{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stock Health Dashboard\n",
        "\n",
        "## Purpose\n",
        "Comprehensive analysis of stock health across all SKUs and warehouses:\n",
        "- **Bucket Classification**: Zero Demand, On Track, Above Target, Below Target, Overstocked\n",
        "- **Actions Tracking**: Price changes, cart rule changes, discounts from all pricing modules\n",
        "- **Trend Analysis**: Is performance improving compared to yesterday?\n",
        "\n",
        "## Output\n",
        "1. Interactive Jupyter analysis with visualizations\n",
        "2. Excel file with two sheets:\n",
        "   - SKU_Details: Full data per SKU/warehouse\n",
        "   - Bucket_Summary: Aggregated metrics by bucket\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# IMPORTS AND SETUP\n",
        "# =============================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import pytz\n",
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add parent directory to path\n",
        "sys.path.append('modules')\n",
        "sys.path.append('..')\n",
        "\n",
        "import setup_environment_2\n",
        "setup_environment_2.initialize_env()\n",
        "\n",
        "# Cairo timezone for consistent timestamps\n",
        "CAIRO_TZ = pytz.timezone('Africa/Cairo')\n",
        "CAIRO_NOW = datetime.now(CAIRO_TZ)\n",
        "TODAY = CAIRO_NOW.date()\n",
        "YESTERDAY = TODAY - timedelta(days=1)\n",
        "\n",
        "print(f\"Stock Health Dashboard\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Current Time: {CAIRO_NOW.strftime('%Y-%m-%d %H:%M:%S')} Cairo\")\n",
        "print(f\"Today: {TODAY}\")\n",
        "print(f\"Yesterday: {YESTERDAY}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SNOWFLAKE CONNECTION\n",
        "# =============================================================================\n",
        "import snowflake.connector\n",
        "\n",
        "def query_snowflake(query):\n",
        "    \"\"\"Execute a query on Snowflake and return results as DataFrame.\"\"\"\n",
        "    con = snowflake.connector.connect(\n",
        "        user=os.environ[\"SNOWFLAKE_USERNAME\"],\n",
        "        account=os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
        "        password=os.environ[\"SNOWFLAKE_PASSWORD\"],\n",
        "        database=os.environ[\"SNOWFLAKE_DATABASE\"]\n",
        "    )\n",
        "    try:\n",
        "        cur = con.cursor()\n",
        "        cur.execute(\"USE WAREHOUSE COMPUTE_WH\")\n",
        "        cur.execute(query)\n",
        "        data = cur.fetchall()\n",
        "        columns = [desc[0].lower() for desc in cur.description]\n",
        "        return pd.DataFrame(data, columns=columns)\n",
        "    finally:\n",
        "        con.close()\n",
        "\n",
        "def get_snowflake_timezone():\n",
        "    result = query_snowflake(\"SHOW PARAMETERS LIKE 'TIMEZONE'\")\n",
        "    return result.value[0] if len(result) > 0 else \"UTC\"\n",
        "\n",
        "TIMEZONE = get_snowflake_timezone()\n",
        "print(f\"Snowflake Timezone: {TIMEZONE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CONFIGURATION\n",
        "# =============================================================================\n",
        "\n",
        "# Warehouse Mapping\n",
        "WAREHOUSE_MAPPING = [\n",
        "    ('Cairo', 'Mostorod', 1, 700),\n",
        "    ('Giza', 'Barageel', 236, 701),\n",
        "    ('Giza', 'Sakkarah', 962, 701),\n",
        "    ('Delta West', 'El-Mahala', 337, 703),\n",
        "    ('Delta West', 'Tanta', 8, 703),\n",
        "    ('Delta East', 'Mansoura FC', 339, 704),\n",
        "    ('Delta East', 'Sharqya', 170, 704),\n",
        "    ('Upper Egypt', 'Assiut FC', 501, 1124),\n",
        "    ('Upper Egypt', 'Bani sweif', 401, 1126),\n",
        "    ('Upper Egypt', 'Menya Samalot', 703, 1123),\n",
        "    ('Upper Egypt', 'Sohag', 632, 1125),\n",
        "    ('Alexandria', 'Khorshed Alex', 797, 702),\n",
        "]\n",
        "\n",
        "COHORT_IDS = [700, 701, 702, 703, 704, 1123, 1124, 1125, 1126]\n",
        "\n",
        "# Bucket thresholds\n",
        "STD_THRESHOLD = 3  # ¬±3 standard deviations\n",
        "OVERSTOCKED_DOH_THRESHOLD = 30  # DOH > 30 = overstocked\n",
        "\n",
        "# Output file\n",
        "OUTPUT_FILE = f'stock_health_output_{TODAY.strftime(\"%Y%m%d\")}.xlsx'\n",
        "\n",
        "print(f\"Output File: {OUTPUT_FILE}\")\n",
        "print(f\"Overstocked DOH Threshold: > {OVERSTOCKED_DOH_THRESHOLD} days\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 1: Base Data Loading\n",
        "Load stocks, prices, WAC, P80/P70 benchmarks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# QUERY 1: CURRENT STOCKS\n",
        "# =============================================================================\n",
        "print(\"Loading current stocks...\")\n",
        "\n",
        "STOCK_QUERY = '''\n",
        "SELECT \n",
        "    pw.warehouse_id,\n",
        "    pw.product_id,\n",
        "    pw.available_stock::INTEGER AS stocks\n",
        "FROM product_warehouse pw\n",
        "WHERE pw.warehouse_id NOT IN (6, 9, 10)\n",
        "    AND pw.is_basic_unit = 1\n",
        "'''\n",
        "\n",
        "df_stocks = query_snowflake(STOCK_QUERY)\n",
        "print(f\"  Loaded {len(df_stocks)} stock records\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# QUERY 2: PRODUCT BASE DATA (SKU info, WAC)\n",
        "# =============================================================================\n",
        "print(\"Loading product base data...\")\n",
        "\n",
        "PRODUCT_BASE_QUERY = f'''\n",
        "WITH skus_prices AS (\n",
        "    WITH local_prices AS (\n",
        "        SELECT  \n",
        "            CASE \n",
        "                WHEN cpu.cohort_id IN (700, 695) THEN 'Cairo'\n",
        "                WHEN cpu.cohort_id IN (701) THEN 'Giza'\n",
        "                WHEN cpu.cohort_id IN (704, 698) THEN 'Delta East'\n",
        "                WHEN cpu.cohort_id IN (703, 697) THEN 'Delta West'\n",
        "                WHEN cpu.cohort_id IN (696, 1123, 1124, 1125, 1126) THEN 'Upper Egypt'\n",
        "                WHEN cpu.cohort_id IN (702, 699) THEN 'Alexandria'\n",
        "            END AS region,\n",
        "            cohort_id,\n",
        "            pu.product_id,\n",
        "            pu.packing_unit_id,\n",
        "            pu.basic_unit_count,\n",
        "            AVG(cpu.price) AS price\n",
        "        FROM cohort_product_packing_units cpu\n",
        "        JOIN PACKING_UNIT_PRODUCTS pu ON pu.id = cpu.product_packing_unit_id\n",
        "        WHERE cpu.cohort_id IN (700,701,702,703,704,695,696,697,698,699,1123,1124,1125,1126)\n",
        "            AND cpu.created_at::date <> '2023-07-31'\n",
        "            AND cpu.is_customized = TRUE\n",
        "        GROUP BY ALL\n",
        "    )\n",
        "    \n",
        "    SELECT region, cohort_id, product_id, price\n",
        "    FROM local_prices\n",
        "    WHERE basic_unit_count = 1\n",
        "        AND ((product_id = 1309 AND packing_unit_id = 2) OR (product_id <> 1309))\n",
        ")\n",
        "\n",
        "SELECT DISTINCT\n",
        "    sp.region, sp.cohort_id, p.id as product_id,\n",
        "    CONCAT(p.name_ar, ' ', p.size, ' ', pu.name_ar) AS sku,\n",
        "    b.name_ar AS brand,\n",
        "    cat.name_ar AS cat,\n",
        "    c.wac1, c.wac_p, sp.price as current_price\n",
        "FROM skus_prices sp\n",
        "JOIN products p ON p.id = sp.product_id\n",
        "JOIN finance.all_cogs c ON c.product_id = sp.product_id \n",
        "    AND CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP()) BETWEEN c.from_date AND c.to_date\n",
        "JOIN categories cat ON cat.id = p.category_id\n",
        "JOIN brands b ON b.id = p.brand_id\n",
        "JOIN product_units pu ON pu.id = p.unit_id\n",
        "WHERE c.wac1 > 0 AND c.wac_p > 0\n",
        "'''\n",
        "\n",
        "df_product_base = query_snowflake(PRODUCT_BASE_QUERY)\n",
        "print(f\"  Loaded {len(df_product_base)} product base records\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# QUERY 3: P80/P70 BENCHMARKS (from Pricing_data_extraction table)\n",
        "# =============================================================================\n",
        "print(\"Loading P80/P70 benchmarks...\")\n",
        "\n",
        "BENCHMARK_QUERY = f'''\n",
        "SELECT DISTINCT\n",
        "    warehouse_id, product_id,\n",
        "    p80_daily_240d,\n",
        "    avg_daily_240d,\n",
        "    std_daily_240d,\n",
        "    p70_daily_retailers_240d,\n",
        "    std_daily_retailers_240d,\n",
        "    normal_refill,\n",
        "    refill_stddev\n",
        "FROM MATERIALIZED_VIEWS.Pricing_data_extraction\n",
        "WHERE created_at = CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE\n",
        "'''\n",
        "\n",
        "df_benchmarks = query_snowflake(BENCHMARK_QUERY)\n",
        "print(f\"  Loaded {len(df_benchmarks)} benchmark records\")\n",
        "\n",
        "# If no data for today, try yesterday\n",
        "if len(df_benchmarks) == 0:\n",
        "    print(\"  No data for today, trying yesterday...\")\n",
        "    BENCHMARK_QUERY_YESTERDAY = f'''\n",
        "    SELECT DISTINCT\n",
        "        warehouse_id, product_id,\n",
        "        p80_daily_240d,\n",
        "        avg_daily_240d,\n",
        "        std_daily_240d,\n",
        "        p70_daily_retailers_240d,\n",
        "        std_daily_retailers_240d,\n",
        "        normal_refill,\n",
        "        refill_stddev\n",
        "    FROM MATERIALIZED_VIEWS.Pricing_data_extraction\n",
        "    WHERE created_at = CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE - 1\n",
        "    '''\n",
        "    df_benchmarks = query_snowflake(BENCHMARK_QUERY_YESTERDAY)\n",
        "    print(f\"  Loaded {len(df_benchmarks)} benchmark records from yesterday\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 2: Yesterday's Running Rate & Today's UTH\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# QUERY 4: YESTERDAY'S SALES (Running Rate)\n",
        "# =============================================================================\n",
        "print(\"Loading yesterday's sales data...\")\n",
        "\n",
        "YESTERDAY_SALES_QUERY = f'''\n",
        "SELECT\n",
        "    pso.warehouse_id,\n",
        "    pso.product_id,\n",
        "    SUM(pso.purchased_item_count) AS yesterday_qty,\n",
        "    SUM(pso.total_price) AS yesterday_nmv,\n",
        "    COUNT(DISTINCT so.retailer_id) AS yesterday_retailers\n",
        "FROM product_sales_order pso\n",
        "JOIN sales_orders so ON so.id = pso.sales_order_id\n",
        "WHERE so.created_at::date = CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE - 1\n",
        "    AND so.sales_order_status_id NOT IN (7, 12)\n",
        "    AND so.channel IN ('telesales', 'retailer')\n",
        "    AND pso.purchased_item_count <> 0\n",
        "GROUP BY pso.warehouse_id, pso.product_id\n",
        "'''\n",
        "\n",
        "df_yesterday_sales = query_snowflake(YESTERDAY_SALES_QUERY)\n",
        "print(f\"  Loaded {len(df_yesterday_sales)} yesterday sales records\")\n",
        "print(f\"  Total Yesterday Qty: {df_yesterday_sales['yesterday_qty'].sum():,.0f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# QUERY 5: TODAY'S UTH (Up-Till-Hour) SALES\n",
        "# =============================================================================\n",
        "print(\"Loading today's UTH sales...\")\n",
        "\n",
        "TODAY_UTH_QUERY = f'''\n",
        "WITH params AS (\n",
        "    SELECT\n",
        "        CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE AS today,\n",
        "        HOUR(CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())) AS current_hour\n",
        ")\n",
        "SELECT\n",
        "    pso.warehouse_id,\n",
        "    pso.product_id,\n",
        "    SUM(pso.purchased_item_count) AS today_uth_qty,\n",
        "    SUM(pso.total_price) AS today_uth_nmv,\n",
        "    COUNT(DISTINCT so.retailer_id) AS today_uth_retailers\n",
        "FROM product_sales_order pso\n",
        "JOIN sales_orders so ON so.id = pso.sales_order_id\n",
        "CROSS JOIN params p\n",
        "WHERE so.created_at::date = p.today\n",
        "    AND HOUR(so.created_at) < p.current_hour\n",
        "    AND so.sales_order_status_id NOT IN (7, 12)\n",
        "    AND so.channel IN ('telesales', 'retailer')\n",
        "    AND pso.purchased_item_count <> 0\n",
        "GROUP BY pso.warehouse_id, pso.product_id\n",
        "'''\n",
        "\n",
        "df_today_uth = query_snowflake(TODAY_UTH_QUERY)\n",
        "print(f\"  Loaded {len(df_today_uth)} today UTH records\")\n",
        "print(f\"  Total Today UTH Qty: {df_today_uth['today_uth_qty'].sum():,.0f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# QUERY 6: HISTORICAL HOURLY DISTRIBUTION (for closing expectation)\n",
        "# =============================================================================\n",
        "print(\"Loading hourly distribution...\")\n",
        "\n",
        "HOURLY_DIST_QUERY = f'''\n",
        "WITH params AS (\n",
        "    SELECT\n",
        "        CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE AS today,\n",
        "        CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE - 120 AS history_start,\n",
        "        HOUR(CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())) AS current_hour\n",
        "),\n",
        "hourly_sales AS (\n",
        "    SELECT\n",
        "        pso.warehouse_id,\n",
        "        c.name_ar AS cat,\n",
        "        so.created_at::date AS sale_date,\n",
        "        HOUR(so.created_at) AS sale_hour,\n",
        "        SUM(pso.purchased_item_count) AS qty\n",
        "    FROM product_sales_order pso\n",
        "    JOIN sales_orders so ON so.id = pso.sales_order_id\n",
        "    JOIN products p ON p.id = pso.product_id\n",
        "    JOIN categories c ON c.id = p.category_id\n",
        "    CROSS JOIN params\n",
        "    WHERE so.created_at::date BETWEEN params.history_start AND params.today - 1\n",
        "        AND so.sales_order_status_id NOT IN (7, 12)\n",
        "        AND so.channel IN ('telesales', 'retailer')\n",
        "        AND pso.purchased_item_count <> 0\n",
        "    GROUP BY pso.warehouse_id, c.name_ar, so.created_at::date, sale_hour\n",
        "),\n",
        "daily_totals AS (\n",
        "    SELECT warehouse_id, cat, sale_date,\n",
        "        SUM(qty) AS day_total_qty\n",
        "    FROM hourly_sales\n",
        "    GROUP BY warehouse_id, cat, sale_date\n",
        "),\n",
        "uth_totals AS (\n",
        "    SELECT hs.warehouse_id, hs.cat, hs.sale_date,\n",
        "        SUM(hs.qty) AS uth_total_qty\n",
        "    FROM hourly_sales hs\n",
        "    CROSS JOIN params p\n",
        "    WHERE hs.sale_hour < p.current_hour\n",
        "    GROUP BY hs.warehouse_id, hs.cat, hs.sale_date\n",
        ")\n",
        "SELECT\n",
        "    dt.warehouse_id, dt.cat,\n",
        "    AVG(COALESCE(ut.uth_total_qty, 0) / NULLIF(dt.day_total_qty, 0)) AS avg_uth_pct\n",
        "FROM daily_totals dt\n",
        "LEFT JOIN uth_totals ut ON dt.warehouse_id = ut.warehouse_id \n",
        "    AND dt.cat = ut.cat AND dt.sale_date = ut.sale_date\n",
        "WHERE dt.day_total_qty > 0\n",
        "GROUP BY dt.warehouse_id, dt.cat\n",
        "'''\n",
        "\n",
        "df_hourly_dist = query_snowflake(HOURLY_DIST_QUERY)\n",
        "print(f\"  Loaded {len(df_hourly_dist)} hourly distribution records\")\n",
        "print(f\"  Average UTH %: {df_hourly_dist['avg_uth_pct'].mean()*100:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 3: Active Discounts (SKU Discounts + Quantity Discounts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# QUERY 7: ACTIVE SKU DISCOUNTS\n",
        "# =============================================================================\n",
        "print(\"Loading active SKU discounts...\")\n",
        "\n",
        "SKU_DISCOUNT_QUERY = f'''\n",
        "SELECT DISTINCT\n",
        "    sdp.product_id,\n",
        "    sd.discount_percentage AS sku_discount_pct,\n",
        "    sd.start_at AS sku_disc_start,\n",
        "    sd.end_at AS sku_disc_end\n",
        "FROM sku_discounts sd\n",
        "JOIN sku_discount_products sdp ON sdp.sku_discount_id = sd.id\n",
        "WHERE sd.active = TRUE\n",
        "    AND sd.deleted_at IS NULL\n",
        "    AND CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP()) BETWEEN sd.start_at AND sd.end_at\n",
        "'''\n",
        "\n",
        "df_sku_discounts = query_snowflake(SKU_DISCOUNT_QUERY)\n",
        "print(f\"  Loaded {len(df_sku_discounts)} active SKU discount records\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# QUERY 8: ACTIVE QUANTITY DISCOUNTS (QD)\n",
        "# =============================================================================\n",
        "print(\"Loading active quantity discounts...\")\n",
        "\n",
        "QD_QUERY = f'''\n",
        "WITH active_qd AS (\n",
        "    SELECT DISTINCT \n",
        "        qd.id AS discount_id,\n",
        "        qd.dynamic_tag_id AS tag_id,\n",
        "        qdt.threshold_quantity,\n",
        "        qdt.discount_percentage AS qd_discount_pct\n",
        "    FROM quantity_discounts qd \n",
        "    JOIN quantity_discount_tiers qdt ON qdt.quantity_discount_id = qd.id\n",
        "    WHERE qd.active = TRUE\n",
        "        AND CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP()) \n",
        "            BETWEEN qd.start_at AND qd.end_at\n",
        "),\n",
        "tag_products AS (\n",
        "    SELECT DISTINCT\n",
        "        dtp.dynamic_tag_id AS tag_id,\n",
        "        dtp.product_id\n",
        "    FROM dynamic_tag_products dtp\n",
        "    WHERE dtp.deleted_at IS NULL\n",
        ")\n",
        "SELECT \n",
        "    tp.product_id,\n",
        "    aq.tag_id,\n",
        "    MAX(aq.qd_discount_pct) AS qd_discount_pct  -- Take max discount available\n",
        "FROM active_qd aq\n",
        "JOIN tag_products tp ON tp.tag_id = aq.tag_id\n",
        "GROUP BY tp.product_id, aq.tag_id\n",
        "'''\n",
        "\n",
        "df_qd_discounts = query_snowflake(QD_QUERY)\n",
        "print(f\"  Loaded {len(df_qd_discounts)} active QD records\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 4: Today's Actions from Pricing Modules\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# QUERY 9: TODAY'S ACTIONS FROM MODULE 3 (Periodic)\n",
        "# =============================================================================\n",
        "print(\"Loading Module 3 actions...\")\n",
        "\n",
        "MODULE3_ACTIONS_QUERY = f'''\n",
        "SELECT \n",
        "    product_id, warehouse_id,\n",
        "    price_action AS module3_price_action,\n",
        "    CASE WHEN new_price IS NOT NULL AND new_price <> current_price THEN 1 ELSE 0 END AS module3_price_changed,\n",
        "    CASE WHEN new_cart_rule IS NOT NULL AND new_cart_rule <> current_cart_rule THEN 1 ELSE 0 END AS module3_cart_changed,\n",
        "    uth_status AS module3_uth_status,\n",
        "    current_price AS module3_current_price,\n",
        "    new_price AS module3_new_price,\n",
        "    created_at AS module3_timestamp\n",
        "FROM MATERIALIZED_VIEWS.pricing_periodic_push\n",
        "WHERE created_at::DATE = CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE\n",
        "QUALIFY ROW_NUMBER() OVER (PARTITION BY product_id, warehouse_id ORDER BY created_at DESC) = 1\n",
        "'''\n",
        "\n",
        "try:\n",
        "    df_module3_actions = query_snowflake(MODULE3_ACTIONS_QUERY)\n",
        "    print(f\"  Loaded {len(df_module3_actions)} Module 3 action records\")\n",
        "except Exception as e:\n",
        "    print(f\"  Warning: Could not load Module 3 actions: {e}\")\n",
        "    df_module3_actions = pd.DataFrame(columns=['product_id', 'warehouse_id', 'module3_price_action'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# QUERY 10: TODAY'S ACTIONS FROM MODULE 4 (Hourly)\n",
        "# =============================================================================\n",
        "print(\"Loading Module 4 actions...\")\n",
        "\n",
        "MODULE4_ACTIONS_QUERY = f'''\n",
        "SELECT \n",
        "    product_id, warehouse_id,\n",
        "    price_action AS module4_price_action,\n",
        "    cart_rule_action AS module4_cart_action,\n",
        "    CASE WHEN new_price IS NOT NULL AND new_price <> current_price THEN 1 ELSE 0 END AS module4_price_changed,\n",
        "    CASE WHEN new_cart_rule IS NOT NULL AND new_cart_rule <> current_cart_rule THEN 1 ELSE 0 END AS module4_cart_changed,\n",
        "    uth_qty_status AS module4_uth_qty_status,\n",
        "    last_hour_qty_status AS module4_last_hour_status,\n",
        "    current_price AS module4_current_price,\n",
        "    new_price AS module4_new_price,\n",
        "    created_at AS module4_timestamp\n",
        "FROM MATERIALIZED_VIEWS.pricing_hourly_push\n",
        "WHERE created_at::DATE = CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE\n",
        "QUALIFY ROW_NUMBER() OVER (PARTITION BY product_id, warehouse_id ORDER BY created_at DESC) = 1\n",
        "'''\n",
        "\n",
        "try:\n",
        "    df_module4_actions = query_snowflake(MODULE4_ACTIONS_QUERY)\n",
        "    print(f\"  Loaded {len(df_module4_actions)} Module 4 action records\")\n",
        "except Exception as e:\n",
        "    print(f\"  Warning: Could not load Module 4 actions: {e}\")\n",
        "    df_module4_actions = pd.DataFrame(columns=['product_id', 'warehouse_id', 'module4_price_action'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 5: Merge All Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# MERGE ALL DATA INTO MASTER DATAFRAME\n",
        "# =============================================================================\n",
        "print(\"Merging all data...\")\n",
        "\n",
        "# Create warehouse mapping dataframe\n",
        "df_warehouse = pd.DataFrame(\n",
        "    WAREHOUSE_MAPPING,\n",
        "    columns=['region', 'warehouse', 'warehouse_id', 'cohort_id']\n",
        ")\n",
        "\n",
        "# Start with product base and add warehouse mapping\n",
        "df = df_product_base.merge(df_warehouse[['cohort_id', 'warehouse_id', 'warehouse']], on='cohort_id')\n",
        "print(f\"  After warehouse mapping: {len(df)} records\")\n",
        "\n",
        "# Merge stocks\n",
        "df = df.merge(df_stocks, on=['warehouse_id', 'product_id'], how='left')\n",
        "df['stocks'] = df['stocks'].fillna(0)\n",
        "print(f\"  After stocks merge: {len(df)} records\")\n",
        "\n",
        "# Merge benchmarks (P80/P70)\n",
        "df = df.merge(df_benchmarks, on=['warehouse_id', 'product_id'], how='left')\n",
        "df['p80_daily_240d'] = pd.to_numeric(df['p80_daily_240d'], errors='coerce').fillna(5)  # Default to 5 if missing\n",
        "df['std_daily_240d'] = pd.to_numeric(df['std_daily_240d'], errors='coerce').fillna(0)\n",
        "df['avg_daily_240d'] = pd.to_numeric(df.get('avg_daily_240d', 0), errors='coerce').fillna(0)\n",
        "print(f\"  After benchmarks merge: {len(df)} records\")\n",
        "\n",
        "# Merge yesterday's sales\n",
        "df = df.merge(df_yesterday_sales, on=['warehouse_id', 'product_id'], how='left')\n",
        "df['yesterday_qty'] = df['yesterday_qty'].fillna(0)\n",
        "df['yesterday_nmv'] = df['yesterday_nmv'].fillna(0)\n",
        "df['yesterday_retailers'] = df['yesterday_retailers'].fillna(0)\n",
        "print(f\"  After yesterday sales merge: {len(df)} records\")\n",
        "\n",
        "# Merge today's UTH\n",
        "df = df.merge(df_today_uth, on=['warehouse_id', 'product_id'], how='left')\n",
        "df['today_uth_qty'] = df['today_uth_qty'].fillna(0)\n",
        "df['today_uth_nmv'] = df['today_uth_nmv'].fillna(0)\n",
        "df['today_uth_retailers'] = df['today_uth_retailers'].fillna(0)\n",
        "print(f\"  After today UTH merge: {len(df)} records\")\n",
        "\n",
        "# Merge hourly distribution (for closing expectation)\n",
        "df = df.merge(df_hourly_dist, on=['warehouse_id', 'cat'], how='left')\n",
        "df['avg_uth_pct'] = pd.to_numeric(df['avg_uth_pct'], errors='coerce').fillna(0.5)  # Default to 50% if missing\n",
        "print(f\"  After hourly dist merge: {len(df)} records\")\n",
        "\n",
        "# Merge SKU discounts\n",
        "df = df.merge(df_sku_discounts[['product_id', 'sku_discount_pct']], on='product_id', how='left')\n",
        "df['sku_discount_pct'] = df['sku_discount_pct'].fillna(0)\n",
        "print(f\"  After SKU discounts merge: {len(df)} records\")\n",
        "\n",
        "# Merge QD discounts (aggregate by product_id to get max discount)\n",
        "df_qd_agg = df_qd_discounts.groupby('product_id')['qd_discount_pct'].max().reset_index()\n",
        "df = df.merge(df_qd_agg, on='product_id', how='left')\n",
        "df['qd_discount_pct'] = df['qd_discount_pct'].fillna(0)\n",
        "print(f\"  After QD merge: {len(df)} records\")\n",
        "\n",
        "# Merge Module 3 actions\n",
        "if len(df_module3_actions) > 0:\n",
        "    df = df.merge(df_module3_actions, on=['warehouse_id', 'product_id'], how='left')\n",
        "else:\n",
        "    df['module3_price_action'] = None\n",
        "    df['module3_price_changed'] = 0\n",
        "    df['module3_cart_changed'] = 0\n",
        "print(f\"  After Module 3 actions merge: {len(df)} records\")\n",
        "\n",
        "# Merge Module 4 actions\n",
        "if len(df_module4_actions) > 0:\n",
        "    df = df.merge(df_module4_actions, on=['warehouse_id', 'product_id'], how='left')\n",
        "else:\n",
        "    df['module4_price_action'] = None\n",
        "    df['module4_price_changed'] = 0\n",
        "    df['module4_cart_changed'] = 0\n",
        "print(f\"  After Module 4 actions merge: {len(df)} records\")\n",
        "\n",
        "# Remove duplicates\n",
        "df = df.drop_duplicates(subset=['warehouse_id', 'product_id'])\n",
        "print(f\"\\n‚úÖ Final merged dataframe: {len(df)} records\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 6: Calculate Derived Metrics & Effective Price\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CALCULATE DERIVED METRICS\n",
        "# =============================================================================\n",
        "print(\"Calculating derived metrics...\")\n",
        "\n",
        "# 1. Stock Value (WAC1)\n",
        "df['stock_value_wac1'] = df['stocks'] * df['wac1']\n",
        "\n",
        "# 2. Days on Hand (DOH)\n",
        "df['running_rate'] = df['yesterday_qty'].fillna(0)\n",
        "df.loc[df['running_rate'] == 0, 'running_rate'] = df.loc[df['running_rate'] == 0, 'avg_daily_240d'].fillna(1)\n",
        "df['doh'] = df['stocks'] / df['running_rate'].replace(0, np.inf)\n",
        "df['doh'] = df['doh'].replace([np.inf, -np.inf], 999)\n",
        "\n",
        "# 3. Effective Price (after all discounts)\n",
        "df['total_discount_pct'] = df['sku_discount_pct'] + df['qd_discount_pct']\n",
        "df['total_discount_pct'] = df['total_discount_pct'].clip(upper=50)  # Cap at 50%\n",
        "df['effective_price'] = df['current_price'] * (1 - df['total_discount_pct'] / 100)\n",
        "\n",
        "# 4. Effective Margin\n",
        "df['effective_margin'] = (df['effective_price'] - df['wac_p']) / df['effective_price']\n",
        "df['effective_margin'] = df['effective_margin'].clip(lower=-1, upper=1)\n",
        "\n",
        "# 5. Closing Expectation\n",
        "df['closing_expectation'] = df['today_uth_qty'] / df['avg_uth_pct'].replace(0, 0.5)\n",
        "\n",
        "# 6. Price/Cart Rule Changed Today\n",
        "df['module3_price_changed'] = df.get('module3_price_changed', 0).fillna(0)\n",
        "df['module4_price_changed'] = df.get('module4_price_changed', 0).fillna(0)\n",
        "df['module3_cart_changed'] = df.get('module3_cart_changed', 0).fillna(0)\n",
        "df['module4_cart_changed'] = df.get('module4_cart_changed', 0).fillna(0)\n",
        "\n",
        "df['price_changed_today'] = ((df['module3_price_changed'] > 0) | (df['module4_price_changed'] > 0)).astype(int)\n",
        "df['cart_rule_changed_today'] = ((df['module3_cart_changed'] > 0) | (df['module4_cart_changed'] > 0)).astype(int)\n",
        "\n",
        "# 7. Has Active Discount\n",
        "df['has_sku_discount'] = (df['sku_discount_pct'] > 0).astype(int)\n",
        "df['has_qd_discount'] = (df['qd_discount_pct'] > 0).astype(int)\n",
        "df['has_any_discount'] = ((df['sku_discount_pct'] > 0) | (df['qd_discount_pct'] > 0)).astype(int)\n",
        "\n",
        "print(\"‚úÖ Derived metrics calculated\")\n",
        "print(f\"  - Stock Value (WAC1): {df['stock_value_wac1'].sum():,.0f} EGP\")\n",
        "print(f\"  - Avg DOH: {df['doh'].median():.1f} days (median)\")\n",
        "print(f\"  - SKUs with discounts: {df['has_any_discount'].sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# BUCKET CLASSIFICATION LOGIC\n",
        "# =============================================================================\n",
        "print(\"Classifying SKUs into buckets...\")\n",
        "\n",
        "def classify_bucket(row):\n",
        "    \"\"\"\n",
        "    Classify SKU into performance bucket based on yesterday's qty vs P80 target.\n",
        "    \"\"\"\n",
        "    stocks = row['stocks']\n",
        "    yesterday_qty = row['yesterday_qty']\n",
        "    target = row['p80_daily_240d']\n",
        "    std = row['std_daily_240d']\n",
        "    \n",
        "    # OOS first\n",
        "    if stocks <= 0:\n",
        "        return 'OOS'\n",
        "    \n",
        "    # Zero Demand: has stock but no sales yesterday\n",
        "    if yesterday_qty == 0:\n",
        "        return 'Zero Demand'\n",
        "    \n",
        "    # Calculate thresholds using ¬±3 std\n",
        "    upper_bound = target + STD_THRESHOLD * std\n",
        "    lower_bound = max(target - STD_THRESHOLD * std, 1)\n",
        "    \n",
        "    # Classify based on yesterday's performance vs target\n",
        "    if yesterday_qty > upper_bound:\n",
        "        return 'Above Target'\n",
        "    elif yesterday_qty < lower_bound:\n",
        "        return 'Below Target'\n",
        "    else:\n",
        "        return 'On Track'\n",
        "\n",
        "# Apply bucket classification\n",
        "df['bucket'] = df.apply(classify_bucket, axis=1)\n",
        "\n",
        "# Add overstocked flag (DOH > 30)\n",
        "df['is_overstocked'] = (df['doh'] > OVERSTOCKED_DOH_THRESHOLD).astype(int)\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"BUCKET DISTRIBUTION\")\n",
        "print(\"=\"*60)\n",
        "print(df['bucket'].value_counts().to_string())\n",
        "print(f\"\\nOverstocked SKUs (DOH > {OVERSTOCKED_DOH_THRESHOLD}): {df['is_overstocked'].sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TREND ANALYSIS: Is performance improving?\n",
        "# =============================================================================\n",
        "print(\"Analyzing trends...\")\n",
        "\n",
        "def analyze_trend(row):\n",
        "    \"\"\"Analyze trend based on bucket type. Compare today's running rate vs yesterday's full day.\"\"\"\n",
        "    bucket = row['bucket']\n",
        "    yesterday_qty = row['yesterday_qty']\n",
        "    closing_exp = row['closing_expectation']\n",
        "    today_uth_qty = row['today_uth_qty']\n",
        "    \n",
        "    # Zero Demand: Is it starting to sell?\n",
        "    if bucket == 'Zero Demand':\n",
        "        if today_uth_qty > 0:\n",
        "            return 'Starting to Sell', True\n",
        "        else:\n",
        "            return 'Still Zero', False\n",
        "    \n",
        "    # OOS: No trend analysis possible\n",
        "    if bucket == 'OOS':\n",
        "        return 'OOS', False\n",
        "    \n",
        "    # For other buckets: compare closing expectation vs yesterday\n",
        "    if yesterday_qty > 0:\n",
        "        ratio = closing_exp / yesterday_qty if yesterday_qty > 0 else 0\n",
        "        if ratio > 1.1:\n",
        "            return 'Improving', True\n",
        "        elif ratio < 0.9:\n",
        "            return 'Declining', False\n",
        "        else:\n",
        "            return 'Stable', True\n",
        "    else:\n",
        "        return 'No Baseline', False\n",
        "\n",
        "# Apply trend analysis\n",
        "trend_results = df.apply(analyze_trend, axis=1, result_type='expand')\n",
        "df['trend_status'] = trend_results[0]\n",
        "df['is_improving'] = trend_results[1].astype(int)\n",
        "\n",
        "# For Above Target: Check if effective price is increasing\n",
        "df['price_direction'] = 'Stable'\n",
        "above_target_mask = df['bucket'] == 'Above Target'\n",
        "df.loc[above_target_mask & (df['total_discount_pct'] == 0), 'price_direction'] = 'No Discount'\n",
        "df.loc[above_target_mask & (df['price_changed_today'] == 1), 'price_direction'] = 'Price Changed'\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TREND DISTRIBUTION\")\n",
        "print(\"=\"*60)\n",
        "print(df['trend_status'].value_counts().to_string())\n",
        "print(f\"\\nSKUs Improving: {df['is_improving'].sum()} ({df['is_improving'].mean()*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SUMMARY DASHBOARD BY BUCKET\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STOCK HEALTH DASHBOARD - SUMMARY\")\n",
        "print(f\"Generated: {CAIRO_NOW.strftime('%Y-%m-%d %H:%M:%S')} Cairo\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create summary by bucket\n",
        "bucket_summary = df.groupby('bucket').agg({\n",
        "    'product_id': 'count',\n",
        "    'stock_value_wac1': 'sum',\n",
        "    'stocks': 'sum',\n",
        "    'doh': 'median',\n",
        "    'yesterday_qty': 'sum',\n",
        "    'today_uth_qty': 'sum',\n",
        "    'closing_expectation': 'sum',\n",
        "    'price_changed_today': 'sum',\n",
        "    'cart_rule_changed_today': 'sum',\n",
        "    'has_any_discount': 'sum',\n",
        "    'is_improving': 'sum',\n",
        "    'effective_margin': 'mean',\n",
        "    'is_overstocked': 'sum'\n",
        "}).rename(columns={\n",
        "    'product_id': 'sku_count',\n",
        "    'stock_value_wac1': 'total_stock_value',\n",
        "    'stocks': 'total_stock_units',\n",
        "    'doh': 'median_doh',\n",
        "    'yesterday_qty': 'total_yesterday_qty',\n",
        "    'today_uth_qty': 'total_today_uth',\n",
        "    'closing_expectation': 'total_closing_exp',\n",
        "    'price_changed_today': 'skus_price_action',\n",
        "    'cart_rule_changed_today': 'skus_cart_action',\n",
        "    'has_any_discount': 'skus_with_discount',\n",
        "    'is_improving': 'skus_improving',\n",
        "    'effective_margin': 'avg_effective_margin',\n",
        "    'is_overstocked': 'skus_overstocked'\n",
        "})\n",
        "\n",
        "# Calculate percentages\n",
        "bucket_summary['pct_improving'] = (bucket_summary['skus_improving'] / bucket_summary['sku_count'] * 100).round(1)\n",
        "bucket_summary['pct_with_discount'] = (bucket_summary['skus_with_discount'] / bucket_summary['sku_count'] * 100).round(1)\n",
        "bucket_summary['avg_effective_margin'] = (bucket_summary['avg_effective_margin'] * 100).round(2)\n",
        "\n",
        "print(\"\\nBUCKET SUMMARY:\")\n",
        "print(\"-\" * 80)\n",
        "display(bucket_summary.round(2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PREPARE OUTPUT DATAFRAMES FOR EXCEL\n",
        "# =============================================================================\n",
        "print(\"Preparing Excel export...\")\n",
        "\n",
        "# Sheet 1: SKU Details - Select and order columns\n",
        "detail_columns = [\n",
        "    'warehouse_id', 'warehouse', 'region', 'cohort_id', 'product_id', 'sku', 'brand', 'cat',\n",
        "    'stocks', 'stock_value_wac1', 'doh', 'is_overstocked',\n",
        "    'wac1', 'wac_p',\n",
        "    'current_price', 'effective_price', 'effective_margin',\n",
        "    'sku_discount_pct', 'qd_discount_pct', 'total_discount_pct',\n",
        "    'p80_daily_240d', 'std_daily_240d', 'avg_daily_240d',\n",
        "    'yesterday_qty', 'yesterday_nmv', 'yesterday_retailers',\n",
        "    'today_uth_qty', 'today_uth_nmv', 'today_uth_retailers', 'closing_expectation',\n",
        "    'bucket', 'trend_status', 'is_improving', 'price_direction',\n",
        "    'price_changed_today', 'cart_rule_changed_today',\n",
        "    'has_sku_discount', 'has_qd_discount', 'has_any_discount',\n",
        "    'module3_price_action', 'module3_uth_status',\n",
        "    'module4_price_action', 'module4_uth_qty_status'\n",
        "]\n",
        "\n",
        "detail_columns = [c for c in detail_columns if c in df.columns]\n",
        "df_details = df[detail_columns].copy()\n",
        "\n",
        "# Sort by bucket and stock value\n",
        "bucket_order = {'Zero Demand': 0, 'Below Target': 1, 'On Track': 2, 'Above Target': 3, 'OOS': 4}\n",
        "df_details['bucket_order'] = df_details['bucket'].map(bucket_order)\n",
        "df_details = df_details.sort_values(['bucket_order', 'stock_value_wac1'], ascending=[True, False])\n",
        "df_details = df_details.drop('bucket_order', axis=1)\n",
        "\n",
        "print(f\"  SKU Details: {len(df_details)} rows, {len(df_details.columns)} columns\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PREPARE BUCKET SUMMARY FOR EXCEL (Sheet 2)\n",
        "# =============================================================================\n",
        "bucket_summary_export = bucket_summary.reset_index()\n",
        "bucket_summary_export.columns = [\n",
        "    'Bucket', 'SKU Count', 'Stock Value (EGP)', 'Total Stock Units', 'Median DOH',\n",
        "    'Yesterday Qty', 'Today UTH Qty', 'Closing Expectation',\n",
        "    'Price Actions', 'Cart Actions', 'With Discount',\n",
        "    'Improving', 'Avg Margin %', 'Overstocked', '% Improving', '% With Discount'\n",
        "]\n",
        "\n",
        "print(f\"  Bucket Summary: {len(bucket_summary_export)} rows\")\n",
        "display(bucket_summary_export)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# EXPORT TO EXCEL\n",
        "# =============================================================================\n",
        "print(f\"\\nExporting to Excel: {OUTPUT_FILE}\")\n",
        "\n",
        "with pd.ExcelWriter(OUTPUT_FILE, engine='openpyxl') as writer:\n",
        "    # Sheet 1: SKU Details\n",
        "    df_details.to_excel(writer, sheet_name='SKU_Details', index=False)\n",
        "    print(f\"  ‚úÖ Sheet 'SKU_Details' written: {len(df_details)} rows\")\n",
        "    \n",
        "    # Sheet 2: Bucket Summary\n",
        "    bucket_summary_export.to_excel(writer, sheet_name='Bucket_Summary', index=False)\n",
        "    print(f\"  ‚úÖ Sheet 'Bucket_Summary' written: {len(bucket_summary_export)} rows\")\n",
        "\n",
        "print(f\"\\n\" + \"=\"*60)\n",
        "print(f\"‚úÖ EXPORT COMPLETE: {OUTPUT_FILE}\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# FINAL SUMMARY\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STOCK HEALTH DASHBOARD - COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nüìÖ Date: {TODAY}\")\n",
        "print(f\"‚è∞ Generated: {datetime.now(CAIRO_TZ).strftime('%Y-%m-%d %H:%M:%S')} Cairo\")\n",
        "print(f\"\\nüìä OVERALL STATS:\")\n",
        "print(f\"  Total SKUs Analyzed: {len(df):,}\")\n",
        "print(f\"  Total Stock Value: {df['stock_value_wac1'].sum():,.0f} EGP\")\n",
        "print(f\"  Total Stock Units: {df['stocks'].sum():,.0f}\")\n",
        "\n",
        "print(f\"\\nüì¶ BUCKET DISTRIBUTION:\")\n",
        "for bucket, count in df['bucket'].value_counts().items():\n",
        "    pct = count / len(df) * 100\n",
        "    print(f\"  {bucket}: {count:,} ({pct:.1f}%)\")\n",
        "\n",
        "print(f\"\\n‚ö†Ô∏è ALERTS:\")\n",
        "print(f\"  Overstocked SKUs (DOH > {OVERSTOCKED_DOH_THRESHOLD}): {df['is_overstocked'].sum()}\")\n",
        "print(f\"  Zero Demand with Stock: {len(df[df['bucket'] == 'Zero Demand'])}\")\n",
        "print(f\"  Below Target: {len(df[df['bucket'] == 'Below Target'])}\")\n",
        "\n",
        "print(f\"\\n‚úÖ POSITIVE SIGNALS:\")\n",
        "print(f\"  SKUs Improving: {df['is_improving'].sum()} ({df['is_improving'].mean()*100:.1f}%)\")\n",
        "print(f\"  Above Target: {len(df[df['bucket'] == 'Above Target'])}\")\n",
        "print(f\"  On Track: {len(df[df['bucket'] == 'On Track'])}\")\n",
        "\n",
        "print(f\"\\nüìÅ OUTPUT: {OUTPUT_FILE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
