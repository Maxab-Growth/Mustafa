{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Market Data Module\n",
    "\n",
    "This module provides fresh market prices and margin tiers data on demand.\n",
    "\n",
    "## Functions\n",
    "- `get_market_prices()` - Fetch and process market prices from all sources\n",
    "- `get_margin_tiers()` - Calculate margin tiers for products\n",
    "- `get_market_and_margin_data()` - Combined function for both\n",
    "\n",
    "## Output Columns\n",
    "\n",
    "**Market Prices:**\n",
    "- `below_market`, `market_min`, `market_25`, `market_50`, `market_75`, `market_max`, `above_market`\n",
    "\n",
    "**Margin Tiers:**\n",
    "- `margin_tier_below`, `margin_tier_1`, `margin_tier_2`, `margin_tier_3`, `margin_tier_4`, `margin_tier_5`, `margin_tier_above_1`, `margin_tier_above_2`\n",
    "\n",
    "## Usage\n",
    "```python\n",
    "%run market_data_module.ipynb\n",
    "df_market = get_market_and_margin_data(df_input)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/snowflake/connector/options.py:104: UserWarning: You have an incompatible version of 'pyarrow' installed (22.0.0), please install a version that adheres to: 'pyarrow<19.0.0; extra == \"pandas\"'\n",
      "  warn_incompatible_dep(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.Renviron\n",
      "/home/ec2-user/service_account_key.json\n",
      "Market Data Module loaded at 2026-01-24 01:55:57 Cairo time\n",
      "Snowflake timezone: America/Los_Angeles\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS & CONFIGURATION\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import snowflake.connector\n",
    "import os\n",
    "\n",
    "# Import setup_environment_2 for credentials\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import setup_environment_2\n",
    "\n",
    "# Initialize environment (loads Snowflake credentials)\n",
    "setup_environment_2.initialize_env()\n",
    "\n",
    "# Cairo timezone\n",
    "CAIRO_TZ = pytz.timezone('Africa/Cairo')\n",
    "CAIRO_NOW = datetime.now(CAIRO_TZ)\n",
    "\n",
    "# =============================================================================\n",
    "# SNOWFLAKE CONNECTION\n",
    "# =============================================================================\n",
    "def query_snowflake(query):\n",
    "    \"\"\"Execute a query on Snowflake and return results as DataFrame.\"\"\"\n",
    "    con = snowflake.connector.connect(\n",
    "        user=os.environ[\"SNOWFLAKE_USERNAME\"],\n",
    "        account=os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "        password=os.environ[\"SNOWFLAKE_PASSWORD\"],\n",
    "        database=os.environ[\"SNOWFLAKE_DATABASE\"]\n",
    "    )\n",
    "    try:\n",
    "        cur = con.cursor()\n",
    "        cur.execute(\"USE WAREHOUSE COMPUTE_WH\")\n",
    "        cur.execute(query)\n",
    "        data = cur.fetchall()\n",
    "        columns = [desc[0].lower() for desc in cur.description]\n",
    "        return pd.DataFrame(data, columns=columns)\n",
    "    finally:\n",
    "        con.close()\n",
    "\n",
    "def get_snowflake_timezone():\n",
    "    result = query_snowflake(\"SHOW PARAMETERS LIKE 'TIMEZONE'\")\n",
    "    return result.value[0] if len(result) > 0 else \"UTC\"\n",
    "\n",
    "# Get timezone for queries\n",
    "TIMEZONE = get_snowflake_timezone()\n",
    "\n",
    "print(f\"Market Data Module loaded at {CAIRO_NOW.strftime('%Y-%m-%d %H:%M:%S')} Cairo time\")\n",
    "print(f\"Snowflake timezone: {TIMEZONE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market price queries defined ✓\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MARKET PRICE QUERIES\n",
    "# =============================================================================\n",
    "# Note: TIMEZONE is set dynamically from Snowflake in the imports cell above\n",
    "# =============================================================================\n",
    "# 1. BEN SOLIMAN PRICES QUERY\n",
    "# =============================================================================\n",
    "BEN_SOLIMAN_QUERY = f'''\n",
    "WITH lower as (\n",
    "    select distinct product_id, sku, new_d*bs_price as ben_soliman_price, INJECTION_DATE\n",
    "    from (\n",
    "        select maxab_product_id as product_id, maxab_sku as sku, INJECTION_DATE, wac1, wac_p,\n",
    "            (bs_price/bs_unit_count) as bs_price, diff, cu_price,\n",
    "            case when p1 > 1 then child_quantity else 0 end as scheck,\n",
    "            round(p1/2)*2 as p1, p2,\n",
    "            case when (ROUND(p1 / scheck) * scheck) = 0 then p1 else (ROUND(p1 / scheck) * scheck) end as new_d\n",
    "        from (\n",
    "            select sm.*, wac1, wac_p, \n",
    "                abs((bs_price/bs_unit_count)-(wac_p*maxab_basic_unit_count))/(wac_p*maxab_basic_unit_count) as diff,\n",
    "                cpc.price as cu_price, pup.child_quantity,\n",
    "                round((cu_price/(bs_price/bs_unit_count))) as p1, \n",
    "                round(((bs_price/bs_unit_count)/cu_price)) as p2\n",
    "            from materialized_views.savvy_mapping sm \n",
    "            join finance.all_cogs f on f.product_id = sm.maxab_product_id \n",
    "                and CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP()) between f.from_Date and f.to_date\n",
    "            join PACKING_UNIT_PRODUCTS pu on pu.product_id = sm.maxab_product_id and pu.IS_BASIC_UNIT = 1 \n",
    "            join cohort_product_packing_units cpc on cpc.PRODUCT_PACKING_UNIT_ID = pu.id and cohort_id = 700 \n",
    "            join packing_unit_products pup on pup.product_id = sm.maxab_product_id and pup.is_basic_unit = 1  \n",
    "            where bs_price is not null \n",
    "                and INJECTION_DATE::date >= CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - 5 \n",
    "                and diff > 0.3 and p1 > 1\n",
    "        )\n",
    "    )\n",
    "    qualify max(INJECTION_DATE) over(partition by product_id) = INJECTION_DATE\n",
    "),\n",
    "\n",
    "m_bs as (\n",
    "    select z.* from (\n",
    "        select maxab_product_id as product_id, maxab_sku as sku, avg(bs_final_price) as ben_soliman_price, INJECTION_DATE\n",
    "        from (\n",
    "            select *, row_number() over(partition by maxab_product_id order by diff) as rnk_2 \n",
    "            from (\n",
    "                select *, (bs_final_price-wac_p)/wac_p as diff_2 \n",
    "                from (\n",
    "                    select *, bs_price/maxab_basic_unit_count as bs_final_price \n",
    "                    from (\n",
    "                        select *, row_number() over(partition by maxab_product_id, maxab_pu order by diff) as rnk \n",
    "                        from (\n",
    "                            select *, max(INJECTION_DATE::date) over(partition by maxab_product_id, maxab_pu) as max_date\n",
    "                            from (\n",
    "                                select sm.*, wac1, wac_p, \n",
    "                                    abs(bs_price-(wac_p*maxab_basic_unit_count))/(wac_p*maxab_basic_unit_count) as diff \n",
    "                                from materialized_views.savvy_mapping sm \n",
    "                                join finance.all_cogs f on f.product_id = sm.maxab_product_id \n",
    "                                    and CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP()) between f.from_Date and f.to_date\n",
    "                                where bs_price is not null \n",
    "                                    and INJECTION_DATE::date >= CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - 5 \n",
    "                                    and diff < 0.3\n",
    "                            )\n",
    "                            qualify max_date = INJECTION_DATE\n",
    "                        ) qualify rnk = 1 \n",
    "                    )\n",
    "                ) where diff_2 between -0.5 and 0.5 \n",
    "            ) qualify rnk_2 = 1 \n",
    "        ) group by all\n",
    "    ) z \n",
    "    join finance.all_cogs f on f.product_id = z.product_id \n",
    "        and CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP()) between f.from_Date and f.to_date\n",
    "    where ben_soliman_price between f.wac_p*0.8 and f.wac_p*1.3\n",
    ")\n",
    "\n",
    "select product_id, avg(ben_soliman_price) as ben_soliman_price\n",
    "from (\n",
    "    select * from (\n",
    "        select * from m_bs \n",
    "        union all\n",
    "        select * from lower\n",
    "    )\n",
    "    qualify max(INJECTION_DATE) over(partition by product_id) = INJECTION_DATE\n",
    ")\n",
    "group by all\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "# Marketplace Prices Query\n",
    "# =============================================================================\n",
    "# 2. MARKETPLACE PRICES QUERY (with region fallback)\n",
    "# =============================================================================\n",
    "MARKETPLACE_PRICES_QUERY = f'''\n",
    "WITH MP as (\n",
    "    select region, product_id,\n",
    "        min(min_price) as min_price, min(max_price) as max_price,\n",
    "        min(mod_price) as mod_price, min(true_min) as true_min, min(true_max) as true_max\n",
    "    from (\n",
    "        select mp.region, mp.product_id, mp.pu_id,\n",
    "            min_price/BASIC_UNIT_COUNT as min_price,\n",
    "            max_price/BASIC_UNIT_COUNT as max_price,\n",
    "            mod_price/BASIC_UNIT_COUNT as mod_price,\n",
    "            TRUE_MIN_PRICE/BASIC_UNIT_COUNT as true_min,\n",
    "            TRUE_MAX_PRICE/BASIC_UNIT_COUNT as true_max\n",
    "        from materialized_views.marketplace_prices mp \n",
    "        join packing_unit_products pup on pup.product_id = mp.product_id and pup.packing_unit_id = mp.pu_id\n",
    "        join finance.all_cogs f on f.product_id = mp.product_id \n",
    "            and CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP()) between f.from_date and f.to_date\n",
    "        where least(min_price, mod_price) between wac_p*0.9 and wac_p*1.3 \n",
    "    )\n",
    "    group by all \n",
    "),\n",
    "\n",
    "region_mapping AS (\n",
    "    SELECT * FROM (VALUES\n",
    "        ('Delta East', 'Delta West'), ('Delta West', 'Delta East'),\n",
    "        ('Alexandria', 'Cairo'), ('Alexandria', 'Giza'),\n",
    "        ('Upper Egypt', 'Cairo'), ('Upper Egypt', 'Giza'),\n",
    "        ('Cairo', 'Giza'), ('Giza', 'Cairo'),\n",
    "        ('Delta West', 'Cairo'), ('Delta East', 'Cairo'),\n",
    "        ('Delta West', 'Giza'), ('Delta East', 'Giza')\n",
    "    ) AS region_mapping(region, fallback_region)\n",
    "),\n",
    "\n",
    "all_regions as (\n",
    "    SELECT * FROM (VALUES\n",
    "        ('Cairo'), ('Giza'), ('Delta West'), ('Delta East'), ('Upper Egypt'), ('Alexandria')\n",
    "    ) AS x(region)\n",
    "),\n",
    "\n",
    "full_data as (\n",
    "    select products.id as product_id, ar.region\n",
    "    from products, all_regions ar\n",
    "    where activation = 'true'\n",
    ")\n",
    "\n",
    "select region, product_id,\n",
    "    min(final_min_price) as final_min_price, \n",
    "    min(final_max_price) as final_max_price,\n",
    "    min(final_mod_price) as final_mod_price, \n",
    "    min(final_true_min) as final_true_min,\n",
    "    min(final_true_max) as final_true_max\n",
    "from (\n",
    "    SELECT distinct w.region, w.product_id,\n",
    "        COALESCE(m1.min_price, m2.min_price) AS final_min_price,\n",
    "        COALESCE(m1.max_price, m2.max_price) AS final_max_price,\n",
    "        COALESCE(m1.mod_price, m2.mod_price) AS final_mod_price,\n",
    "        COALESCE(m1.true_min, m2.true_min) AS final_true_min,\n",
    "        COALESCE(m1.true_max, m2.true_max) AS final_true_max\n",
    "    FROM full_data w\n",
    "    LEFT JOIN MP m1 ON w.region = m1.region and w.product_id = m1.product_id\n",
    "    LEFT JOIN region_mapping rm ON w.region = rm.region\n",
    "    LEFT JOIN MP m2 ON rm.fallback_region = m2.region AND w.product_id = m2.product_id\n",
    ")\n",
    "where final_min_price is not null \n",
    "group by all\n",
    "'''\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. SCRAPPED DATA QUERY (Competitor prices from scraping)\n",
    "# =============================================================================\n",
    "SCRAPPED_QUERY = f'''\n",
    "select product_id, region,\n",
    "    MIN(market_price) AS min_scrapped,\n",
    "    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY market_price) AS scrapped25,\n",
    "    PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY market_price) AS scrapped50,\n",
    "    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY market_price) AS scrapped75,\n",
    "    MAX(market_price) AS max_scrapped\n",
    "from (\n",
    "    select distinct cmp.*, max(date) over(partition by region, cmp.product_id, competitor) as max_date\n",
    "    from MATERIALIZED_VIEWS.CLEANED_MARKET_PRICES cmp\n",
    "    join finance.all_cogs f on f.product_id = cmp.product_id \n",
    "        and CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP()) between f.from_date and f.to_date \n",
    "    where date >= CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - 7 \n",
    "        and MARKET_PRICE between f.wac_p * 0.8 and wac_p * 1.3\n",
    "    qualify date = max_date \n",
    ")\n",
    "group by all\n",
    "'''\n",
    "\n",
    "\n",
    "# Product Groups Query\n",
    "GROUPS_QUERY = '''\n",
    "SELECT * FROM materialized_views.sku_commercial_groups\n",
    "'''\n",
    "\n",
    "print(\"Market price queries defined ✓\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price analysis helper functions defined ✓\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PRICE ANALYSIS HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def price_analysis(row):\n",
    "    \"\"\"\n",
    "    Analyze prices and calculate percentiles for a product.\n",
    "    \n",
    "    Collects prices from all sources (Ben Soliman, Marketplace, Scrapped),\n",
    "    filters for valid prices within acceptable range, and calculates percentiles.\n",
    "    \n",
    "    Args:\n",
    "        row: DataFrame row containing price columns and wac_p\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (minimum, percentile_25, percentile_50, percentile_75, maximum)\n",
    "    \"\"\"\n",
    "    wac = row['wac_p']\n",
    "    avg_margin = row['avg_margin'] if row['avg_margin'] >= 0.01 else row['target_margin']\n",
    "    std = np.maximum(row['std'], 0.0025)\n",
    "    target_margin = row['target_margin']\n",
    "    max_marg = np.maximum(avg_margin, target_margin)\n",
    "    \n",
    "    # Collect all price points from different sources\n",
    "    price_list = [\n",
    "        row.get('ben_soliman_price'), \n",
    "        row.get('final_min_price'), \n",
    "        row.get('final_mod_price'),\n",
    "        row.get('final_max_price'), \n",
    "        row.get('final_true_min'), \n",
    "        row.get('final_true_max'),\n",
    "        row.get('min_scrapped'), \n",
    "        row.get('scrapped25'), \n",
    "        row.get('scrapped50'), \n",
    "        row.get('scrapped75'), \n",
    "        row.get('max_scrapped')\n",
    "    ]\n",
    "    \n",
    "    # Filter valid prices within acceptable range\n",
    "    valid_prices = sorted({\n",
    "        x for x in price_list \n",
    "        if x and not pd.isna(x) and x != 0 \n",
    "        and wac / (1 - (avg_margin - (10 * std))) <= x <= wac / (1 - (max_marg + 10 * std))\n",
    "        and x >= wac * (0.9 + target_margin * 0.7)\n",
    "    })\n",
    "    \n",
    "    if not valid_prices:\n",
    "        return np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "    \n",
    "    return (\n",
    "        np.min(valid_prices),\n",
    "        np.percentile(valid_prices, 25),\n",
    "        np.percentile(valid_prices, 50),\n",
    "        np.percentile(valid_prices, 75),\n",
    "        np.max(valid_prices)\n",
    "    )\n",
    "\n",
    "\n",
    "def calculate_step_bounds(row):\n",
    "    \"\"\"\n",
    "    Calculate below/above market bounds based on price steps.\n",
    "    \n",
    "    Analyzes the steps between market price tiers and calculates\n",
    "    bounds one step below minimum and one step above maximum.\n",
    "    \n",
    "    Args:\n",
    "        row: DataFrame row with percentile columns\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (below_market_price, above_market_price)\n",
    "    \"\"\"\n",
    "    wac = row['wac_p']\n",
    "    std = row['std']\n",
    "    target_margin = row.get('target_margin', 0.05)\n",
    "    \n",
    "    prices = [\n",
    "        row['minimum'], \n",
    "        row['percentile_25'], \n",
    "        row['percentile_50'], \n",
    "        row['percentile_75'], \n",
    "        row['maximum']\n",
    "    ]\n",
    "    \n",
    "    # Calculate valid steps between price points\n",
    "    valid_steps = []\n",
    "    for i in range(len(prices) - 1):\n",
    "        step = prices[i + 1] - prices[i]\n",
    "        if (step / wac) <= std * 1.2:\n",
    "            valid_steps.append(step)\n",
    "    \n",
    "    avg_step = np.mean(valid_steps) if valid_steps else min(2 * std, 0.2 * target_margin)\n",
    "    \n",
    "    new_min = prices[0] - avg_step if (prices[0] - avg_step) >= wac else prices[0]\n",
    "    new_max = prices[-1] + avg_step if (prices[-1] + avg_step) >= wac else prices[-1]\n",
    "    \n",
    "    return new_min, new_max\n",
    "\n",
    "\n",
    "def weighted_median(series, weights):\n",
    "    \"\"\"\n",
    "    Calculate weighted median of a series.\n",
    "    \n",
    "    Args:\n",
    "        series: pd.Series of values\n",
    "        weights: pd.Series of weights (e.g., NMV contribution)\n",
    "        \n",
    "    Returns:\n",
    "        Weighted median value or NaN if no valid data\n",
    "    \"\"\"\n",
    "    valid = ~series.isna() & ~weights.isna()\n",
    "    s = series[valid]\n",
    "    w = weights[valid]\n",
    "    if len(s) == 0:\n",
    "        return np.nan\n",
    "    order = np.argsort(s)\n",
    "    s, w = s.iloc[order], w.iloc[order]\n",
    "    return s.iloc[np.searchsorted(np.cumsum(w), w.sum() / 2)]\n",
    "\n",
    "\n",
    "def fill_missing_prices_from_groups(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fill missing market prices using group-level aggregation.\n",
    "    \n",
    "    Fetches product groups from Snowflake, then for products in a group,\n",
    "    calculates weighted median prices based on NMV contribution and \n",
    "    uses these to fill missing values.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with market prices (group_id will be fetched from Snowflake)\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with missing prices filled from group data\n",
    "    \"\"\"\n",
    "    # Price columns to process\n",
    "    price_cols = [\n",
    "        'ben_soliman_price', 'final_min_price', 'final_max_price', \n",
    "        'final_mod_price', 'final_true_min', 'final_true_max',\n",
    "        'min_scrapped', 'scrapped25', 'scrapped50', 'scrapped75', 'max_scrapped'\n",
    "    ]\n",
    "    \n",
    "    # Fetch product groups from Snowflake\n",
    "    df_groups = fetch_product_groups()\n",
    "    \n",
    "    if len(df_groups) == 0:\n",
    "        print(\"  No product groups found in database\")\n",
    "        return df\n",
    "    \n",
    "    # Merge groups with df (drop existing group_id if any)\n",
    "    df = df.drop(columns=['group_id'], errors='ignore')\n",
    "    df = df.merge(\n",
    "        df_groups[['product_id', 'group_id']], \n",
    "        on='product_id', \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Get products with groups\n",
    "    groups_data = df[~df['group_id'].isna()].copy()\n",
    "    \n",
    "    if len(groups_data) == 0:\n",
    "        print(\"  No products with groups found, skipping group processing\")\n",
    "        return df\n",
    "    \n",
    "    print(f\"  Processing {len(groups_data)} products with groups...\")\n",
    "    \n",
    "    # Calculate NMV contribution within each group\n",
    "    # Use 'nmv' if available, otherwise use equal weights\n",
    "    if 'nmv' in groups_data.columns:\n",
    "        groups_data['group_nmv'] = groups_data.groupby(['group_id', 'cohort_id'])['nmv'].transform('sum')\n",
    "        groups_data['cntrb'] = (groups_data['nmv'] / groups_data['group_nmv']).fillna(1)\n",
    "    else:\n",
    "        # Equal weights if no NMV\n",
    "        groups_data['cntrb'] = 1\n",
    "    \n",
    "    # Flag if any price column is non-NaN\n",
    "    available_price_cols = [c for c in price_cols if c in groups_data.columns]\n",
    "    groups_data['flag_non_nan'] = groups_data[available_price_cols].notna().any(axis=1).astype(int)\n",
    "    \n",
    "    # Perform weighted aggregation for groups\n",
    "    groups_agg = (\n",
    "        groups_data[groups_data['flag_non_nan'] == 1]\n",
    "        .groupby(['group_id', 'cohort_id'])\n",
    "        .apply(lambda g: pd.Series({\n",
    "            col: weighted_median(g[col], g['cntrb']) for col in available_price_cols if col in g.columns\n",
    "        }))\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    if len(groups_agg) == 0:\n",
    "        print(\"  No group aggregations computed\")\n",
    "        return df\n",
    "    \n",
    "    # Fill missing prices with group-level prices\n",
    "    merged = df.merge(groups_agg, on=['group_id', 'cohort_id'], how='left', suffixes=('', '_group'))\n",
    "    \n",
    "    for col in available_price_cols:\n",
    "        group_col = f'{col}_group'\n",
    "        if group_col in merged.columns:\n",
    "            merged[col] = merged[col].fillna(merged[group_col])\n",
    "    \n",
    "    # Drop the group columns\n",
    "    group_cols_to_drop = [f'{c}_group' for c in available_price_cols if f'{c}_group' in merged.columns]\n",
    "    merged = merged.drop(columns=group_cols_to_drop, errors='ignore')\n",
    "    \n",
    "    print(f\"  Group processing complete\")\n",
    "    return merged\n",
    "\n",
    "\n",
    "print(\"Price analysis helper functions defined ✓\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fetching functions defined ✓\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DATA FETCHING FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_ben_soliman_prices():\n",
    "    \"\"\"Fetch Ben Soliman competitor prices from Snowflake.\"\"\"\n",
    "    print(\"  Fetching Ben Soliman prices...\")\n",
    "    df = query_snowflake(BEN_SOLIMAN_QUERY)\n",
    "    print(f\"    Loaded {len(df)} records\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def fetch_marketplace_prices():\n",
    "    \"\"\"Fetch marketplace prices from Snowflake.\"\"\"\n",
    "    print(\"  Fetching marketplace prices...\")\n",
    "    df = query_snowflake(MARKETPLACE_QUERY)\n",
    "    print(f\"    Loaded {len(df)} records\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def fetch_scrapped_prices():\n",
    "    \"\"\"Fetch scrapped competitor prices from Snowflake.\"\"\"\n",
    "    print(\"  Fetching scrapped prices...\")\n",
    "    df = query_snowflake(SCRAPPED_QUERY)\n",
    "    print(f\"    Loaded {len(df)} records\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def fetch_product_groups():\n",
    "    \"\"\"Fetch product groups from Snowflake.\"\"\"\n",
    "    print(\"  Fetching product groups...\")\n",
    "    df = query_snowflake(GROUPS_QUERY)\n",
    "    print(f\"    Loaded {len(df)} records\")\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"Data fetching functions defined ✓\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MAIN PROCESSING FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def get_market_prices(df_base: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch and process all market prices.\n",
    "    \n",
    "    Merges Ben Soliman, Marketplace, and Scrapped prices with the base dataframe.\n",
    "    Calculates market price percentiles and margin tiers.\n",
    "    \n",
    "    Args:\n",
    "        df_base: DataFrame with product_id, warehouse_id, wac_p, avg_margin, \n",
    "                 std, target_margin columns\n",
    "                 \n",
    "    Returns:\n",
    "        DataFrame with market price columns added:\n",
    "        - below_market, market_min, market_25, market_50, market_75, market_max, above_market\n",
    "        (These are MARGIN values, not prices)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FETCHING MARKET PRICES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get warehouse-region mapping\n",
    "    df = df_base.copy()\n",
    "    \n",
    "    # Ensure required columns exist\n",
    "    required_cols = ['product_id', 'wac_p', 'avg_margin', 'std', 'target_margin']\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "    \n",
    "    # 1. Fetch all market price sources\n",
    "    df_bs = fetch_ben_soliman_prices()\n",
    "    df_mp = fetch_marketplace_prices()\n",
    "    df_scrapped = fetch_scrapped_prices()\n",
    "    \n",
    "    # 2. Merge Ben Soliman prices (product level)\n",
    "    df = df.merge(\n",
    "        df_bs[['product_id', 'ben_soliman_price']],\n",
    "        on='product_id',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # 3. For marketplace and scrapped, we need region mapping\n",
    "    # Get region from warehouse if available\n",
    "    if 'region' not in df.columns:\n",
    "        # Try to get region from warehouse_id\n",
    "        REGION_QUERY = '''\n",
    "        SELECT id as warehouse_id, region \n",
    "        FROM warehouses \n",
    "        WHERE is_active = TRUE\n",
    "        '''\n",
    "        print(\"  Fetching warehouse regions...\")\n",
    "        df_regions = query_snowflake(REGION_QUERY)\n",
    "        if 'warehouse_id' in df.columns:\n",
    "            df = df.merge(df_regions, on='warehouse_id', how='left')\n",
    "        else:\n",
    "            # Default to 'cairo' if no warehouse info\n",
    "            df['region'] = 'cairo'\n",
    "    \n",
    "    # 4. Merge marketplace prices\n",
    "    df = df.merge(\n",
    "        df_mp[['product_id', 'region', 'final_min_price', 'final_max_price', \n",
    "               'final_mod_price', 'final_true_min', 'final_true_max']],\n",
    "        on=['product_id', 'region'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # 5. Merge scrapped prices\n",
    "    df = df.merge(\n",
    "        df_scrapped[['product_id', 'region', 'min_scrapped', 'scrapped25', \n",
    "                     'scrapped50', 'scrapped75', 'max_scrapped']],\n",
    "        on=['product_id', 'region'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # 6. Fill missing prices from group-level data\n",
    "    print(\"\\n  Processing group-level prices...\")\n",
    "    df = fill_missing_prices_from_groups(df)\n",
    "    \n",
    "    # 7. Apply price analysis to get percentiles\n",
    "    print(\"\\n  Calculating market price percentiles...\")\n",
    "    df[['minimum', 'percentile_25', 'percentile_50', 'percentile_75', 'maximum']] = \\\n",
    "        df.apply(price_analysis, axis=1, result_type='expand')\n",
    "    \n",
    "    # 8. Calculate below/above market bounds\n",
    "    # Only for rows with valid percentiles\n",
    "    mask = df['minimum'].notna()\n",
    "    df.loc[mask, ['below_market_price', 'above_market_price']] = \\\n",
    "        df[mask].apply(calculate_step_bounds, axis=1, result_type='expand')\n",
    "    \n",
    "    # 9. Convert prices to margins\n",
    "    print(\"  Converting prices to margins...\")\n",
    "    \n",
    "    # below_market = (price - wac) / price  (margin formula)\n",
    "    df['below_market'] = (df['below_market_price'] - df['wac_p']) / df['below_market_price']\n",
    "    df['market_min'] = (df['minimum'] - df['wac_p']) / df['minimum']\n",
    "    df['market_25'] = (df['percentile_25'] - df['wac_p']) / df['percentile_25']\n",
    "    df['market_50'] = (df['percentile_50'] - df['wac_p']) / df['percentile_50']\n",
    "    df['market_75'] = (df['percentile_75'] - df['wac_p']) / df['percentile_75']\n",
    "    df['market_max'] = (df['maximum'] - df['wac_p']) / df['maximum']\n",
    "    df['above_market'] = (df['above_market_price'] - df['wac_p']) / df['above_market_price']\n",
    "    \n",
    "    # 10. Select only the market columns to return\n",
    "    market_cols = [\n",
    "        'product_id', 'warehouse_id',\n",
    "        # Raw prices\n",
    "        'minimum', 'percentile_25', 'percentile_50', 'percentile_75', 'maximum',\n",
    "        # Margin tiers\n",
    "        'below_market', 'market_min', 'market_25', 'market_50', \n",
    "        'market_75', 'market_max', 'above_market'\n",
    "    ]\n",
    "    \n",
    "    # Keep only columns that exist\n",
    "    market_cols = [c for c in market_cols if c in df.columns]\n",
    "    \n",
    "    print(f\"\\n  Market prices processed for {len(df)} records\")\n",
    "    print(f\"  Columns: {market_cols}\")\n",
    "    \n",
    "    return df[market_cols]\n",
    "\n",
    "\n",
    "print(\"get_market_prices function defined ✓\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MARGIN TIERS CALCULATION\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_margin_tiers(df_base: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate margin tiers for products.\n",
    "    \n",
    "    Creates 8 margin tiers based on effective_min_margin and max_boundary:\n",
    "    - margin_tier_below: 1 step below minimum\n",
    "    - margin_tier_1 to margin_tier_5: Within range\n",
    "    - margin_tier_above_1, margin_tier_above_2: Above maximum\n",
    "    \n",
    "    Args:\n",
    "        df_base: DataFrame with product_id, warehouse_id, effective_min_margin,\n",
    "                 max_boundary, margin_step columns\n",
    "                 \n",
    "    Returns:\n",
    "        DataFrame with margin tier columns added\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CALCULATING MARGIN TIERS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    df = df_base.copy()\n",
    "    \n",
    "    # Check required columns\n",
    "    required_cols = ['product_id', 'effective_min_margin', 'max_boundary', 'margin_step']\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        print(f\"  ⚠️ Missing columns for margin tiers: {missing}\")\n",
    "        print(\"  Using avg_margin to create default tiers...\")\n",
    "        \n",
    "        # Create default margin step if not available\n",
    "        if 'margin_step' not in df.columns:\n",
    "            df['margin_step'] = df['std'].fillna(0.01) * 2  # Default step\n",
    "            \n",
    "        if 'effective_min_margin' not in df.columns:\n",
    "            df['effective_min_margin'] = df['target_margin'] - df['margin_step']\n",
    "            \n",
    "        if 'max_boundary' not in df.columns:\n",
    "            df['max_boundary'] = df['target_margin'] + (4 * df['margin_step'])\n",
    "    \n",
    "    # Calculate margin tiers\n",
    "    print(\"  Creating margin tier structure...\")\n",
    "    \n",
    "    # Below minimum (1 step below)\n",
    "    df['margin_tier_below'] = df['effective_min_margin'] - df['margin_step']\n",
    "    \n",
    "    # 5 tiers in range (equally spaced)\n",
    "    df['margin_tier_1'] = df['effective_min_margin']  # Min\n",
    "    df['margin_tier_2'] = df['effective_min_margin'] + df['margin_step']\n",
    "    df['margin_tier_3'] = df['effective_min_margin'] + 2 * df['margin_step']\n",
    "    df['margin_tier_4'] = df['effective_min_margin'] + 3 * df['margin_step']\n",
    "    df['margin_tier_5'] = df['max_boundary']  # Max\n",
    "    \n",
    "    # Above maximum (2 steps above)\n",
    "    df['margin_tier_above_1'] = df['max_boundary'] + df['margin_step']\n",
    "    df['margin_tier_above_2'] = df['max_boundary'] + 2 * df['margin_step']\n",
    "    \n",
    "    # Select margin tier columns\n",
    "    margin_tier_cols = [\n",
    "        'product_id', 'warehouse_id',\n",
    "        'margin_tier_below', 'margin_tier_1', 'margin_tier_2', 'margin_tier_3',\n",
    "        'margin_tier_4', 'margin_tier_5', 'margin_tier_above_1', 'margin_tier_above_2'\n",
    "    ]\n",
    "    \n",
    "    # Keep only columns that exist\n",
    "    margin_tier_cols = [c for c in margin_tier_cols if c in df.columns]\n",
    "    \n",
    "    print(f\"\\n  Margin tiers calculated for {len(df)} records\")\n",
    "    print(f\"  Tier structure:\")\n",
    "    print(f\"    margin_tier_below:   effective_min - step\")\n",
    "    print(f\"    margin_tier_1:       effective_min_margin\")\n",
    "    print(f\"    margin_tier_2:       effective_min + 1*step\")\n",
    "    print(f\"    margin_tier_3:       effective_min + 2*step\")\n",
    "    print(f\"    margin_tier_4:       effective_min + 3*step\")\n",
    "    print(f\"    margin_tier_5:       max_boundary\")\n",
    "    print(f\"    margin_tier_above_1: max_boundary + 1*step\")\n",
    "    print(f\"    margin_tier_above_2: max_boundary + 2*step\")\n",
    "    \n",
    "    return df[margin_tier_cols]\n",
    "\n",
    "\n",
    "print(\"calculate_margin_tiers function defined ✓\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMBINED FUNCTION - GET ALL MARKET AND MARGIN DATA\n",
    "# =============================================================================\n",
    "\n",
    "def get_market_and_margin_data(df_base: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combined function to fetch fresh market prices and calculate margin tiers.\n",
    "    \n",
    "    This is the main entry point for getting all market/margin data refreshed.\n",
    "    \n",
    "    Args:\n",
    "        df_base: DataFrame with at least:\n",
    "            - product_id, warehouse_id\n",
    "            - wac_p, avg_margin, std, target_margin\n",
    "            - effective_min_margin, max_boundary, margin_step (optional)\n",
    "            \n",
    "    Returns:\n",
    "        DataFrame with market prices and margin tiers added:\n",
    "        \n",
    "        Market Prices (as margins):\n",
    "            - below_market, market_min, market_25, market_50, market_75, market_max, above_market\n",
    "            \n",
    "        Price Percentiles (as EGP):\n",
    "            - minimum, percentile_25, percentile_50, percentile_75, maximum\n",
    "            \n",
    "        Margin Tiers:\n",
    "            - margin_tier_below, margin_tier_1, margin_tier_2, margin_tier_3,\n",
    "              margin_tier_4, margin_tier_5, margin_tier_above_1, margin_tier_above_2\n",
    "              \n",
    "    Usage:\n",
    "        %run market_data_module.ipynb\n",
    "        df_with_market = get_market_and_margin_data(df_input)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"REFRESHING MARKET PRICES AND MARGIN TIERS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Timestamp: {datetime.now(CAIRO_TZ).strftime('%Y-%m-%d %H:%M:%S')} Cairo time\")\n",
    "    print(f\"Input records: {len(df_base)}\")\n",
    "    \n",
    "    df = df_base.copy()\n",
    "    \n",
    "    # =================================\n",
    "    # STEP 1: Get Market Prices\n",
    "    # =================================\n",
    "    try:\n",
    "        df_market = get_market_prices(df)\n",
    "        \n",
    "        # Drop existing market columns to avoid duplicates\n",
    "        market_cols_to_drop = [\n",
    "            'below_market', 'market_min', 'market_25', 'market_50', \n",
    "            'market_75', 'market_max', 'above_market',\n",
    "            'minimum', 'percentile_25', 'percentile_50', 'percentile_75', 'maximum'\n",
    "        ]\n",
    "        df = df.drop(columns=[c for c in market_cols_to_drop if c in df.columns], errors='ignore')\n",
    "        \n",
    "        # Merge market data\n",
    "        merge_keys = ['product_id']\n",
    "        if 'warehouse_id' in df_market.columns and 'warehouse_id' in df.columns:\n",
    "            merge_keys.append('warehouse_id')\n",
    "        \n",
    "        df = df.merge(df_market, on=merge_keys, how='left')\n",
    "        print(f\"\\n  ✓ Market prices merged\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n  ⚠️ Error fetching market prices: {e}\")\n",
    "        print(\"  Continuing without market price refresh...\")\n",
    "    \n",
    "    # =================================\n",
    "    # STEP 2: Calculate Margin Tiers\n",
    "    # =================================\n",
    "    try:\n",
    "        df_tiers = calculate_margin_tiers(df)\n",
    "        \n",
    "        # Drop existing margin tier columns to avoid duplicates\n",
    "        tier_cols_to_drop = [\n",
    "            'margin_tier_below', 'margin_tier_1', 'margin_tier_2', 'margin_tier_3',\n",
    "            'margin_tier_4', 'margin_tier_5', 'margin_tier_above_1', 'margin_tier_above_2'\n",
    "        ]\n",
    "        df = df.drop(columns=[c for c in tier_cols_to_drop if c in df.columns], errors='ignore')\n",
    "        \n",
    "        # Merge margin tiers\n",
    "        merge_keys = ['product_id']\n",
    "        if 'warehouse_id' in df_tiers.columns and 'warehouse_id' in df.columns:\n",
    "            merge_keys.append('warehouse_id')\n",
    "            \n",
    "        df = df.merge(df_tiers, on=merge_keys, how='left')\n",
    "        print(f\"\\n  ✓ Margin tiers merged\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n  ⚠️ Error calculating margin tiers: {e}\")\n",
    "        print(\"  Continuing without margin tier refresh...\")\n",
    "    \n",
    "    # =================================\n",
    "    # SUMMARY\n",
    "    # =================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MARKET DATA REFRESH COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Output records: {len(df)}\")\n",
    "    \n",
    "    # List available market/margin columns\n",
    "    market_cols = [c for c in df.columns if 'market' in c.lower() or 'margin_tier' in c.lower() or 'percentile' in c.lower()]\n",
    "    print(f\"Market/Margin columns available: {len(market_cols)}\")\n",
    "    for col in market_cols:\n",
    "        non_null = df[col].notna().sum()\n",
    "        print(f\"  - {col}: {non_null} non-null values ({non_null/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"get_market_and_margin_data function defined ✓\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MARKET DATA MODULE READY\")\n",
    "print(\"=\"*70)\n",
    "print(\"Available functions:\")\n",
    "print(\"  - get_market_prices(df_base)      : Fetch market prices only\")\n",
    "print(\"  - calculate_margin_tiers(df_base) : Calculate margin tiers only\")\n",
    "print(\"  - get_market_and_margin_data(df)  : Combined refresh (recommended)\")\n",
    "print(\"=\"*70)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
