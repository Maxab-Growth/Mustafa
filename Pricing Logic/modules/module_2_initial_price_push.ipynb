{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2: Initial Price Push (Daily 6 AM Reset)\n",
    "\n",
    "## Purpose\n",
    "This module runs once daily at 8 AM Cairo time to:\n",
    "1. **Load and prepare data** from Snowflake (MATERIALIZED_VIEWS.Pricing_data_extraction)\n",
    "2. Reset prices for all SKUs based on ABC classification\n",
    "3. Set initial cart rules based on normal_refill and stddev\n",
    "4. Apply status-based adjustments (combined_status + yesterday_status)\n",
    "5. Push cart rules and prices via API\n",
    "\n",
    "## Data Flow\n",
    "```\n",
    "data_extraction.ipynb â†’ Snowflake (Pricing_data_extraction) â†’ Module 2 (this module)\n",
    "                                                        â”œâ”€â”€ Data Preparation\n",
    "                                                        â”œâ”€â”€ Price Logic\n",
    "                                                        â”œâ”€â”€ Cart Rule Logic\n",
    "                                                        â””â”€â”€ Push to API\n",
    "```\n",
    "\n",
    "## Price Setting Logic\n",
    "- **Zero demand SKUs**: Market minimum price + SKU discount\n",
    "- **With market data**: A=25th percentile, B=50th, C=75th\n",
    "- **Without market data**: A=50% margin range, B=75%, C=90%\n",
    "- **No data SKUs**: Average margin of their category\n",
    "\n",
    "## Status Adjustment Logic\n",
    "- Both below On Track: -1 step from current price\n",
    "- Both above On Track: +1 step from current price\n",
    "- Combined lower, Yesterday higher: No action (oscillation prevention)\n",
    "- Combined higher, Yesterday lower: No action (trend observation)\n",
    "- On Track: No action\n",
    "- Above On Track + Yesterday On Track: No action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T06:09:56.111046Z",
     "iopub.status.busy": "2026-02-18T06:09:56.110859Z",
     "iopub.status.idle": "2026-02-18T06:09:59.641485Z",
     "shell.execute_reply": "2026-02-18T06:09:59.640644Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.12/site-packages/snowflake/connector/options.py:104: UserWarning: You have an incompatible version of 'pyarrow' installed (20.0.0), please install a version that adheres to: 'pyarrow<19.0.0; extra == \"pandas\"'\n",
      "  warn_incompatible_dep(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.Renviron\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/service_account_key.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries Module | Timezone: America/Los_Angeles\n",
      "âœ… UTH and Last Hour functions defined\n",
      "\n",
      "==================================================\n",
      "QUERIES MODULE READY\n",
      "==================================================\n",
      "\n",
      "Live Data Functions:\n",
      "  â€¢ get_current_stocks()\n",
      "  â€¢ get_packing_units()\n",
      "  â€¢ get_current_prices()\n",
      "  â€¢ get_current_wac()\n",
      "  â€¢ get_current_cart_rules()\n",
      "\n",
      "UTH Performance Functions:\n",
      "  â€¢ get_uth_performance()         - UTH qty/retailers (Snowflake)\n",
      "  â€¢ get_hourly_distribution()     - Historical hour contributions (Snowflake)\n",
      "  â€¢ get_last_hour_performance()   - Last hour qty/retailers (DWH)\n",
      "\n",
      "Note: Market prices use MODULE_1_INPUT data\n",
      "Retailer Selection Queries defined âœ“\n",
      "  - get_churned_dropped_retailers()\n",
      "  - get_category_not_product_retailers()\n",
      "  - get_out_of_cycle_retailers()\n",
      "  - get_view_no_orders_retailers()\n",
      "  - get_excluded_retailers()\n",
      "  - get_retailers_with_quantity_discount()\n",
      "  - get_retailer_main_warehouse()\n",
      "Module 2: Initial Price Push\n",
      "Run Time (Cairo): 2026-02-18 08:09:59\n",
      "Input: MATERIALIZED_VIEWS.Pricing_data_extraction (today's data)\n",
      "Output: module_2_output_20260218_0809.xlsx\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS AND SETUP\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "%run queries_module.ipynb\n",
    "# Cairo timezone\n",
    "CAIRO_TZ = pytz.timezone('Africa/Cairo')\n",
    "CAIRO_NOW = datetime.now(CAIRO_TZ)\n",
    "TODAY = CAIRO_NOW.date()\n",
    "CURRENT_HOUR = CAIRO_NOW.hour\n",
    "\n",
    "# Configuration constants\n",
    "ABC_MARKET_PERCENTILES = {'A': 25, 'B': 50, 'C': 75}\n",
    "ABC_MARGIN_PERCENTILES = {'A': 50, 'B': 75, 'C': 90}\n",
    "# Cart rule constraints\n",
    "MIN_CART_RULE = 10  # Minimum for Module 2 (fallback when no percentile data)\n",
    "MIN_CART_RULE_OTHER = 5  # Minimum for other cases\n",
    "MAX_CART_RULE = 500\n",
    "LOW_STOCK_DOH_THRESHOLD = 2  # SKUs with DOH <= this are protected from price reduction\n",
    "STATUS_BELOW_ON_TRACK = ['No Data', 'Critical', 'Struggling', 'Underperforming']\n",
    "STATUS_ABOVE_ON_TRACK = ['Over Achiever', 'Star Performer']\n",
    "STATUS_ON_TRACK = ['On Track']\n",
    "\n",
    "# Input/Output configuration\n",
    "# Data is now loaded from Snowflake instead of Excel\n",
    "INPUT_TABLE = 'MATERIALIZED_VIEWS.Pricing_data_extraction'\n",
    "OUTPUT_FILE = f'module_2_output_{CAIRO_NOW.strftime(\"%Y%m%d_%H%M\")}.xlsx'\n",
    "\n",
    "print(f\"Module 2: Initial Price Push\")\n",
    "print(f\"Run Time (Cairo): {CAIRO_NOW.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Input: {INPUT_TABLE} (today's data)\")\n",
    "print(f\"Output: {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T06:09:59.643864Z",
     "iopub.status.busy": "2026-02-18T06:09:59.643497Z",
     "iopub.status.idle": "2026-02-18T06:10:06.927750Z",
     "shell.execute_reply": "2026-02-18T06:10:06.926981Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from Snowflake...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 28794 records from Snowflake\n",
      "\n",
      "Preparing data (structuring columns, handling nulls)...\n",
      "âœ… Data prepared: 28794 records\n",
      "\n",
      "ABC Class Distribution:\n",
      "abc_class\n",
      "C    23438\n",
      "B     4656\n",
      "A      700\n",
      "\n",
      "Combined Status Distribution:\n",
      "combined_status\n",
      "No Data            7744\n",
      "Struggling         5253\n",
      "Underperforming    4698\n",
      "On Track           3822\n",
      "Critical           3347\n",
      "Over Achiever      2200\n",
      "Star Performer     1730\n",
      "Fetching percentile data for cart rules...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded 17493 percentile records\n",
      "   Percentiles available for 3379 unique products\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD DATA FROM SNOWFLAKE (Instead of Excel file)\n",
    "# =============================================================================\n",
    "print(\"Loading data from Snowflake...\")\n",
    "\n",
    "# Query to get today's data from Pricing_data_extraction\n",
    "LOAD_QUERY = f\"\"\"\n",
    "SELECT * FROM {INPUT_TABLE}\n",
    "WHERE created_at = '{datetime.now(CAIRO_TZ).date()}'\n",
    "\"\"\"\n",
    "\n",
    "df_raw = query_snowflake(LOAD_QUERY)\n",
    "print(f\"Loaded {len(df_raw)} records from Snowflake\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# DATA PREPARATION: Transform raw data to structured format\n",
    "# This replicates the data structuring from pricing_action_engine.ipynb\n",
    "# -------------------------------------------------------------------------\n",
    "print(\"\\nPreparing data (structuring columns, handling nulls)...\")\n",
    "\n",
    "# Create a clean DataFrame with all required columns and proper defaults\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Identifiers\n",
    "df['warehouse_id'] = df_raw['warehouse_id']\n",
    "df['product_id'] = df_raw['product_id']\n",
    "df['sku'] = df_raw['sku']\n",
    "df['cohort_id'] = df_raw['cohort_id'] if 'cohort_id' in df_raw.columns else None\n",
    "\n",
    "# Product info\n",
    "df['abc_class'] = df_raw['abc_class'].fillna('C')\n",
    "df['brand'] = df_raw['brand'] if 'brand' in df_raw.columns else None\n",
    "df['cat'] = df_raw['cat'] if 'cat' in df_raw.columns else None\n",
    "df['sensitivity'] = df_raw['sensitivity'] if 'sensitivity' in df_raw.columns else None\n",
    "\n",
    "# Current state - with null handling\n",
    "df['current_price'] = pd.to_numeric(df_raw['current_price'], errors='coerce').fillna(0)\n",
    "df['current_cart_rule'] = pd.to_numeric(df_raw['current_cart_rule'], errors='coerce').fillna(999)\n",
    "df['normal_refill'] = pd.to_numeric(df_raw.get('normal_refill', 0), errors='coerce').fillna(0)\n",
    "df['refill_stddev'] = pd.to_numeric(df_raw.get('refill_stddev', 0), errors='coerce').fillna(0)\n",
    "df['wac_p'] = pd.to_numeric(df_raw['wac_p'], errors='coerce').fillna(0)\n",
    "df['commercial_min_price'] = pd.to_numeric(df_raw.get('commercial_min_price', 0), errors='coerce').fillna(0)\n",
    "\n",
    "# Performance status\n",
    "df['combined_status'] = df_raw['combined_status'].fillna('No Data')\n",
    "df['yesterday_status'] = df_raw['yesterday_status'].fillna('No Data')\n",
    "df['oos_yesterday'] = df_raw['oos_yesterday'].fillna(0).astype(int)\n",
    "\n",
    "# Stock and demand\n",
    "df['stocks'] = pd.to_numeric(df_raw['stocks'], errors='coerce').fillna(0)\n",
    "df['zero_demand'] = df_raw['zero_demand'].fillna(0).astype(int)\n",
    "df['doh'] = pd.to_numeric(df_raw.get('doh', 999), errors='coerce').fillna(999)  # Days on Hand for low stock protection\n",
    "\n",
    "# Margin data (for price tier calculations)\n",
    "df['target_margin'] = pd.to_numeric(df_raw.get('target_margin', 0), errors='coerce').fillna(0)\n",
    "#df['target_margin_std'] = pd.to_numeric(df_raw.get('target_margin_std', 0), errors='coerce').fillna(0)\n",
    "\n",
    "# Market margins (for price tiers)\n",
    "market_margin_cols = ['below_market', 'market_min', 'market_25', 'market_50', \n",
    "                      'market_75', 'market_max', 'above_market']\n",
    "for col in market_margin_cols:\n",
    "    if col in df_raw.columns:\n",
    "        df[col] = pd.to_numeric(df_raw[col], errors='coerce')\n",
    "    else:\n",
    "        df[col] = np.nan\n",
    "\n",
    "# Internal margin tiers\n",
    "margin_tier_cols = ['margin_tier_below', 'margin_tier_1', 'margin_tier_2', 'margin_tier_3',\n",
    "                    'margin_tier_4', 'margin_tier_5', 'margin_tier_above_1', 'margin_tier_above_2']\n",
    "for col in margin_tier_cols:\n",
    "    if col in df_raw.columns:\n",
    "        df[col] = pd.to_numeric(df_raw[col], errors='coerce')\n",
    "    else:\n",
    "        df[col] = np.nan\n",
    "\n",
    "# All-time high margin (price ceiling for increases)\n",
    "df['all_time_high_margin'] = pd.to_numeric(df_raw.get('all_time_high_margin', np.nan), errors='coerce')\n",
    "\n",
    "# P80/P70 for cart rules fallback\n",
    "df['p80_daily_240d'] = pd.to_numeric(df_raw.get('p80_daily_240d', 0), errors='coerce').fillna(0)\n",
    "df['p70_daily_retailers_240d'] = pd.to_numeric(df_raw.get('p70_daily_retailers_240d', 1), errors='coerce').fillna(1)\n",
    "\n",
    "print(f\"âœ… Data prepared: {len(df)} records\")\n",
    "print(f\"\\nABC Class Distribution:\")\n",
    "print(df['abc_class'].value_counts().to_string())\n",
    "print(f\"\\nCombined Status Distribution:\")\n",
    "print(df['combined_status'].value_counts().to_string())\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD PERCENTILE DATA FOR CART RULES\n",
    "# =============================================================================\n",
    "df_percentiles = get_percentile_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T06:10:06.930300Z",
     "iopub.status.busy": "2026-02-18T06:10:06.930069Z",
     "iopub.status.idle": "2026-02-18T06:10:06.941395Z",
     "shell.execute_reply": "2026-02-18T06:10:06.940697Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions loaded.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "# Minimum price change constant (ensure it's defined in this cell for function access)\n",
    "MIN_PRICE_CHANGE_EGP = 0.25  # Minimum 0.25 EGP for any price increase or decrease\n",
    "\n",
    "def is_below_on_track(status):\n",
    "    \"\"\"Check if status is below On Track.\"\"\"\n",
    "    return str(status).strip() in STATUS_BELOW_ON_TRACK\n",
    "\n",
    "def is_above_on_track(status):\n",
    "    \"\"\"Check if status is above On Track.\"\"\"\n",
    "    return str(status).strip() in STATUS_ABOVE_ON_TRACK\n",
    "\n",
    "def is_on_track(status):\n",
    "    \"\"\"Check if status is On Track.\"\"\"\n",
    "    return str(status).strip() in STATUS_ON_TRACK\n",
    "\n",
    "def calculate_margin(price, wac):\n",
    "    \"\"\"Calculate margin from price and WAC.\"\"\"\n",
    "    if pd.isna(price) or pd.isna(wac) or price == 0:\n",
    "        return None\n",
    "    return (price - wac) / price\n",
    "\n",
    "def get_market_tiers(row):\n",
    "    \"\"\"Get sorted list of market price tiers.\"\"\"\n",
    "    tiers = []\n",
    "    for col in ['minimum', 'percentile_25', 'percentile_50', 'percentile_75', 'maximum']:\n",
    "        val = row.get(col)\n",
    "        if pd.notna(val) and val > 0:\n",
    "            tiers.append(val)\n",
    "    return sorted(set(tiers))\n",
    "\n",
    "def get_margin_tiers(row):\n",
    "    \"\"\"Get sorted list of margin-based price tiers (converted to prices).\"\"\"\n",
    "    tiers = []\n",
    "    wac = row.get('wac_p', 0)\n",
    "    if wac <= 0:\n",
    "        return tiers\n",
    "    \n",
    "    for tier_col in ['margin_tier_below','margin_tier_1', 'margin_tier_2', 'margin_tier_3', \n",
    "                     'margin_tier_4', 'margin_tier_5', 'margin_tier_above_1', 'margin_tier_above_2']:\n",
    "        margin = row.get(tier_col)\n",
    "        if pd.notna(margin) and 0 < margin < 1:\n",
    "            price = wac / (1 - margin)\n",
    "            tiers.append(round(price, 2))\n",
    "    return sorted(set(tiers))\n",
    "\n",
    "def find_next_price_above(current_price, row):\n",
    "    \"\"\"\n",
    "    Find the first price tier ABOVE current_price by at least MIN_PRICE_CHANGE_EGP.\n",
    "    Priority: Market tiers first, then margin tiers, then all_time_high_margin.\n",
    "    Returns current_price if nothing found or already at ceiling.\n",
    "    Skips tiers that are less than 0.25 EGP above current.\n",
    "    \"\"\"\n",
    "    if pd.isna(current_price) or current_price <= 0:\n",
    "        return current_price\n",
    "    \n",
    "    wac = row.get('wac_p', 0)\n",
    "    all_time_high_margin = row.get('all_time_high_margin', 0)\n",
    "    \n",
    "    # Calculate ceiling price from all_time_high_margin\n",
    "    ceiling_price = None\n",
    "    if pd.notna(all_time_high_margin) and all_time_high_margin > 0 and wac > 0:\n",
    "        ceiling_price = wac / (1 - all_time_high_margin)\n",
    "    \n",
    "    # Check if already at or above ceiling\n",
    "    if ceiling_price and current_price >= ceiling_price:\n",
    "        return current_price  # Already at ceiling, no increase\n",
    "    \n",
    "    # Try market tiers first - skip tiers less than MIN_PRICE_CHANGE_EGP above current\n",
    "    market_tiers = get_market_tiers(row)\n",
    "    for tier in market_tiers:\n",
    "        if tier > current_price + MIN_PRICE_CHANGE_EGP:  # Must be at least 0.25 EGP above\n",
    "            # Ensure we don't exceed ceiling\n",
    "            if ceiling_price and tier > ceiling_price:\n",
    "                # Use ceiling if it's at least MIN_PRICE_CHANGE_EGP above current\n",
    "                if ceiling_price > current_price + MIN_PRICE_CHANGE_EGP:\n",
    "                    return round(ceiling_price, 2)\n",
    "                return current_price\n",
    "            return round(tier, 2)\n",
    "    \n",
    "    # Try margin tiers - skip tiers less than MIN_PRICE_CHANGE_EGP above current\n",
    "    margin_tiers = get_margin_tiers(row)\n",
    "    for tier in margin_tiers:\n",
    "        if tier > current_price + MIN_PRICE_CHANGE_EGP:  # Must be at least 0.25 EGP above\n",
    "            # Ensure we don't exceed ceiling\n",
    "            if ceiling_price and tier > ceiling_price:\n",
    "                if ceiling_price > current_price + MIN_PRICE_CHANGE_EGP:\n",
    "                    return round(ceiling_price, 2)\n",
    "                return current_price\n",
    "            return round(tier, 2)\n",
    "    \n",
    "    # No tier found above - use all_time_high_margin as ceiling (fallback)\n",
    "    if ceiling_price and ceiling_price > current_price + MIN_PRICE_CHANGE_EGP:\n",
    "        return round(ceiling_price, 2)\n",
    "    \n",
    "    # Nothing found above with sufficient difference, keep current\n",
    "    return current_price\n",
    "\n",
    "def find_next_price_below(current_price, row):\n",
    "    \"\"\"\n",
    "    Find the first price tier BELOW current_price by at least MIN_PRICE_CHANGE_EGP.\n",
    "    Priority: Market tiers first, then margin tiers.\n",
    "    Returns current_price if nothing found.\n",
    "    Skips tiers that are less than 0.25 EGP below current.\n",
    "    \"\"\"\n",
    "    if pd.isna(current_price) or current_price <= 0:\n",
    "        return current_price\n",
    "    \n",
    "    # Try market tiers first (reverse order to find highest below with sufficient diff)\n",
    "    market_tiers = get_market_tiers(row)\n",
    "    for tier in reversed(market_tiers):\n",
    "        if tier < current_price - MIN_PRICE_CHANGE_EGP:  # Must be at least 0.25 EGP below\n",
    "            return round(tier, 2)\n",
    "    \n",
    "    # Try margin tiers (reverse order) - skip tiers less than MIN_PRICE_CHANGE_EGP below\n",
    "    if len(market_tiers) ==0:\n",
    "        margin_tiers = get_margin_tiers(row)\n",
    "        for tier in reversed(margin_tiers):\n",
    "            if tier < current_price - MIN_PRICE_CHANGE_EGP:  # Must be at least 0.25 EGP below\n",
    "                return round(tier, 2)\n",
    "    \n",
    "    # Nothing found below with sufficient difference, keep current\n",
    "    return current_price\n",
    "\n",
    "print(\"Helper functions loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T06:10:06.943704Z",
     "iopub.status.busy": "2026-02-18T06:10:06.943494Z",
     "iopub.status.idle": "2026-02-18T06:10:07.138735Z",
     "shell.execute_reply": "2026-02-18T06:10:07.137988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status adjustment and price functions loaded.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STATUS ADJUSTMENT & PRICE FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def get_price_action(combined_status, yesterday_status):\n",
    "    \"\"\"\n",
    "    Determine price action based on status combination.\n",
    "    \n",
    "    Returns:\n",
    "        str: 'increase', 'decrease', or 'hold'\n",
    "        str: Reason for action\n",
    "    \"\"\"\n",
    "    combined_below = is_below_on_track(combined_status)\n",
    "    combined_above = is_above_on_track(combined_status)\n",
    "    combined_on = is_on_track(combined_status)\n",
    "    yesterday_below = is_below_on_track(yesterday_status)\n",
    "    yesterday_above = is_above_on_track(yesterday_status)\n",
    "    yesterday_on = is_on_track(yesterday_status)\n",
    "    \n",
    "    # On Track = no action\n",
    "    if combined_on and yesterday_on:\n",
    "        return 'hold', \"On Track - no price change\"\n",
    "    if combined_on and yesterday_above:\n",
    "        return 'increase', f\"yesterday above - combined on  ({combined_status}, {yesterday_status}) - increase\"\n",
    "    \n",
    "    # Both ABOVE On Track: INCREASE price (only if both are above, not on track)\n",
    "    if combined_above and yesterday_above:\n",
    "        return 'increase', f\"Both above ({combined_status}, {yesterday_status}) - increase\"\n",
    "    \n",
    "    # Combined above, Yesterday on track: HOLD (changed from increase)\n",
    "    if combined_above and yesterday_on:\n",
    "        return 'hold', f\"Above + On Track ({combined_status}, {yesterday_status}) - hold\"\n",
    "    \n",
    "    # Both below On Track: DECREASE price (go to first tier below current)\n",
    "    if combined_below and (yesterday_below or yesterday_on):\n",
    "        return 'decrease', f\"Both below/on ({combined_status}, {yesterday_status}) - decrease\"\n",
    "    \n",
    "    # Combined below, Yesterday above: No action (oscillation prevention)\n",
    "    if combined_below and yesterday_above:\n",
    "        return 'hold', f\"Oscillation prevention ({combined_status} vs {yesterday_status}) - hold\"\n",
    "    \n",
    "    # Combined above, Yesterday below: HOLD (observe trend before reacting)\n",
    "    if combined_above and yesterday_below:\n",
    "        return 'hold', f\"Trend observation ({combined_status} vs {yesterday_status}) - hold\"\n",
    "    \n",
    "    return 'hold', \"Default - no price change\"\n",
    "\n",
    "def apply_price_action(current_price, action, row):\n",
    "    \"\"\"\n",
    "    Apply price action: find next tier above/below current price.\n",
    "    \n",
    "    Args:\n",
    "        current_price: Current SKU price\n",
    "        action: 'increase', 'decrease', or 'hold'\n",
    "        row: DataFrame row with tier data\n",
    "    \n",
    "    Returns:\n",
    "        float: New price\n",
    "        str: Source of new price (market/margin/unchanged)\n",
    "    \"\"\"\n",
    "    if action == 'hold' or pd.isna(current_price):\n",
    "        return current_price, 'unchanged'\n",
    "    \n",
    "    if action == 'increase':\n",
    "        # Check if already at or above all_time_high ceiling\n",
    "        wac = row.get('wac_p', 0)\n",
    "        all_time_high_margin = row.get('all_time_high_margin', 0)\n",
    "        ceiling_price = None\n",
    "        if pd.notna(all_time_high_margin) and all_time_high_margin > 0 and wac > 0:\n",
    "            ceiling_price = wac / (1 - all_time_high_margin)\n",
    "        \n",
    "        # If current price is already at or above ceiling, HOLD\n",
    "        if ceiling_price and current_price >= ceiling_price:\n",
    "            return current_price, 'unchanged (at all_time_high ceiling)'\n",
    "        \n",
    "        new_price = find_next_price_above(current_price, row)\n",
    "        if new_price > current_price:\n",
    "            # Determine source\n",
    "            market_tiers = get_market_tiers(row)\n",
    "            if new_price in market_tiers:\n",
    "                source = 'market'\n",
    "            elif ceiling_price and abs(new_price - ceiling_price) < 0.0025:\n",
    "                source = 'all_time_high_margin'\n",
    "            else:\n",
    "                source = 'margin'\n",
    "            return new_price, source\n",
    "        return current_price, 'unchanged (no tier above)'\n",
    "    \n",
    "    if action == 'decrease':\n",
    "        new_price = find_next_price_below(current_price, row)\n",
    "        if new_price < current_price:\n",
    "            # Determine source\n",
    "            market_tiers = get_market_tiers(row)\n",
    "            source = 'market' if new_price in market_tiers else 'margin'\n",
    "            \n",
    "            # Apply commercial minimum floor\n",
    "            commercial_min = row.get('commercial_min_price', row.get('minimum', 0))\n",
    "            if pd.notna(commercial_min) and commercial_min > 0:\n",
    "                new_price = max(new_price, commercial_min)\n",
    "            \n",
    "            return new_price, source\n",
    "        return current_price, 'unchanged (no tier below)'\n",
    "    \n",
    "    return current_price, 'unchanged'\n",
    "\n",
    "def get_initial_cart_rule(row, percentile_data, is_oos=False, is_zero_demand=False):\n",
    "    \"\"\"\n",
    "    Calculate initial cart rule using percentile-based approach.\n",
    "    \n",
    "    Primary: Use percentiles from historical order data\n",
    "    - Normal case: 95th percentile\n",
    "    - OOS: 95th percentile\n",
    "    - Zero Demand: 95th percentile\n",
    "    - Low Stock (DOH <= 2): 50th percentile\n",
    "    \n",
    "    Fallback: If percentile data unavailable, use 10 (minimum for Module 2)\n",
    "    \"\"\"\n",
    "    cohort_id = row.get('cohort_id')\n",
    "    product_id = row.get('product_id')\n",
    "    \n",
    "    # Try to get percentile data\n",
    "    if len(percentile_data) > 0:\n",
    "        percentile_row = percentile_data[\n",
    "            (percentile_data['cohort_id'] == cohort_id) & \n",
    "            (percentile_data['product_id'] == product_id)\n",
    "        ]\n",
    "        \n",
    "        if len(percentile_row) > 0:\n",
    "            # Special case: OOS - Use 95th percentile\n",
    "            if is_oos:\n",
    "                perc_95 = percentile_row.iloc[0]['perc_95']\n",
    "                if pd.notna(perc_95) and perc_95 > 0:\n",
    "                    return max(MIN_CART_RULE, min(MAX_CART_RULE, int(round(perc_95))))\n",
    "            \n",
    "            # Special case: Zero Demand - Use 95th percentile\n",
    "            if is_zero_demand:\n",
    "                perc_95 = percentile_row.iloc[0]['perc_95']\n",
    "                if pd.notna(perc_95) and perc_95 > 0:\n",
    "                    return max(MIN_CART_RULE, min(MAX_CART_RULE, int(round(perc_95))))\n",
    "            \n",
    "            # Special case: Low Stock (DOH <= 2) - Use 50th percentile\n",
    "            doh = row.get('doh', 999)\n",
    "            if doh <= LOW_STOCK_DOH_THRESHOLD:\n",
    "                perc_50 = percentile_row.iloc[0]['perc_50']\n",
    "                if pd.notna(perc_50) and perc_50 > 0:\n",
    "                    return max(MIN_CART_RULE, min(MAX_CART_RULE, int(round(perc_50))))\n",
    "            \n",
    "            # Normal case: Use 95th percentile\n",
    "            perc_95 = percentile_row.iloc[0]['perc_95']\n",
    "            if pd.notna(perc_95) and perc_95 > 0:\n",
    "                return max(MIN_CART_RULE, min(MAX_CART_RULE, int(round(perc_95))))\n",
    "    \n",
    "    # Fallback: no percentile data available\n",
    "    return MIN_CART_RULE\n",
    "\n",
    "print(\"Status adjustment and price functions loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T06:10:07.141044Z",
     "iopub.status.busy": "2026-02-18T06:10:07.140826Z",
     "iopub.status.idle": "2026-02-18T06:10:07.156128Z",
     "shell.execute_reply": "2026-02-18T06:10:07.155412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main engine function loaded.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MAIN ENGINE: GENERATE INITIAL PRICE PUSH\n",
    "# =============================================================================\n",
    "\n",
    "def get_max_price(row):\n",
    "    \"\"\"Get maximum price: market_max first, then highest margin tier.\"\"\"\n",
    "    # Try market max first\n",
    "    market_max = row.get('maximum')\n",
    "    if pd.notna(market_max) and market_max > 0:\n",
    "        return market_max, 'market_max'\n",
    "    \n",
    "    # Fallback: highest margin tier\n",
    "    margin_tiers = get_margin_tiers(row)\n",
    "    if margin_tiers:\n",
    "        return margin_tiers[-1], 'margin_max'  # Last = highest\n",
    "    \n",
    "    # Fallback: current price\n",
    "    return row.get('current_price', 0), 'unchanged'\n",
    "\n",
    "def find_price_n_steps_below(current_price, n_steps, row):\n",
    "    \"\"\"Find price N steps below current (iteratively find next tier below).\"\"\"\n",
    "    price = current_price\n",
    "    for _ in range(n_steps):\n",
    "        next_price = find_next_price_below(price, row)\n",
    "        if next_price >= price:  # No tier below found\n",
    "            break\n",
    "        price = next_price\n",
    "    return price\n",
    "\n",
    "def generate_initial_price_push(row, percentile_data):\n",
    "    \"\"\"\n",
    "    Generate initial price push action for a single SKU.\n",
    "    \n",
    "    Logic:\n",
    "    - Stocks = 0: Set to market_max or margin_max (highest price)\n",
    "    - Zero demand + yesterday below on track: Go 2 steps below current\n",
    "    - Zero demand + yesterday above on track: Keep current price\n",
    "    - Otherwise: Adjust price relative to CURRENT price based on status\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'product_id': row.get('product_id'),\n",
    "        'warehouse_id': row.get('warehouse_id'),\n",
    "        'cohort_id': row.get('cohort_id'),\n",
    "        'sku': row.get('sku'),\n",
    "        'brand': row.get('brand'),\n",
    "        'cat': row.get('cat'),\n",
    "        'abc_class': row.get('abc_class', 'C'),\n",
    "        'current_price': row.get('current_price'),\n",
    "        'current_cart_rule': row.get('current_cart_rule'),\n",
    "        'wac_p': row.get('wac_p'),\n",
    "        'stocks': row.get('stocks', 0),\n",
    "        'combined_status': row.get('combined_status'),\n",
    "        'yesterday_status': row.get('yesterday_status'),\n",
    "        'zero_demand': row.get('zero_demand', 0),\n",
    "        'sensitivity': row.get('sensitivity', row.get('product_sensitivity')),\n",
    "        'new_price': None,\n",
    "        'new_cart_rule': None,\n",
    "        'new_margin': None,\n",
    "        'current_margin': None,\n",
    "        'price_source': None,\n",
    "        'price_action': None,\n",
    "        'price_reason': None,\n",
    "    }\n",
    "    \n",
    "    wac = row.get('wac_p', 0)\n",
    "    current_price = row.get('current_price', 0)\n",
    "    result['current_margin'] = calculate_margin(current_price, wac)\n",
    "    yesterday_status = row.get('yesterday_status', 'No Data')\n",
    "    \n",
    "    # CASE 1: Out of Stock (stocks = 0) - Set to MAX price, capped at target_margin + 3*std\n",
    "    if row.get('stocks', 0) == 0:\n",
    "        max_price, price_source = get_max_price(row)\n",
    "        \n",
    "        # Calculate cap price from target_margin + 3*std\n",
    "        target_margin = row.get('target_margin', 0)\n",
    "        margin_std = row.get('std', 0)  # margin std from data extraction\n",
    "        cap_margin = target_margin + (5 * margin_std) if pd.notna(target_margin) and pd.notna(margin_std) else None\n",
    "        cap_price = wac / (1 - cap_margin) if cap_margin and cap_margin < 1 and wac > 0 else None\n",
    "        \n",
    "        # Apply cap logic\n",
    "        if cap_price and max_price > cap_price:\n",
    "            # Max price exceeds cap - use max(current_price, cap_price)\n",
    "            final_price = max(current_price, cap_price) if pd.notna(current_price) else cap_price\n",
    "            result['new_price'] = round(final_price, 2)\n",
    "            result['price_source'] = 'capped_at_target+3std'\n",
    "            result['price_action'] = 'oos_capped'\n",
    "            result['price_reason'] = f'OOS - max ({max_price:.2f}) > cap ({cap_price:.2f}), using max(current, cap)'\n",
    "        else:\n",
    "            # Max price is within cap or no cap - use max price\n",
    "            result['new_price'] = max_price\n",
    "            result['price_source'] = price_source\n",
    "            result['price_action'] = 'oos_max'\n",
    "            result['price_reason'] = 'OOS - set to max price (within cap)'\n",
    "        \n",
    "        result['new_cart_rule'] = get_initial_cart_rule(row, percentile_data, is_oos=True)  # OOS cart: 95th percentile\n",
    "        result['new_margin'] = calculate_margin(result['new_price'], wac)\n",
    "        return result\n",
    "    \n",
    "    # CASE 2: Zero Demand SKUs (has stock but no recent sales)\n",
    "    if row.get('zero_demand', 0) == 1:\n",
    "        yesterday_below = is_below_on_track(yesterday_status)\n",
    "        yesterday_above = is_above_on_track(yesterday_status)\n",
    "        \n",
    "        if yesterday_below:\n",
    "            # Yesterday below on track: Go 2 steps below current price\n",
    "            new_price = find_price_n_steps_below(current_price, 2, row)\n",
    "            \n",
    "            # Apply commercial minimum floor\n",
    "            commercial_min = row.get('commercial_min_price', row.get('minimum', 0))\n",
    "            if pd.notna(commercial_min) and commercial_min > 0:\n",
    "                new_price = max(new_price, commercial_min)\n",
    "            \n",
    "            result['new_price'] = new_price\n",
    "            result['price_source'] = 'market' if new_price in get_market_tiers(row) else 'margin'\n",
    "            result['price_action'] = 'zero_demand_decrease'\n",
    "            result['price_reason'] = f'Zero demand + yesterday below ({yesterday_status}) - 2 steps below'\n",
    "        elif yesterday_above:\n",
    "            # Yesterday above on track: Keep current price\n",
    "            result['new_price'] = current_price\n",
    "            result['price_source'] = 'unchanged'\n",
    "            result['price_action'] = 'zero_demand_hold'\n",
    "            result['price_reason'] = f'Zero demand + yesterday above ({yesterday_status}) - keep current'\n",
    "        else:\n",
    "            # Yesterday on track or no data: Go 1 step below\n",
    "            new_price = find_next_price_below(current_price, row)\n",
    "            commercial_min = row.get('commercial_min_price', row.get('minimum', 0))\n",
    "            if pd.notna(commercial_min) and commercial_min > 0:\n",
    "                new_price = max(new_price, commercial_min)\n",
    "            \n",
    "            result['new_price'] = new_price\n",
    "            result['price_source'] = 'market' if new_price in get_market_tiers(row) else 'margin'\n",
    "            result['price_action'] = 'zero_demand_decrease'\n",
    "            result['price_reason'] = f'Zero demand + yesterday on track ({yesterday_status}) - 1 step below'\n",
    "        \n",
    "        result['new_cart_rule'] = get_initial_cart_rule(row, percentile_data, is_zero_demand=True)  # Zero demand: 95th percentile\n",
    "        result['new_margin'] = calculate_margin(result['new_price'], wac)\n",
    "        return result\n",
    "    \n",
    "    # CASE 2.5: LOW STOCK PROTECTION (DOH <= 2 with demand)\n",
    "    # Protect inventory until next receiving - no price reduction, cap cart at normal_refill\n",
    "    # But still allow price INCREASE if status indicates strong performance\n",
    "    doh = row.get('doh', 999)\n",
    "    if doh <= LOW_STOCK_DOH_THRESHOLD and row.get('zero_demand', 0) == 0 and row.get('stocks', 0) > 0:\n",
    "        combined_status = row.get('combined_status', 'No Data')\n",
    "        normal_refill = row.get('normal_refill', 5) or 5\n",
    "        \n",
    "        # Check if we should increase price (status above on track)\n",
    "        if is_above_on_track(combined_status):\n",
    "            new_price = find_next_price_above(current_price, row)\n",
    "            result['new_price'] = new_price\n",
    "            result['price_source'] = 'market' if new_price in get_market_tiers(row) else 'margin'\n",
    "            result['price_action'] = 'low_stock_increase'\n",
    "            result['price_reason'] = f'Low stock (DOH={doh:.1f}) + above on track ({combined_status}) - increase allowed'\n",
    "        else:\n",
    "            # Hold price - no reduction for low stock\n",
    "            result['new_price'] = current_price\n",
    "            result['price_source'] = 'unchanged'\n",
    "            result['price_action'] = 'low_stock_hold'\n",
    "            result['price_reason'] = f'Low stock (DOH={doh:.1f}) - hold price (no reduction allowed)'\n",
    "        \n",
    "        # Cap cart at 50th percentile for low stock\n",
    "        result['new_cart_rule'] = get_initial_cart_rule(row, percentile_data)  # Will use 50th percentile for low stock\n",
    "        result['new_margin'] = calculate_margin(result['new_price'], wac)\n",
    "        return result\n",
    "    \n",
    "    # CASE 3: Normal SKUs - Determine price action based on status\n",
    "    combined_status = row.get('combined_status', 'No Data')\n",
    "    \n",
    "    # Handle 'No Data' with stocks as Critical (below on track)\n",
    "    if combined_status == 'No Data' and row.get('stocks', 0) > 0:\n",
    "        combined_status = 'Critical'\n",
    "    \n",
    "    # Get price action (increase/decrease/hold)\n",
    "    action, reason = get_price_action(combined_status, yesterday_status)\n",
    "    \n",
    "    # Apply price action - find next tier above/below current price\n",
    "    new_price, price_source = apply_price_action(current_price, action, row)\n",
    "    \n",
    "    result['new_price'] = new_price\n",
    "    result['price_source'] = price_source\n",
    "    result['price_action'] = action\n",
    "    result['price_reason'] = reason\n",
    "    result['new_cart_rule'] = get_initial_cart_rule(row, percentile_data)\n",
    "    result['new_margin'] = calculate_margin(new_price, wac)\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"Main engine function loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T06:10:07.158088Z",
     "iopub.status.busy": "2026-02-18T06:10:07.157873Z",
     "iopub.status.idle": "2026-02-18T06:10:27.432148Z",
     "shell.execute_reply": "2026-02-18T06:10:27.431248Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 28794 SKUs...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000/28794 SKUs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20000/28794 SKUs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Processed 28794 SKUs\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXECUTE MODULE 2\n",
    "# =============================================================================\n",
    "print(f\"Processing {len(df)} SKUs...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = []\n",
    "for idx, row in df.iterrows():\n",
    "    result = generate_initial_price_push(row, df_percentiles)\n",
    "    results.append(result)\n",
    "    \n",
    "    if (idx + 1) % 10000 == 0:\n",
    "        print(f\"Processed {idx + 1}/{len(df)} SKUs...\")\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(f\"\\nâœ… Processed {len(df_results)} SKUs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T06:10:27.434153Z",
     "iopub.status.busy": "2026-02-18T06:10:27.433928Z",
     "iopub.status.idle": "2026-02-18T06:10:27.457426Z",
     "shell.execute_reply": "2026-02-18T06:10:27.456710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODULE 2 SUMMARY\n",
      "============================================================\n",
      "\n",
      "Total SKUs processed: 28794\n",
      "\n",
      "By ABC Class:\n",
      "abc_class\n",
      "C    23438\n",
      "B     4656\n",
      "A      700\n",
      "\n",
      "By Price Source:\n",
      "price_source\n",
      "capped_at_target+3std                   8854\n",
      "margin                                  7603\n",
      "unchanged (no tier below)               6306\n",
      "unchanged                               5102\n",
      "margin_max                               471\n",
      "unchanged (at all_time_high ceiling)     364\n",
      "all_time_high_margin                      69\n",
      "unchanged (no tier above)                 25\n",
      "\n",
      "Price Change Distribution:\n",
      "  Increases: 4254\n",
      "  Decreases: 3703\n",
      "  No change: 20837\n",
      "\n",
      "Avg price change: 0.22%\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(\"=\"*60)\n",
    "print(\"MODULE 2 SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nTotal SKUs processed: {len(df_results)}\")\n",
    "print(f\"\\nBy ABC Class:\")\n",
    "print(df_results['abc_class'].value_counts().to_string())\n",
    "print(f\"\\nBy Price Source:\")\n",
    "print(df_results['price_source'].value_counts().to_string())\n",
    "\n",
    "# Price change analysis\n",
    "df_results['price_change'] = df_results['new_price'] - df_results['current_price']\n",
    "df_results['price_change_pct'] = (df_results['price_change'] / df_results['current_price'] * 100).round(2)\n",
    "\n",
    "print(f\"\\nPrice Change Distribution:\")\n",
    "print(f\"  Increases: {len(df_results[df_results['price_change'] > 0])}\")\n",
    "print(f\"  Decreases: {len(df_results[df_results['price_change'] < 0])}\")\n",
    "print(f\"  No change: {len(df_results[df_results['price_change'] == 0])}\")\n",
    "print(f\"\\nAvg price change: {df_results['price_change_pct'].mean():.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T06:10:27.459316Z",
     "iopub.status.busy": "2026-02-18T06:10:27.459098Z",
     "iopub.status.idle": "2026-02-18T06:10:27.483667Z",
     "shell.execute_reply": "2026-02-18T06:10:27.482953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Saved 28794 rows for Slack upload\n",
      "Total records: 28794 (after removing 0 duplicates)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXPORT RESULTS\n",
    "# =============================================================================\n",
    "output_cols = [\n",
    "    'product_id', 'warehouse_id', 'cohort_id', 'sku', 'brand', 'cat', 'abc_class', 'sensitivity',\n",
    "    'stocks', 'zero_demand', 'combined_status', 'yesterday_status',\n",
    "    'current_price', 'new_price', 'price_change', 'price_change_pct',\n",
    "    'wac_p', 'current_margin', 'new_margin','current_cart_rule',\n",
    "    'new_cart_rule', 'price_action', 'price_source', 'price_reason'\n",
    "]\n",
    "\n",
    "# Filter to only columns that exist\n",
    "output_cols = [c for c in output_cols if c in df_results.columns]\n",
    "\n",
    "# Drop duplicates before saving\n",
    "df_output = df_results[output_cols].drop_duplicates(subset=['product_id', 'warehouse_id'], keep='first')\n",
    "# Save df_output state before any manipulation for Slack upload later\n",
    "temp_df_for_slack = df_output.copy()\n",
    "print(f\"\\nâœ… Saved {len(temp_df_for_slack)} rows for Slack upload\")\n",
    "print(f\"Total records: {len(df_output)} (after removing {len(df_results) - len(df_output)} duplicates)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T06:10:27.485724Z",
     "iopub.status.busy": "2026-02-18T06:10:27.485501Z",
     "iopub.status.idle": "2026-02-18T06:12:53.437737Z",
     "shell.execute_reply": "2026-02-18T06:12:53.436978Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Push Cart Rules Handler loaded at 2026-02-18 08:10:27 Cairo time\n",
      "âœ“ API credentials loaded successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Push Prices Handler loaded at 2026-02-18 08:10:27 Cairo time\n",
      "âœ“ API credentials loaded successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Google Sheets client initialized\n",
      "Fetching packing_units ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded 35273 records\n",
      "\n",
      "======================================================================\n",
      "STEP 1: PUSHING CART RULES\n",
      "======================================================================\n",
      "\n",
      "ðŸš€ MODE: LIVE\n",
      "   Files will be prepared AND uploaded to API\n",
      "\n",
      "============================================================\n",
      "PUSH CART RULES - Source: module_2\n",
      "============================================================\n",
      "Total received: 28794\n",
      "Cart rule changes to push: 19163\n",
      "Skipped (no change): 9631\n",
      "\n",
      "Cart rule changes summary:\n",
      "  Increases: 2609\n",
      "  Decreases: 16554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“‹ Prepared 20863 packing unit cart rules\n",
      "\n",
      "Sample cart rule adjustments (showing products with multiple PUs):\n",
      " product_id  basic_unit_count  final_cart_rule  final_pu_cart_rule\n",
      "          3                 1               10                  10\n",
      "          3                 1               10                  10\n",
      "          3                 1               10                  10\n",
      "          3                 1               10                  10\n",
      "          3                 1               10                  10\n",
      "          3                 1               10                  10\n",
      "          3                 1               10                  10\n",
      "          3                 1               10                  10\n",
      "          3                 1               10                  10\n",
      "          9                 1               10                  10\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 700\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved: uploads/module_2_cart_rules_700.xlsx (3262 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  8.68it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  8.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ“ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 701\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved: uploads/module_2_cart_rules_701.xlsx (3712 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.43it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ“ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 702\n",
      "==================================================\n",
      "  Saved: uploads/module_2_cart_rules_702.xlsx (1995 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 13.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ“ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 703\n",
      "==================================================\n",
      "  Saved: uploads/module_2_cart_rules_703.xlsx (2799 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 10.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ“ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 704\n",
      "==================================================\n",
      "  Saved: uploads/module_2_cart_rules_704.xlsx (2787 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.74it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.73it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ“ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1123\n",
      "==================================================\n",
      "  Saved: uploads/module_2_cart_rules_1123.xlsx (1605 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 16.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ“ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1124\n",
      "==================================================\n",
      "  Saved: uploads/module_2_cart_rules_1124.xlsx (1640 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 16.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ“ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1125\n",
      "==================================================\n",
      "  Saved: uploads/module_2_cart_rules_1125.xlsx (1402 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 18.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ“ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1126\n",
      "==================================================\n",
      "  Saved: uploads/module_2_cart_rules_1126.xlsx (1661 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 15.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ“ Chunk 1 uploaded successfully\n",
      "\n",
      "============================================================\n",
      "ðŸš€ UPLOAD COMPLETE\n",
      "============================================================\n",
      "Mode: live\n",
      "Total prepared: 20863\n",
      "Total failed: 0\n",
      "\n",
      "============================================================\n",
      "CART RULES RESULT\n",
      "============================================================\n",
      "Mode: live\n",
      "Cart rule changes: 19163\n",
      "Pushed: 20863\n",
      "Failed: 0\n",
      "\n",
      "======================================================================\n",
      "STEP 2: PUSHING PRICES\n",
      "======================================================================\n",
      "\n",
      "ðŸš€ MODE: LIVE\n",
      "   Files will be prepared AND uploaded to API\n",
      "Loading disable_pu_visibility from Google Sheets...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ Loaded 88 products to disable min PU visibility\n",
      "\n",
      "============================================================\n",
      "PUSH PRICES - Source: module_2\n",
      "============================================================\n",
      "Total received: 28794\n",
      "Price changes to push: 7957\n",
      "Skipped (no change): 20837\n",
      "\n",
      "Price changes summary:\n",
      "  Increases: 4254\n",
      "  Decreases: 3703\n",
      "\n",
      "ðŸ“‹ Prepared 9563 packing unit prices\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 700\n",
      "==================================================\n",
      "  Saved: uploads/module_2_700.xlsx (1364 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  9.20it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  9.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ“ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 701\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved: uploads/module_2_701.xlsx (1926 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  6.55it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  6.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ“ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 702\n",
      "==================================================\n",
      "  Saved: uploads/module_2_702.xlsx (763 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 16.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ“ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 704\n",
      "==================================================\n",
      "  Saved: uploads/module_2_704.xlsx (1524 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  8.15it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  8.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ“ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1123\n",
      "==================================================\n",
      "  Saved: uploads/module_2_1123.xlsx (677 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 17.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ“ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1124\n",
      "==================================================\n",
      "  Saved: uploads/module_2_1124.xlsx (594 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 23.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ“ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1125\n",
      "==================================================\n",
      "  Saved: uploads/module_2_1125.xlsx (594 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 23.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ“ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1126\n",
      "==================================================\n",
      "  Saved: uploads/module_2_1126.xlsx (606 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 23.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ“ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 703\n",
      "==================================================\n",
      "  Saved: uploads/module_2_703.xlsx (1515 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.70it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.69it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ“ Chunk 1 uploaded successfully\n",
      "\n",
      "============================================================\n",
      "ðŸš€ UPLOAD COMPLETE\n",
      "============================================================\n",
      "Mode: live\n",
      "Total prepared: 9563\n",
      "Total failed: 0\n",
      "\n",
      "============================================================\n",
      "PRICES RESULT\n",
      "============================================================\n",
      "Mode: live\n",
      "Source: module_2\n",
      "Timestamp: 2026-02-18 08:11:15\n",
      "Total received: 28794\n",
      "Price changes: 7957\n",
      "Pushed: 9563\n",
      "Skipped: 20837\n",
      "Failed: 0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PUSH CART RULES & PRICES\n",
    "# =============================================================================\n",
    "# Push cart rules FIRST, then prices\n",
    "# If cart rules fail for certain cohorts, skip those cohorts for prices\n",
    "\n",
    "%run push_cart_rules_handler.ipynb\n",
    "%run push_prices_handler.ipynb\n",
    "pus = get_packing_units()\n",
    "\n",
    "# âš ï¸ MODE CONFIGURATION:\n",
    "# - 'testing' (default): Prepare files but DON'T upload to API\n",
    "# - 'live': Prepare files AND upload to MaxAB API\n",
    "PUSH_MODE = 'live'  # Change to 'live' when ready to push\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: Push Cart Rules First\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 1: PUSHING CART RULES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "cart_result = push_cart_rules(df_output, pus, source_module='module_2', mode=PUSH_MODE)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CART RULES RESULT\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Mode: {cart_result['mode']}\")\n",
    "print(f\"Cart rule changes: {cart_result['cart_rule_changes']}\")\n",
    "print(f\"Pushed: {cart_result['pushed']}\")\n",
    "print(f\"Failed: {cart_result['failed']}\")\n",
    "if cart_result['failed_cohorts']:\n",
    "    print(f\"âš ï¸ Failed cohorts: {cart_result['failed_cohorts']}\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: Push Prices (skip failed cohorts)\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: PUSHING PRICES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get failed cohorts from cart rules to skip in price push\n",
    "failed_cohorts = cart_result.get('failed_cohorts', [])\n",
    "\n",
    "# Call push_prices with the results, skipping failed cohorts\n",
    "push_result = push_prices(df_output, pus, source_module='module_2', mode=PUSH_MODE, skip_cohorts=failed_cohorts)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PRICES RESULT\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Mode: {push_result['mode']}\")\n",
    "print(f\"Source: {push_result['source_module']}\")\n",
    "print(f\"Timestamp: {push_result['timestamp']}\")\n",
    "print(f\"Total received: {push_result['total_received']}\")\n",
    "print(f\"Price changes: {push_result['price_changes']}\")\n",
    "print(f\"Pushed: {push_result['pushed']}\")\n",
    "print(f\"Skipped: {push_result['skipped']}\")\n",
    "print(f\"Failed: {push_result['failed']}\")\n",
    "if push_result.get('skipped_cohorts'):\n",
    "    print(f\"âš ï¸ Skipped cohorts (cart rules failed): {push_result['skipped_cohorts']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T06:12:53.440110Z",
     "iopub.status.busy": "2026-02-18T06:12:53.439732Z",
     "iopub.status.idle": "2026-02-18T06:13:16.591541Z",
     "shell.execute_reply": "2026-02-18T06:13:16.590662Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "UPLOADING RESULTS TO SNOWFLAKE\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/service_account_key.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.12/site-packages/slack/deprecation.py:14: UserWarning: slack package is deprecated. Please use slack_sdk.web/webhook/rtm package instead. For more info, go to https://docs.slack.dev/tools/python-slack-sdk/v3-migration/\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/service_account_key.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message Sent\n",
      "âœ… Slack notification sent!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/service_account_key.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File module2_initial_20260218_0813.xlsx sent to Slack\n",
      "âœ… Output file sent to Slack\n",
      "âœ… 28794 records uploaded to Snowflake\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# UPLOAD RESULTS TO SNOWFLAKE AND SEND SLACK NOTIFICATION\n",
    "# =============================================================================\n",
    "from common_functions import upload_dataframe_to_snowflake, send_text_slack, send_file_slack\n",
    "\n",
    "# Add created_at as DATE (module runs once per day at 8 AM)\n",
    "df_output['created_at'] = datetime.now(CAIRO_TZ).date()\n",
    "\n",
    "# Upload to Snowflake\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"UPLOADING RESULTS TO SNOWFLAKE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "upload_status = upload_dataframe_to_snowflake(\n",
    "    \"Egypt\", \n",
    "    df_output, \n",
    "    \"MATERIALIZED_VIEWS\", \n",
    "    \"pricing_initial_push\", \n",
    "    \"append\", \n",
    "    auto_create_table=True, \n",
    "    conn=None\n",
    ")\n",
    "\n",
    "# Prepare Slack notification\n",
    "prices_pushed = push_result.get('pushed', 0)\n",
    "prices_failed = push_result.get('failed', 0)\n",
    "cart_rules_pushed = cart_result.get('pushed', 0)\n",
    "cart_rules_failed = cart_result.get('failed', 0)\n",
    "\n",
    "if upload_status:\n",
    "    slack_message = f\"\"\"âœ… *Module 2 - Initial Price Push Completed*\n",
    "\n",
    "ðŸ“… Date: {datetime.now(CAIRO_TZ).strftime('%Y-%m-%d')}\n",
    "â° Completed at: {datetime.now(CAIRO_TZ).strftime('%H:%M:%S')} Cairo time\n",
    "ðŸ”§ Mode: {PUSH_MODE.upper()}\n",
    "\n",
    "ðŸ“Š *Results:*\n",
    "â€¢ Total SKUs processed: {len(df_output):,}\n",
    "â€¢ Price changes: {push_result.get('price_changes', 0):,}\n",
    "â€¢ Cart rule changes: {cart_result.get('cart_rule_changes', 0):,}\n",
    "\n",
    "ðŸ“¤ *Push Status:*\n",
    "â€¢ ðŸ’° Prices: âœ… {prices_pushed} pushed | âŒ {prices_failed} failed\n",
    "â€¢ ðŸ›’ Cart Rules: âœ… {cart_rules_pushed} pushed | âŒ {cart_rules_failed} failed\n",
    "\n",
    "ðŸ—„ï¸ Results uploaded to: MATERIALIZED_VIEWS.pricing_initial_push\"\"\"\n",
    "    \n",
    "    send_text_slack('new-pricing-logic', slack_message)\n",
    "    print(\"âœ… Slack notification sent!\")\n",
    "    \n",
    "    # Send output file to Slack after the text message (using saved copy before manipulation)\n",
    "    SLACK_CHANNEL_ID = 'C0AAWK97Z3Q'\n",
    "    send_file_slack(\n",
    "        temp_df_for_slack, \n",
    "        f'ðŸ“Ž Module 2 Output: {len(temp_df_for_slack)} SKUs processed', \n",
    "        SLACK_CHANNEL_ID,\n",
    "        filename=f'module2_initial_{datetime.now(CAIRO_TZ).strftime(\"%Y%m%d_%H%M\")}.xlsx'\n",
    "    )\n",
    "    print(\"âœ… Output file sent to Slack\")\n",
    "    \n",
    "    print(f\"âœ… {len(df_output)} records uploaded to Snowflake\")\n",
    "else:\n",
    "    error_message = f\"\"\"âŒ *Module 2 - Initial Price Push Failed*\n",
    "\n",
    "ðŸ“… Date: {datetime.now(CAIRO_TZ).strftime('%Y-%m-%d')}\n",
    "â° Failed at: {datetime.now(CAIRO_TZ).strftime('%H:%M:%S')} Cairo time\n",
    "âš ï¸ Upload to Snowflake failed - please check logs\n",
    "\n",
    "ðŸ“¤ *Push Status (before upload failure):*\n",
    "â€¢ ðŸ’° Prices: âœ… {prices_pushed} pushed | âŒ {prices_failed} failed\n",
    "â€¢ ðŸ›’ Cart Rules: âœ… {cart_rules_pushed} pushed | âŒ {cart_rules_failed} failed\"\"\"\n",
    "    \n",
    "    send_text_slack('new-pricing-logic', error_message)\n",
    "    print(\"âŒ Error notification sent to Slack!\")\n",
    "    \n",
    "    # Still send output file even on error for debugging (using saved copy before manipulation)\n",
    "    send_file_slack(\n",
    "        temp_df_for_slack, \n",
    "        f'âš ï¸ Module 2 ERROR: {len(temp_df_for_slack)} SKUs', \n",
    "        SLACK_CHANNEL_ID,\n",
    "        filename=f'module2_initial_ERROR_{datetime.now(CAIRO_TZ).strftime(\"%Y%m%d_%H%M\")}.xlsx'\n",
    "    )\n",
    "    print(\"âœ… Error file sent to Slack\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
