{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2: Initial Price Push (Daily 6 AM Reset)\n",
    "\n",
    "## Purpose\n",
    "This module runs once daily at 8 AM Cairo time to:\n",
    "1. **Load and prepare data** from Snowflake (MATERIALIZED_VIEWS.Pricing_data_extraction)\n",
    "2. Reset prices for all SKUs based on ABC classification\n",
    "3. Set initial cart rules based on normal_refill and stddev\n",
    "4. Apply status-based adjustments (combined_status + yesterday_status)\n",
    "5. Push cart rules and prices via API\n",
    "\n",
    "## Data Flow\n",
    "```\n",
    "data_extraction.ipynb â†’ Snowflake (Pricing_data_extraction) â†’ Module 2 (this module)\n",
    "                                                        â”œâ”€â”€ Data Preparation\n",
    "                                                        â”œâ”€â”€ Price Logic\n",
    "                                                        â”œâ”€â”€ Cart Rule Logic\n",
    "                                                        â””â”€â”€ Push to API\n",
    "```\n",
    "\n",
    "## Price Setting Logic\n",
    "- **Zero demand SKUs**: Market minimum price + SKU discount\n",
    "- **With market data**: A=25th percentile, B=50th, C=75th\n",
    "- **Without market data**: A=50% margin range, B=75%, C=90%\n",
    "- **No data SKUs**: Average margin of their category\n",
    "\n",
    "## Status Adjustment Logic\n",
    "- Both below On Track: -1 step from current price\n",
    "- Both above On Track: +1 step from current price\n",
    "- Combined lower, Yesterday higher: No action (oscillation prevention)\n",
    "- Combined higher, Yesterday lower: No action (trend observation)\n",
    "- On Track: No action\n",
    "- Above On Track + Yesterday On Track: No action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.12/site-packages/snowflake/connector/options.py:104: UserWarning: You have an incompatible version of 'pyarrow' installed (20.0.0), please install a version that adheres to: 'pyarrow<19.0.0; extra == \"pandas\"'\n",
      "  warn_incompatible_dep(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.Renviron\n",
      "/home/ec2-user/service_account_key.json\n",
      "Queries Module | Timezone: America/Los_Angeles\n",
      "âœ… UTH and Last Hour functions defined\n",
      "\n",
      "==================================================\n",
      "QUERIES MODULE READY\n",
      "==================================================\n",
      "\n",
      "Live Data Functions:\n",
      "  â€¢ get_current_stocks()\n",
      "  â€¢ get_packing_units()\n",
      "  â€¢ get_current_prices()\n",
      "  â€¢ get_current_wac()\n",
      "  â€¢ get_current_cart_rules()\n",
      "\n",
      "UTH Performance Functions:\n",
      "  â€¢ get_uth_performance()         - UTH qty/retailers (Snowflake)\n",
      "  â€¢ get_hourly_distribution()     - Historical hour contributions (Snowflake)\n",
      "  â€¢ get_last_hour_performance()   - Last hour qty/retailers (DWH)\n",
      "\n",
      "Note: Market prices use MODULE_1_INPUT data\n",
      "Retailer Selection Queries defined âœ“\n",
      "  - get_churned_dropped_retailers()\n",
      "  - get_category_not_product_retailers()\n",
      "  - get_out_of_cycle_retailers()\n",
      "  - get_view_no_orders_retailers()\n",
      "  - get_excluded_retailers()\n",
      "  - get_retailers_with_quantity_discount()\n",
      "  - get_retailer_main_warehouse()\n",
      "Module 2: Initial Price Push\n",
      "Run Time (Cairo): 2026-01-27 00:49:09\n",
      "Input: MATERIALIZED_VIEWS.Pricing_data_extraction (today's data)\n",
      "Output: module_2_output_20260127_0049.xlsx\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS AND SETUP\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "%run queries_module.ipynb\n",
    "# Cairo timezone\n",
    "CAIRO_TZ = pytz.timezone('Africa/Cairo')\n",
    "CAIRO_NOW = datetime.now(CAIRO_TZ)\n",
    "TODAY = CAIRO_NOW.date()\n",
    "CURRENT_HOUR = CAIRO_NOW.hour\n",
    "\n",
    "# Configuration constants\n",
    "ABC_MARKET_PERCENTILES = {'A': 25, 'B': 50, 'C': 75}\n",
    "ABC_MARGIN_PERCENTILES = {'A': 50, 'B': 75, 'C': 90}\n",
    "ABC_CART_STD_MULTIPLIERS = {'A': 1, 'B': 2, 'C': 5}\n",
    "MIN_CART_RULE = 2\n",
    "MAX_CART_RULE = 150\n",
    "STATUS_BELOW_ON_TRACK = ['No Data', 'Critical', 'Struggling', 'Underperforming']\n",
    "STATUS_ABOVE_ON_TRACK = ['Over Achiever', 'Star Performer']\n",
    "STATUS_ON_TRACK = ['On Track']\n",
    "\n",
    "# Input/Output configuration\n",
    "# Data is now loaded from Snowflake instead of Excel\n",
    "INPUT_TABLE = 'MATERIALIZED_VIEWS.Pricing_data_extraction'\n",
    "OUTPUT_FILE = f'module_2_output_{CAIRO_NOW.strftime(\"%Y%m%d_%H%M\")}.xlsx'\n",
    "\n",
    "print(f\"Module 2: Initial Price Push\")\n",
    "print(f\"Run Time (Cairo): {CAIRO_NOW.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Input: {INPUT_TABLE} (today's data)\")\n",
    "print(f\"Output: {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from Snowflake...\n",
      "Loaded 28436 records from Snowflake\n",
      "\n",
      "Preparing data (structuring columns, handling nulls)...\n",
      "âœ… Data prepared: 28436 records\n",
      "\n",
      "ABC Class Distribution:\n",
      "abc_class\n",
      "C    23407\n",
      "B     4372\n",
      "A      657\n",
      "\n",
      "Combined Status Distribution:\n",
      "combined_status\n",
      "No Data            7154\n",
      "Struggling         5903\n",
      "Critical           5727\n",
      "Underperforming    3028\n",
      "Over Achiever      2772\n",
      "On Track           2474\n",
      "Star Performer     1378\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD DATA FROM SNOWFLAKE (Instead of Excel file)\n",
    "# =============================================================================\n",
    "print(\"Loading data from Snowflake...\")\n",
    "\n",
    "# Query to get today's data from Pricing_data_extraction\n",
    "LOAD_QUERY = f\"\"\"\n",
    "SELECT * FROM {INPUT_TABLE}\n",
    "WHERE created_at = '{datetime.now(CAIRO_TZ).date()}'\n",
    "\"\"\"\n",
    "\n",
    "df_raw = query_snowflake(LOAD_QUERY)\n",
    "print(f\"Loaded {len(df_raw)} records from Snowflake\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# DATA PREPARATION: Transform raw data to structured format\n",
    "# This replicates the data structuring from pricing_action_engine.ipynb\n",
    "# -------------------------------------------------------------------------\n",
    "print(\"\\nPreparing data (structuring columns, handling nulls)...\")\n",
    "\n",
    "# Create a clean DataFrame with all required columns and proper defaults\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Identifiers\n",
    "df['warehouse_id'] = df_raw['warehouse_id']\n",
    "df['product_id'] = df_raw['product_id']\n",
    "df['sku'] = df_raw['sku']\n",
    "df['cohort_id'] = df_raw['cohort_id'] if 'cohort_id' in df_raw.columns else None\n",
    "\n",
    "# Product info\n",
    "df['abc_class'] = df_raw['abc_class'].fillna('C')\n",
    "df['brand'] = df_raw['brand'] if 'brand' in df_raw.columns else None\n",
    "df['cat'] = df_raw['cat'] if 'cat' in df_raw.columns else None\n",
    "df['sensitivity'] = df_raw['sensitivity'] if 'sensitivity' in df_raw.columns else None\n",
    "\n",
    "# Current state - with null handling\n",
    "df['current_price'] = pd.to_numeric(df_raw['current_price'], errors='coerce').fillna(0)\n",
    "df['current_cart_rule'] = pd.to_numeric(df_raw['current_cart_rule'], errors='coerce').fillna(999)\n",
    "df['normal_refill'] = pd.to_numeric(df_raw.get('normal_refill', 0), errors='coerce').fillna(0)\n",
    "df['refill_stddev'] = pd.to_numeric(df_raw.get('refill_stddev', 0), errors='coerce').fillna(0)\n",
    "df['wac_p'] = pd.to_numeric(df_raw['wac_p'], errors='coerce').fillna(0)\n",
    "df['commercial_min_price'] = pd.to_numeric(df_raw.get('commercial_min_price', 0), errors='coerce').fillna(0)\n",
    "\n",
    "# Performance status\n",
    "df['combined_status'] = df_raw['combined_status'].fillna('No Data')\n",
    "df['yesterday_status'] = df_raw['yesterday_status'].fillna('No Data')\n",
    "df['oos_yesterday'] = df_raw['oos_yesterday'].fillna(0).astype(int)\n",
    "\n",
    "# Stock and demand\n",
    "df['stocks'] = pd.to_numeric(df_raw['stocks'], errors='coerce').fillna(0)\n",
    "df['zero_demand'] = df_raw['zero_demand'].fillna(0).astype(int)\n",
    "\n",
    "# Margin data (for price tier calculations)\n",
    "df['target_margin'] = pd.to_numeric(df_raw.get('target_margin', 0), errors='coerce').fillna(0)\n",
    "#df['target_margin_std'] = pd.to_numeric(df_raw.get('target_margin_std', 0), errors='coerce').fillna(0)\n",
    "\n",
    "# Market margins (for price tiers)\n",
    "market_margin_cols = ['below_market', 'market_min', 'market_25', 'market_50', \n",
    "                      'market_75', 'market_max', 'above_market']\n",
    "for col in market_margin_cols:\n",
    "    if col in df_raw.columns:\n",
    "        df[col] = pd.to_numeric(df_raw[col], errors='coerce')\n",
    "    else:\n",
    "        df[col] = np.nan\n",
    "\n",
    "# Internal margin tiers\n",
    "margin_tier_cols = ['margin_tier_below', 'margin_tier_1', 'margin_tier_2', 'margin_tier_3',\n",
    "                    'margin_tier_4', 'margin_tier_5', 'margin_tier_above_1', 'margin_tier_above_2']\n",
    "for col in margin_tier_cols:\n",
    "    if col in df_raw.columns:\n",
    "        df[col] = pd.to_numeric(df_raw[col], errors='coerce')\n",
    "    else:\n",
    "        df[col] = np.nan\n",
    "\n",
    "# All-time high margin (price ceiling for increases)\n",
    "df['all_time_high_margin'] = pd.to_numeric(df_raw.get('all_time_high_margin', np.nan), errors='coerce')\n",
    "\n",
    "# P80/P70 for cart rules fallback\n",
    "df['p80_daily_240d'] = pd.to_numeric(df_raw.get('p80_daily_240d', 0), errors='coerce').fillna(0)\n",
    "df['p70_daily_retailers_240d'] = pd.to_numeric(df_raw.get('p70_daily_retailers_240d', 1), errors='coerce').fillna(1)\n",
    "\n",
    "print(f\"âœ… Data prepared: {len(df)} records\")\n",
    "print(f\"\\nABC Class Distribution:\")\n",
    "print(df['abc_class'].value_counts().to_string())\n",
    "print(f\"\\nCombined Status Distribution:\")\n",
    "print(df['combined_status'].value_counts().to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions loaded.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "# Minimum price change constant (ensure it's defined in this cell for function access)\n",
    "MIN_PRICE_CHANGE_EGP = 0.25  # Minimum 0.25 EGP for any price increase or decrease\n",
    "\n",
    "def is_below_on_track(status):\n",
    "    \"\"\"Check if status is below On Track.\"\"\"\n",
    "    return str(status).strip() in STATUS_BELOW_ON_TRACK\n",
    "\n",
    "def is_above_on_track(status):\n",
    "    \"\"\"Check if status is above On Track.\"\"\"\n",
    "    return str(status).strip() in STATUS_ABOVE_ON_TRACK\n",
    "\n",
    "def is_on_track(status):\n",
    "    \"\"\"Check if status is On Track.\"\"\"\n",
    "    return str(status).strip() in STATUS_ON_TRACK\n",
    "\n",
    "def calculate_margin(price, wac):\n",
    "    \"\"\"Calculate margin from price and WAC.\"\"\"\n",
    "    if pd.isna(price) or pd.isna(wac) or price == 0:\n",
    "        return None\n",
    "    return (price - wac) / price\n",
    "\n",
    "def get_market_tiers(row):\n",
    "    \"\"\"Get sorted list of market price tiers.\"\"\"\n",
    "    tiers = []\n",
    "    for col in ['minimum', 'percentile_25', 'percentile_50', 'percentile_75', 'maximum']:\n",
    "        val = row.get(col)\n",
    "        if pd.notna(val) and val > 0:\n",
    "            tiers.append(val)\n",
    "    return sorted(set(tiers))\n",
    "\n",
    "def get_margin_tiers(row):\n",
    "    \"\"\"Get sorted list of margin-based price tiers (converted to prices).\"\"\"\n",
    "    tiers = []\n",
    "    wac = row.get('wac_p', 0)\n",
    "    if wac <= 0:\n",
    "        return tiers\n",
    "    \n",
    "    for tier_col in ['margin_tier_below','margin_tier_1', 'margin_tier_2', 'margin_tier_3', \n",
    "                     'margin_tier_4', 'margin_tier_5', 'margin_tier_above_1', 'margin_tier_above_2']:\n",
    "        margin = row.get(tier_col)\n",
    "        if pd.notna(margin) and 0 < margin < 1:\n",
    "            price = wac / (1 - margin)\n",
    "            tiers.append(round(price, 2))\n",
    "    return sorted(set(tiers))\n",
    "\n",
    "def find_next_price_above(current_price, row):\n",
    "    \"\"\"\n",
    "    Find the first price tier ABOVE current_price by at least MIN_PRICE_CHANGE_EGP.\n",
    "    Priority: Market tiers first, then margin tiers, then all_time_high_margin.\n",
    "    Returns current_price if nothing found or already at ceiling.\n",
    "    Skips tiers that are less than 0.25 EGP above current.\n",
    "    \"\"\"\n",
    "    if pd.isna(current_price) or current_price <= 0:\n",
    "        return current_price\n",
    "    \n",
    "    wac = row.get('wac_p', 0)\n",
    "    all_time_high_margin = row.get('all_time_high_margin', 0)\n",
    "    \n",
    "    # Calculate ceiling price from all_time_high_margin\n",
    "    ceiling_price = None\n",
    "    if pd.notna(all_time_high_margin) and all_time_high_margin > 0 and wac > 0:\n",
    "        ceiling_price = wac / (1 - all_time_high_margin)\n",
    "    \n",
    "    # Check if already at or above ceiling\n",
    "    if ceiling_price and current_price >= ceiling_price:\n",
    "        return current_price  # Already at ceiling, no increase\n",
    "    \n",
    "    # Try market tiers first - skip tiers less than MIN_PRICE_CHANGE_EGP above current\n",
    "    market_tiers = get_market_tiers(row)\n",
    "    for tier in market_tiers:\n",
    "        if tier > current_price + MIN_PRICE_CHANGE_EGP:  # Must be at least 0.25 EGP above\n",
    "            # Ensure we don't exceed ceiling\n",
    "            if ceiling_price and tier > ceiling_price:\n",
    "                # Use ceiling if it's at least MIN_PRICE_CHANGE_EGP above current\n",
    "                if ceiling_price > current_price + MIN_PRICE_CHANGE_EGP:\n",
    "                    return round(ceiling_price, 2)\n",
    "                return current_price\n",
    "            return round(tier, 2)\n",
    "    \n",
    "    # Try margin tiers - skip tiers less than MIN_PRICE_CHANGE_EGP above current\n",
    "    margin_tiers = get_margin_tiers(row)\n",
    "    for tier in margin_tiers:\n",
    "        if tier > current_price + MIN_PRICE_CHANGE_EGP:  # Must be at least 0.25 EGP above\n",
    "            # Ensure we don't exceed ceiling\n",
    "            if ceiling_price and tier > ceiling_price:\n",
    "                if ceiling_price > current_price + MIN_PRICE_CHANGE_EGP:\n",
    "                    return round(ceiling_price, 2)\n",
    "                return current_price\n",
    "            return round(tier, 2)\n",
    "    \n",
    "    # No tier found above - use all_time_high_margin as ceiling (fallback)\n",
    "    if ceiling_price and ceiling_price > current_price + MIN_PRICE_CHANGE_EGP:\n",
    "        return round(ceiling_price, 2)\n",
    "    \n",
    "    # Nothing found above with sufficient difference, keep current\n",
    "    return current_price\n",
    "\n",
    "def find_next_price_below(current_price, row):\n",
    "    \"\"\"\n",
    "    Find the first price tier BELOW current_price by at least MIN_PRICE_CHANGE_EGP.\n",
    "    Priority: Market tiers first, then margin tiers.\n",
    "    Returns current_price if nothing found.\n",
    "    Skips tiers that are less than 0.25 EGP below current.\n",
    "    \"\"\"\n",
    "    if pd.isna(current_price) or current_price <= 0:\n",
    "        return current_price\n",
    "    \n",
    "    # Try market tiers first (reverse order to find highest below with sufficient diff)\n",
    "    market_tiers = get_market_tiers(row)\n",
    "    for tier in reversed(market_tiers):\n",
    "        if tier < current_price - MIN_PRICE_CHANGE_EGP:  # Must be at least 0.25 EGP below\n",
    "            return round(tier, 2)\n",
    "    \n",
    "    # Try margin tiers (reverse order) - skip tiers less than MIN_PRICE_CHANGE_EGP below\n",
    "    margin_tiers = get_margin_tiers(row)\n",
    "    for tier in reversed(margin_tiers):\n",
    "        if tier < current_price - MIN_PRICE_CHANGE_EGP:  # Must be at least 0.25 EGP below\n",
    "            return round(tier, 2)\n",
    "    \n",
    "    # Nothing found below with sufficient difference, keep current\n",
    "    return current_price\n",
    "\n",
    "print(\"Helper functions loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status adjustment and price functions loaded.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STATUS ADJUSTMENT & PRICE FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def get_price_action(combined_status, yesterday_status):\n",
    "    \"\"\"\n",
    "    Determine price action based on status combination.\n",
    "    \n",
    "    Returns:\n",
    "        str: 'increase', 'decrease', or 'hold'\n",
    "        str: Reason for action\n",
    "    \"\"\"\n",
    "    combined_below = is_below_on_track(combined_status)\n",
    "    combined_above = is_above_on_track(combined_status)\n",
    "    combined_on = is_on_track(combined_status)\n",
    "    yesterday_below = is_below_on_track(yesterday_status)\n",
    "    yesterday_above = is_above_on_track(yesterday_status)\n",
    "    yesterday_on = is_on_track(yesterday_status)\n",
    "    \n",
    "    # On Track = no action\n",
    "    if combined_on and yesterday_on:\n",
    "        return 'hold', \"On Track - no price change\"\n",
    "    if combined_on and yesterday_above:\n",
    "        return 'increase', f\"yesterday above - combined on  ({combined_status}, {yesterday_status}) - increase\"\n",
    "    \n",
    "    # Both ABOVE On Track: INCREASE price (only if both are above, not on track)\n",
    "    if combined_above and yesterday_above:\n",
    "        return 'increase', f\"Both above ({combined_status}, {yesterday_status}) - increase\"\n",
    "    \n",
    "    # Combined above, Yesterday on track: HOLD (changed from increase)\n",
    "    if combined_above and yesterday_on:\n",
    "        return 'hold', f\"Above + On Track ({combined_status}, {yesterday_status}) - hold\"\n",
    "    \n",
    "    # Both below On Track: DECREASE price (go to first tier below current)\n",
    "    if combined_below and (yesterday_below or yesterday_on):\n",
    "        return 'decrease', f\"Both below/on ({combined_status}, {yesterday_status}) - decrease\"\n",
    "    \n",
    "    # Combined below, Yesterday above: No action (oscillation prevention)\n",
    "    if combined_below and yesterday_above:\n",
    "        return 'hold', f\"Oscillation prevention ({combined_status} vs {yesterday_status}) - hold\"\n",
    "    \n",
    "    # Combined above, Yesterday below: HOLD (observe trend before reacting)\n",
    "    if combined_above and yesterday_below:\n",
    "        return 'hold', f\"Trend observation ({combined_status} vs {yesterday_status}) - hold\"\n",
    "    \n",
    "    return 'hold', \"Default - no price change\"\n",
    "\n",
    "def apply_price_action(current_price, action, row):\n",
    "    \"\"\"\n",
    "    Apply price action: find next tier above/below current price.\n",
    "    \n",
    "    Args:\n",
    "        current_price: Current SKU price\n",
    "        action: 'increase', 'decrease', or 'hold'\n",
    "        row: DataFrame row with tier data\n",
    "    \n",
    "    Returns:\n",
    "        float: New price\n",
    "        str: Source of new price (market/margin/unchanged)\n",
    "    \"\"\"\n",
    "    if action == 'hold' or pd.isna(current_price):\n",
    "        return current_price, 'unchanged'\n",
    "    \n",
    "    if action == 'increase':\n",
    "        # Check if already at or above all_time_high ceiling\n",
    "        wac = row.get('wac_p', 0)\n",
    "        all_time_high_margin = row.get('all_time_high_margin', 0)\n",
    "        ceiling_price = None\n",
    "        if pd.notna(all_time_high_margin) and all_time_high_margin > 0 and wac > 0:\n",
    "            ceiling_price = wac / (1 - all_time_high_margin)\n",
    "        \n",
    "        # If current price is already at or above ceiling, HOLD\n",
    "        if ceiling_price and current_price >= ceiling_price:\n",
    "            return current_price, 'unchanged (at all_time_high ceiling)'\n",
    "        \n",
    "        new_price = find_next_price_above(current_price, row)\n",
    "        if new_price > current_price:\n",
    "            # Determine source\n",
    "            market_tiers = get_market_tiers(row)\n",
    "            if new_price in market_tiers:\n",
    "                source = 'market'\n",
    "            elif ceiling_price and abs(new_price - ceiling_price) < 0.01:\n",
    "                source = 'all_time_high_margin'\n",
    "            else:\n",
    "                source = 'margin'\n",
    "            return new_price, source\n",
    "        return current_price, 'unchanged (no tier above)'\n",
    "    \n",
    "    if action == 'decrease':\n",
    "        new_price = find_next_price_below(current_price, row)\n",
    "        if new_price < current_price:\n",
    "            # Determine source\n",
    "            market_tiers = get_market_tiers(row)\n",
    "            source = 'market' if new_price in market_tiers else 'margin'\n",
    "            \n",
    "            # Apply commercial minimum floor\n",
    "            commercial_min = row.get('commercial_min_price', row.get('minimum', 0))\n",
    "            if pd.notna(commercial_min) and commercial_min > 0:\n",
    "                new_price = max(new_price, commercial_min)\n",
    "            \n",
    "            return new_price, source\n",
    "        return current_price, 'unchanged (no tier below)'\n",
    "    \n",
    "    return current_price, 'unchanged'\n",
    "\n",
    "# Cart rule multipliers based on ABC class + yesterday status\n",
    "CART_MULTIPLIERS = {\n",
    "    # (abc_class, status_category) -> multiplier\n",
    "    ('A', 'above'): 2,\n",
    "    ('A', 'below'): 5,\n",
    "    ('A', 'on_track'): 3,\n",
    "    ('B', 'above'): 2,\n",
    "    ('B', 'below'): 7,\n",
    "    ('B', 'on_track'): 5,\n",
    "    ('C', 'above'): 5,\n",
    "    ('C', 'below'): 10,\n",
    "    ('C', 'on_track'): 7,\n",
    "}\n",
    "\n",
    "def get_cart_multiplier(abc_class, yesterday_status):\n",
    "    \"\"\"Get cart rule multiplier based on ABC class and yesterday status.\"\"\"\n",
    "    abc = str(abc_class).upper()\n",
    "    if abc not in ['A', 'B', 'C']:\n",
    "        abc = 'C'\n",
    "    \n",
    "    if is_above_on_track(yesterday_status):\n",
    "        status_cat = 'above'\n",
    "    elif is_below_on_track(yesterday_status):\n",
    "        status_cat = 'below'\n",
    "    else:\n",
    "        status_cat = 'on_track'\n",
    "    \n",
    "    return CART_MULTIPLIERS.get((abc, status_cat), 7)  # Default to 7\n",
    "\n",
    "def get_initial_cart_rule(row, is_oos=False, is_zero_demand=False):\n",
    "    \"\"\"\n",
    "    Calculate initial cart rule based on ABC class + yesterday performance.\n",
    "    \n",
    "    Multipliers:\n",
    "    - A + above: 2, A + below: 5, A + on_track: 3\n",
    "    - B + above: 2, B + below: 7, B + on_track: 5\n",
    "    - C + above: 5, C + below: 10, C + on_track: 7\n",
    "    \n",
    "    Special cases:\n",
    "    - OOS: normal_refill + 3*std (min 2)\n",
    "    - Zero Demand: normal_refill + 10*std (open cart)\n",
    "    \n",
    "    Fallback (if normal_refill or stddev is null):\n",
    "    - cart = p80_daily_240d / p70_daily_retailers_240d\n",
    "    \"\"\"\n",
    "    normal_refill = row.get('normal_refill')\n",
    "    stddev = row.get('refill_stddev')\n",
    "    \n",
    "    # Fallback: if normal_refill or stddev is null, use p80_qty / p70_retailers\n",
    "    if pd.isna(normal_refill) or pd.isna(stddev):\n",
    "        p80_qty = row.get('p80_daily_240d', 0)\n",
    "        p70_retailers = row.get('p70_daily_retailers_240d', 1)  # Avoid division by zero\n",
    "        \n",
    "        if pd.notna(p80_qty) and pd.notna(p70_retailers) and p70_retailers > 0:\n",
    "            target_cart = p80_qty / p70_retailers\n",
    "        else:\n",
    "            target_cart = 25  # Default fallback\n",
    "        \n",
    "        return max(MIN_CART_RULE, min(MAX_CART_RULE, int(target_cart)))\n",
    "    \n",
    "    # Special case: OOS - set to refill + 3*std\n",
    "    if is_oos:\n",
    "        target_cart = normal_refill + (3 * stddev)\n",
    "        return max(MIN_CART_RULE, min(MAX_CART_RULE, int(target_cart)))\n",
    "    \n",
    "    # Special case: Zero Demand - open cart to refill + 10*std\n",
    "    if is_zero_demand:\n",
    "        target_cart = normal_refill + (10 * stddev)\n",
    "        return max(MIN_CART_RULE, min(MAX_CART_RULE, int(target_cart)))\n",
    "    \n",
    "    # Normal case: use ABC class + yesterday status multiplier\n",
    "    abc_class = row.get('abc_class', 'C')\n",
    "    yesterday_status = row.get('yesterday_status', 'No Data')\n",
    "    \n",
    "    multiplier = get_cart_multiplier(abc_class, yesterday_status)\n",
    "    target_cart = normal_refill + (multiplier * stddev)\n",
    "    \n",
    "    return max(MIN_CART_RULE, min(MAX_CART_RULE, int(target_cart)))\n",
    "\n",
    "print(\"Status adjustment and price functions loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main engine function loaded.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MAIN ENGINE: GENERATE INITIAL PRICE PUSH\n",
    "# =============================================================================\n",
    "\n",
    "def get_max_price(row):\n",
    "    \"\"\"Get maximum price: market_max first, then highest margin tier.\"\"\"\n",
    "    # Try market max first\n",
    "    market_max = row.get('maximum')\n",
    "    if pd.notna(market_max) and market_max > 0:\n",
    "        return market_max, 'market_max'\n",
    "    \n",
    "    # Fallback: highest margin tier\n",
    "    margin_tiers = get_margin_tiers(row)\n",
    "    if margin_tiers:\n",
    "        return margin_tiers[-1], 'margin_max'  # Last = highest\n",
    "    \n",
    "    # Fallback: current price\n",
    "    return row.get('current_price', 0), 'unchanged'\n",
    "\n",
    "def find_price_n_steps_below(current_price, n_steps, row):\n",
    "    \"\"\"Find price N steps below current (iteratively find next tier below).\"\"\"\n",
    "    price = current_price\n",
    "    for _ in range(n_steps):\n",
    "        next_price = find_next_price_below(price, row)\n",
    "        if next_price >= price:  # No tier below found\n",
    "            break\n",
    "        price = next_price\n",
    "    return price\n",
    "\n",
    "def generate_initial_price_push(row):\n",
    "    \"\"\"\n",
    "    Generate initial price push action for a single SKU.\n",
    "    \n",
    "    Logic:\n",
    "    - Stocks = 0: Set to market_max or margin_max (highest price)\n",
    "    - Zero demand + yesterday below on track: Go 2 steps below current\n",
    "    - Zero demand + yesterday above on track: Keep current price\n",
    "    - Otherwise: Adjust price relative to CURRENT price based on status\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'product_id': row.get('product_id'),\n",
    "        'warehouse_id': row.get('warehouse_id'),\n",
    "        'cohort_id': row.get('cohort_id'),\n",
    "        'sku': row.get('sku'),\n",
    "        'brand': row.get('brand'),\n",
    "        'cat': row.get('cat'),\n",
    "        'abc_class': row.get('abc_class', 'C'),\n",
    "        'current_price': row.get('current_price'),\n",
    "        'current_cart_rule': row.get('current_cart_rule'),\n",
    "        'wac_p': row.get('wac_p'),\n",
    "        'stocks': row.get('stocks', 0),\n",
    "        'combined_status': row.get('combined_status'),\n",
    "        'yesterday_status': row.get('yesterday_status'),\n",
    "        'zero_demand': row.get('zero_demand', 0),\n",
    "        'sensitivity': row.get('sensitivity', row.get('product_sensitivity')),\n",
    "        'new_price': None,\n",
    "        'new_cart_rule': None,\n",
    "        'new_margin': None,\n",
    "        'current_margin': None,\n",
    "        'price_source': None,\n",
    "        'price_action': None,\n",
    "        'price_reason': None,\n",
    "    }\n",
    "    \n",
    "    wac = row.get('wac_p', 0)\n",
    "    current_price = row.get('current_price', 0)\n",
    "    result['current_margin'] = calculate_margin(current_price, wac)\n",
    "    yesterday_status = row.get('yesterday_status', 'No Data')\n",
    "    \n",
    "    # CASE 1: Out of Stock (stocks = 0) - Set to MAX price, capped at target_margin + 3*std\n",
    "    if row.get('stocks', 0) == 0:\n",
    "        max_price, price_source = get_max_price(row)\n",
    "        \n",
    "        # Calculate cap price from target_margin + 3*std\n",
    "        target_margin = row.get('target_margin', 0)\n",
    "        margin_std = row.get('std', 0)  # margin std from data extraction\n",
    "        cap_margin = target_margin + (5 * margin_std) if pd.notna(target_margin) and pd.notna(margin_std) else None\n",
    "        cap_price = wac / (1 - cap_margin) if cap_margin and cap_margin < 1 and wac > 0 else None\n",
    "        \n",
    "        # Apply cap logic\n",
    "        if cap_price and max_price > cap_price:\n",
    "            # Max price exceeds cap - use max(current_price, cap_price)\n",
    "            final_price = max(current_price, cap_price) if pd.notna(current_price) else cap_price\n",
    "            result['new_price'] = round(final_price, 2)\n",
    "            result['price_source'] = 'capped_at_target+3std'\n",
    "            result['price_action'] = 'oos_capped'\n",
    "            result['price_reason'] = f'OOS - max ({max_price:.2f}) > cap ({cap_price:.2f}), using max(current, cap)'\n",
    "        else:\n",
    "            # Max price is within cap or no cap - use max price\n",
    "            result['new_price'] = max_price\n",
    "            result['price_source'] = price_source\n",
    "            result['price_action'] = 'oos_max'\n",
    "            result['price_reason'] = 'OOS - set to max price (within cap)'\n",
    "        \n",
    "        result['new_cart_rule'] = get_initial_cart_rule(row, is_oos=True)  # OOS cart: refill + 3*std\n",
    "        result['new_margin'] = calculate_margin(result['new_price'], wac)\n",
    "        return result\n",
    "    \n",
    "    # CASE 2: Zero Demand SKUs (has stock but no recent sales)\n",
    "    if row.get('zero_demand', 0) == 1:\n",
    "        yesterday_below = is_below_on_track(yesterday_status)\n",
    "        yesterday_above = is_above_on_track(yesterday_status)\n",
    "        \n",
    "        if yesterday_below:\n",
    "            # Yesterday below on track: Go 2 steps below current price\n",
    "            new_price = find_price_n_steps_below(current_price, 2, row)\n",
    "            \n",
    "            # Apply commercial minimum floor\n",
    "            commercial_min = row.get('commercial_min_price', row.get('minimum', 0))\n",
    "            if pd.notna(commercial_min) and commercial_min > 0:\n",
    "                new_price = max(new_price, commercial_min)\n",
    "            \n",
    "            result['new_price'] = new_price\n",
    "            result['price_source'] = 'market' if new_price in get_market_tiers(row) else 'margin'\n",
    "            result['price_action'] = 'zero_demand_decrease'\n",
    "            result['price_reason'] = f'Zero demand + yesterday below ({yesterday_status}) - 2 steps below'\n",
    "        elif yesterday_above:\n",
    "            # Yesterday above on track: Keep current price\n",
    "            result['new_price'] = current_price\n",
    "            result['price_source'] = 'unchanged'\n",
    "            result['price_action'] = 'zero_demand_hold'\n",
    "            result['price_reason'] = f'Zero demand + yesterday above ({yesterday_status}) - keep current'\n",
    "        else:\n",
    "            # Yesterday on track or no data: Go 1 step below\n",
    "            new_price = find_next_price_below(current_price, row)\n",
    "            commercial_min = row.get('commercial_min_price', row.get('minimum', 0))\n",
    "            if pd.notna(commercial_min) and commercial_min > 0:\n",
    "                new_price = max(new_price, commercial_min)\n",
    "            \n",
    "            result['new_price'] = new_price\n",
    "            result['price_source'] = 'market' if new_price in get_market_tiers(row) else 'margin'\n",
    "            result['price_action'] = 'zero_demand_decrease'\n",
    "            result['price_reason'] = f'Zero demand + yesterday on track ({yesterday_status}) - 1 step below'\n",
    "        \n",
    "        result['new_cart_rule'] = get_initial_cart_rule(row, is_zero_demand=True)  # Zero demand: refill + 10*std\n",
    "        result['new_margin'] = calculate_margin(result['new_price'], wac)\n",
    "        return result\n",
    "    \n",
    "    # CASE 3: Normal SKUs - Determine price action based on status\n",
    "    combined_status = row.get('combined_status', 'No Data')\n",
    "    \n",
    "    # Handle 'No Data' with stocks as Critical (below on track)\n",
    "    if combined_status == 'No Data' and row.get('stocks', 0) > 0:\n",
    "        combined_status = 'Critical'\n",
    "    \n",
    "    # Get price action (increase/decrease/hold)\n",
    "    action, reason = get_price_action(combined_status, yesterday_status)\n",
    "    \n",
    "    # Apply price action - find next tier above/below current price\n",
    "    new_price, price_source = apply_price_action(current_price, action, row)\n",
    "    \n",
    "    result['new_price'] = new_price\n",
    "    result['price_source'] = price_source\n",
    "    result['price_action'] = action\n",
    "    result['price_reason'] = reason\n",
    "    result['new_cart_rule'] = get_initial_cart_rule(row)\n",
    "    result['new_margin'] = calculate_margin(new_price, wac)\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"Main engine function loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 28436 SKUs...\n",
      "============================================================\n",
      "Processed 10000/28436 SKUs...\n",
      "Processed 20000/28436 SKUs...\n",
      "\n",
      "âœ… Processed 28436 SKUs\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXECUTE MODULE 2\n",
    "# =============================================================================\n",
    "print(f\"Processing {len(df)} SKUs...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = []\n",
    "for idx, row in df.iterrows():\n",
    "    result = generate_initial_price_push(row)\n",
    "    results.append(result)\n",
    "    \n",
    "    if (idx + 1) % 10000 == 0:\n",
    "        print(f\"Processed {idx + 1}/{len(df)} SKUs...\")\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(f\"\\nâœ… Processed {len(df_results)} SKUs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODULE 2 SUMMARY\n",
      "============================================================\n",
      "\n",
      "Total SKUs processed: 28436\n",
      "\n",
      "By ABC Class:\n",
      "abc_class\n",
      "C    23407\n",
      "B     4372\n",
      "A      657\n",
      "\n",
      "By Price Source:\n",
      "price_source\n",
      "margin                                  9038\n",
      "unchanged (no tier below)               7717\n",
      "capped_at_target+3std                   7174\n",
      "unchanged                               3338\n",
      "unchanged (at all_time_high ceiling)     555\n",
      "margin_max                               356\n",
      "all_time_high_margin                     171\n",
      "unchanged (no tier above)                 87\n",
      "\n",
      "Price Change Distribution:\n",
      "  Increases: 5038\n",
      "  Decreases: 4142\n",
      "  No change: 19256\n",
      "\n",
      "Avg price change: 0.07%\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(\"=\"*60)\n",
    "print(\"MODULE 2 SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nTotal SKUs processed: {len(df_results)}\")\n",
    "print(f\"\\nBy ABC Class:\")\n",
    "print(df_results['abc_class'].value_counts().to_string())\n",
    "print(f\"\\nBy Price Source:\")\n",
    "print(df_results['price_source'].value_counts().to_string())\n",
    "\n",
    "# Price change analysis\n",
    "df_results['price_change'] = df_results['new_price'] - df_results['current_price']\n",
    "df_results['price_change_pct'] = (df_results['price_change'] / df_results['current_price'] * 100).round(2)\n",
    "\n",
    "print(f\"\\nPrice Change Distribution:\")\n",
    "print(f\"  Increases: {len(df_results[df_results['price_change'] > 0])}\")\n",
    "print(f\"  Decreases: {len(df_results[df_results['price_change'] < 0])}\")\n",
    "print(f\"  No change: {len(df_results[df_results['price_change'] == 0])}\")\n",
    "print(f\"\\nAvg price change: {df_results['price_change_pct'].mean():.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Results exported to: module_2_output_20260127_0049.xlsx\n",
      "Total records: 28436 (after removing 0 duplicates)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXPORT RESULTS\n",
    "# =============================================================================\n",
    "output_cols = [\n",
    "    'product_id', 'warehouse_id', 'cohort_id', 'sku', 'brand', 'cat', 'abc_class', 'sensitivity',\n",
    "    'stocks', 'zero_demand', 'combined_status', 'yesterday_status',\n",
    "    'current_price', 'new_price', 'price_change', 'price_change_pct',\n",
    "    'wac_p', 'current_margin', 'new_margin','current_cart_rule',\n",
    "    'new_cart_rule', 'price_action', 'price_source', 'price_reason'\n",
    "]\n",
    "\n",
    "# Filter to only columns that exist\n",
    "output_cols = [c for c in output_cols if c in df_results.columns]\n",
    "\n",
    "# Drop duplicates before saving\n",
    "df_output = df_results[output_cols].drop_duplicates(subset=['product_id', 'warehouse_id'], keep='first')\n",
    "df_output.to_excel(OUTPUT_FILE, index=False)\n",
    "print(f\"\\nâœ… Results exported to: {OUTPUT_FILE}\")\n",
    "print(f\"Total records: {len(df_output)} (after removing {len(df_results) - len(df_output)} duplicates)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Push Cart Rules Handler loaded at 2026-01-27 01:12:22 Cairo time\n",
      "âœ“ API credentials loaded successfully\n",
      "Push Prices Handler loaded at 2026-01-27 01:12:22 Cairo time\n",
      "âœ“ API credentials loaded successfully\n",
      "âœ“ Google Sheets client initialized\n",
      "Fetching packing_units ...\n",
      "  Loaded 34849 records\n",
      "\n",
      "======================================================================\n",
      "STEP 1: PUSHING CART RULES\n",
      "======================================================================\n",
      "\n",
      "ðŸš€ MODE: LIVE\n",
      "   Files will be prepared AND uploaded to API\n",
      "\n",
      "============================================================\n",
      "PUSH CART RULES - Source: module_2\n",
      "============================================================\n",
      "Total received: 28436\n",
      "Cart rule changes to push: 26333\n",
      "Skipped (no change): 2103\n",
      "\n",
      "Cart rule changes summary:\n",
      "  Increases: 2478\n",
      "  Decreases: 23855\n",
      "\n",
      "ðŸ“‹ Prepared 29331 packing unit cart rules\n",
      "\n",
      "Sample cart rule adjustments (showing products with multiple PUs):\n",
      " product_id  basic_unit_count  final_cart_rule  final_pu_cart_rule\n",
      "          3                 1               35                  35\n",
      "          3                 1               14                  14\n",
      "          3                 1                5                   5\n",
      "          3                 1               19                  19\n",
      "          3                 1                7                   7\n",
      "          3                 1               20                  20\n",
      "          3                 1               25                  25\n",
      "          3                 1               29                  29\n",
      "          3                 1               15                  15\n",
      "          9                 1                6                   6\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 700\n",
      "==================================================\n",
      "  Saved: uploads/module_2_cart_rules_700.xlsx (4497 rows)\n",
      "  Split into 2 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 14.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ“ Chunk 1 uploaded successfully\n",
      "    âœ“ Chunk 2 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 701\n",
      "==================================================\n",
      "  Saved: uploads/module_2_cart_rules_701.xlsx (5038 rows)\n",
      "  Split into 2 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ“ Chunk 1 uploaded successfully\n",
      "    âœ“ Chunk 2 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 702\n",
      "==================================================\n",
      "  Saved: uploads/module_2_cart_rules_702.xlsx (2869 rows)\n",
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 11.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ“ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 703\n",
      "==================================================\n",
      "  Saved: uploads/module_2_cart_rules_703.xlsx (4000 rows)\n",
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  8.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ“ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 704\n",
      "==================================================\n",
      "  Saved: uploads/module_2_cart_rules_704.xlsx (4085 rows)\n",
      "  Split into 2 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ“ Chunk 1 uploaded successfully\n",
      "    âœ“ Chunk 2 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1123\n",
      "==================================================\n",
      "  Saved: uploads/module_2_cart_rules_1123.xlsx (2243 rows)\n",
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ“ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1124\n",
      "==================================================\n",
      "  Saved: uploads/module_2_cart_rules_1124.xlsx (2193 rows)\n",
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 15.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ“ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1125\n",
      "==================================================\n",
      "  Saved: uploads/module_2_cart_rules_1125.xlsx (2187 rows)\n",
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 15.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ“ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1126\n",
      "==================================================\n",
      "  Saved: uploads/module_2_cart_rules_1126.xlsx (2219 rows)\n",
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ“ Chunk 1 uploaded successfully\n",
      "\n",
      "============================================================\n",
      "ðŸš€ UPLOAD COMPLETE\n",
      "============================================================\n",
      "Mode: live\n",
      "Total prepared: 29331\n",
      "Total failed: 0\n",
      "\n",
      "============================================================\n",
      "CART RULES RESULT\n",
      "============================================================\n",
      "Mode: live\n",
      "Cart rule changes: 26333\n",
      "Pushed: 29331\n",
      "Failed: 0\n",
      "\n",
      "======================================================================\n",
      "STEP 2: PUSHING PRICES\n",
      "======================================================================\n",
      "\n",
      "ðŸš€ MODE: LIVE\n",
      "   Files will be prepared AND uploaded to API\n",
      "Loading disable_pu_visibility from Google Sheets...\n",
      "  âœ“ Loaded 89 products to disable min PU visibility\n",
      "\n",
      "============================================================\n",
      "PUSH PRICES - Source: module_2\n",
      "============================================================\n",
      "Total received: 28436\n",
      "Price changes to push: 9180\n",
      "Skipped (no change): 19256\n",
      "\n",
      "Price changes summary:\n",
      "  Increases: 5038\n",
      "  Decreases: 4142\n",
      "\n",
      "ðŸ“‹ Prepared 10983 packing unit prices\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 701\n",
      "==================================================\n",
      "  Saved: uploads/module_2_701.xlsx (2249 rows)\n",
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  6.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ“ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 702\n",
      "==================================================\n",
      "  Saved: uploads/module_2_702.xlsx (875 rows)\n",
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 16.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ“ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 703\n",
      "==================================================\n",
      "  Saved: uploads/module_2_703.xlsx (1623 rows)\n",
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  9.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ“ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 704\n",
      "==================================================\n",
      "  Saved: uploads/module_2_704.xlsx (1709 rows)\n",
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  8.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ“ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 700\n",
      "==================================================\n",
      "  Saved: uploads/module_2_700.xlsx (1688 rows)\n",
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  8.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ“ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1124\n",
      "==================================================\n",
      "  Saved: uploads/module_2_1124.xlsx (699 rows)\n",
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 20.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ“ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1125\n",
      "==================================================\n",
      "  Saved: uploads/module_2_1125.xlsx (696 rows)\n",
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n",
      "    âœ“ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1123\n",
      "==================================================\n",
      "  Saved: uploads/module_2_1123.xlsx (720 rows)\n",
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 19.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ“ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1126\n",
      "==================================================\n",
      "  Saved: uploads/module_2_1126.xlsx (724 rows)\n",
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Saving chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 19.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ“ Chunk 1 uploaded successfully\n",
      "\n",
      "============================================================\n",
      "ðŸš€ UPLOAD COMPLETE\n",
      "============================================================\n",
      "Mode: live\n",
      "Total prepared: 10983\n",
      "Total failed: 0\n",
      "\n",
      "============================================================\n",
      "PRICES RESULT\n",
      "============================================================\n",
      "Mode: live\n",
      "Source: module_2\n",
      "Timestamp: 2026-01-27 01:13:12\n",
      "Total received: 28436\n",
      "Price changes: 9180\n",
      "Pushed: 10983\n",
      "Skipped: 19256\n",
      "Failed: 0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PUSH CART RULES & PRICES\n",
    "# =============================================================================\n",
    "# Push cart rules FIRST, then prices\n",
    "# If cart rules fail for certain cohorts, skip those cohorts for prices\n",
    "\n",
    "%run push_cart_rules_handler.ipynb\n",
    "%run push_prices_handler.ipynb\n",
    "pus = get_packing_units()\n",
    "\n",
    "# âš ï¸ MODE CONFIGURATION:\n",
    "# - 'testing' (default): Prepare files but DON'T upload to API\n",
    "# - 'live': Prepare files AND upload to MaxAB API\n",
    "PUSH_MODE = 'live'  # Change to 'live' when ready to push\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: Push Cart Rules First\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 1: PUSHING CART RULES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "cart_result = push_cart_rules(df_output, pus, source_module='module_2', mode=PUSH_MODE)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CART RULES RESULT\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Mode: {cart_result['mode']}\")\n",
    "print(f\"Cart rule changes: {cart_result['cart_rule_changes']}\")\n",
    "print(f\"Pushed: {cart_result['pushed']}\")\n",
    "print(f\"Failed: {cart_result['failed']}\")\n",
    "if cart_result['failed_cohorts']:\n",
    "    print(f\"âš ï¸ Failed cohorts: {cart_result['failed_cohorts']}\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: Push Prices (skip failed cohorts)\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: PUSHING PRICES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get failed cohorts from cart rules to skip in price push\n",
    "failed_cohorts = cart_result.get('failed_cohorts', [])\n",
    "\n",
    "# Call push_prices with the results, skipping failed cohorts\n",
    "push_result = push_prices(df_output, pus, source_module='module_2', mode=PUSH_MODE, skip_cohorts=failed_cohorts)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PRICES RESULT\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Mode: {push_result['mode']}\")\n",
    "print(f\"Source: {push_result['source_module']}\")\n",
    "print(f\"Timestamp: {push_result['timestamp']}\")\n",
    "print(f\"Total received: {push_result['total_received']}\")\n",
    "print(f\"Price changes: {push_result['price_changes']}\")\n",
    "print(f\"Pushed: {push_result['pushed']}\")\n",
    "print(f\"Skipped: {push_result['skipped']}\")\n",
    "print(f\"Failed: {push_result['failed']}\")\n",
    "if push_result.get('skipped_cohorts'):\n",
    "    print(f\"âš ï¸ Skipped cohorts (cart rules failed): {push_result['skipped_cohorts']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.13.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.12/site-packages (from aiohttp) (25.4.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp)\n",
      "  Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp)\n",
      "  Downloading multidict-6.7.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp)\n",
      "  Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp)\n",
      "  Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "Requirement already satisfied: idna>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.11)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.12/site-packages (from aiosignal>=1.4.0->aiohttp) (4.15.0)\n",
      "Downloading aiohttp-3.13.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.7.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)\n",
      "Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (377 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (242 kB)\n",
      "Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (221 kB)\n",
      "Installing collected packages: propcache, multidict, frozenlist, aiohappyeyeballs, yarl, aiosignal, aiohttp\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7/7\u001b[0m [aiohttp]m6/7\u001b[0m [aiohttp]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.3 aiosignal-1.4.0 frozenlist-1.8.0 multidict-6.7.1 propcache-0.4.1 yarl-1.22.0\n"
     ]
    }
   ],
   "source": [
    "!pip install aiohttp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "UPLOADING RESULTS TO SNOWFLAKE\n",
      "============================================================\n",
      "/home/ec2-user/service_account_key.json\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mUPLOADING RESULTS TO SNOWFLAKE\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m upload_status = \u001b[43mupload_dataframe_to_snowflake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEgypt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMATERIALIZED_VIEWS\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpricing_initial_push\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mappend\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauto_create_table\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m     22\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Prepare Slack notification\u001b[39;00m\n\u001b[32m     25\u001b[39m prices_pushed = push_result.get(\u001b[33m'\u001b[39m\u001b[33mpushed\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/Pricing Runs/Prediction_Scripts_2/Happy_hour/git/Mustafa/Pricing Logic/modules/../common_functions.py:675\u001b[39m, in \u001b[36mupload_dataframe_to_snowflake\u001b[39m\u001b[34m(country, df, schema_name, table_name, method, auto_create_table, conn)\u001b[39m\n\u001b[32m    672\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msnowflake\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconnector\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpandas_tools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m write_pandas\n\u001b[32m    673\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m675\u001b[39m \u001b[43minitialize_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    677\u001b[39m schema_name = schema_name.upper()\n\u001b[32m    678\u001b[39m table_name = table_name.upper()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/Pricing Runs/Prediction_Scripts_2/Happy_hour/git/Mustafa/Pricing Logic/modules/../common_functions.py:252\u001b[39m, in \u001b[36minitialize_env\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    250\u001b[39m json_path = \u001b[38;5;28mstr\u001b[39m(Path.home())+\u001b[33m\"\u001b[39m\u001b[33m/service_account_key.json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;28mprint\u001b[39m(json_path)\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m bigquery_key = \u001b[43mget_secret\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprod/bigquery/sagemaker\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    253\u001b[39m f = \u001b[38;5;28mopen\u001b[39m(json_path, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    254\u001b[39m f.write(bigquery_key)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/Pricing Runs/Prediction_Scripts_2/Happy_hour/git/Mustafa/Pricing Logic/modules/../common_functions.py:32\u001b[39m, in \u001b[36mget_secret\u001b[39m\u001b[34m(secret_name)\u001b[39m\n\u001b[32m     29\u001b[39m client = session.client(service_name=\u001b[33m\"\u001b[39m\u001b[33msecretsmanager\u001b[39m\u001b[33m\"\u001b[39m, region_name=region_name)\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     get_secret_value_response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_secret_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSecretId\u001b[49m\u001b[43m=\u001b[49m\u001b[43msecret_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/python3/lib/python3.12/site-packages/botocore/client.py:602\u001b[39m, in \u001b[36mClientCreator._create_api_method.<locals>._api_call\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    598\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    599\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m() only accepts keyword arguments.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    600\u001b[39m     )\n\u001b[32m    601\u001b[39m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/python3/lib/python3.12/site-packages/botocore/context.py:123\u001b[39m, in \u001b[36mwith_current_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hook:\n\u001b[32m    122\u001b[39m     hook()\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/python3/lib/python3.12/site-packages/botocore/client.py:1060\u001b[39m, in \u001b[36mBaseClient._make_api_call\u001b[39m\u001b[34m(self, operation_name, api_params)\u001b[39m\n\u001b[32m   1056\u001b[39m     maybe_compress_request(\n\u001b[32m   1057\u001b[39m         \u001b[38;5;28mself\u001b[39m.meta.config, request_dict, operation_model\n\u001b[32m   1058\u001b[39m     )\n\u001b[32m   1059\u001b[39m     apply_request_checksum(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m1060\u001b[39m     http, parsed_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1061\u001b[39m \u001b[43m        \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_context\u001b[49m\n\u001b[32m   1062\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1064\u001b[39m \u001b[38;5;28mself\u001b[39m.meta.events.emit(\n\u001b[32m   1065\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mafter-call.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mservice_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m,\n\u001b[32m   1066\u001b[39m     http_response=http,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1069\u001b[39m     context=request_context,\n\u001b[32m   1070\u001b[39m )\n\u001b[32m   1072\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http.status_code >= \u001b[32m300\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/python3/lib/python3.12/site-packages/botocore/client.py:1084\u001b[39m, in \u001b[36mBaseClient._make_request\u001b[39m\u001b[34m(self, operation_model, request_dict, request_context)\u001b[39m\n\u001b[32m   1082\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_make_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, operation_model, request_dict, request_context):\n\u001b[32m   1083\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1084\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_endpoint\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1085\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1086\u001b[39m         \u001b[38;5;28mself\u001b[39m.meta.events.emit(\n\u001b[32m   1087\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mafter-call-error.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._service_model.service_id.hyphenize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperation_model.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m,\n\u001b[32m   1088\u001b[39m             exception=e,\n\u001b[32m   1089\u001b[39m             context=request_context,\n\u001b[32m   1090\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/python3/lib/python3.12/site-packages/botocore/endpoint.py:119\u001b[39m, in \u001b[36mEndpoint.make_request\u001b[39m\u001b[34m(self, operation_model, request_dict)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmake_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, operation_model, request_dict):\n\u001b[32m    114\u001b[39m     logger.debug(\n\u001b[32m    115\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMaking request for \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m with params: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    116\u001b[39m         operation_model,\n\u001b[32m    117\u001b[39m         request_dict,\n\u001b[32m    118\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/python3/lib/python3.12/site-packages/botocore/endpoint.py:200\u001b[39m, in \u001b[36mEndpoint._send_request\u001b[39m\u001b[34m(self, request_dict, operation_model)\u001b[39m\n\u001b[32m    196\u001b[39m request = \u001b[38;5;28mself\u001b[39m.create_request(request_dict, operation_model)\n\u001b[32m    197\u001b[39m success_response, exception = \u001b[38;5;28mself\u001b[39m._get_response(\n\u001b[32m    198\u001b[39m     request, operation_model, context\n\u001b[32m    199\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_needs_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattempts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43msuccess_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    207\u001b[39m     attempts += \u001b[32m1\u001b[39m\n\u001b[32m    208\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_retries_context(context, attempts, success_response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/python3/lib/python3.12/site-packages/botocore/endpoint.py:360\u001b[39m, in \u001b[36mEndpoint._needs_retry\u001b[39m\u001b[34m(self, attempts, operation_model, request_dict, response, caught_exception)\u001b[39m\n\u001b[32m    358\u001b[39m service_id = operation_model.service_model.service_id.hyphenize()\n\u001b[32m    359\u001b[39m event_name = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mneeds-retry.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mservice_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperation_model.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m360\u001b[39m responses = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_event_emitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43memit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m    \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattempts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattempts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcaught_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcaught_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    369\u001b[39m handler_response = first_non_none_response(responses)\n\u001b[32m    370\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m handler_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/python3/lib/python3.12/site-packages/botocore/hooks.py:412\u001b[39m, in \u001b[36mEventAliaser.emit\u001b[39m\u001b[34m(self, event_name, **kwargs)\u001b[39m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34memit\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name, **kwargs):\n\u001b[32m    411\u001b[39m     aliased_event_name = \u001b[38;5;28mself\u001b[39m._alias_event_name(event_name)\n\u001b[32m--> \u001b[39m\u001b[32m412\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_emitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43memit\u001b[49m\u001b[43m(\u001b[49m\u001b[43maliased_event_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/python3/lib/python3.12/site-packages/botocore/hooks.py:256\u001b[39m, in \u001b[36mHierarchicalEmitter.emit\u001b[39m\u001b[34m(self, event_name, **kwargs)\u001b[39m\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34memit\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name, **kwargs):\n\u001b[32m    246\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03m    Emit an event by name with arguments passed as keyword args.\u001b[39;00m\n\u001b[32m    248\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    254\u001b[39m \u001b[33;03m             handlers.\u001b[39;00m\n\u001b[32m    255\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_emit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/python3/lib/python3.12/site-packages/botocore/hooks.py:239\u001b[39m, in \u001b[36mHierarchicalEmitter._emit\u001b[39m\u001b[34m(self, event_name, kwargs, stop_on_response)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers_to_call:\n\u001b[32m    238\u001b[39m     logger.debug(\u001b[33m'\u001b[39m\u001b[33mEvent \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m: calling handler \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m, event_name, handler)\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m     response = \u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    240\u001b[39m     responses.append((handler, response))\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stop_on_response \u001b[38;5;129;01mand\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/python3/lib/python3.12/site-packages/botocore/retryhandler.py:207\u001b[39m, in \u001b[36mRetryHandler.__call__\u001b[39m\u001b[34m(self, attempts, response, caught_exception, **kwargs)\u001b[39m\n\u001b[32m    204\u001b[39m     retries_context = kwargs[\u001b[33m'\u001b[39m\u001b[33mrequest_dict\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m'\u001b[39m].get(\u001b[33m'\u001b[39m\u001b[33mretries\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    205\u001b[39m     checker_kwargs.update({\u001b[33m'\u001b[39m\u001b[33mretries_context\u001b[39m\u001b[33m'\u001b[39m: retries_context})\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mchecker_kwargs\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    208\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._action(attempts=attempts)\n\u001b[32m    209\u001b[39m     logger.debug(\u001b[33m\"\u001b[39m\u001b[33mRetry needed, action of: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/python3/lib/python3.12/site-packages/botocore/retryhandler.py:284\u001b[39m, in \u001b[36mMaxAttemptsDecorator.__call__\u001b[39m\u001b[34m(self, attempt_number, response, caught_exception, retries_context)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m retries_context:\n\u001b[32m    280\u001b[39m     retries_context[\u001b[33m'\u001b[39m\u001b[33mmax\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mmax\u001b[39m(\n\u001b[32m    281\u001b[39m         retries_context.get(\u001b[33m'\u001b[39m\u001b[33mmax\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0\u001b[39m), \u001b[38;5;28mself\u001b[39m._max_attempts\n\u001b[32m    282\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m should_retry = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattempt_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaught_exception\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m should_retry:\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attempt_number >= \u001b[38;5;28mself\u001b[39m._max_attempts:\n\u001b[32m    289\u001b[39m         \u001b[38;5;66;03m# explicitly set MaxAttemptsReached\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/python3/lib/python3.12/site-packages/botocore/retryhandler.py:307\u001b[39m, in \u001b[36mMaxAttemptsDecorator._should_retry\u001b[39m\u001b[34m(self, attempt_number, response, caught_exception)\u001b[39m\n\u001b[32m    305\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retryable_exceptions \u001b[38;5;129;01mand\u001b[39;00m attempt_number < \u001b[38;5;28mself\u001b[39m._max_attempts:\n\u001b[32m    306\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_checker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattempt_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaught_exception\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    310\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retryable_exceptions \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    311\u001b[39m         logger.debug(\n\u001b[32m    312\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mretry needed, retryable exception caught: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    313\u001b[39m             e,\n\u001b[32m    314\u001b[39m             exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    315\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/python3/lib/python3.12/site-packages/botocore/retryhandler.py:363\u001b[39m, in \u001b[36mMultiChecker.__call__\u001b[39m\u001b[34m(self, attempt_number, response, caught_exception)\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attempt_number, response, caught_exception):\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m checker \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._checkers:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         checker_response = \u001b[43mchecker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattempt_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaught_exception\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    366\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m checker_response:\n\u001b[32m    367\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m checker_response\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# UPLOAD RESULTS TO SNOWFLAKE AND SEND SLACK NOTIFICATION\n",
    "# =============================================================================\n",
    "from common_functions import upload_dataframe_to_snowflake, send_text_slack\n",
    "\n",
    "# Add created_at as DATE (module runs once per day at 8 AM)\n",
    "df_output['created_at'] = datetime.now(CAIRO_TZ).date()\n",
    "\n",
    "# Upload to Snowflake\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"UPLOADING RESULTS TO SNOWFLAKE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "upload_status = upload_dataframe_to_snowflake(\n",
    "    \"Egypt\", \n",
    "    df_output, \n",
    "    \"MATERIALIZED_VIEWS\", \n",
    "    \"pricing_initial_push\", \n",
    "    \"append\", \n",
    "    auto_create_table=True, \n",
    "    conn=None\n",
    ")\n",
    "\n",
    "# Prepare Slack notification\n",
    "prices_pushed = push_result.get('pushed', 0)\n",
    "prices_failed = push_result.get('failed', 0)\n",
    "cart_rules_pushed = cart_result.get('pushed', 0)\n",
    "cart_rules_failed = cart_result.get('failed', 0)\n",
    "\n",
    "if upload_status:\n",
    "    slack_message = f\"\"\"âœ… *Module 2 - Initial Price Push Completed*\n",
    "\n",
    "ðŸ“… Date: {datetime.now(CAIRO_TZ).strftime('%Y-%m-%d')}\n",
    "â° Completed at: {datetime.now(CAIRO_TZ).strftime('%H:%M:%S')} Cairo time\n",
    "ðŸ”§ Mode: {PUSH_MODE.upper()}\n",
    "\n",
    "ðŸ“Š *Results:*\n",
    "â€¢ Total SKUs processed: {len(df_output):,}\n",
    "â€¢ Price changes: {push_result.get('price_changes', 0):,}\n",
    "â€¢ Cart rule changes: {cart_result.get('cart_rule_changes', 0):,}\n",
    "\n",
    "ðŸ“¤ *Push Status:*\n",
    "â€¢ ðŸ’° Prices: âœ… {prices_pushed} pushed | âŒ {prices_failed} failed\n",
    "â€¢ ðŸ›’ Cart Rules: âœ… {cart_rules_pushed} pushed | âŒ {cart_rules_failed} failed\n",
    "\n",
    "ðŸ—„ï¸ Results uploaded to: MATERIALIZED_VIEWS.pricing_initial_push\"\"\"\n",
    "    \n",
    "    send_text_slack('new-pricing-logic', slack_message)\n",
    "    print(\"âœ… Slack notification sent!\")\n",
    "    print(f\"âœ… {len(df_output)} records uploaded to Snowflake\")\n",
    "else:\n",
    "    error_message = f\"\"\"âŒ *Module 2 - Initial Price Push Failed*\n",
    "\n",
    "ðŸ“… Date: {datetime.now(CAIRO_TZ).strftime('%Y-%m-%d')}\n",
    "â° Failed at: {datetime.now(CAIRO_TZ).strftime('%H:%M:%S')} Cairo time\n",
    "âš ï¸ Upload to Snowflake failed - please check logs\n",
    "\n",
    "ðŸ“¤ *Push Status (before upload failure):*\n",
    "â€¢ ðŸ’° Prices: âœ… {prices_pushed} pushed | âŒ {prices_failed} failed\n",
    "â€¢ ðŸ›’ Cart Rules: âœ… {cart_rules_pushed} pushed | âŒ {cart_rules_failed} failed\"\"\"\n",
    "    \n",
    "    send_text_slack('new-pricing-logic', error_message)\n",
    "    print(\"âŒ Error notification sent to Slack!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
