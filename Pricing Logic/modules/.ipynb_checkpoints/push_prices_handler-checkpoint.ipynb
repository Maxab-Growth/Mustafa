{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push Prices Handler\n",
    "\n",
    "This module receives price changes from Module 2 or Module 3 and pushes them to the MaxAB API.\n",
    "\n",
    "## Workflow\n",
    "1. Receive price data from Module 2 (daily reset) or Module 3 (periodic adjustments)\n",
    "2. Filter to only SKUs where price actually changed\n",
    "3. Aggregate prices by cohort using stock-weighted average\n",
    "4. Expand to all packing units (multiply by basic_unit_count)\n",
    "5. Apply visibility rules (remove minimum PU for multi-PU products)\n",
    "6. Split into chunks and upload via MaxAB API\n",
    "\n",
    "## Required Input Columns\n",
    "- `product_id`: Product ID\n",
    "- `sku`: SKU name\n",
    "- `new_price`: New price to set (basic unit price)\n",
    "- `warehouse_id`: Warehouse ID\n",
    "- `cohort_id`: Cohort ID for pricing\n",
    "- `stocks`: Current stock level (used for weighted price aggregation)\n",
    "- `current_price`: Current price (for filtering changed prices)\n",
    "\n",
    "## Output\n",
    "- Uploads pricing sheets to MaxAB API per cohort\n",
    "- Returns summary dict with counts of pushed/failed/skipped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import base64\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "# AWS for secrets management\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# HTTP requests for API calls\n",
    "import requests\n",
    "\n",
    "# Progress bar for chunk uploads\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Google Sheets integration\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "# Import setup_environment_2 for Google Sheets credentials\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import setup_environment_2\n",
    "\n",
    "# Cairo timezone\n",
    "CAIRO_TZ = pytz.timezone('Africa/Cairo')\n",
    "CAIRO_NOW = datetime.now(CAIRO_TZ)\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "CHUNK_SIZE_DEFAULT = 4000   # Default chunk size for API uploads\n",
    "CHUNK_SIZE_SPECIAL = 2000   # Smaller chunk size for specific cohorts (e.g., cohort 61)\n",
    "UPLOAD_DIR = 'uploads'      # Directory to save full upload files\n",
    "MANUAL_DIR = 'manual'       # Directory to save chunk files\n",
    "\n",
    "# Google Sheets configuration\n",
    "GSHEET_NAME = 'Demand Based Dynamic Pricing'\n",
    "DISABLE_PU_SHEET = 'disable_pu_visibility'\n",
    "\n",
    "print(f\"Push Prices Handler loaded at {CAIRO_NOW.strftime('%Y-%m-%d %H:%M:%S')} Cairo time\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# AWS & API FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def get_secret(secret_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve a secret from AWS Secrets Manager.\n",
    "    \n",
    "    This function connects to AWS Secrets Manager to fetch sensitive credentials\n",
    "    (like API keys, passwords) securely without hardcoding them in the code.\n",
    "    \n",
    "    Args:\n",
    "        secret_name: The name/path of the secret in AWS Secrets Manager\n",
    "                    (e.g., \"prod/pricing/api/\")\n",
    "    \n",
    "    Returns:\n",
    "        Secret string (JSON format) or decoded binary\n",
    "        \n",
    "    Raises:\n",
    "        ClientError: If the secret cannot be retrieved (permissions, not found, etc.)\n",
    "    \"\"\"\n",
    "    region_name = \"us-east-1\"\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(service_name='secretsmanager', region_name=region_name)\n",
    "\n",
    "    try:\n",
    "        response = client.get_secret_value(SecretId=secret_name)\n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        error_messages = {\n",
    "            'DecryptionFailureException': \"Can't decrypt secret using provided KMS key\",\n",
    "            'InternalServiceErrorException': \"Server-side error occurred\",\n",
    "            'InvalidParameterException': \"Invalid parameter value provided\",\n",
    "            'InvalidRequestException': \"Invalid request for current resource state\",\n",
    "            'ResourceNotFoundException': \"Requested resource not found\"\n",
    "        }\n",
    "        if error_code in error_messages:\n",
    "            print(f\"AWS Error: {error_messages[error_code]}\")\n",
    "        raise e\n",
    "    \n",
    "    if 'SecretString' in response:\n",
    "        return response['SecretString']\n",
    "    return base64.b64decode(response['SecretBinary'])\n",
    "\n",
    "\n",
    "def get_access_token(url: str, client_id: str, client_secret: str) -> str:\n",
    "    \"\"\"\n",
    "    Get OAuth2 access token for MaxAB API authentication.\n",
    "    \n",
    "    Uses password grant type to exchange credentials for an access token.\n",
    "    The token is used in subsequent API calls for authorization.\n",
    "    \n",
    "    Args:\n",
    "        url: The OAuth token endpoint URL\n",
    "        client_id: OAuth client ID for the application\n",
    "        client_secret: OAuth client secret for the application\n",
    "    \n",
    "    Returns:\n",
    "        Access token string to be used in Authorization header\n",
    "    \"\"\"\n",
    "    response = requests.post(\n",
    "        url,\n",
    "        data={\n",
    "            \"grant_type\": \"password\",\n",
    "            \"username\": API_USERNAME,\n",
    "            \"password\": API_PASSWORD\n",
    "        },\n",
    "        auth=(client_id, client_secret),\n",
    "    )\n",
    "    return response.json()[\"access_token\"]\n",
    "\n",
    "\n",
    "def _get_api_token() -> str:\n",
    "    \"\"\"\n",
    "    Get a fresh API token for MaxAB API requests.\n",
    "    \n",
    "    This is a convenience wrapper that calls get_access_token with\n",
    "    the correct MaxAB SSO endpoint and client credentials.\n",
    "    \n",
    "    Returns:\n",
    "        Fresh access token string\n",
    "    \"\"\"\n",
    "    return get_access_token(\n",
    "        'https://sso.maxab.info/auth/realms/maxab/protocol/openid-connect/token',\n",
    "        'main-system-externals',\n",
    "        API_SECRET\n",
    "    )\n",
    "\n",
    "\n",
    "def post_prices(cohort_id: int, file_name: str) -> requests.Response:\n",
    "    \"\"\"\n",
    "    Upload a pricing Excel sheet to MaxAB API for a specific cohort.\n",
    "    \n",
    "    This function:\n",
    "    1. Gets a fresh API token\n",
    "    2. Prepares the file for multipart upload\n",
    "    3. POSTs to the cohort pricing endpoint\n",
    "    \n",
    "    Args:\n",
    "        cohort_id: The target cohort ID to upload prices for\n",
    "        file_name: Path to the Excel file containing pricing data\n",
    "                  Expected columns: Product ID, Product Name, Packing Unit ID, \n",
    "                                   Price, Visibility (YES/NO)\n",
    "    \n",
    "    Returns:\n",
    "        requests.Response object with API response\n",
    "        Check response.content for '{\"success\":true}' to verify upload\n",
    "    \"\"\"\n",
    "    token = _get_api_token()\n",
    "    url = f\"https://api.maxab.info/main-system/api/admin-portal/cohorts/{cohort_id}/pricing\"\n",
    "    \n",
    "    files = [('sheet', (file_name, open(file_name, 'rb'), \n",
    "              'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'))]\n",
    "    headers = {'Authorization': f'bearer {token}'}\n",
    "    \n",
    "    return requests.post(url, headers=headers, data={}, files=files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# API CREDENTIALS INITIALIZATION\n",
    "# =============================================================================\n",
    "# Load API credentials from AWS Secrets Manager\n",
    "# These are used by get_access_token() to authenticate with MaxAB API\n",
    "\n",
    "pricing_api_secret = json.loads(get_secret(\"prod/pricing/api/\"))\n",
    "API_USERNAME = pricing_api_secret[\"egypt_username\"]\n",
    "API_PASSWORD = pricing_api_secret[\"egypt_password\"]\n",
    "API_SECRET = pricing_api_secret[\"egypt_secret\"]\n",
    "\n",
    "print(\"âœ“ API credentials loaded successfully\")\n",
    "\n",
    "# =============================================================================\n",
    "# GOOGLE SHEETS CLIENT INITIALIZATION\n",
    "# =============================================================================\n",
    "# Initialize Google Sheets client for reading disable_pu_visibility data\n",
    "\n",
    "GSHEET_SCOPE = [\n",
    "    \"https://spreadsheets.google.com/feeds\",\n",
    "    'https://www.googleapis.com/auth/spreadsheets',\n",
    "    \"https://www.googleapis.com/auth/drive.file\",\n",
    "    \"https://www.googleapis.com/auth/drive\"\n",
    "]\n",
    "\n",
    "gsheet_creds = ServiceAccountCredentials.from_json_keyfile_dict(\n",
    "    json.loads(setup_environment_2.get_secret(\"prod/maxab-sheets\")), \n",
    "    GSHEET_SCOPE\n",
    ")\n",
    "gsheet_client = gspread.authorize(gsheet_creds)\n",
    "\n",
    "print(\"âœ“ Google Sheets client initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GOOGLE SHEETS DATA LOADING\n",
    "# =============================================================================\n",
    "\n",
    "def get_disable_pu_visibility() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load disable PU visibility data from Google Sheets.\n",
    "    \n",
    "    This function reads from the 'disable_pu_visibility' sheet in the \n",
    "    'Demand Based Dynamic Pricing' Google Sheet. This sheet contains products \n",
    "    whose minimum packing unit should be hidden from retailers.\n",
    "    \n",
    "    Sheet columns:\n",
    "        - cohort_id: Cohort identifier (dropped - not needed)\n",
    "        - product_id: Product identifier\n",
    "        - packing_unit_id: Packing unit identifier (kept for reference)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns: product_id, packing_unit_id\n",
    "        Duplicates are removed based on all columns.\n",
    "    \"\"\"\n",
    "    print(\"Loading disable_pu_visibility from Google Sheets...\")\n",
    "    \n",
    "    # Open the Google Sheet and get the specific worksheet\n",
    "    sheet = gsheet_client.open(GSHEET_NAME).worksheet(DISABLE_PU_SHEET)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(sheet.get_all_records())\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"  âš ï¸ No data in disable_pu_visibility sheet\")\n",
    "        return pd.DataFrame(columns=['product_id', 'packing_unit_id'])\n",
    "    \n",
    "    # Drop cohort_id column (not needed for visibility logic)\n",
    "    if 'cohort_id' in df.columns:\n",
    "        df = df.drop(columns=['cohort_id'])\n",
    "    \n",
    "    # Drop duplicates\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    # Add remove_min flag (all rows in this sheet should have visibility disabled)\n",
    "    df['remove_min'] = 1\n",
    "    \n",
    "    print(f\"  âœ“ Loaded {len(df)} products to disable min PU visibility\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PRICE HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def round_price(price):\n",
    "    \"\"\"\n",
    "    Round price to the nearest 0.25 EGP.\n",
    "    \n",
    "    MaxAB pricing system uses quarter-pound increments (0.25 EGP).\n",
    "    This function ensures all prices conform to this requirement.\n",
    "    \n",
    "    Formula: round(price * 4) / 4\n",
    "    - Multiply by 4 to convert to quarter units\n",
    "    - Round to nearest integer\n",
    "    - Divide by 4 to convert back to EGP\n",
    "    \n",
    "    Examples:\n",
    "        10.13 -> 10.25\n",
    "        10.37 -> 10.50\n",
    "        10.62 -> 10.75\n",
    "        10.87 -> 11.00\n",
    "    \n",
    "    Args:\n",
    "        price: The price value to round\n",
    "        \n",
    "    Returns:\n",
    "        Price rounded to nearest 0.25 EGP\n",
    "    \"\"\"\n",
    "    return round(price * 4) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23291/1016856075.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mprices_preparation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_prices\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdf_prices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cohort_total_stocks'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_prices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'product_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'cohort_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stocks'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DATA PREPARATION FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def prices_preparation(df_prices: pd.DataFrame, pus: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare price data for API upload by aggregating and expanding to packing units.\n",
    "    \n",
    "    This function performs a critical transformation:\n",
    "    1. Since each cohort may have multiple warehouses, we need to calculate a \n",
    "       single price per product-cohort combination\n",
    "    2. We use stock-weighted average: warehouses with more stock have more \n",
    "       influence on the final price\n",
    "    3. Then we expand to all packing units (multiply by basic_unit_count)\n",
    "    \n",
    "    Stock-Weighted Price Calculation:\n",
    "    ---------------------------------\n",
    "    For each product in a cohort:\n",
    "    - If warehouse A has 70% of stock and price 10 EGP\n",
    "    - And warehouse B has 30% of stock and price 12 EGP\n",
    "    - Final price = (0.7 * 10) + (0.3 * 12) = 10.6 EGP\n",
    "    \n",
    "    If stocks are 0 or null, we fall back to equal weights (1/num_warehouses).\n",
    "    \n",
    "    Args:\n",
    "        df_prices: DataFrame with columns:\n",
    "            - product_id: Product identifier\n",
    "            - sku: SKU name  \n",
    "            - cohort_id: Target cohort\n",
    "            - warehouse_id: Source warehouse\n",
    "            - stocks: Stock quantity (for weighting)\n",
    "            - new_price: New basic unit price\n",
    "            \n",
    "        pus: Packing units DataFrame with columns:\n",
    "            - product_id: Product identifier\n",
    "            - packing_unit_id: Packing unit ID\n",
    "            - basic_unit_count: Number of basic units in this packing unit\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns:\n",
    "            - product_id, sku, cohort_id: Identifiers\n",
    "            - final_price: Stock-weighted average price (basic unit)\n",
    "            - packing_unit_id: Packing unit ID\n",
    "            - basic_unit_count: Units per pack\n",
    "            - final_pu_price: final_price * basic_unit_count\n",
    "    \"\"\"\n",
    "    # Step 1: Calculate total stocks per product-cohort for weighting\n",
    "    df_prices['cohort_total_stocks'] = df_prices.groupby(['product_id', 'cohort_id'])['stocks'].transform('sum')\n",
    "    \n",
    "    # Step 2: Count unique warehouses per product-cohort (fallback for zero stocks)\n",
    "    df_prices['cohort_total_whs'] = df_prices.groupby(['product_id', 'cohort_id'])['warehouse_id'].transform('nunique')\n",
    "    \n",
    "    # Step 3: Calculate stock contribution (weight) for each warehouse\n",
    "    # If total stocks = 0, use equal weights (1 / number of warehouses)\n",
    "    df_prices['stocks_cntrb'] = df_prices['stocks'] / df_prices['cohort_total_stocks']\n",
    "    df_prices['stocks_cntrb'] = df_prices['stocks_cntrb'].fillna(1 / df_prices['cohort_total_whs'])\n",
    "    \n",
    "    # Step 4: Calculate weighted price contribution from each warehouse\n",
    "    df_prices['final_price'] = df_prices['stocks_cntrb'] * df_prices['new_price']\n",
    "    \n",
    "    # Step 5: Aggregate to get single price per product-cohort\n",
    "    df_final = df_prices.groupby(['product_id', 'sku', 'cohort_id'])['final_price'].sum().reset_index()\n",
    "    \n",
    "    # Step 6: Round to nearest 0.25 EGP\n",
    "    df_final['final_price'] = df_final['final_price'].apply(round_price)\n",
    "    \n",
    "    # Step 7: Expand to all packing units\n",
    "    df_final = df_final.merge(pus, on=['product_id'])\n",
    "    \n",
    "    # Step 8: Calculate packing unit price (basic price * units per pack)\n",
    "    df_final['final_pu_price'] = df_final['final_price'] * df_final['basic_unit_count']\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MAIN PUSH PRICES FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "# Mode constants for clarity\n",
    "MODE_TESTING = 'testing'  # Prepare files but DON'T upload to API\n",
    "MODE_LIVE = 'live'        # Prepare files AND upload to API\n",
    "\n",
    "def push_prices(df_prices: pd.DataFrame, pus: pd.DataFrame, \n",
    "                remove_min_pu: pd.DataFrame = None,\n",
    "                source_module: str = 'unknown',\n",
    "                mode: str = 'testing',\n",
    "                skip_cohorts: list = None) -> dict:\n",
    "    \"\"\"\n",
    "    Main entry point: Push price changes to MaxAB API.\n",
    "    \n",
    "    This function orchestrates the entire price upload workflow:\n",
    "    1. Filter to only changed prices\n",
    "    2. Prepare data (stock-weighted aggregation, packing unit expansion)\n",
    "    3. Apply visibility rules (hide min PU for multi-PU products)\n",
    "    4. Split into chunks and upload via API (only if mode='live')\n",
    "    \n",
    "    âš ï¸ IMPORTANT: Default mode is 'testing' - prices will NOT be pushed!\n",
    "    Set mode='live' to actually upload prices to the API.\n",
    "    \n",
    "    Visibility Rules:\n",
    "    -----------------\n",
    "    For products with multiple packing units (e.g., single bottle + pack of 6):\n",
    "    - The minimum PU (ind=1, smallest basic_unit_count) is hidden (Visibility=NO)\n",
    "    - This prevents retailers from buying single units when packs are available\n",
    "    - Products in remove_min_pu list also get their min PU hidden\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_prices : pd.DataFrame\n",
    "        DataFrame with price recommendations from Module 2 or 3\n",
    "        Required columns: product_id, sku, new_price, warehouse_id, cohort_id, \n",
    "                         stocks, current_price\n",
    "        \n",
    "    pus : pd.DataFrame\n",
    "        Packing units lookup table\n",
    "        Required columns: product_id, packing_unit_id, basic_unit_count\n",
    "        \n",
    "    remove_min_pu : pd.DataFrame, optional\n",
    "        Products whose min PU should always be hidden\n",
    "        Required columns: product_id\n",
    "        \n",
    "    source_module : str\n",
    "        Identifier for calling module ('module_2' or 'module_3')\n",
    "        Used in logging and file naming\n",
    "        \n",
    "    mode : str\n",
    "        'testing' (default) - Prepare and save files, but DON'T upload to API\n",
    "        'live' - Prepare files AND upload to MaxAB API\n",
    "        \n",
    "    skip_cohorts : list, optional\n",
    "        List of cohort IDs to skip (e.g., cohorts where cart rules failed)\n",
    "        Prices for these cohorts will not be pushed\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict with results:\n",
    "        - total_received: Input row count\n",
    "        - price_changes: Rows with actual price changes\n",
    "        - skipped: Rows with no change\n",
    "        - pushed: Successfully uploaded (or would be in testing mode)\n",
    "        - failed: Upload failures\n",
    "        - source_module: Which module called this\n",
    "        - timestamp: When upload completed\n",
    "        - mode: 'testing' or 'live'\n",
    "        - skipped_cohorts: Cohorts that were skipped due to cart rule failures\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validate mode parameter\n",
    "    if mode not in [MODE_TESTING, MODE_LIVE]:\n",
    "        print(f\"âš ï¸ Invalid mode '{mode}'. Using 'testing' mode.\")\n",
    "        mode = MODE_TESTING\n",
    "    \n",
    "    # Print clear mode indicator\n",
    "    print(f\"\\n{'ðŸ§ª' if mode == MODE_TESTING else 'ðŸš€'} MODE: {mode.upper()}\")\n",
    "    if mode == MODE_TESTING:\n",
    "        print(\"   Files will be prepared but NOT uploaded to API\")\n",
    "    else:\n",
    "        print(\"   Files will be prepared AND uploaded to API\")\n",
    "    \n",
    "    # Initialize skip_cohorts if not provided\n",
    "    if skip_cohorts is None:\n",
    "        skip_cohorts = []\n",
    "    \n",
    "    if skip_cohorts:\n",
    "        print(f\"   âš ï¸ Skipping cohorts (cart rules failed): {skip_cohorts}\")\n",
    "    \n",
    "    # Load disable PU visibility data from Google Sheets if not provided\n",
    "    if remove_min_pu is None:\n",
    "        remove_min_pu = get_disable_pu_visibility()\n",
    "    \n",
    "    # Initialize result tracking\n",
    "    result = {\n",
    "        'total_received': len(df_prices),\n",
    "        'price_changes': 0,\n",
    "        'pushed': 0,\n",
    "        'failed': 0,\n",
    "        'skipped': 0,\n",
    "        'source_module': source_module,\n",
    "        'timestamp': datetime.now(CAIRO_TZ).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'mode': mode,\n",
    "        'skipped_cohorts': skip_cohorts.copy()  # Cohorts skipped due to cart rule failures\n",
    "    }\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 1: Validate input\n",
    "    # =========================================================================\n",
    "    if df_prices.empty:\n",
    "        print(f\"âš ï¸ No data received from {source_module}\")\n",
    "        return result\n",
    "    \n",
    "    # Filter to only rows where price actually changed\n",
    "    df_to_push = df_prices[\n",
    "        (df_prices['new_price'].notna()) & \n",
    "        (df_prices['new_price'] != df_prices['current_price'])\n",
    "    ].copy()\n",
    "    \n",
    "    result['price_changes'] = len(df_to_push)\n",
    "    result['skipped'] = len(df_prices) - len(df_to_push)\n",
    "    \n",
    "    if df_to_push.empty:\n",
    "        print(f\"â„¹ï¸ No price changes to push from {source_module}\")\n",
    "        return result\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PUSH PRICES - Source: {source_module}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total received: {result['total_received']}\")\n",
    "    print(f\"Price changes to push: {result['price_changes']}\")\n",
    "    print(f\"Skipped (no change): {result['skipped']}\")\n",
    "    \n",
    "    # Check required columns\n",
    "    required_cols = ['product_id', 'sku', 'new_price', 'warehouse_id', 'cohort_id', 'stocks']\n",
    "    missing_cols = [c for c in required_cols if c not in df_to_push.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"âŒ ERROR: Missing required columns: {missing_cols}\")\n",
    "        result['failed'] = len(df_to_push)\n",
    "        return result\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 2: Log price change summary\n",
    "    # =========================================================================\n",
    "    print(f\"\\nPrice changes summary:\")\n",
    "    if 'current_price' in df_to_push.columns:\n",
    "        increases = len(df_to_push[df_to_push['new_price'] > df_to_push['current_price']])\n",
    "        decreases = len(df_to_push[df_to_push['new_price'] < df_to_push['current_price']])\n",
    "        print(f\"  Increases: {increases}\")\n",
    "        print(f\"  Decreases: {decreases}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 3: Prepare data (aggregate by cohort, expand to packing units)\n",
    "    # =========================================================================\n",
    "    push_data = df_to_push[required_cols].copy()\n",
    "    final_data = prices_preparation(push_data, pus)\n",
    "    \n",
    "    # Store for debugging/inspection\n",
    "    global prices_to_push\n",
    "    prices_to_push = final_data\n",
    "    \n",
    "    print(f\"\\nðŸ“‹ Prepared {len(final_data)} packing unit prices\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 4: Apply visibility rules (hide min PU for multi-PU products)\n",
    "    # =========================================================================\n",
    "    # Create index for packing unit ordering (1 = smallest/min PU)\n",
    "    final_data.sort_values(['cohort_id', 'product_id', 'basic_unit_count']).reset_index(drop=True)\n",
    "    final_data['ind'] = 1\n",
    "    final_data['ind'] = final_data.groupby(['cohort_id', 'product_id'])['ind'].cumsum()\n",
    "    \n",
    "    # Apply remove_min rules if provided\n",
    "    if remove_min_pu is not None and not remove_min_pu.empty:\n",
    "        if 'remove_min' not in remove_min_pu.columns:\n",
    "            remove_min_pu['remove_min'] = 1\n",
    "        final_data = final_data.merge(\n",
    "            remove_min_pu[['product_id', 'remove_min']], \n",
    "            on='product_id', \n",
    "            how='left'\n",
    "        )\n",
    "    else:\n",
    "        final_data['remove_min'] = np.nan\n",
    "    \n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 5: Upload to API per cohort\n",
    "    # =========================================================================\n",
    "    # Ensure output directories exist\n",
    "    os.makedirs(UPLOAD_DIR, exist_ok=True)\n",
    "    os.makedirs(MANUAL_DIR, exist_ok=True)\n",
    "    \n",
    "    total_pushed = 0\n",
    "    total_failed = 0\n",
    "    \n",
    "    for cohort in final_data.cohort_id.unique():\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Processing cohort: {cohort}\")\n",
    "        print('='*50)\n",
    "        \n",
    "        # Skip cohorts where cart rules failed\n",
    "        if cohort in skip_cohorts:\n",
    "            print(f\"  â­ï¸ SKIPPED - Cart rules failed for this cohort\")\n",
    "            continue\n",
    "        \n",
    "        # Filter to this cohort\n",
    "        cohort_data = final_data[final_data['cohort_id'] == cohort].copy()\n",
    "        \n",
    "        # Prepare output DataFrame with API-expected columns\n",
    "        out = cohort_data[['product_id', 'sku', 'packing_unit_id', 'final_pu_price', 'ind', 'remove_min']].copy()\n",
    "        out.columns = ['Product ID', 'Product Name', 'Packing Unit ID', 'Price', 'ind', 'remove_min']\n",
    "        \n",
    "        # Set visibility based on remove_min flag\n",
    "        out['Visibility (YES/NO)'] = 'YES'\n",
    "        out.loc[(out['ind'] == 1) & (out['remove_min'] == 1), 'Visibility (YES/NO)'] = 'NO'\n",
    "        \n",
    "        # Drop helper columns and duplicates\n",
    "        out = out.drop(columns=['ind', 'remove_min']).drop_duplicates()\n",
    "        \n",
    "        # Add required empty columns for API\n",
    "        out['Execute At (format:dd/mm/yyyy HH:mm)'] = None\n",
    "        out['Tags'] = None\n",
    "        \n",
    "        # Filter out invalid prices\n",
    "        out = out[out['Price'] > 1].reset_index(drop=True)\n",
    "        \n",
    "        if len(out) == 0:\n",
    "            print(f\"  No valid prices for cohort {cohort}\")\n",
    "            continue\n",
    "        \n",
    "        # Save full file for reference\n",
    "        file_name_ = f'{UPLOAD_DIR}/{source_module}_{cohort}.xlsx'\n",
    "        out.to_excel(file_name_, index=False)\n",
    "        print(f\"  Saved: {file_name_} ({len(out)} rows)\")\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # In testing mode, skip the actual API upload\n",
    "        if mode == MODE_TESTING:\n",
    "            print(f\"  ðŸ§ª [TESTING] Would upload {len(out)} prices (skipped)\")\n",
    "            total_pushed += len(out)\n",
    "            continue\n",
    "        \n",
    "        # Split into chunks for API upload\n",
    "        chunk_size = CHUNK_SIZE_SPECIAL if cohort == 61 else CHUNK_SIZE_DEFAULT\n",
    "        chunks = [out[i:i + chunk_size] for i in range(0, len(out), chunk_size)]\n",
    "        print(f\"  Split into {len(chunks)} chunks (size: {chunk_size})\")\n",
    "        \n",
    "        # Save and upload chunks\n",
    "        fileslist = []\n",
    "        for i, chunk in tqdm(enumerate(chunks), total=len(chunks), desc=\"  Saving chunks\"):\n",
    "            output_file = f'{MANUAL_DIR}/output_{cohort}_chunk_{i + 1}.xlsx'\n",
    "            fileslist.append(output_file)\n",
    "            chunk.to_excel(output_file, index=False)\n",
    "        \n",
    "        # Upload each chunk\n",
    "        print(\"  Uploading...\")\n",
    "        upload_success = True\n",
    "        \n",
    "        for file in fileslist:\n",
    "            chunk_num = file.split('chunk_')[1].split('.xls')[0]\n",
    "            response = post_prices(cohort, file)\n",
    "            \n",
    "            if '\"success\":true' in str(response.content).lower():\n",
    "                print(f\"    âœ“ Chunk {chunk_num} uploaded successfully\")\n",
    "                total_pushed += len(pd.read_excel(file))\n",
    "            else:\n",
    "                print(f\"    âœ— ERROR chunk {chunk_num}\")\n",
    "                print(f\"      Response: {response.content}\")\n",
    "                upload_success = False\n",
    "                total_failed += len(pd.read_excel(file))\n",
    "                break\n",
    "        \n",
    "        if not upload_success:\n",
    "            print(f\"  Upload failed for cohort {cohort}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 6: Final summary\n",
    "    # =========================================================================\n",
    "    result['pushed'] = total_pushed\n",
    "    result['failed'] = total_failed\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    if mode == MODE_TESTING:\n",
    "        print(\"ðŸ§ª TESTING MODE COMPLETE - NO PRICES WERE UPLOADED\")\n",
    "    else:\n",
    "        print(\"ðŸš€ UPLOAD COMPLETE\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Mode: {mode}\")\n",
    "    print(f\"Total prepared: {total_pushed}\")\n",
    "    if mode == MODE_LIVE:\n",
    "        print(f\"Total failed: {total_failed}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Global variable to store prepared prices for debugging/inspection\n",
    "prices_to_push = pd.DataFrame()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
