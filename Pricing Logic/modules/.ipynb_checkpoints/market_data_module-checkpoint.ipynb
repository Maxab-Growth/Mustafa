{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Market Data Module\n",
    "\n",
    "This module provides fresh market prices and margin tiers data on demand.\n",
    "\n",
    "**NO INPUT REQUIRED** - All data is fetched directly from Snowflake.\n",
    "\n",
    "## Functions\n",
    "- `get_market_data()` - Fetch and process market prices from all sources (Ben Soliman, Marketplace, Scrapped)\n",
    "- `get_margin_tiers()` - Calculate margin tiers for products from PRODUCT_STATISTICS\n",
    "\n",
    "## Output Columns\n",
    "\n",
    "**Market Data (from `get_market_data()`):**\n",
    "- Raw prices: `ben_soliman_price`, `final_min_price`, `final_max_price`, etc.\n",
    "- Price percentiles: `minimum`, `percentile_25`, `percentile_50`, `percentile_75`, `maximum`\n",
    "- Margin tiers: `below_market`, `market_min`, `market_25`, `market_50`, `market_75`, `market_max`, `above_market`\n",
    "\n",
    "**Margin Tiers (from `get_margin_tiers()`):**\n",
    "- `margin_tier_below`, `margin_tier_1`, `margin_tier_2`, `margin_tier_3`, `margin_tier_4`, `margin_tier_5`, `margin_tier_above_1`, `margin_tier_above_2`\n",
    "\n",
    "## Usage\n",
    "```python\n",
    "%run market_data_module.ipynb\n",
    "\n",
    "# Get market data (no input required)\n",
    "df_market = get_market_data()\n",
    "\n",
    "# Get margin tiers (no input required)\n",
    "df_margin_tiers = get_margin_tiers()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/snowflake/connector/options.py:104: UserWarning: You have an incompatible version of 'pyarrow' installed (22.0.0), please install a version that adheres to: 'pyarrow<19.0.0; extra == \"pandas\"'\n",
      "  warn_incompatible_dep(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.Renviron\n",
      "/home/ec2-user/service_account_key.json\n",
      "Market Data Module loaded at 2026-01-24 01:55:57 Cairo time\n",
      "Snowflake timezone: America/Los_Angeles\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS & CONFIGURATION\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import snowflake.connector\n",
    "import os\n",
    "\n",
    "# Import setup_environment_2 for credentials\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import setup_environment_2\n",
    "\n",
    "# Initialize environment (loads Snowflake credentials)\n",
    "setup_environment_2.initialize_env()\n",
    "\n",
    "# Cairo timezone\n",
    "CAIRO_TZ = pytz.timezone('Africa/Cairo')\n",
    "CAIRO_NOW = datetime.now(CAIRO_TZ)\n",
    "\n",
    "# =============================================================================\n",
    "# SNOWFLAKE CONNECTION\n",
    "# =============================================================================\n",
    "def query_snowflake(query):\n",
    "    \"\"\"Execute a query on Snowflake and return results as DataFrame.\"\"\"\n",
    "    con = snowflake.connector.connect(\n",
    "        user=os.environ[\"SNOWFLAKE_USERNAME\"],\n",
    "        account=os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "        password=os.environ[\"SNOWFLAKE_PASSWORD\"],\n",
    "        database=os.environ[\"SNOWFLAKE_DATABASE\"]\n",
    "    )\n",
    "    try:\n",
    "        cur = con.cursor()\n",
    "        cur.execute(\"USE WAREHOUSE COMPUTE_WH\")\n",
    "        cur.execute(query)\n",
    "        data = cur.fetchall()\n",
    "        columns = [desc[0].lower() for desc in cur.description]\n",
    "        df = pd.DataFrame(data, columns=columns)\n",
    "        # Convert decimal.Decimal to float\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == object:\n",
    "                try:\n",
    "                    df[col] = df[col].apply(lambda x: float(x) if hasattr(x, '__float__') else x)\n",
    "                except:\n",
    "                    pass\n",
    "        return df\n",
    "    finally:\n",
    "        con.close()\n",
    "\n",
    "def get_snowflake_timezone():\n",
    "    result = query_snowflake(\"SHOW PARAMETERS LIKE 'TIMEZONE'\")\n",
    "    return result['value'].iloc[0] if len(result) > 0 else \"UTC\"\n",
    "\n",
    "# Get timezone for queries\n",
    "TIMEZONE = get_snowflake_timezone()\n",
    "\n",
    "# Region-Cohort mapping\n",
    "REGION_COHORT_DF = pd.DataFrame({\n",
    "    'region': ['Cairo', 'Giza', 'Delta West', 'Delta East', \n",
    "               'Upper Egypt', 'Upper Egypt', 'Upper Egypt', 'Upper Egypt', 'Alexandria'],\n",
    "    'cohort_id': [700, 701, 703, 704, 1124, 1126, 1123, 1125, 702]\n",
    "})\n",
    "\n",
    "print(f\"Market Data Module loaded at {CAIRO_NOW.strftime('%Y-%m-%d %H:%M:%S')} Cairo time\")\n",
    "print(f\"Snowflake timezone: {TIMEZONE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market price queries defined ✓\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ALL QUERIES FOR MARKET DATA MODULE\n",
    "# =============================================================================\n",
    "# Note: TIMEZONE is set dynamically from Snowflake in the imports cell above\n",
    "\n",
    "# =============================================================================\n",
    "# 1. BEN SOLIMAN PRICES QUERY\n",
    "# =============================================================================\n",
    "BEN_SOLIMAN_QUERY = f'''\n",
    "WITH lower as (\n",
    "    select distinct product_id, new_d*bs_price as ben_soliman_price, INJECTION_DATE\n",
    "    from (\n",
    "        select maxab_product_id as product_id, INJECTION_DATE, wac1, wac_p,\n",
    "            (bs_price) as bs_price, diff, cu_price,\n",
    "            case when p1 > 1 then child_quantity else 0 end as scheck,\n",
    "            round(p1/2)*2 as p1, p2,\n",
    "            case when (ROUND(p1 / scheck) * scheck) = 0 then p1 else (ROUND(p1 / scheck) * scheck) end as new_d\n",
    "        from (\n",
    "            select sm.*, wac1, wac_p, \n",
    "                abs((bs_price)-(wac_p*maxab_basic_unit_count))/(wac_p*maxab_basic_unit_count) as diff,\n",
    "                cpc.price as cu_price, pup.child_quantity,\n",
    "                round((cu_price/bs_price)) as p1, \n",
    "                round(((bs_price)/cu_price)) as p2\n",
    "            from materialized_views.savvy_mapping sm \n",
    "            join finance.all_cogs f on f.product_id = sm.maxab_product_id \n",
    "                and CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP()) between f.from_Date and f.to_date\n",
    "            join PACKING_UNIT_PRODUCTS pu on pu.product_id = sm.maxab_product_id and pu.IS_BASIC_UNIT = 1 \n",
    "            join cohort_product_packing_units cpc on cpc.PRODUCT_PACKING_UNIT_ID = pu.id and cohort_id = 700 \n",
    "            join packing_unit_products pup on pup.product_id = sm.maxab_product_id and pup.is_basic_unit = 1  \n",
    "            where bs_price is not null \n",
    "                and INJECTION_DATE::date >= CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - 5 \n",
    "                and diff > 0.3 and p1 > 1\n",
    "        )\n",
    "    )\n",
    "    qualify max(INJECTION_DATE) over(partition by product_id) = INJECTION_DATE\n",
    "),\n",
    "\n",
    "m_bs as (\n",
    "    select z.* from (\n",
    "        select maxab_product_id as product_id, avg(bs_final_price) as ben_soliman_price, INJECTION_DATE\n",
    "        from (\n",
    "            select *, row_number() over(partition by maxab_product_id order by diff) as rnk_2 \n",
    "            from (\n",
    "                select *, (bs_final_price-wac_p)/wac_p as diff_2 \n",
    "                from (\n",
    "                    select *, bs_price/maxab_basic_unit_count as bs_final_price \n",
    "                    from (\n",
    "                        select *, row_number() over(partition by maxab_product_id, maxab_pu order by diff) as rnk \n",
    "                        from (\n",
    "                            select *, max(INJECTION_DATE::date) over(partition by maxab_product_id, maxab_pu) as max_date\n",
    "                            from (\n",
    "                                select sm.*, wac1, wac_p, \n",
    "                                    abs(bs_price-(wac_p*maxab_basic_unit_count))/(wac_p*maxab_basic_unit_count) as diff \n",
    "                                from materialized_views.savvy_mapping sm \n",
    "                                join finance.all_cogs f on f.product_id = sm.maxab_product_id \n",
    "                                    and CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP()) between f.from_Date and f.to_date\n",
    "                                where bs_price is not null \n",
    "                                    and INJECTION_DATE::date >= CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - 5 \n",
    "                                    and diff < 0.3\n",
    "                            )\n",
    "                            qualify max_date = INJECTION_DATE\n",
    "                        ) qualify rnk = 1 \n",
    "                    )\n",
    "                ) where diff_2 between -0.5 and 0.5 \n",
    "            ) qualify rnk_2 = 1 \n",
    "        ) group by all\n",
    "    ) z \n",
    "    join finance.all_cogs f on f.product_id = z.product_id \n",
    "        and CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP()) between f.from_Date and f.to_date\n",
    "    where ben_soliman_price between f.wac_p*0.8 and f.wac_p*1.3\n",
    ")\n",
    "select product_id,avg(ben_soliman_price) as ben_soliman_price\n",
    "from (\n",
    "select product_id,ben_soliman_price,INJECTION_DATE\n",
    "from (\n",
    "    select * from (\n",
    "        select *,1 as prio from m_bs \n",
    "        union all\n",
    "        select *, 2 as prio from lower\n",
    "    )\n",
    "    qualify max(INJECTION_DATE) over(partition by product_id) = INJECTION_DATE\n",
    ")\n",
    "qualify prio = min(prio)over(partition by product_id)\n",
    ")\n",
    "group by all\n",
    "'''\n",
    "\n",
    "# =============================================================================\n",
    "# 2. MARKETPLACE PRICES QUERY (with region fallback)\n",
    "# =============================================================================\n",
    "MARKETPLACE_PRICES_QUERY = f'''\n",
    "WITH MP as (\n",
    "    select region, product_id,\n",
    "        min(min_price) as min_price, min(max_price) as max_price,\n",
    "        min(mod_price) as mod_price, min(true_min) as true_min, min(true_max) as true_max\n",
    "    from (\n",
    "        select mp.region, mp.product_id, mp.pu_id,\n",
    "            min_price/BASIC_UNIT_COUNT as min_price,\n",
    "            max_price/BASIC_UNIT_COUNT as max_price,\n",
    "            mod_price/BASIC_UNIT_COUNT as mod_price,\n",
    "            TRUE_MIN_PRICE/BASIC_UNIT_COUNT as true_min,\n",
    "            TRUE_MAX_PRICE/BASIC_UNIT_COUNT as true_max\n",
    "        from materialized_views.marketplace_prices mp \n",
    "        join packing_unit_products pup on pup.product_id = mp.product_id and pup.packing_unit_id = mp.pu_id\n",
    "        join finance.all_cogs f on f.product_id = mp.product_id \n",
    "            and CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP()) between f.from_date and f.to_date\n",
    "        where least(min_price, mod_price) between wac_p*0.9 and wac_p*1.3 \n",
    "    )\n",
    "    group by all \n",
    "),\n",
    "\n",
    "region_mapping AS (\n",
    "    SELECT * FROM (VALUES\n",
    "        ('Delta East', 'Delta West'), ('Delta West', 'Delta East'),\n",
    "        ('Alexandria', 'Cairo'), ('Alexandria', 'Giza'),\n",
    "        ('Upper Egypt', 'Cairo'), ('Upper Egypt', 'Giza'),\n",
    "        ('Cairo', 'Giza'), ('Giza', 'Cairo'),\n",
    "        ('Delta West', 'Cairo'), ('Delta East', 'Cairo'),\n",
    "        ('Delta West', 'Giza'), ('Delta East', 'Giza')\n",
    "    ) AS region_mapping(region, fallback_region)\n",
    "),\n",
    "\n",
    "all_regions as (\n",
    "    SELECT * FROM (VALUES\n",
    "        ('Cairo'), ('Giza'), ('Delta West'), ('Delta East'), ('Upper Egypt'), ('Alexandria')\n",
    "    ) AS x(region)\n",
    "),\n",
    "\n",
    "full_data as (\n",
    "    select products.id as product_id, ar.region\n",
    "    from products, all_regions ar\n",
    "    where activation = 'true'\n",
    ")\n",
    "\n",
    "select region, product_id,\n",
    "    min(final_min_price) as final_min_price, \n",
    "    min(final_max_price) as final_max_price,\n",
    "    min(final_mod_price) as final_mod_price, \n",
    "    min(final_true_min) as final_true_min,\n",
    "    min(final_true_max) as final_true_max\n",
    "from (\n",
    "    SELECT distinct w.region, w.product_id,\n",
    "        COALESCE(m1.min_price, m2.min_price) AS final_min_price,\n",
    "        COALESCE(m1.max_price, m2.max_price) AS final_max_price,\n",
    "        COALESCE(m1.mod_price, m2.mod_price) AS final_mod_price,\n",
    "        COALESCE(m1.true_min, m2.true_min) AS final_true_min,\n",
    "        COALESCE(m1.true_max, m2.true_max) AS final_true_max\n",
    "    FROM full_data w\n",
    "    LEFT JOIN MP m1 ON w.region = m1.region and w.product_id = m1.product_id\n",
    "    LEFT JOIN region_mapping rm ON w.region = rm.region\n",
    "    LEFT JOIN MP m2 ON rm.fallback_region = m2.region AND w.product_id = m2.product_id\n",
    ")\n",
    "where final_min_price is not null \n",
    "group by all\n",
    "'''\n",
    "\n",
    "# =============================================================================\n",
    "# 3. SCRAPPED DATA QUERY (Competitor prices from scraping)\n",
    "# =============================================================================\n",
    "SCRAPPED_QUERY = f'''\n",
    "select product_id, region,\n",
    "    MIN(market_price) AS min_scrapped,\n",
    "    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY market_price) AS scrapped25,\n",
    "    PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY market_price) AS scrapped50,\n",
    "    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY market_price) AS scrapped75,\n",
    "    MAX(market_price) AS max_scrapped\n",
    "from (\n",
    "    select distinct cmp.*, max(date) over(partition by region, cmp.product_id, competitor) as max_date\n",
    "    from MATERIALIZED_VIEWS.CLEANED_MARKET_PRICES cmp\n",
    "    join finance.all_cogs f on f.product_id = cmp.product_id \n",
    "        and CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP()) between f.from_date and f.to_date \n",
    "    where date >= CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - 7 \n",
    "        and MARKET_PRICE between f.wac_p * 0.8 and wac_p * 1.3\n",
    "    qualify date = max_date \n",
    ")\n",
    "group by all\n",
    "'''\n",
    "\n",
    "# =============================================================================\n",
    "# 4. PRODUCT GROUPS QUERY\n",
    "# =============================================================================\n",
    "GROUPS_QUERY = '''\n",
    "SELECT * FROM materialized_views.sku_commercial_groups\n",
    "'''\n",
    "\n",
    "# =============================================================================\n",
    "# 5. SALES DATA QUERY (for NMV weighting in group processing)\n",
    "# =============================================================================\n",
    "SALES_QUERY = f'''\n",
    "SELECT DISTINCT cpc.cohort_id, pso.product_id,\n",
    "    CONCAT(products.name_ar,' ',products.size,' ',product_units.name_ar) as sku,\n",
    "    brands.name_ar as brand, categories.name_ar as cat,\n",
    "    sum(pso.total_price) as nmv\n",
    "FROM product_sales_order pso\n",
    "JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "JOIN COHORT_PRICING_CHANGES cpc ON cpc.id = pso.COHORT_PRICING_CHANGE_id\n",
    "JOIN products ON products.id = pso.product_id\n",
    "JOIN brands ON products.brand_id = brands.id \n",
    "JOIN categories ON products.category_id = categories.id\n",
    "JOIN product_units ON product_units.id = products.unit_id \n",
    "WHERE so.created_at::date BETWEEN CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - 120 \n",
    "    AND CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - 1 \n",
    "    AND so.sales_order_status_id NOT IN (7, 12)\n",
    "    AND so.channel IN ('telesales', 'retailer')\n",
    "    AND pso.purchased_item_count <> 0\n",
    "    AND cpc.cohort_id IN (700,701,702,703,704,1123,1124,1125,1126)\n",
    "GROUP BY ALL\n",
    "'''\n",
    "\n",
    "# =============================================================================\n",
    "# 6. MARGIN STATS QUERY (STD and average margins)\n",
    "# =============================================================================\n",
    "MARGIN_STATS_QUERY = f'''\n",
    "select product_id, cohort_id, \n",
    "    (0.6*product_std) + (0.3*brand_std) + (0.1*cat_std) as std, \n",
    "    avg_margin\n",
    "from (\n",
    "    select product_id, cohort_id, \n",
    "        stddev(product_margin) as product_std, \n",
    "        stddev(brand_margin) as brand_std,\n",
    "        stddev(cat_margin) as cat_std, \n",
    "        avg(product_margin) as avg_margin\n",
    "    from (\n",
    "        select distinct product_id, order_date, cohort_id,\n",
    "            (nmv-cogs_p)/nmv as product_margin, \n",
    "            (brand_nmv-brand_cogs)/brand_nmv as brand_margin,\n",
    "            (cat_nmv-cat_cogs)/cat_nmv as cat_margin\n",
    "        from (\n",
    "            SELECT DISTINCT so.created_at::date as order_date, cpc.cohort_id, pso.product_id,\n",
    "                brands.name_ar as brand, categories.name_ar as cat,\n",
    "                sum(COALESCE(f.wac_p,0) * pso.purchased_item_count * pso.basic_unit_count) as cogs_p,\n",
    "                sum(pso.total_price) as nmv,\n",
    "                sum(nmv) over(partition by order_date, cat, brand) as brand_nmv,\n",
    "                sum(cogs_p) over(partition by order_date, cat, brand) as brand_cogs,\n",
    "                sum(nmv) over(partition by order_date, cat) as cat_nmv,\n",
    "                sum(cogs_p) over(partition by order_date, cat) as cat_cogs\n",
    "            FROM product_sales_order pso\n",
    "            JOIN sales_orders so ON so.id = pso.sales_order_id   \n",
    "            JOIN COHORT_PRICING_CHANGES cpc on cpc.id = pso.cohort_pricing_change_id\n",
    "            JOIN products on products.id = pso.product_id\n",
    "            JOIN brands on products.brand_id = brands.id \n",
    "            JOIN categories ON products.category_id = categories.id\n",
    "            JOIN finance.all_cogs f ON f.product_id = pso.product_id\n",
    "                AND f.from_date::date <= so.created_at::date AND f.to_date::date > so.created_at::date\n",
    "            WHERE so.created_at::date between \n",
    "                date_trunc('month', CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - 120) \n",
    "                and CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date\n",
    "                AND so.sales_order_status_id not in (7,12)\n",
    "                AND so.channel IN ('telesales','retailer')\n",
    "                AND pso.purchased_item_count <> 0\n",
    "            GROUP BY ALL\n",
    "        )\n",
    "    ) group by all \n",
    ")\n",
    "'''\n",
    "\n",
    "# =============================================================================\n",
    "# 7. TARGET MARGINS QUERY\n",
    "# =============================================================================\n",
    "TARGET_MARGINS_QUERY = f'''\n",
    "WITH cat_brand_target as (\n",
    "    SELECT DISTINCT cat, brand, margin as target_bm\n",
    "    FROM performance.commercial_targets cplan\n",
    "    QUALIFY CASE \n",
    "        WHEN DATE_TRUNC('month', MAX(DATE) OVER()) = DATE_TRUNC('month', CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date) \n",
    "        THEN DATE_TRUNC('month', CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date)\n",
    "        ELSE DATE_TRUNC('month', CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - INTERVAL '1 month') \n",
    "    END = DATE_TRUNC('month', date)\n",
    "),\n",
    "cat_target as (\n",
    "    select cat, sum(target_bm * (target_nmv/cat_total)) as cat_target_margin\n",
    "    from (\n",
    "        select *, sum(target_nmv) over(partition by cat) as cat_total\n",
    "        from (\n",
    "            select cat, brand, avg(target_bm) as target_bm, sum(target_nmv) as target_nmv\n",
    "            from (\n",
    "                SELECT DISTINCT date, city as region, cat, brand, margin as target_bm, nmv as target_nmv\n",
    "                FROM performance.commercial_targets cplan\n",
    "                QUALIFY CASE \n",
    "                    WHEN DATE_TRUNC('month', MAX(DATE) OVER()) = DATE_TRUNC('month', CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date) \n",
    "                    THEN DATE_TRUNC('month', CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date)\n",
    "                    ELSE DATE_TRUNC('month', CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - INTERVAL '1 month') \n",
    "                END = DATE_TRUNC('month', date)\n",
    "            ) group by all\n",
    "        )\n",
    "    ) group by all \n",
    ")\n",
    "SELECT DISTINCT cbt.cat, cbt.brand, cbt.target_bm, ct.cat_target_margin\n",
    "FROM cat_brand_target cbt\n",
    "LEFT JOIN cat_target ct ON ct.cat = cbt.cat\n",
    "'''\n",
    "\n",
    "# =============================================================================\n",
    "# 8. PRODUCT BASE QUERY (WAC data)\n",
    "# =============================================================================\n",
    "PRODUCT_BASE_QUERY = f'''\n",
    "SELECT DISTINCT\n",
    "    CASE \n",
    "        WHEN cohort_id IN (700, 695) THEN 'Cairo'\n",
    "        WHEN cohort_id IN (701) THEN 'Giza'\n",
    "        WHEN cohort_id IN (704, 698) THEN 'Delta East'\n",
    "        WHEN cohort_id IN (703, 697) THEN 'Delta West'\n",
    "        WHEN cohort_id IN (696, 1123, 1124, 1125, 1126) THEN 'Upper Egypt'\n",
    "        WHEN cohort_id IN (702, 699) THEN 'Alexandria'\n",
    "    END AS region,\n",
    "    cohort_id,\n",
    "    f.product_id,\n",
    "    brands.name_ar as brand,\n",
    "    categories.name_ar as cat,\n",
    "    f.wac1,\n",
    "    f.wac_p\n",
    "FROM finance.all_cogs f\n",
    "JOIN products ON products.id = f.product_id\n",
    "JOIN brands ON products.brand_id = brands.id\n",
    "JOIN categories ON products.category_id = categories.id\n",
    "CROSS JOIN (\n",
    "    SELECT DISTINCT cohort_id \n",
    "    FROM COHORT_PRICING_CHANGES \n",
    "    WHERE cohort_id IN (700,701,702,703,704,1123,1124,1125,1126)\n",
    ") cohorts\n",
    "WHERE CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP()) BETWEEN f.from_date AND f.to_date\n",
    "    AND products.activation = 'true'\n",
    "'''\n",
    "\n",
    "# =============================================================================\n",
    "# 9. MARGIN BOUNDARIES QUERY (for margin tiers)\n",
    "# =============================================================================\n",
    "MARGIN_BOUNDARIES_QUERY = f'''\n",
    "SELECT \n",
    "    region,\n",
    "    product_id,\n",
    "    optimal_bm,\n",
    "    MIN_BOUNDARY,\n",
    "    MAX_BOUNDARY,\n",
    "    MEDIAN_BM\n",
    "FROM (\n",
    "    SELECT \n",
    "        region,\n",
    "        product_id,\n",
    "        target_bm,\n",
    "        optimal_bm,\n",
    "        MIN_BOUNDARY,\n",
    "        MAX_BOUNDARY,\n",
    "        MEDIAN_BM,\n",
    "        MAX(created_at) OVER (PARTITION BY product_id, region) AS max_date,\n",
    "        created_at\n",
    "    FROM materialized_views.PRODUCT_STATISTICS\n",
    "    WHERE created_at::DATE >= DATE_TRUNC('month', CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE - 60)\n",
    "    QUALIFY max_date = created_at\n",
    ")\n",
    "'''\n",
    "\n",
    "print(\"All queries defined ✓\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price analysis helper functions defined ✓\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def price_analysis(row):\n",
    "    \"\"\"\n",
    "    Analyze prices and calculate percentiles for a product.\n",
    "    \n",
    "    Collects prices from all sources (Ben Soliman, Marketplace, Scrapped),\n",
    "    filters for valid prices within acceptable range, and calculates percentiles.\n",
    "    \"\"\"\n",
    "    wac = row['wac_p']\n",
    "    avg_margin = row['avg_margin'] if row['avg_margin'] >= 0.01 else row['target_margin']\n",
    "    std = np.maximum(row['std'], 0.0025)\n",
    "    target_margin = row['target_margin']\n",
    "    max_marg = np.maximum(avg_margin, target_margin)\n",
    "    \n",
    "    # Collect all price points from different sources\n",
    "    price_list = [\n",
    "        row.get('ben_soliman_price'), \n",
    "        row.get('final_min_price'), \n",
    "        row.get('final_mod_price'),\n",
    "        row.get('final_max_price'), \n",
    "        row.get('final_true_min'), \n",
    "        row.get('final_true_max'),\n",
    "        row.get('min_scrapped'), \n",
    "        row.get('scrapped25'), \n",
    "        row.get('scrapped50'), \n",
    "        row.get('scrapped75'), \n",
    "        row.get('max_scrapped')\n",
    "    ]\n",
    "    \n",
    "    # Filter valid prices within acceptable range\n",
    "    valid_prices = sorted({\n",
    "        x for x in price_list \n",
    "        if x and not pd.isna(x) and x != 0 \n",
    "        and wac / (1 - (avg_margin - (10 * std))) <= x <= wac / (1 - (max_marg + 10 * std))\n",
    "        and x >= wac * (0.9 + target_margin * 0.7)\n",
    "    })\n",
    "    \n",
    "    if not valid_prices:\n",
    "        return np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "    \n",
    "    return (\n",
    "        np.min(valid_prices),\n",
    "        np.percentile(valid_prices, 25),\n",
    "        np.percentile(valid_prices, 50),\n",
    "        np.percentile(valid_prices, 75),\n",
    "        np.max(valid_prices)\n",
    "    )\n",
    "\n",
    "\n",
    "def calculate_step_bounds(row):\n",
    "    \"\"\"Calculate below/above market bounds based on price steps.\"\"\"\n",
    "    wac = row['wac_p']\n",
    "    std = row['std']\n",
    "    target_margin = row.get('target_margin', 0.05)\n",
    "    \n",
    "    prices = [\n",
    "        row['minimum'], \n",
    "        row['percentile_25'], \n",
    "        row['percentile_50'], \n",
    "        row['percentile_75'], \n",
    "        row['maximum']\n",
    "    ]\n",
    "    \n",
    "    # Calculate valid steps between price points\n",
    "    valid_steps = []\n",
    "    for i in range(len(prices) - 1):\n",
    "        step = prices[i + 1] - prices[i]\n",
    "        if (step / wac) <= std * 1.2:\n",
    "            valid_steps.append(step)\n",
    "    \n",
    "    avg_step = np.mean(valid_steps) if valid_steps else min(2 * std, 0.2 * target_margin)\n",
    "    \n",
    "    new_min = prices[0] - avg_step if (prices[0] - avg_step) >= wac else prices[0]\n",
    "    new_max = prices[-1] + avg_step if (prices[-1] + avg_step) >= wac else prices[-1]\n",
    "    \n",
    "    return new_min, new_max\n",
    "\n",
    "\n",
    "def weighted_median(series, weights):\n",
    "    \"\"\"Calculate weighted median of a series.\"\"\"\n",
    "    valid = ~series.isna() & ~weights.isna()\n",
    "    s = series[valid]\n",
    "    w = weights[valid]\n",
    "    if len(s) == 0:\n",
    "        return np.nan\n",
    "    order = np.argsort(s)\n",
    "    s, w = s.iloc[order], w.iloc[order]\n",
    "    return s.iloc[np.searchsorted(np.cumsum(w), w.sum() / 2)]\n",
    "\n",
    "\n",
    "print(\"Helper functions defined ✓\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fetching functions defined ✓\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MAIN FUNCTION 1: get_market_data()\n",
    "# =============================================================================\n",
    "# This function fetches and processes all market price data from Snowflake\n",
    "# NO INPUT REQUIRED - all data is fetched directly\n",
    "\n",
    "def get_market_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch and process all market prices from Snowflake.\n",
    "    \n",
    "    NO INPUT REQUIRED - All data is fetched directly from Snowflake.\n",
    "    \n",
    "    Process:\n",
    "    1. Fetch Ben Soliman, Marketplace, and Scrapped prices\n",
    "    2. Outer join all price sources\n",
    "    3. Add cohort IDs and supporting data (sales, margin stats, targets)\n",
    "    4. Process group-level prices (weighted median)\n",
    "    5. Apply price coverage filtering\n",
    "    6. Calculate price percentiles\n",
    "    7. Convert prices to margins\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns:\n",
    "        - cohort_id, product_id, region\n",
    "        - Raw prices: ben_soliman_price, final_min_price, etc.\n",
    "        - Percentiles: minimum, percentile_25, percentile_50, percentile_75, maximum\n",
    "        - Margins: below_market, market_min, market_25, market_50, market_75, market_max, above_market\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FETCHING MARKET DATA\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Timestamp: {datetime.now(CAIRO_TZ).strftime('%Y-%m-%d %H:%M:%S')} Cairo time\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Step 1: Fetch all raw price data\n",
    "    # =========================================================================\n",
    "    print(\"\\nStep 1: Fetching raw price data...\")\n",
    "    \n",
    "    print(\"  1.1 Ben Soliman prices...\")\n",
    "    df_ben_soliman = query_snowflake(BEN_SOLIMAN_QUERY)\n",
    "    print(f\"      Loaded {len(df_ben_soliman)} records\")\n",
    "    \n",
    "    print(\"  1.2 Marketplace prices...\")\n",
    "    df_marketplace = query_snowflake(MARKETPLACE_PRICES_QUERY)\n",
    "    df_marketplace['final_true_min'] = np.nan\n",
    "    print(f\"      Loaded {len(df_marketplace)} records\")\n",
    "    \n",
    "    print(\"  1.3 Scrapped prices...\")\n",
    "    df_scrapped = query_snowflake(SCRAPPED_QUERY)\n",
    "    print(f\"      Loaded {len(df_scrapped)} records\")\n",
    "    \n",
    "    print(\"  1.4 Product groups...\")\n",
    "    df_groups = query_snowflake(GROUPS_QUERY)\n",
    "    print(f\"      Loaded {len(df_groups)} records\")\n",
    "    \n",
    "    print(\"  1.5 Sales data (for NMV weighting)...\")\n",
    "    df_sales = query_snowflake(SALES_QUERY)\n",
    "    print(f\"      Loaded {len(df_sales)} records\")\n",
    "    \n",
    "    print(\"  1.6 Margin stats...\")\n",
    "    df_margin_stats = query_snowflake(MARGIN_STATS_QUERY)\n",
    "    print(f\"      Loaded {len(df_margin_stats)} records\")\n",
    "    \n",
    "    print(\"  1.7 Target margins...\")\n",
    "    df_targets = query_snowflake(TARGET_MARGINS_QUERY)\n",
    "    print(f\"      Loaded {len(df_targets)} records\")\n",
    "    \n",
    "    print(\"  1.8 Product base (WAC)...\")\n",
    "    df_product_base = query_snowflake(PRODUCT_BASE_QUERY)\n",
    "    print(f\"      Loaded {len(df_product_base)} records\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Step 2: Outer join all market price sources\n",
    "    # =========================================================================\n",
    "    print(\"\\nStep 2: Joining all market price sources (outer join)...\")\n",
    "    \n",
    "    # Start with marketplace prices (has region + product_id)\n",
    "    market_data = df_marketplace.copy()\n",
    "    \n",
    "    # Outer join with scrapped data (by region + product_id)\n",
    "    market_data = market_data.merge(df_scrapped, on=['region', 'product_id'], how='outer')\n",
    "    \n",
    "    # Outer join with Ben Soliman prices (by product_id only - expand to all regions)\n",
    "    all_regions = pd.DataFrame({'region': ['Cairo', 'Giza', 'Delta West', 'Delta East', 'Upper Egypt', 'Alexandria']})\n",
    "    df_ben_soliman_expanded = df_ben_soliman.merge(all_regions, how='cross')\n",
    "    \n",
    "    # Outer join with Ben Soliman\n",
    "    market_data = market_data.merge(df_ben_soliman_expanded, on=['region', 'product_id'], how='outer')\n",
    "    \n",
    "    print(f\"    Market prices base: {len(market_data)} records\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Step 3: Add cohort IDs and supporting data\n",
    "    # =========================================================================\n",
    "    print(\"\\nStep 3: Adding cohort IDs and supporting data...\")\n",
    "    \n",
    "    market_data = market_data.merge(REGION_COHORT_DF, on='region')\n",
    "    \n",
    "    # Add sales data for NMV weighting\n",
    "    market_data = market_data.merge(\n",
    "        df_sales[['cohort_id', 'product_id', 'nmv', 'sku', 'brand', 'cat']], \n",
    "        on=['cohort_id', 'product_id'], \n",
    "        how='left'\n",
    "    )\n",
    "    market_data['nmv'] = market_data['nmv'].fillna(0)\n",
    "    \n",
    "    # Merge product groups\n",
    "    market_data = market_data.merge(df_groups, on='product_id', how='left')\n",
    "    \n",
    "    # Remove duplicates\n",
    "    market_data = market_data.drop_duplicates(subset=['cohort_id', 'product_id'])\n",
    "    \n",
    "    print(f\"    Records after adding cohorts: {len(market_data)}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Step 4: Group-level price processing\n",
    "    # =========================================================================\n",
    "    print(\"\\nStep 4: Processing group-level prices...\")\n",
    "    \n",
    "    price_cols = [\n",
    "        'ben_soliman_price', 'final_min_price', 'final_max_price', 'final_mod_price', \n",
    "        'final_true_min', 'final_true_max', 'min_scrapped', 'scrapped25', \n",
    "        'scrapped50', 'scrapped75', 'max_scrapped'\n",
    "    ]\n",
    "    \n",
    "    groups_data = market_data[~market_data['group_id'].isna()].copy()\n",
    "    \n",
    "    if len(groups_data) > 0:\n",
    "        groups_data['group_nmv'] = groups_data.groupby(['group_id', 'cohort_id'])['nmv'].transform('sum')\n",
    "        groups_data['cntrb'] = (groups_data['nmv'] / groups_data['group_nmv']).fillna(1)\n",
    "        \n",
    "        # Flag if any price column is non-NaN\n",
    "        groups_data['flag_non_nan'] = groups_data[price_cols].notna().any(axis=1).astype(int)\n",
    "        \n",
    "        # Perform weighted aggregation\n",
    "        groups_agg = (\n",
    "            groups_data[groups_data['flag_non_nan'] == 1]\n",
    "            .groupby(['group_id', 'cohort_id'])\n",
    "            .apply(lambda g: pd.Series({\n",
    "                col: weighted_median(g[col], g['cntrb']) for col in price_cols\n",
    "            }))\n",
    "            .reset_index()\n",
    "        )\n",
    "        \n",
    "        # Fill missing prices with group-level prices\n",
    "        merged = market_data.merge(groups_agg, on=['group_id', 'cohort_id'], how='left', suffixes=('', '_group'))\n",
    "        for col in price_cols:\n",
    "            if f'{col}_group' in merged.columns:\n",
    "                merged[col] = merged[col].fillna(merged[f'{col}_group'])\n",
    "        \n",
    "        market_data = merged.drop(columns=[f'{c}_group' for c in price_cols if f'{c}_group' in merged.columns], errors='ignore')\n",
    "        \n",
    "        # Add missing group SKUs\n",
    "        missing_groups_skus = df_groups.merge(groups_agg, on='group_id')\n",
    "        missing_groups_skus = missing_groups_skus.merge(REGION_COHORT_DF, on='cohort_id')\n",
    "        market_data = pd.concat([market_data, missing_groups_skus])\n",
    "        market_data = market_data.drop_duplicates(subset=['cohort_id', 'product_id'], keep='first')\n",
    "    \n",
    "    print(f\"    Records after group processing: {len(market_data)}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Step 5: Add WAC and margin data\n",
    "    # =========================================================================\n",
    "    print(\"\\nStep 5: Adding WAC and margin data...\")\n",
    "    \n",
    "    # Drop nmv and re-merge sales\n",
    "    market_data = market_data.drop(columns=['nmv'], errors='ignore')\n",
    "    market_data = market_data.merge(\n",
    "        df_sales[['cohort_id', 'product_id', 'nmv']], \n",
    "        on=['cohort_id', 'product_id'], \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Add WAC from product base\n",
    "    market_data = market_data.merge(\n",
    "        df_product_base[['cohort_id', 'product_id', 'wac_p', 'brand', 'cat']].drop_duplicates(), \n",
    "        on=['cohort_id', 'product_id'], \n",
    "        how='left',\n",
    "        suffixes=('', '_base')\n",
    "    )\n",
    "    # Fill brand/cat from base if missing\n",
    "    if 'brand_base' in market_data.columns:\n",
    "        market_data['brand'] = market_data['brand'].fillna(market_data['brand_base'])\n",
    "        market_data['cat'] = market_data['cat'].fillna(market_data['cat_base'])\n",
    "        market_data = market_data.drop(columns=['brand_base', 'cat_base'], errors='ignore')\n",
    "    \n",
    "    # Add margin stats\n",
    "    market_data = market_data.merge(df_margin_stats, on=['cohort_id', 'product_id'], how='left')\n",
    "    \n",
    "    # Add target margins\n",
    "    market_data = market_data.merge(df_targets, on=['brand', 'cat'], how='left')\n",
    "    market_data['target_margin'] = market_data['target_bm'].fillna(market_data['cat_target_margin']).fillna(0)\n",
    "    market_data = market_data.drop(columns=['target_bm', 'cat_target_margin'], errors='ignore')\n",
    "    \n",
    "    # Fill NaN values with defaults\n",
    "    market_data['std'] = market_data['std'].fillna(0.01)\n",
    "    market_data['avg_margin'] = market_data['avg_margin'].fillna(0)\n",
    "    \n",
    "    # Filter out records without WAC\n",
    "    market_data = market_data[~market_data['wac_p'].isna()]\n",
    "    \n",
    "    print(f\"    Records with WAC: {len(market_data)}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Step 6: Price coverage filtering\n",
    "    # =========================================================================\n",
    "    print(\"\\nStep 6: Filtering by price coverage...\")\n",
    "    \n",
    "    market_data['ben'] = 0\n",
    "    market_data['MP'] = 0\n",
    "    market_data['sp'] = 0\n",
    "    \n",
    "    # Ben Soliman: 1 point if present\n",
    "    market_data.loc[~market_data['ben_soliman_price'].isna(), 'ben'] = 1\n",
    "    \n",
    "    # Marketplace: 1 point if single price, 3 points if range\n",
    "    market_data.loc[(market_data['final_min_price'] == market_data['final_max_price']) & \n",
    "                    (~market_data['final_min_price'].isna()), 'MP'] = 1\n",
    "    market_data.loc[(market_data['final_min_price'] != market_data['final_max_price']) & \n",
    "                    (~market_data['final_min_price'].isna()), 'MP'] = 3\n",
    "    \n",
    "    # Scrapped: 1 point if single price, 5 points if range\n",
    "    market_data.loc[(market_data['min_scrapped'] == market_data['max_scrapped']) & \n",
    "                    (~market_data['min_scrapped'].isna()), 'sp'] = 1\n",
    "    market_data.loc[(market_data['min_scrapped'] != market_data['max_scrapped']) & \n",
    "                    (~market_data['min_scrapped'].isna()), 'sp'] = 5\n",
    "    \n",
    "    market_data['total_p'] = market_data['ben'] + market_data['MP'] + market_data['sp']\n",
    "    \n",
    "    # Filter: keep only products with total_p > 2\n",
    "    market_data = market_data[market_data['total_p'] > 2]\n",
    "    \n",
    "    print(f\"    Records after price coverage filter: {len(market_data)}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Step 7: Apply price analysis\n",
    "    # =========================================================================\n",
    "    print(\"\\nStep 7: Calculating price percentiles...\")\n",
    "    \n",
    "    market_data[['minimum', 'percentile_25', 'percentile_50', 'percentile_75', 'maximum']] = \\\n",
    "        market_data.apply(price_analysis, axis=1, result_type='expand')\n",
    "    \n",
    "    # Filter out records without valid price analysis\n",
    "    market_data = market_data[~market_data['minimum'].isna()]\n",
    "    \n",
    "    # Calculate below/above market bounds\n",
    "    market_data[['below_market', 'above_market']] = market_data.apply(calculate_step_bounds, axis=1, result_type='expand')\n",
    "    \n",
    "    print(f\"    Records after price analysis: {len(market_data)}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Step 8: Convert prices to margins\n",
    "    # =========================================================================\n",
    "    print(\"\\nStep 8: Converting prices to margins...\")\n",
    "    \n",
    "    market_data['below_market'] = (market_data['below_market'] - market_data['wac_p']) / market_data['below_market']\n",
    "    market_data['market_min'] = (market_data['minimum'] - market_data['wac_p']) / market_data['minimum']\n",
    "    market_data['market_25'] = (market_data['percentile_25'] - market_data['wac_p']) / market_data['percentile_25']\n",
    "    market_data['market_50'] = (market_data['percentile_50'] - market_data['wac_p']) / market_data['percentile_50']\n",
    "    market_data['market_75'] = (market_data['percentile_75'] - market_data['wac_p']) / market_data['percentile_75']\n",
    "    market_data['market_max'] = (market_data['maximum'] - market_data['wac_p']) / market_data['maximum']\n",
    "    market_data['above_market'] = (market_data['above_market'] - market_data['wac_p']) / market_data['above_market']\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Step 9: Select output columns\n",
    "    # =========================================================================\n",
    "    market_columns = [\n",
    "        'cohort_id', 'product_id', 'region',\n",
    "        # Raw prices\n",
    "        'ben_soliman_price', \n",
    "        'final_min_price', 'final_max_price', 'final_mod_price', 'final_true_min', 'final_true_max',\n",
    "        'min_scrapped', 'scrapped25', 'scrapped50', 'scrapped75', 'max_scrapped',\n",
    "        # Price Percentiles\n",
    "        'minimum', 'percentile_25', 'percentile_50', 'percentile_75', 'maximum',\n",
    "        # Margin Tiers\n",
    "        'below_market', 'market_min', 'market_25', 'market_50', 'market_75', 'market_max', 'above_market'\n",
    "    ]\n",
    "    market_data = market_data[[c for c in market_columns if c in market_data.columns]]\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Summary\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MARKET DATA COMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total records: {len(market_data)}\")\n",
    "    print(f\"  - With marketplace prices: {(~market_data['final_min_price'].isna()).sum()}\")\n",
    "    print(f\"  - With scrapped prices: {(~market_data['min_scrapped'].isna()).sum()}\")\n",
    "    print(f\"  - With Ben Soliman prices: {(~market_data['ben_soliman_price'].isna()).sum()}\")\n",
    "    \n",
    "    return market_data\n",
    "\n",
    "\n",
    "print(\"get_market_data() function defined ✓\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MAIN FUNCTION 2: get_margin_tiers()\n",
    "# =============================================================================\n",
    "# This function fetches margin boundaries and calculates margin tiers\n",
    "# NO INPUT REQUIRED - all data is fetched directly\n",
    "\n",
    "def get_margin_tiers() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch margin boundaries and calculate margin tiers from Snowflake.\n",
    "    \n",
    "    NO INPUT REQUIRED - All data is fetched directly from Snowflake.\n",
    "    \n",
    "    Process:\n",
    "    1. Fetch margin boundaries from PRODUCT_STATISTICS\n",
    "    2. Calculate 8 margin tiers:\n",
    "       - margin_tier_below: 1 step below minimum\n",
    "       - margin_tier_1 to margin_tier_5: Within range\n",
    "       - margin_tier_above_1, margin_tier_above_2: Above maximum\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns:\n",
    "        - product_id, region, cohort_id\n",
    "        - optimal_bm, min_boundary, max_boundary, median_bm\n",
    "        - effective_min_margin, margin_step\n",
    "        - margin_tier_below, margin_tier_1, margin_tier_2, margin_tier_3,\n",
    "          margin_tier_4, margin_tier_5, margin_tier_above_1, margin_tier_above_2\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FETCHING MARGIN TIERS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Timestamp: {datetime.now(CAIRO_TZ).strftime('%Y-%m-%d %H:%M:%S')} Cairo time\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Step 1: Fetch margin boundaries\n",
    "    # =========================================================================\n",
    "    print(\"\\nStep 1: Fetching margin boundaries from PRODUCT_STATISTICS...\")\n",
    "    \n",
    "    df_margin_boundaries = query_snowflake(MARGIN_BOUNDARIES_QUERY)\n",
    "    print(f\"    Loaded {len(df_margin_boundaries)} records\")\n",
    "    \n",
    "    if len(df_margin_boundaries) == 0:\n",
    "        print(\"    ⚠️ No margin boundaries found!\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Step 2: Add cohort IDs\n",
    "    # =========================================================================\n",
    "    print(\"\\nStep 2: Adding cohort IDs...\")\n",
    "    \n",
    "    df = df_margin_boundaries.merge(REGION_COHORT_DF, on='region', how='left')\n",
    "    print(f\"    Records with cohorts: {len(df)}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Step 3: Calculate margin tiers\n",
    "    # =========================================================================\n",
    "    print(\"\\nStep 3: Calculating margin tiers...\")\n",
    "    \n",
    "    # Calculate the effective minimum margin (min of MIN_BOUNDARY and optimal_bm)\n",
    "    df['effective_min_margin'] = df[['min_boundary', 'optimal_bm']].min(axis=1)\n",
    "    \n",
    "    # Calculate step size: (max_boundary - effective_min_margin) / 4\n",
    "    df['margin_step'] = (df['max_boundary'] - df['effective_min_margin']) / 4\n",
    "    \n",
    "    # Calculate the 8 margin tiers:\n",
    "    # Below minimum (1 step below)\n",
    "    df['margin_tier_below'] = df['effective_min_margin'] - df['margin_step']\n",
    "    \n",
    "    # 5 tiers in range (equally spaced)\n",
    "    df['margin_tier_1'] = df['effective_min_margin']  # Min\n",
    "    df['margin_tier_2'] = df['effective_min_margin'] + df['margin_step']\n",
    "    df['margin_tier_3'] = df['effective_min_margin'] + 2 * df['margin_step']\n",
    "    df['margin_tier_4'] = df['effective_min_margin'] + 3 * df['margin_step']\n",
    "    df['margin_tier_5'] = df['max_boundary']  # Max\n",
    "    \n",
    "    # Above maximum (2 steps above)\n",
    "    df['margin_tier_above_1'] = df['max_boundary'] + df['margin_step']\n",
    "    df['margin_tier_above_2'] = df['max_boundary'] + 2 * df['margin_step']\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Step 4: Select output columns\n",
    "    # =========================================================================\n",
    "    output_cols = [\n",
    "        'product_id', 'region', 'cohort_id',\n",
    "        'optimal_bm', 'min_boundary', 'max_boundary', 'median_bm',\n",
    "        'effective_min_margin', 'margin_step',\n",
    "        'margin_tier_below', 'margin_tier_1', 'margin_tier_2', 'margin_tier_3',\n",
    "        'margin_tier_4', 'margin_tier_5', 'margin_tier_above_1', 'margin_tier_above_2'\n",
    "    ]\n",
    "    df = df[[c for c in output_cols if c in df.columns]]\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Summary\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MARGIN TIERS COMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total records: {len(df)}\")\n",
    "    print(f\"\\nMargin Tier Structure:\")\n",
    "    print(f\"  margin_tier_below:   effective_min - step (1 below)\")\n",
    "    print(f\"  margin_tier_1:       effective_min_margin\")\n",
    "    print(f\"  margin_tier_2:       effective_min + 1*step\")\n",
    "    print(f\"  margin_tier_3:       effective_min + 2*step\")\n",
    "    print(f\"  margin_tier_4:       effective_min + 3*step\")\n",
    "    print(f\"  margin_tier_5:       max_boundary\")\n",
    "    print(f\"  margin_tier_above_1: max_boundary + 1*step\")\n",
    "    print(f\"  margin_tier_above_2: max_boundary + 2*step\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"get_margin_tiers() function defined ✓\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODULE READY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MARKET DATA MODULE READY\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nAvailable functions (NO INPUT REQUIRED):\")\n",
    "print(\"  - get_market_data()   : Fetch and process all market prices\")\n",
    "print(\"  - get_margin_tiers()  : Fetch and calculate margin tiers\")\n",
    "print(\"\\nUsage:\")\n",
    "print(\"  %run market_data_module.ipynb\")\n",
    "print(\"  df_market = get_market_data()\")\n",
    "print(\"  df_tiers = get_margin_tiers()\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell intentionally left empty - old combined function removed\n",
    "# Use get_market_data() and get_margin_tiers() instead\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
