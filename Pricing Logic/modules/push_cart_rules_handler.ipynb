{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Push Cart Rules Handler\n",
        "\n",
        "This module receives cart rule changes from Module 2 or Module 3 and pushes them to the MaxAB API.\n",
        "\n",
        "## Workflow\n",
        "1. Receive cart rule data from Module 2 (daily reset) or Module 3 (periodic adjustments)\n",
        "2. Filter to only SKUs where cart rule actually changed\n",
        "3. Aggregate cart rules by cohort using stock-weighted average\n",
        "4. Expand to all packing units with adjusted cart rules:\n",
        "   - Min PU (basic unit): Uses the original cart rule\n",
        "   - Bigger PUs: cart_rule / basic_unit_count, rounded, then +10%\n",
        "5. Split into chunks and upload via MaxAB API\n",
        "\n",
        "## Required Input Columns\n",
        "- `product_id`: Product ID\n",
        "- `sku`: SKU name\n",
        "- `new_cart_rule`: New cart rule to set (for basic/min packing unit)\n",
        "- `warehouse_id`: Warehouse ID\n",
        "- `cohort_id`: Cohort ID for pricing\n",
        "- `stocks`: Current stock level (used for weighted aggregation)\n",
        "- `current_cart_rule`: Current cart rule (for filtering changed rules)\n",
        "\n",
        "## Output\n",
        "- Uploads cart rules sheets to MaxAB API per cohort\n",
        "- Returns summary dict with counts of pushed/failed/skipped\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# IMPORTS\n",
        "# =============================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import time\n",
        "import base64\n",
        "import os\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "# AWS for secrets management\n",
        "import boto3\n",
        "from botocore.exceptions import ClientError\n",
        "\n",
        "# HTTP requests for API calls\n",
        "import requests\n",
        "\n",
        "# Progress bar for chunk uploads\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Import setup_environment_2 for credentials\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "import setup_environment_2\n",
        "\n",
        "# Cairo timezone\n",
        "CAIRO_TZ = pytz.timezone('Africa/Cairo')\n",
        "CAIRO_NOW = datetime.now(CAIRO_TZ)\n",
        "\n",
        "# =============================================================================\n",
        "# CONFIGURATION\n",
        "# =============================================================================\n",
        "CHUNK_SIZE_DEFAULT = 4000   # Default chunk size for API uploads\n",
        "CHUNK_SIZE_SPECIAL = 2000   # Smaller chunk size for specific cohorts (e.g., cohort 61)\n",
        "UPLOAD_DIR = 'uploads_cart_rules'      # Directory to save full upload files\n",
        "MANUAL_DIR = 'manual_cart_rules'       # Directory to save chunk files\n",
        "\n",
        "# Cart rule adjustment for bigger packing units\n",
        "BIGGER_PU_MULTIPLIER = 1.10  # 10% increase for bigger packing units\n",
        "\n",
        "print(f\"Push Cart Rules Handler loaded at {CAIRO_NOW.strftime('%Y-%m-%d %H:%M:%S')} Cairo time\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# AWS & API FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "def get_secret(secret_name: str) -> str:\n",
        "    \"\"\"\n",
        "    Retrieve a secret from AWS Secrets Manager.\n",
        "    \n",
        "    This function connects to AWS Secrets Manager to fetch sensitive credentials\n",
        "    (like API keys, passwords) securely without hardcoding them in the code.\n",
        "    \n",
        "    Args:\n",
        "        secret_name: The name/path of the secret in AWS Secrets Manager\n",
        "                    (e.g., \"prod/pricing/api/\")\n",
        "    \n",
        "    Returns:\n",
        "        Secret string (JSON format) or decoded binary\n",
        "        \n",
        "    Raises:\n",
        "        ClientError: If the secret cannot be retrieved (permissions, not found, etc.)\n",
        "    \"\"\"\n",
        "    region_name = \"us-east-1\"\n",
        "    session = boto3.session.Session()\n",
        "    client = session.client(service_name='secretsmanager', region_name=region_name)\n",
        "\n",
        "    try:\n",
        "        response = client.get_secret_value(SecretId=secret_name)\n",
        "    except ClientError as e:\n",
        "        error_code = e.response['Error']['Code']\n",
        "        error_messages = {\n",
        "            'DecryptionFailureException': \"Can't decrypt secret using provided KMS key\",\n",
        "            'InternalServiceErrorException': \"Server-side error occurred\",\n",
        "            'InvalidParameterException': \"Invalid parameter value provided\",\n",
        "            'InvalidRequestException': \"Invalid request for current resource state\",\n",
        "            'ResourceNotFoundException': \"Requested resource not found\"\n",
        "        }\n",
        "        if error_code in error_messages:\n",
        "            print(f\"AWS Error: {error_messages[error_code]}\")\n",
        "        raise e\n",
        "    \n",
        "    if 'SecretString' in response:\n",
        "        return response['SecretString']\n",
        "    return base64.b64decode(response['SecretBinary'])\n",
        "\n",
        "\n",
        "def get_access_token(url: str, client_id: str, client_secret: str) -> str:\n",
        "    \"\"\"\n",
        "    Get OAuth2 access token for MaxAB API authentication.\n",
        "    \n",
        "    Uses password grant type to exchange credentials for an access token.\n",
        "    The token is used in subsequent API calls for authorization.\n",
        "    \n",
        "    Args:\n",
        "        url: The OAuth token endpoint URL\n",
        "        client_id: OAuth client ID for the application\n",
        "        client_secret: OAuth client secret for the application\n",
        "    \n",
        "    Returns:\n",
        "        Access token string to be used in Authorization header\n",
        "    \"\"\"\n",
        "    response = requests.post(\n",
        "        url,\n",
        "        data={\n",
        "            \"grant_type\": \"password\",\n",
        "            \"username\": API_USERNAME,\n",
        "            \"password\": API_PASSWORD\n",
        "        },\n",
        "        auth=(client_id, client_secret),\n",
        "    )\n",
        "    return response.json()[\"access_token\"]\n",
        "\n",
        "\n",
        "def _get_api_token() -> str:\n",
        "    \"\"\"\n",
        "    Get a fresh API token for MaxAB API requests.\n",
        "    \n",
        "    This is a convenience wrapper that calls get_access_token with\n",
        "    the correct MaxAB SSO endpoint and client credentials.\n",
        "    \n",
        "    Returns:\n",
        "        Fresh access token string\n",
        "    \"\"\"\n",
        "    return get_access_token(\n",
        "        'https://sso.maxab.info/auth/realms/maxab/protocol/openid-connect/token',\n",
        "        'main-system-externals',\n",
        "        API_SECRET\n",
        "    )\n",
        "\n",
        "\n",
        "def post_cart_rules(cohort_id: int, file_name: str) -> requests.Response:\n",
        "    \"\"\"\n",
        "    Upload a cart rules Excel sheet to MaxAB API for a specific cohort.\n",
        "    \n",
        "    This function:\n",
        "    1. Gets a fresh API token\n",
        "    2. Prepares the file for multipart upload\n",
        "    3. POSTs to the cohort cart-rules endpoint\n",
        "    \n",
        "    Args:\n",
        "        cohort_id: The target cohort ID to upload cart rules for\n",
        "        file_name: Path to the Excel file containing cart rules data\n",
        "                  Expected columns: Product ID, Packing Unit ID, Cart Rules\n",
        "    \n",
        "    Returns:\n",
        "        requests.Response object with API response\n",
        "        Check response.content for 'Cart Rules Updated Successfully!' to verify upload\n",
        "    \"\"\"\n",
        "    token = _get_api_token()\n",
        "    url = f\"https://api.maxab.info/main-system/api/admin-portal/cohorts/{cohort_id}/cart-rules\"\n",
        "    \n",
        "    files = [('sheet', (file_name, open(file_name, 'rb'), \n",
        "              'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'))]\n",
        "    headers = {'Authorization': f'bearer {token}'}\n",
        "    \n",
        "    return requests.post(url, headers=headers, data={}, files=files)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# API CREDENTIALS INITIALIZATION\n",
        "# =============================================================================\n",
        "# Load API credentials from AWS Secrets Manager\n",
        "# These are used by get_access_token() to authenticate with MaxAB API\n",
        "\n",
        "pricing_api_secret = json.loads(get_secret(\"prod/pricing/api/\"))\n",
        "API_USERNAME = pricing_api_secret[\"egypt_username\"]\n",
        "API_PASSWORD = pricing_api_secret[\"egypt_password\"]\n",
        "API_SECRET = pricing_api_secret[\"egypt_secret\"]\n",
        "\n",
        "print(\"âœ“ API credentials loaded successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# DATA PREPARATION FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "def cart_rules_preparation(df_cart_rules: pd.DataFrame, pus: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Prepare cart rule data for API upload by aggregating and expanding to packing units.\n",
        "    \n",
        "    This function performs the following transformation:\n",
        "    1. Since each cohort may have multiple warehouses, we take the MINIMUM\n",
        "       cart rule per product-cohort combination (most restrictive)\n",
        "    2. Then we expand to all packing units with DIFFERENT cart rules:\n",
        "       - Min/basic PU: Uses the original cart rule\n",
        "       - Bigger PUs: cart_rule / basic_unit_count, rounded, then +10%\n",
        "    \n",
        "    Cart Rule Calculation for Bigger Packing Units:\n",
        "    ------------------------------------------------\n",
        "    The cart rule from the main script is for the minimum packing unit (e.g., single bottle).\n",
        "    For bigger packing units (e.g., pack of 6):\n",
        "    - Divide the cart rule by basic_unit_count\n",
        "    - Round to nearest integer\n",
        "    - Increase by 10% (to give slightly more flexibility for bulk purchases)\n",
        "    \n",
        "    Example:\n",
        "    - Min PU cart rule = 30 (single bottles)\n",
        "    - Pack of 6: 30 / 6 = 5, then 5 * 1.10 = 5.5, rounded = 6 packs\n",
        "    \n",
        "    Minimum Cart Rule Across Warehouses:\n",
        "    ------------------------------------\n",
        "    For each product in a cohort, we take the MINIMUM cart rule across all warehouses.\n",
        "    This ensures the most restrictive limit applies.\n",
        "    \n",
        "    Example:\n",
        "    - Product 1, Warehouse A: cart_rule = 10\n",
        "    - Product 1, Warehouse B: cart_rule = 5\n",
        "    - Final cart rule = 5 (the minimum)\n",
        "    \n",
        "    Args:\n",
        "        df_cart_rules: DataFrame with columns:\n",
        "            - product_id: Product identifier\n",
        "            - sku: SKU name  \n",
        "            - cohort_id: Target cohort\n",
        "            - warehouse_id: Source warehouse\n",
        "            - stocks: Stock quantity (not used for cart rule aggregation)\n",
        "            - new_cart_rule: New cart rule for basic/min packing unit\n",
        "            \n",
        "        pus: Packing units DataFrame with columns:\n",
        "            - product_id: Product identifier\n",
        "            - packing_unit_id: Packing unit ID\n",
        "            - basic_unit_count: Number of basic units in this packing unit\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with columns:\n",
        "            - product_id, sku, cohort_id: Identifiers\n",
        "            - final_cart_rule: Minimum cart rule across warehouses (for min PU)\n",
        "            - packing_unit_id: Packing unit ID\n",
        "            - basic_unit_count: Units per pack\n",
        "            - final_pu_cart_rule: Adjusted cart rule for this packing unit\n",
        "    \"\"\"\n",
        "    # Step 1: Aggregate to get MINIMUM cart rule per product-cohort\n",
        "    # Take the minimum cart rule across all warehouses (most restrictive)\n",
        "    df_final = df_cart_rules.groupby(['product_id', 'sku', 'cohort_id']).agg({\n",
        "        'new_cart_rule': 'min'  # Take minimum across warehouses\n",
        "    }).reset_index()\n",
        "    \n",
        "    # Rename column for clarity\n",
        "    df_final = df_final.rename(columns={'new_cart_rule': 'final_cart_rule'})\n",
        "    \n",
        "    # Step 2: Ensure cart rule is integer\n",
        "    df_final['final_cart_rule'] = df_final['final_cart_rule'].round().astype(int)\n",
        "    \n",
        "    # Step 3: Expand to all packing units\n",
        "    df_final = df_final.merge(pus, on=['product_id'])\n",
        "    \n",
        "    # Step 4: Calculate packing unit cart rule\n",
        "    # For min PU (basic_unit_count = 1): use the original cart rule\n",
        "    # For bigger PUs: divide by basic_unit_count, multiply by 1.10, THEN round\n",
        "    df_final['final_pu_cart_rule'] = df_final.apply(\n",
        "        lambda row: row['final_cart_rule'] if row['basic_unit_count'] == 1 \n",
        "        else int(round((row['final_cart_rule'] / row['basic_unit_count']) * BIGGER_PU_MULTIPLIER)),\n",
        "        axis=1\n",
        "    )\n",
        "    \n",
        "    # Ensure cart rules are within valid range: min 2, max 150\n",
        "    df_final['final_pu_cart_rule'] = df_final['final_pu_cart_rule'].clip(lower=2, upper=150)\n",
        "    \n",
        "    return df_final\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# MAIN PUSH CART RULES FUNCTION\n",
        "# =============================================================================\n",
        "\n",
        "# Mode constants for clarity\n",
        "MODE_TESTING = 'testing'  # Prepare files but DON'T upload to API\n",
        "MODE_LIVE = 'live'        # Prepare files AND upload to API\n",
        "\n",
        "def push_cart_rules(df_cart_rules: pd.DataFrame, pus: pd.DataFrame, \n",
        "                    source_module: str = 'unknown',\n",
        "                    mode: str = 'testing') -> dict:\n",
        "    \"\"\"\n",
        "    Main entry point: Push cart rule changes to MaxAB API.\n",
        "    \n",
        "    This function orchestrates the entire cart rules upload workflow:\n",
        "    1. Filter to only changed cart rules\n",
        "    2. Prepare data (stock-weighted aggregation, packing unit expansion)\n",
        "    3. Apply cart rule adjustments for bigger packing units\n",
        "    4. Split into chunks and upload via API (only if mode='live')\n",
        "    \n",
        "    âš ï¸ IMPORTANT: Default mode is 'testing' - cart rules will NOT be pushed!\n",
        "    Set mode='live' to actually upload cart rules to the API.\n",
        "    \n",
        "    Cart Rule Logic for Packing Units:\n",
        "    -----------------------------------\n",
        "    - Min/basic PU (basic_unit_count=1): Uses original cart rule\n",
        "    - Bigger PUs: cart_rule / basic_unit_count, rounded, then +10%\n",
        "    \n",
        "    Example:\n",
        "    - Product has cart rule = 30 (for single bottles)\n",
        "    - Single bottle (basic_unit_count=1): cart = 30\n",
        "    - Pack of 6 (basic_unit_count=6): cart = round(30/6) * 1.10 = 6\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    df_cart_rules : pd.DataFrame\n",
        "        DataFrame with cart rule recommendations from Module 2 or 3\n",
        "        Required columns: product_id, sku, new_cart_rule, warehouse_id, cohort_id, \n",
        "                         stocks, current_cart_rule\n",
        "        \n",
        "    pus : pd.DataFrame\n",
        "        Packing units lookup table\n",
        "        Required columns: product_id, packing_unit_id, basic_unit_count\n",
        "        \n",
        "    source_module : str\n",
        "        Identifier for calling module ('module_2' or 'module_3')\n",
        "        Used in logging and file naming\n",
        "        \n",
        "    mode : str\n",
        "        'testing' (default) - Prepare and save files, but DON'T upload to API\n",
        "        'live' - Prepare files AND upload to MaxAB API\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    dict with results:\n",
        "        - total_received: Input row count\n",
        "        - cart_rule_changes: Rows with actual cart rule changes\n",
        "        - skipped: Rows with no change\n",
        "        - pushed: Successfully uploaded (or would be in testing mode)\n",
        "        - failed: Upload failures\n",
        "        - source_module: Which module called this\n",
        "        - timestamp: When upload completed\n",
        "        - mode: 'testing' or 'live'\n",
        "    \"\"\"\n",
        "    \n",
        "    # Validate mode parameter\n",
        "    if mode not in [MODE_TESTING, MODE_LIVE]:\n",
        "        print(f\"âš ï¸ Invalid mode '{mode}'. Using 'testing' mode.\")\n",
        "        mode = MODE_TESTING\n",
        "    \n",
        "    # Print clear mode indicator\n",
        "    print(f\"\\n{'ðŸ§ª' if mode == MODE_TESTING else 'ðŸš€'} MODE: {mode.upper()}\")\n",
        "    if mode == MODE_TESTING:\n",
        "        print(\"   Files will be prepared but NOT uploaded to API\")\n",
        "    else:\n",
        "        print(\"   Files will be prepared AND uploaded to API\")\n",
        "    \n",
        "    # Initialize result tracking\n",
        "    result = {\n",
        "        'total_received': len(df_cart_rules),\n",
        "        'cart_rule_changes': 0,\n",
        "        'pushed': 0,\n",
        "        'failed': 0,\n",
        "        'skipped': 0,\n",
        "        'source_module': source_module,\n",
        "        'timestamp': datetime.now(CAIRO_TZ).strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        'mode': mode,\n",
        "        'failed_cohorts': []  # List of cohorts that failed to upload\n",
        "    }\n",
        "    \n",
        "    # =========================================================================\n",
        "    # STEP 1: Validate input\n",
        "    # =========================================================================\n",
        "    if df_cart_rules.empty:\n",
        "        print(f\"âš ï¸ No data received from {source_module}\")\n",
        "        return result\n",
        "    \n",
        "    # Filter to only rows where cart rule actually changed\n",
        "    df_to_push = df_cart_rules[\n",
        "        (df_cart_rules['new_cart_rule'].notna()) & \n",
        "        (df_cart_rules['new_cart_rule'] != df_cart_rules['current_cart_rule'])\n",
        "    ].copy()\n",
        "    \n",
        "    result['cart_rule_changes'] = len(df_to_push)\n",
        "    result['skipped'] = len(df_cart_rules) - len(df_to_push)\n",
        "    \n",
        "    if df_to_push.empty:\n",
        "        print(f\"â„¹ï¸ No cart rule changes to push from {source_module}\")\n",
        "        return result\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"PUSH CART RULES - Source: {source_module}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Total received: {result['total_received']}\")\n",
        "    print(f\"Cart rule changes to push: {result['cart_rule_changes']}\")\n",
        "    print(f\"Skipped (no change): {result['skipped']}\")\n",
        "    \n",
        "    # Check required columns\n",
        "    required_cols = ['product_id', 'sku', 'new_cart_rule', 'warehouse_id', 'cohort_id', 'stocks']\n",
        "    missing_cols = [c for c in required_cols if c not in df_to_push.columns]\n",
        "    if missing_cols:\n",
        "        print(f\"âŒ ERROR: Missing required columns: {missing_cols}\")\n",
        "        result['failed'] = len(df_to_push)\n",
        "        return result\n",
        "    \n",
        "    # =========================================================================\n",
        "    # STEP 2: Log cart rule change summary\n",
        "    # =========================================================================\n",
        "    print(f\"\\nCart rule changes summary:\")\n",
        "    if 'current_cart_rule' in df_to_push.columns:\n",
        "        increases = len(df_to_push[df_to_push['new_cart_rule'] > df_to_push['current_cart_rule']])\n",
        "        decreases = len(df_to_push[df_to_push['new_cart_rule'] < df_to_push['current_cart_rule']])\n",
        "        print(f\"  Increases: {increases}\")\n",
        "        print(f\"  Decreases: {decreases}\")\n",
        "    \n",
        "    # =========================================================================\n",
        "    # STEP 3: Prepare data (aggregate by cohort, expand to packing units)\n",
        "    # =========================================================================\n",
        "    push_data = df_to_push[required_cols].copy()\n",
        "    final_data = cart_rules_preparation(push_data, pus)\n",
        "    \n",
        "    # Store for debugging/inspection\n",
        "    global cart_rules_to_push\n",
        "    cart_rules_to_push = final_data\n",
        "    \n",
        "    print(f\"\\nðŸ“‹ Prepared {len(final_data)} packing unit cart rules\")\n",
        "    \n",
        "    # Show sample of cart rule adjustments for different packing units\n",
        "    sample_products = final_data.groupby('product_id').filter(lambda x: len(x) > 1).head(10)\n",
        "    if not sample_products.empty:\n",
        "        print(f\"\\nSample cart rule adjustments (showing products with multiple PUs):\")\n",
        "        print(sample_products[['product_id', 'basic_unit_count', 'final_cart_rule', 'final_pu_cart_rule']].to_string(index=False))\n",
        "    \n",
        "    # =========================================================================\n",
        "    # STEP 4: Upload to API per cohort\n",
        "    # =========================================================================\n",
        "    # Ensure output directories exist\n",
        "    os.makedirs(UPLOAD_DIR, exist_ok=True)\n",
        "    os.makedirs(MANUAL_DIR, exist_ok=True)\n",
        "    \n",
        "    total_pushed = 0\n",
        "    total_failed = 0\n",
        "    \n",
        "    for cohort in final_data.cohort_id.unique():\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Processing cohort: {cohort}\")\n",
        "        print('='*50)\n",
        "        \n",
        "        # Filter to this cohort\n",
        "        cohort_data = final_data[final_data['cohort_id'] == cohort].copy()\n",
        "        \n",
        "        # Prepare output DataFrame with API-expected columns\n",
        "        # Cart rules API expects: Product ID, Packing Unit ID, Cart Rules\n",
        "        out = cohort_data[['product_id', 'packing_unit_id', 'final_pu_cart_rule']].copy()\n",
        "        out.columns = ['Product ID', 'Packing Unit ID', 'Cart Rules']\n",
        "        \n",
        "        # Drop duplicates\n",
        "        out = out.drop_duplicates()\n",
        "        \n",
        "        # Filter out invalid cart rules (min 2, max 150)\n",
        "        out = out[(out['Cart Rules'] >= 2) & (out['Cart Rules'] <= 150)].reset_index(drop=True)\n",
        "        \n",
        "        if len(out) == 0:\n",
        "            print(f\"  No valid cart rules for cohort {cohort}\")\n",
        "            continue\n",
        "        \n",
        "        # Save full file for reference\n",
        "        file_name_ = f'{UPLOAD_DIR}/{source_module}_cart_rules_{cohort}.xlsx'\n",
        "        out.to_excel(file_name_, index=False, engine='xlsxwriter')\n",
        "        print(f\"  Saved: {file_name_} ({len(out)} rows)\")\n",
        "        time.sleep(2)\n",
        "        \n",
        "        # In testing mode, skip the actual API upload\n",
        "        if mode == MODE_TESTING:\n",
        "            print(f\"  ðŸ§ª [TESTING] Would upload {len(out)} cart rules (skipped)\")\n",
        "            total_pushed += len(out)\n",
        "            continue\n",
        "        \n",
        "        # Split into chunks for API upload\n",
        "        chunk_size = CHUNK_SIZE_SPECIAL if cohort == 61 else CHUNK_SIZE_DEFAULT\n",
        "        chunks = [out[i:i + chunk_size] for i in range(0, len(out), chunk_size)]\n",
        "        print(f\"  Split into {len(chunks)} chunks (size: {chunk_size})\")\n",
        "        \n",
        "        # Save and upload chunks\n",
        "        fileslist = []\n",
        "        for i, chunk in tqdm(enumerate(chunks), total=len(chunks), desc=\"  Saving chunks\"):\n",
        "            output_file = f'{MANUAL_DIR}/cart_rules_{cohort}_chunk_{i + 1}.xlsx'\n",
        "            fileslist.append(output_file)\n",
        "            chunk.to_excel(output_file, index=False, engine='xlsxwriter')\n",
        "        \n",
        "        # Upload each chunk\n",
        "        print(\"  Uploading...\")\n",
        "        upload_success = True\n",
        "        \n",
        "        for file in fileslist:\n",
        "            chunk_num = file.split('chunk_')[1].split('.xls')[0]\n",
        "            response = post_cart_rules(cohort, file)\n",
        "            \n",
        "            if 'Cart Rules Updated Successfully!' in str(response.content):\n",
        "                print(f\"    âœ“ Chunk {chunk_num} uploaded successfully\")\n",
        "                total_pushed += len(pd.read_excel(file))\n",
        "            else:\n",
        "                print(f\"    âœ— ERROR chunk {chunk_num}\")\n",
        "                print(f\"      Response: {response.content}\")\n",
        "                upload_success = False\n",
        "                total_failed += len(pd.read_excel(file))\n",
        "                break\n",
        "        \n",
        "        if not upload_success:\n",
        "            print(f\"  Upload failed for cohort {cohort}\")\n",
        "            result['failed_cohorts'].append(cohort)\n",
        "    \n",
        "    # =========================================================================\n",
        "    # STEP 5: Final summary\n",
        "    # =========================================================================\n",
        "    result['pushed'] = total_pushed\n",
        "    result['failed'] = total_failed\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    if mode == MODE_TESTING:\n",
        "        print(\"ðŸ§ª TESTING MODE COMPLETE - NO CART RULES WERE UPLOADED\")\n",
        "    else:\n",
        "        print(\"ðŸš€ UPLOAD COMPLETE\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Mode: {mode}\")\n",
        "    print(f\"Total prepared: {total_pushed}\")\n",
        "    if mode == MODE_LIVE:\n",
        "        print(f\"Total failed: {total_failed}\")\n",
        "        if result['failed_cohorts']:\n",
        "            print(f\"Failed cohorts: {result['failed_cohorts']}\")\n",
        "    \n",
        "    return result\n",
        "\n",
        "\n",
        "# Global variable to store prepared cart rules for debugging/inspection\n",
        "cart_rules_to_push = pd.DataFrame()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
