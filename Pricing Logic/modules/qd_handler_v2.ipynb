{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Quantity Discount (QD) Handler V2\n",
        "\n",
        "Handles calculation, creation, activation, and deactivation of Quantity Discounts with improved logic.\n",
        "\n",
        "## Key Changes from V1\n",
        "1. **Priority-based selection**: High DOH SKUs (inv>10K) ranked first, then others - total 400 tier limit\n",
        "2. **Sequential performance adjustment**: Lower qty first (2x max), then boost discount (3x max)\n",
        "3. **Tier-specific thresholds**: LOW (T1:3%, T2:7%, T3:5%), HIGH (LOW+10%)\n",
        "4. **Margin-based fallbacks**: For SKUs without market/margin data\n",
        "5. **Elasticity constraint**: qty_ratio < discount_ratio (T1/T2 only, NOT T3)\n",
        "6. **More accessible wholesale**: 2x minimum multiplier (was 3x)\n",
        "\n",
        "## Usage\n",
        "```python\n",
        "%run qd_handler_v2.ipynb\n",
        "\n",
        "# Process QD with DataFrame from Module 3\n",
        "result = process_qd_v2(df_qd, dry_run=True)\n",
        "```\n",
        "\n",
        "## Input Requirements (DataFrame columns from Module 3)\n",
        "**Identifiers:**\n",
        "- `product_id`, `warehouse_id`, `cohort_id`, `sku`, `brand`, `cat`\n",
        "\n",
        "**Pricing Data:**\n",
        "- `wac_p`, `current_price`, `new_price`, `target_margin`, `min_boundary`\n",
        "\n",
        "**Market Margins:**\n",
        "- `below_market`, `market_min`, `market_25`, `market_50`, `market_75`, `market_max`, `above_market`\n",
        "\n",
        "**Margin Tiers:**\n",
        "- `margin_tier_1` through `margin_tier_above_2`\n",
        "\n",
        "**Performance Data (NEW):**\n",
        "- `responsive_doh`, `stocks`, `doh` - for DOH prioritization\n",
        "- `t1_cntrb_uth`, `t2_cntrb_uth`, `t3_cntrb_uth` - tier contribution\n",
        "- `has_active_qd` - existing QD flag\n",
        "- `mtd_qty` - for ranking\n",
        "\n",
        "**QD Configuration:**\n",
        "- `keep_qd_tiers` - List of tiers to keep, e.g., `['T1', 'T2']`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# IMPORTS & CONFIGURATION\n",
        "# =============================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from datetime import datetime, timedelta\n",
        "import pytz\n",
        "import os\n",
        "import ast\n",
        "import json\n",
        "import time\n",
        "import base64\n",
        "import boto3\n",
        "from botocore.exceptions import ClientError\n",
        "import snowflake.connector\n",
        "import sys\n",
        "\n",
        "%run queries_module.ipynb\n",
        "# Add parent directory for imports\n",
        "sys.path.insert(0, '..')\n",
        "import setup_environment_2\n",
        "from common_functions import send_file_slack\n",
        "\n",
        "# Initialize environment variables (loads Snowflake credentials)\n",
        "setup_environment_2.initialize_env()\n",
        "\n",
        "# Cairo Timezone\n",
        "CAIRO_TZ = pytz.timezone('Africa/Cairo')\n",
        "CAIRO_NOW = datetime.now(CAIRO_TZ)\n",
        "TODAY = CAIRO_NOW.date()\n",
        "\n",
        "# =============================================================================\n",
        "# SNOWFLAKE CONNECTION\n",
        "# =============================================================================\n",
        "def query_snowflake(query):\n",
        "    \"\"\"Execute a query on Snowflake and return results as DataFrame.\"\"\"\n",
        "    con = snowflake.connector.connect(\n",
        "        user=os.environ[\"SNOWFLAKE_USERNAME\"],\n",
        "        account=os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
        "        password=os.environ[\"SNOWFLAKE_PASSWORD\"],\n",
        "        database=os.environ[\"SNOWFLAKE_DATABASE\"]\n",
        "    )\n",
        "    try:\n",
        "        cur = con.cursor()\n",
        "        cur.execute(\"USE WAREHOUSE COMPUTE_WH\")\n",
        "        cur.execute(query)\n",
        "        data = cur.fetchall()\n",
        "        columns = [desc[0].lower() for desc in cur.description]\n",
        "        return pd.DataFrame(data, columns=columns)\n",
        "    finally:\n",
        "        con.close()\n",
        "\n",
        "def get_snowflake_timezone():\n",
        "    result = query_snowflake(\"SHOW PARAMETERS LIKE 'TIMEZONE'\")\n",
        "    return result['value'].iloc[0] if len(result) > 0 else \"UTC\"\n",
        "\n",
        "TIMEZONE = get_snowflake_timezone()\n",
        "\n",
        "# =============================================================================\n",
        "# AWS & API FUNCTIONS\n",
        "# =============================================================================\n",
        "def get_secret(secret_name: str) -> str:\n",
        "    \"\"\"Retrieve a secret from AWS Secrets Manager.\"\"\"\n",
        "    region_name = \"us-east-1\"\n",
        "    session = boto3.session.Session()\n",
        "    client = session.client(service_name='secretsmanager', region_name=region_name)\n",
        "\n",
        "    try:\n",
        "        response = client.get_secret_value(SecretId=secret_name)\n",
        "    except ClientError as e:\n",
        "        print(f\"AWS Error: {e}\")\n",
        "        raise e\n",
        "    \n",
        "    if 'SecretString' in response:\n",
        "        return response['SecretString']\n",
        "    return base64.b64decode(response['SecretBinary'])\n",
        "\n",
        "\n",
        "def get_access_token(url: str, client_id: str, client_secret: str) -> str:\n",
        "    \"\"\"Get OAuth2 access token for MaxAB API authentication.\"\"\"\n",
        "    response = requests.post(\n",
        "        url,\n",
        "        data={\n",
        "            'grant_type': 'password',\n",
        "            'client_id': client_id,\n",
        "            'client_secret': client_secret,\n",
        "            'username': API_USERNAME,\n",
        "            'password': API_PASSWORD\n",
        "        }\n",
        "    )\n",
        "    return response.json()['access_token']\n",
        "\n",
        "\n",
        "def _get_api_token() -> str:\n",
        "    \"\"\"Get a fresh API token for MaxAB API requests.\"\"\"\n",
        "    return get_access_token(\n",
        "        'https://sso.maxab.info/auth/realms/maxab/protocol/openid-connect/token',\n",
        "        'main-system-externals',\n",
        "        API_SECRET\n",
        "    )\n",
        "\n",
        "# =============================================================================\n",
        "# API CREDENTIALS INITIALIZATION\n",
        "# =============================================================================\n",
        "pricing_api_secret = json.loads(get_secret(\"prod/pricing/api/\"))\n",
        "API_USERNAME = pricing_api_secret[\"egypt_username\"]\n",
        "API_PASSWORD = pricing_api_secret[\"egypt_password\"]\n",
        "API_SECRET = pricing_api_secret[\"egypt_secret\"]\n",
        "\n",
        "# =============================================================================\n",
        "# API CONFIGURATION\n",
        "# =============================================================================\n",
        "QD_API_URL = 'https://api.maxab.info/commerce/api/admins/v1/quantity-discounts/'\n",
        "\n",
        "print(\"✓ QD Handler V2 initialized\")\n",
        "print(f\"  Timezone: {TIMEZONE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# V2 CONFIGURATION CONSTANTS\n",
        "# =============================================================================\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# CONVERSION THRESHOLDS (tier-specific)\n",
        "# -----------------------------------------------------------------------------\n",
        "# LOW thresholds - below this, take action to improve conversion\n",
        "T1_LOW_CONVERSION_THRESHOLD = 3   # %\n",
        "T2_LOW_CONVERSION_THRESHOLD = 7   # %\n",
        "T3_LOW_CONVERSION_THRESHOLD = 5   # %\n",
        "\n",
        "# HIGH thresholds (LOW + 10%) - above this, reduce discount to save margin\n",
        "T1_HIGH_CONVERSION_THRESHOLD = 13  # % (3 + 10)\n",
        "T2_HIGH_CONVERSION_THRESHOLD = 17  # % (7 + 10)\n",
        "T3_HIGH_CONVERSION_THRESHOLD = 15  # % (5 + 10)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# QUANTITY ADJUSTMENT (applied FIRST when conversion is low)\n",
        "# -----------------------------------------------------------------------------\n",
        "QTY_REDUCTION_PCT = 0.10          # Reduce qty by 10% each attempt\n",
        "QTY_REDUCTION_MIN = 1             # Minimum reduction of 1 unit\n",
        "MAX_QTY_LOWER_COUNT = 2           # Max 2 times to lower quantity\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# DISCOUNT ADJUSTMENT (applied AFTER qty adjustments exhausted)\n",
        "# -----------------------------------------------------------------------------\n",
        "ZERO_CONVERSION_MULTIPLIER = 1.25  # Boost discount by 25% when 0% contribution\n",
        "LOW_CONVERSION_MULTIPLIER = 1.15   # Boost discount by 15% when below threshold\n",
        "HIGH_CONVERSION_MULTIPLIER = 0.8   # Reduce discount by 20% when above HIGH\n",
        "MAX_DISC_BOOST_COUNT = 3           # Max 3 times to boost discount\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# MARGIN-BASED FALLBACK (when no market/margin tier prices available)\n",
        "# -----------------------------------------------------------------------------\n",
        "# Percentage of current margin to KEEP (give up the rest as discount)\n",
        "FALLBACK_T1_MARGIN_KEEP = 0.90   # Keep 90% margin, give up 10%\n",
        "FALLBACK_T2_MARGIN_KEEP = 0.75   # Keep 75% margin, give up 25%\n",
        "FALLBACK_T3_MARGIN_KEEP = 0.60   # Keep 60% margin, give up 40%\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# DISCOUNT BOUNDS\n",
        "# -----------------------------------------------------------------------------\n",
        "MAX_DISCOUNT_PCT = 5.0    # Maximum allowed discount from current price\n",
        "MIN_DISCOUNT_PCT = 0.35   # Minimum required discount from current price\n",
        "\n",
        "# Elasticity ratio constraints (T1/T2 ONLY - T3 exempt)\n",
        "MIN_RATIO = 1.1  # Minimum elasticity ratio\n",
        "MAX_RATIO = 3.0  # Maximum elasticity ratio\n",
        "\n",
        "# Minimum gap between tier prices\n",
        "MIN_GAP_PCT = 0.25\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# WHOLESALE (TIER 3) CONFIGURATION\n",
        "# -----------------------------------------------------------------------------\n",
        "WS_CAR_COST = 1400           # Cost per delivery (EGP)\n",
        "WS_CAR_CAPACITY_TONS = 1.8   # Max car capacity in tons\n",
        "WS_MAX_TICKET_SIZE = 35000   # Maximum ticket size (EGP)\n",
        "WS_MIN_MARGIN = -0.02        # Minimum margin (-2%) above WAC\n",
        "WS_MIN_MULTIPLIER = 2        # Minimum multiplier (was 3 in V1)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# HIGH DOH PRIORITIZATION\n",
        "# -----------------------------------------------------------------------------\n",
        "HIGH_DOH_THRESHOLD = 30              # Days on hand threshold\n",
        "HIGH_DOH_INVENTORY_THRESHOLD = 10000 # EGP - minimum inventory value for priority\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# SELECTION LIMITS\n",
        "# -----------------------------------------------------------------------------\n",
        "TOP_TIERS_PER_WAREHOUSE = 400  # Total tier entries per warehouse (not SKUs)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# UPLOAD FORMAT CONFIGURATION\n",
        "# -----------------------------------------------------------------------------\n",
        "MAX_GROUP_SIZE = 200         # Max items per discount group in API\n",
        "MAX_DISCOUNT_CAP_T1 = 4.0    # Maximum discount cap for Tier 1\n",
        "MAX_DISCOUNT_CAP_T2 = 5.0    # Maximum discount cap for Tier 2\n",
        "MAX_DISCOUNT_CAP_WS = 6.0    # Maximum discount cap for Wholesale\n",
        "\n",
        "# QD Duration\n",
        "QD_DURATION_HOURS = 14      # QD valid for 14 hours\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# OUTPUT DIRECTORY CONFIGURATION\n",
        "# -----------------------------------------------------------------------------\n",
        "QD_OUTPUT_DIR = 'qd_uploads_v2'  # Dedicated directory for QD handler V2 output\n",
        "os.makedirs(QD_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# WAREHOUSE TO TAG ID MAPPING\n",
        "# -----------------------------------------------------------------------------\n",
        "WAREHOUSE_TAG_MAPPING = {\n",
        "    501: {'name': 'Assiut FC', 'tag_id': 3301},\n",
        "    401: {'name': 'Bani sweif', 'tag_id': 3302},\n",
        "    236: {'name': 'Barageel', 'tag_id': 3303},\n",
        "    337: {'name': 'El-Mahala', 'tag_id': 3304},\n",
        "    797: {'name': 'Khorshed Alex', 'tag_id': 3305},\n",
        "    339: {'name': 'Mansoura FC', 'tag_id': 3306},\n",
        "    703: {'name': 'Menya Samalot', 'tag_id': 3307},\n",
        "    1: {'name': 'Mostorod', 'tag_id': 3308},\n",
        "    962: {'name': 'Sakkarah', 'tag_id': 3309},\n",
        "    170: {'name': 'Sharqya', 'tag_id': 3310},\n",
        "    632: {'name': 'Sohag', 'tag_id': 3311},\n",
        "    8: {'name': 'Tanta', 'tag_id': 3312},\n",
        "    38: {'name': 'El-Marg', 'tag_id': 3313},\n",
        "}\n",
        "\n",
        "print(\"✓ V2 Configuration loaded:\")\n",
        "print(f\"  LOW thresholds: T1={T1_LOW_CONVERSION_THRESHOLD}%, T2={T2_LOW_CONVERSION_THRESHOLD}%, T3={T3_LOW_CONVERSION_THRESHOLD}%\")\n",
        "print(f\"  HIGH thresholds: T1={T1_HIGH_CONVERSION_THRESHOLD}%, T2={T2_HIGH_CONVERSION_THRESHOLD}%, T3={T3_HIGH_CONVERSION_THRESHOLD}%\")\n",
        "print(f\"  Qty reduction: {QTY_REDUCTION_PCT*100}% (max {MAX_QTY_LOWER_COUNT}x)\")\n",
        "print(f\"  Disc boost: x{LOW_CONVERSION_MULTIPLIER}/x{ZERO_CONVERSION_MULTIPLIER} (max {MAX_DISC_BOOST_COUNT}x)\")\n",
        "print(f\"  Wholesale min multiplier: {WS_MIN_MULTIPLIER}x\")\n",
        "print(f\"  High DOH priority: DOH>{HIGH_DOH_THRESHOLD} AND inv>{HIGH_DOH_INVENTORY_THRESHOLD}\")\n",
        "print(f\"  Selection limit: {TOP_TIERS_PER_WAREHOUSE} tiers/warehouse\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# DATA FETCHING: PACKING UNITS & TIER QUANTITIES\n",
        "# =============================================================================\n",
        "\n",
        "def get_top_selling_packing_units(product_warehouse_list: list) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Get the top-selling packing unit per product per warehouse (last 90 days).\n",
        "    \n",
        "    Args:\n",
        "        product_warehouse_list: List of (product_id, warehouse_id) tuples\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame with product_id, warehouse_id, packing_unit_id, basic_unit_count\n",
        "    \"\"\"\n",
        "    if not product_warehouse_list:\n",
        "        return pd.DataFrame(columns=['product_id', 'warehouse_id', 'packing_unit_id', 'basic_unit_count'])\n",
        "    \n",
        "    tuples_str = ','.join([f\"({int(p)}, {int(w)})\" for p, w in product_warehouse_list])\n",
        "    \n",
        "    query = f'''\n",
        "    WITH input_products AS (\n",
        "        SELECT product_id, warehouse_id\n",
        "        FROM (VALUES {tuples_str}) AS x(product_id, warehouse_id)\n",
        "    ),\n",
        "    \n",
        "    pack_products as(\n",
        "        select product_id,packing_unit_id,basic_unit_count,max(deleted_at) as deleted_at\n",
        "        from packing_unit_products pup \n",
        "        group by all\n",
        "    ),\n",
        "    \n",
        "    sales_by_pu AS (\n",
        "        SELECT \n",
        "            pso.product_id,\n",
        "            so.warehouse_id,\n",
        "            pso.packing_unit_id,\n",
        "            SUM(pso.total_price) as nmv\n",
        "        FROM product_sales_order pso\n",
        "        JOIN sales_orders so ON so.id = pso.sales_order_id\n",
        "        JOIN input_products ip ON ip.product_id = pso.product_id AND ip.warehouse_id = so.warehouse_id\n",
        "        WHERE so.created_at >= CURRENT_DATE - 90\n",
        "            AND so.sales_order_status_id NOT IN (7, 12)\n",
        "            AND so.channel IN ('telesales', 'retailer')\n",
        "            AND pso.purchased_item_count <> 0\n",
        "        GROUP BY 1, 2, 3\n",
        "    ),\n",
        "    \n",
        "    ranked_pu AS (\n",
        "        SELECT \n",
        "            s.product_id, \n",
        "            s.warehouse_id, \n",
        "            s.packing_unit_id,\n",
        "            pup.basic_unit_count,\n",
        "            s.nmv,\n",
        "            ROW_NUMBER() OVER (PARTITION BY s.product_id, s.warehouse_id ORDER BY s.nmv DESC) as rnk\n",
        "        FROM sales_by_pu s\n",
        "        JOIN pack_products pup \n",
        "            ON pup.product_id = s.product_id \n",
        "            AND pup.packing_unit_id = s.packing_unit_id\n",
        "        WHERE pup.deleted_at IS NULL \n",
        "    )\n",
        "    \n",
        "    SELECT product_id, warehouse_id, packing_unit_id, basic_unit_count\n",
        "    FROM ranked_pu\n",
        "    WHERE rnk = 1\n",
        "    '''\n",
        "    \n",
        "    print(\"  Fetching top-selling packing units (last 90 days)...\")\n",
        "    df = query_snowflake(query)\n",
        "    \n",
        "    for col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='ignore')\n",
        "    \n",
        "    print(f\"    Found packing units for {len(df)} product-warehouse combinations\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def get_tier_quantities(product_warehouse_pu_list: list) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calculate tier quantities based on historical order data.\n",
        "    \n",
        "    Args:\n",
        "        product_warehouse_pu_list: List of (warehouse_id, product_id, packing_unit_id) tuples\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame with tier_1_qty, tier_2_qty per product-warehouse\n",
        "    \"\"\"\n",
        "    if not product_warehouse_pu_list:\n",
        "        return pd.DataFrame(columns=['warehouse_id', 'product_id', 'packing_unit_id', 'tier_1_qty', 'tier_2_qty'])\n",
        "    \n",
        "    tuples_str = ','.join([f\"({int(w)}, {int(p)}, {int(pu)})\" for w, p, pu in product_warehouse_pu_list])\n",
        "    \n",
        "    query = f'''\n",
        "    WITH selected_products AS (\n",
        "        SELECT warehouse_id, product_id, packing_unit_id\n",
        "        FROM (VALUES {tuples_str}) AS x(warehouse_id, product_id, packing_unit_id)\n",
        "    ),\n",
        "    \n",
        "    base AS (\n",
        "        SELECT *, ROW_NUMBER() OVER (PARTITION BY retailer_id ORDER BY priority) as rnk \n",
        "        FROM (\n",
        "            SELECT x.*, TAGGABLE_ID as retailer_id \n",
        "            FROM (\n",
        "                SELECT id as cohort_id, name as cohort_name, priority, dynamic_tag_id \n",
        "                FROM cohorts \n",
        "                WHERE is_active = 'true'\n",
        "                    AND id IN (700,701,702,703,704,1123,1124,1125,1126)\n",
        "            ) x \n",
        "            JOIN DYNAMIC_TAGgables dt ON x.dynamic_tag_id = dt.dynamic_tag_id\n",
        "            WHERE dt.taggable_id not IN (\n",
        "                SELECT taggable_id FROM DYNAMIC_TAGgables \n",
        "                WHERE dynamic_tag_id IN (2807, 2808, 2809, 2810, 2811, 2812)\n",
        "            )\n",
        "        )\n",
        "        QUALIFY rnk = 1 \n",
        "    ),\n",
        "    \n",
        "    warehouse_mapping AS (\n",
        "        SELECT * FROM (VALUES\n",
        "            ('Cairo', 'Mostorod', 1),\n",
        "            ('Giza', 'Barageel', 236),\n",
        "            ('Giza', 'Sakkarah', 962),\n",
        "            ('Delta West', 'El-Mahala', 337),\n",
        "            ('Delta West', 'Tanta', 8),\n",
        "            ('Delta East', 'Mansoura FC', 339),\n",
        "            ('Delta East', 'Sharqya', 170),\n",
        "            ('Upper Egypt', 'Assiut FC', 501),\n",
        "            ('Upper Egypt', 'Bani sweif', 401),\n",
        "            ('Upper Egypt', 'Menya Samalot', 703),\n",
        "            ('Upper Egypt', 'Sohag', 632),\n",
        "            ('Alexandria', 'Khorshed Alex', 797)\n",
        "        ) x(region_name, wh, warehouse_id)\n",
        "    ),\n",
        "    \n",
        "    raw_order_quantities AS (\n",
        "        SELECT \n",
        "            whs.warehouse_id,\n",
        "            pso.product_id,\n",
        "            pso.packing_unit_id,\n",
        "            so.parent_sales_order_id,\n",
        "            so.retailer_id,\n",
        "            so.created_at::date as order_date,\n",
        "            SUM(pso.purchased_item_count) as order_qty,\n",
        "            EXP(-0.02 * DATEDIFF('day', so.created_at::date, CURRENT_DATE)) as recency_weight\n",
        "        FROM product_sales_order pso\n",
        "        JOIN sales_orders so ON so.id = pso.sales_order_id\n",
        "        JOIN base ON base.retailer_id = so.retailer_id\n",
        "        JOIN materialized_views.retailer_polygon ON materialized_views.retailer_polygon.retailer_id = so.retailer_id\n",
        "        JOIN districts ON districts.id = materialized_views.retailer_polygon.district_id\n",
        "        JOIN cities ON cities.id = districts.city_id\n",
        "        JOIN states ON states.id = cities.state_id\n",
        "        JOIN regions ON regions.id = states.region_id\n",
        "        JOIN warehouse_mapping whs ON whs.region_name = CASE WHEN regions.id = 2 THEN states.name_en ELSE regions.name_en END\n",
        "        JOIN selected_products sp ON sp.warehouse_id = whs.warehouse_id \n",
        "            AND sp.product_id = pso.product_id\n",
        "            AND sp.packing_unit_id = pso.packing_unit_id\n",
        "        WHERE TRUE\n",
        "            AND so.created_at::date BETWEEN DATE_TRUNC('month', CURRENT_DATE - INTERVAL '4 months') AND CURRENT_DATE - 1\n",
        "            AND so.sales_order_status_id NOT IN (7, 12)\n",
        "            AND so.channel IN ('telesales', 'retailer')\n",
        "            AND pso.purchased_item_count <> 0\n",
        "        GROUP BY 1, 2, 3, 4, 5, 6\n",
        "    ),\n",
        "    \n",
        "    retailer_frequency AS (\n",
        "        SELECT \n",
        "            warehouse_id, product_id, packing_unit_id, retailer_id,\n",
        "            COUNT(DISTINCT parent_sales_order_id) as order_count,\n",
        "            COUNT(DISTINCT DATE_TRUNC('week', order_date)) as weeks_ordered\n",
        "        FROM raw_order_quantities\n",
        "        GROUP BY 1, 2, 3, 4\n",
        "    ),\n",
        "    \n",
        "    frequent_buyers AS (\n",
        "        SELECT warehouse_id, product_id, packing_unit_id, retailer_id\n",
        "        FROM retailer_frequency\n",
        "        WHERE order_count >= 2 OR weeks_ordered >= 2\n",
        "    ),\n",
        "    \n",
        "    filtered_orders AS (\n",
        "        SELECT roq.*\n",
        "        FROM raw_order_quantities roq\n",
        "        JOIN frequent_buyers fb ON fb.warehouse_id = roq.warehouse_id\n",
        "            AND fb.product_id = roq.product_id\n",
        "            AND fb.packing_unit_id = roq.packing_unit_id\n",
        "            AND fb.retailer_id = roq.retailer_id\n",
        "    ),\n",
        "    \n",
        "    initial_stats AS (\n",
        "        SELECT \n",
        "            warehouse_id, product_id, packing_unit_id,\n",
        "            PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY order_qty) as q1,\n",
        "            PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY order_qty) as q3,\n",
        "            MEDIAN(order_qty) as median_qty,\n",
        "            STDDEV_POP(order_qty) as stddev_qty,\n",
        "            AVG(order_qty) as avg_qty\n",
        "        FROM filtered_orders\n",
        "        GROUP BY 1, 2, 3\n",
        "    ),\n",
        "    \n",
        "    cleaned_orders AS (\n",
        "        SELECT fo.*\n",
        "        FROM filtered_orders fo\n",
        "        JOIN initial_stats ist ON ist.warehouse_id = fo.warehouse_id\n",
        "            AND ist.product_id = fo.product_id\n",
        "            AND ist.packing_unit_id = fo.packing_unit_id\n",
        "        WHERE fo.order_qty >= ist.q1 - 1.5 * (ist.q3 - ist.q1)\n",
        "            AND fo.order_qty <= ist.q3 + 1.5 * (ist.q3 - ist.q1)\n",
        "            AND (ist.stddev_qty = 0 OR ABS(fo.order_qty - ist.avg_qty) <= 3 * ist.stddev_qty)\n",
        "    ),\n",
        "    \n",
        "    recent_trends AS (\n",
        "        SELECT \n",
        "            warehouse_id, product_id, packing_unit_id,\n",
        "            SUM(order_qty * recency_weight) / NULLIF(SUM(recency_weight), 0) as weighted_avg_qty,\n",
        "            AVG(CASE WHEN order_date >= CURRENT_DATE - 15 THEN order_qty END) as last_15d_avg,\n",
        "            MEDIAN(CASE WHEN order_date >= CURRENT_DATE - 15 THEN order_qty END) as last_15d_median,\n",
        "            MAX(CASE WHEN order_date >= CURRENT_DATE - 15 THEN order_qty END) as last_15d_max,\n",
        "            COUNT(CASE WHEN order_date >= CURRENT_DATE - 15 THEN 1 END) as last_15d_orders\n",
        "        FROM cleaned_orders\n",
        "        GROUP BY 1, 2, 3\n",
        "    ),\n",
        "    \n",
        "    quantity_stats AS (\n",
        "        SELECT \n",
        "            warehouse_id, product_id, packing_unit_id,\n",
        "            COUNT(DISTINCT parent_sales_order_id) as total_orders,\n",
        "            MEDIAN(order_qty) as median_qty,\n",
        "            STDDEV_POP(order_qty) as stddev_qty,\n",
        "            PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY order_qty) as q3_qty,\n",
        "            PERCENTILE_CONT(0.85) WITHIN GROUP (ORDER BY order_qty) as p85_qty,\n",
        "            PERCENTILE_CONT(0.90) WITHIN GROUP (ORDER BY order_qty) as p90_qty,\n",
        "            PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY order_qty) as p95_qty\n",
        "        FROM cleaned_orders\n",
        "        GROUP BY 1, 2, 3\n",
        "    ),\n",
        "    \n",
        "    most_frequent_qty AS (\n",
        "        SELECT warehouse_id, product_id, packing_unit_id, order_qty as mode_qty\n",
        "        FROM (\n",
        "            SELECT warehouse_id, product_id, packing_unit_id, order_qty,\n",
        "                   COUNT(*) as freq,\n",
        "                   ROW_NUMBER() OVER (PARTITION BY warehouse_id, product_id, packing_unit_id ORDER BY COUNT(*) DESC, order_qty DESC) as rn\n",
        "            FROM cleaned_orders\n",
        "            GROUP BY 1, 2, 3, 4\n",
        "        )\n",
        "        WHERE rn = 1\n",
        "    ),\n",
        "    \n",
        "    tier_calculations AS (\n",
        "        SELECT \n",
        "            qs.warehouse_id, qs.product_id, qs.packing_unit_id,\n",
        "            qs.median_qty, qs.stddev_qty, qs.q3_qty, qs.p85_qty, qs.p90_qty, qs.p95_qty,\n",
        "            COALESCE(mf.mode_qty, qs.median_qty) as mode_qty,\n",
        "            rt.weighted_avg_qty, rt.last_15d_median, rt.last_15d_max, rt.last_15d_orders,\n",
        "            \n",
        "            CEIL(GREATEST(\n",
        "                (0.7 * qs.median_qty + 0.3 * COALESCE(rt.weighted_avg_qty, qs.median_qty)) + 1.0 * COALESCE(qs.stddev_qty, 1),\n",
        "                qs.q3_qty,\n",
        "                COALESCE(mf.mode_qty, qs.median_qty) + GREATEST(3, qs.median_qty * 0.3),\n",
        "                CASE \n",
        "                    WHEN rt.last_15d_orders >= 2 AND rt.last_15d_median > qs.median_qty \n",
        "                    THEN rt.last_15d_median * 1.1\n",
        "                    ELSE qs.median_qty * 1.1\n",
        "                END,\n",
        "                qs.median_qty + 2\n",
        "            )) as tier_1_qty,\n",
        "            \n",
        "            CEIL(GREATEST(\n",
        "                qs.q3_qty + 1.5 * COALESCE(qs.stddev_qty, 1),\n",
        "                qs.p85_qty + 1.0 * COALESCE(qs.stddev_qty, 1),\n",
        "                qs.p90_qty + 0.5 * COALESCE(qs.stddev_qty, 1),\n",
        "                qs.p95_qty,\n",
        "                (0.6 * qs.median_qty + 0.4 * COALESCE(rt.weighted_avg_qty, qs.median_qty)) * 2.0,\n",
        "                CASE \n",
        "                    WHEN rt.last_15d_orders >= 2 AND rt.last_15d_max > qs.p90_qty \n",
        "                    THEN rt.last_15d_max * 1.1\n",
        "                    ELSE qs.median_qty * 1.6\n",
        "                END\n",
        "            )) as tier_2_qty_base\n",
        "            \n",
        "        FROM quantity_stats qs\n",
        "        LEFT JOIN most_frequent_qty mf ON mf.warehouse_id = qs.warehouse_id \n",
        "            AND mf.product_id = qs.product_id AND mf.packing_unit_id = qs.packing_unit_id\n",
        "        LEFT JOIN recent_trends rt ON rt.warehouse_id = qs.warehouse_id\n",
        "            AND rt.product_id = qs.product_id AND rt.packing_unit_id = qs.packing_unit_id\n",
        "    )\n",
        "    \n",
        "    SELECT \n",
        "        warehouse_id, product_id, packing_unit_id,\n",
        "        tier_1_qty,\n",
        "        LEAST(\n",
        "            CEIL(GREATEST(tier_2_qty_base, tier_1_qty * 1.6)),\n",
        "            GREATEST(tier_1_qty * 3.5, tier_1_qty + 20)\n",
        "        ) as tier_2_qty,\n",
        "        median_qty, stddev_qty\n",
        "    FROM tier_calculations\n",
        "    '''\n",
        "    \n",
        "    print(\"  Calculating tier quantities from order history...\")\n",
        "    df = query_snowflake(query)\n",
        "    \n",
        "    for col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='ignore')\n",
        "    \n",
        "    print(f\"    Calculated tiers for {len(df)} product-warehouse combinations\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def get_warehouse_ticket_stats() -> pd.DataFrame:\n",
        "    \"\"\"Get warehouse-level ticket size statistics for wholesale calculations.\"\"\"\n",
        "    query = f'''\n",
        "    WITH base AS (\n",
        "        SELECT *, ROW_NUMBER() OVER (PARTITION BY retailer_id ORDER BY priority) as rnk \n",
        "        FROM (\n",
        "            SELECT x.*, TAGGABLE_ID as retailer_id \n",
        "            FROM (\n",
        "                SELECT id as cohort_id, name as cohort_name, priority, dynamic_tag_id \n",
        "                FROM cohorts \n",
        "                WHERE is_active = 'true'\n",
        "                    AND id IN (700,701,702,703,704,1123,1124,1125,1126)\n",
        "            ) x \n",
        "            JOIN DYNAMIC_TAGgables dt ON x.dynamic_tag_id = dt.dynamic_tag_id\n",
        "            WHERE dt.taggable_id not IN (\n",
        "                SELECT taggable_id FROM DYNAMIC_TAGgables \n",
        "                WHERE dynamic_tag_id IN (2807, 2808, 2809, 2810, 2811, 2812)\n",
        "            )\n",
        "        )\n",
        "        QUALIFY rnk = 1 \n",
        "    ),\n",
        "\n",
        "    whs AS (\n",
        "        SELECT * FROM (VALUES\n",
        "            ('Cairo', 'El-Marg', 38),\n",
        "            ('Cairo', 'Mostorod', 1),\n",
        "            ('Giza', 'Barageel', 236),\n",
        "            ('Giza', 'Sakkarah', 962),\n",
        "            ('Delta West', 'El-Mahala', 337),\n",
        "            ('Delta West', 'Tanta', 8),\n",
        "            ('Delta East', 'Mansoura FC', 339),\n",
        "            ('Delta East', 'Sharqya', 170),\n",
        "            ('Upper Egypt', 'Assiut FC', 501),\n",
        "            ('Upper Egypt', 'Bani sweif', 401),\n",
        "            ('Upper Egypt', 'Menya Samalot', 703),\n",
        "            ('Upper Egypt', 'Sohag', 632),\n",
        "            ('Alexandria', 'Khorshed Alex', 797)\n",
        "        ) x(region_name, wh, warehouse_id)\n",
        "    ),\n",
        "\n",
        "    ticket_sizes AS (\n",
        "        SELECT \n",
        "            whs.warehouse_id,\n",
        "            whs.wh as warehouse_name,\n",
        "            so.parent_sales_order_id,\n",
        "            so.retailer_id,\n",
        "            SUM(pso.total_price) as ticket_size,\n",
        "            SUM(pso.purchased_item_count * pup.weight / 1000) as order_weight_kg\n",
        "        FROM product_sales_order pso\n",
        "        JOIN sales_orders so ON so.id = pso.sales_order_id\n",
        "        JOIN base ON base.retailer_id = so.retailer_id\n",
        "        JOIN packing_unit_products pup ON pup.product_id = pso.product_id \n",
        "            AND pup.packing_unit_id = pso.packing_unit_id\n",
        "        JOIN materialized_views.retailer_polygon rp ON rp.retailer_id = so.retailer_id\n",
        "        JOIN districts ON districts.id = rp.district_id\n",
        "        JOIN cities ON cities.id = districts.city_id\n",
        "        JOIN states ON states.id = cities.state_id\n",
        "        JOIN regions ON regions.id = states.region_id\n",
        "        JOIN whs ON whs.region_name = CASE WHEN regions.id = 2 THEN states.name_en ELSE regions.name_en END\n",
        "        WHERE so.created_at::date BETWEEN DATE_TRUNC('month', CURRENT_DATE - INTERVAL '4 months') AND CURRENT_DATE - 1\n",
        "            AND so.sales_order_status_id NOT IN (7, 12)\n",
        "            AND so.channel IN ('telesales', 'retailer')\n",
        "            AND pso.purchased_item_count > 0\n",
        "        GROUP BY whs.warehouse_id, whs.wh, so.parent_sales_order_id, so.retailer_id\n",
        "    ),\n",
        "\n",
        "    warehouse_stats AS (\n",
        "        SELECT \n",
        "            warehouse_id,\n",
        "            warehouse_name,\n",
        "            COUNT(DISTINCT parent_sales_order_id) as total_orders,\n",
        "            COUNT(DISTINCT retailer_id) as total_retailers,\n",
        "            AVG(ticket_size) as avg_ticket_size,\n",
        "            MEDIAN(ticket_size) as median_ticket_size,\n",
        "            AVG(order_weight_kg) as avg_order_weight_kg\n",
        "        FROM ticket_sizes\n",
        "        WHERE ticket_size > 0\n",
        "        GROUP BY warehouse_id, warehouse_name\n",
        "    )\n",
        "\n",
        "    SELECT \n",
        "        warehouse_id,\n",
        "        warehouse_name,\n",
        "        ROUND(avg_ticket_size, 2) as avg_ticket_size,\n",
        "        ROUND(median_ticket_size, 2) as median_ticket_size,\n",
        "        ROUND(avg_order_weight_kg, 2) as avg_order_weight_kg,\n",
        "        ROUND({WS_CAR_CAPACITY_TONS * 1000} / NULLIF(avg_order_weight_kg, 0), 1) as orders_per_car_by_weight\n",
        "    FROM warehouse_stats\n",
        "    ORDER BY warehouse_id\n",
        "    '''\n",
        "    \n",
        "    print(\"  Fetching warehouse ticket statistics...\")\n",
        "    df = query_snowflake(query)\n",
        "    \n",
        "    for col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='ignore')\n",
        "    \n",
        "    print(f\"    Got stats for {len(df)} warehouses\")\n",
        "    return df\n",
        "\n",
        "print(\"✓ Data fetching functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# V2 TIER PRICE CALCULATION (with margin-based fallback)\n",
        "# =============================================================================\n",
        "\n",
        "def calculate_tier_prices_v2(row):\n",
        "    \"\"\"\n",
        "    Calculate tier 1 and tier 2 prices for a single row.\n",
        "    \n",
        "    V2 CHANGES:\n",
        "    - Added margin-based fallback when no market/margin tier prices available\n",
        "    - Fallback: T1 keeps 90%, T2 keeps 75%, T3 keeps 60% of current margin\n",
        "    \n",
        "    Args:\n",
        "        row: DataFrame row with wac_p, current_price, market margins, margin tiers\n",
        "        \n",
        "    Returns:\n",
        "        Series with tier_1_price, tier_2_price, price_source\n",
        "    \"\"\"\n",
        "    current_price = row.get('packing_unit_price')\n",
        "    wac = row.get('wac_pu')\n",
        "    tier_1_qty = row.get('tier_1_qty')\n",
        "    tier_2_qty = row.get('tier_2_qty')\n",
        "    \n",
        "    # Validation\n",
        "    if pd.isna(current_price) or current_price <= 0:\n",
        "        return pd.Series({'tier_1_price': np.nan, 'tier_2_price': np.nan, 'price_source': 'invalid_current_price'})\n",
        "    \n",
        "    if pd.isna(wac) or wac <= 0:\n",
        "        return pd.Series({'tier_1_price': np.nan, 'tier_2_price': np.nan, 'price_source': 'invalid_wac'})\n",
        "    \n",
        "    if current_price <= wac:\n",
        "        return pd.Series({'tier_1_price': np.nan, 'tier_2_price': np.nan, 'price_source': 'current_price_below_wac'})\n",
        "    \n",
        "    # Calculate discount bounds\n",
        "    max_discount_price = current_price * (1 - MAX_DISCOUNT_PCT / 100)\n",
        "    min_discount_price = current_price * (1 - MIN_DISCOUNT_PCT / 100)\n",
        "    \n",
        "    # Collect candidate prices from market margins and margin tiers\n",
        "    candidate_prices = []\n",
        "    \n",
        "    # Market margin columns\n",
        "    market_margin_cols = ['below_market', 'market_min', 'market_25', 'market_50', \n",
        "                          'market_75', 'market_max', 'above_market']\n",
        "    \n",
        "    for col in market_margin_cols:\n",
        "        margin = row.get(col)\n",
        "        if pd.notna(margin) and 0 < margin < 1:\n",
        "            price = wac / (1 - margin)\n",
        "            if max_discount_price <= price <= min_discount_price and price > wac:\n",
        "                candidate_prices.append(('market', col, price))\n",
        "    \n",
        "    # Margin tier columns\n",
        "    margin_tier_cols = ['margin_tier_1', 'margin_tier_2', 'margin_tier_3', 'margin_tier_4',\n",
        "                        'margin_tier_5', 'margin_tier_above_1', 'margin_tier_above_2']\n",
        "    \n",
        "    for col in margin_tier_cols:\n",
        "        margin = row.get(col)\n",
        "        if pd.notna(margin) and 0 < margin < 1:\n",
        "            price = wac / (1 - margin)\n",
        "            if max_discount_price <= price <= min_discount_price and price > wac:\n",
        "                candidate_prices.append(('margin_tier', col, price))\n",
        "    \n",
        "    # Remove duplicates and sort by price descending\n",
        "    unique_prices = {}\n",
        "    for source_type, source_col, price in candidate_prices:\n",
        "        price_rounded = round(price, 2)\n",
        "        if price_rounded not in unique_prices:\n",
        "            unique_prices[price_rounded] = (source_type, source_col)\n",
        "    \n",
        "    valid_prices = sorted(unique_prices.keys(), reverse=True)\n",
        "    \n",
        "    tier_1 = None\n",
        "    tier_2 = None\n",
        "    source = ''\n",
        "    \n",
        "    # =========================================================================\n",
        "    # V2 FALLBACK: Use margin-based pricing if no valid prices found\n",
        "    # =========================================================================\n",
        "    if len(valid_prices) < 2:\n",
        "        # Calculate current margin\n",
        "        current_margin = (current_price - wac) / current_price\n",
        "        \n",
        "        if current_margin > MIN_DISCOUNT_PCT / 100:\n",
        "            # T1: Keep 90% of margin (give up 10%)\n",
        "            t1_margin = current_margin * FALLBACK_T1_MARGIN_KEEP\n",
        "            tier_1 = wac / (1 - t1_margin)\n",
        "            \n",
        "            # T2: Keep 75% of margin (give up 25%)\n",
        "            t2_margin = current_margin * FALLBACK_T2_MARGIN_KEEP\n",
        "            tier_2 = wac / (1 - t2_margin)\n",
        "            \n",
        "            # Validate within bounds\n",
        "            tier_1_valid = max_discount_price <= tier_1 <= min_discount_price and tier_1 > wac\n",
        "            tier_2_valid = max_discount_price <= tier_2 <= min_discount_price and tier_2 > wac\n",
        "            \n",
        "            if tier_1_valid and tier_2_valid and tier_2 < tier_1:\n",
        "                source = 'margin_fallback'\n",
        "            else:\n",
        "                # Try with adjusted bounds\n",
        "                tier_1 = max(tier_1, max_discount_price) if tier_1 < max_discount_price else min(tier_1, min_discount_price)\n",
        "                tier_2 = max(tier_2, max_discount_price) if tier_2 < max_discount_price else tier_2\n",
        "                \n",
        "                if tier_2 < tier_1 and tier_1 > wac and tier_2 > wac:\n",
        "                    source = 'margin_fallback_adjusted'\n",
        "                else:\n",
        "                    return pd.Series({'tier_1_price': np.nan, 'tier_2_price': np.nan, \n",
        "                                     'price_source': 'fallback_failed'})\n",
        "        else:\n",
        "            return pd.Series({'tier_1_price': np.nan, 'tier_2_price': np.nan, \n",
        "                             'price_source': 'insufficient_margin_for_fallback'})\n",
        "    else:\n",
        "        # Strategy: Find two prices with minimum gap\n",
        "        for i, p1 in enumerate(valid_prices):\n",
        "            for p2 in valid_prices[i+1:]:\n",
        "                if p2 < p1 * (1 - MIN_GAP_PCT / 100):\n",
        "                    tier_1 = p1\n",
        "                    tier_2 = p2\n",
        "                    source = f\"{unique_prices[p1][0]}_{unique_prices[p2][0]}\"\n",
        "                    break\n",
        "            if tier_1 is not None:\n",
        "                break\n",
        "        \n",
        "        # If no pair with minimum gap, take top two\n",
        "        if tier_1 is None and len(valid_prices) >= 2:\n",
        "            tier_1 = valid_prices[0]\n",
        "            tier_2 = valid_prices[1]\n",
        "            source = 'top_two_prices'\n",
        "    \n",
        "    # Final validation\n",
        "    if tier_1 is None or tier_2 is None:\n",
        "        return pd.Series({'tier_1_price': np.nan, 'tier_2_price': np.nan, 'price_source': 'no_valid_pair'})\n",
        "    \n",
        "    # Ensure correct ordering\n",
        "    if tier_2 >= tier_1:\n",
        "        tier_1, tier_2 = max(tier_1, tier_2), min(tier_1, tier_2)\n",
        "    \n",
        "    # Check basic constraints\n",
        "    if not (wac < tier_2 < tier_1 < current_price):\n",
        "        return pd.Series({'tier_1_price': np.nan, 'tier_2_price': np.nan, 'price_source': 'invalid_ordering'})\n",
        "    \n",
        "    # =========================================================================\n",
        "    # ELASTICITY RATIO ADJUSTMENT (T1/T2 ONLY - T3 exempt)\n",
        "    # Ensure: qty_ratio < discount_ratio\n",
        "    # =========================================================================\n",
        "    if pd.notna(tier_1_qty) and pd.notna(tier_2_qty) and tier_1_qty > 0:\n",
        "        tier_1_discount = current_price - tier_1\n",
        "        tier_2_discount = current_price - tier_2\n",
        "        \n",
        "        if tier_1_discount > 0:\n",
        "            qty_ratio = tier_2_qty / tier_1_qty\n",
        "            discount_ratio = tier_2_discount / tier_1_discount\n",
        "            \n",
        "            if qty_ratio > 0:\n",
        "                # V2: Ensure qty_ratio < discount_ratio (elasticity > 1)\n",
        "                if discount_ratio <= qty_ratio:\n",
        "                    # Need to increase T2 discount\n",
        "                    target_discount_ratio = qty_ratio * MIN_RATIO\n",
        "                    target_tier_2_discount = target_discount_ratio * tier_1_discount\n",
        "                    adjusted_tier_2 = current_price - target_tier_2_discount\n",
        "                    \n",
        "                    if adjusted_tier_2 > wac and adjusted_tier_2 < tier_1:\n",
        "                        tier_2 = round(adjusted_tier_2, 2)\n",
        "                        source += '_elasticity_adj'\n",
        "                    else:\n",
        "                        return pd.Series({'tier_1_price': np.nan, 'tier_2_price': np.nan, \n",
        "                                         'price_source': f'cannot_adjust_elasticity'})\n",
        "                \n",
        "                # Also cap at MAX_RATIO\n",
        "                elasticity_ratio = discount_ratio / qty_ratio\n",
        "                if elasticity_ratio > MAX_RATIO:\n",
        "                    target_discount_ratio = MAX_RATIO * qty_ratio\n",
        "                    target_tier_2_discount = target_discount_ratio * tier_1_discount\n",
        "                    adjusted_tier_2 = current_price - target_tier_2_discount\n",
        "                    \n",
        "                    if adjusted_tier_2 > wac and adjusted_tier_2 < tier_1:\n",
        "                        tier_2 = round(adjusted_tier_2, 2)\n",
        "                        source += '_ratio_capped'\n",
        "    \n",
        "    # Final rounding\n",
        "    tier_1 = round(tier_1, 2)\n",
        "    tier_2 = round(tier_2, 2)\n",
        "    \n",
        "    return pd.Series({\n",
        "        'tier_1_price': tier_1,\n",
        "        'tier_2_price': tier_2,\n",
        "        'price_source': source\n",
        "    })\n",
        "\n",
        "\n",
        "def calculate_wholesale_tier_v2(row):\n",
        "    \"\"\"\n",
        "    Calculate wholesale (Tier 3) pricing based on delivery cost savings.\n",
        "    \n",
        "    V2 CHANGES:\n",
        "    - Minimum multiplier reduced from 3x to 2x (more accessible)\n",
        "    - Fallback to margin-based pricing if delivery calc fails\n",
        "    \n",
        "    Args:\n",
        "        row: DataFrame row with pricing and warehouse stats\n",
        "        \n",
        "    Returns:\n",
        "        Series with ws_qty, ws_price, ws_discount_pct, ws_margin, etc.\n",
        "    \"\"\"\n",
        "    current_price = row.get('packing_unit_price')\n",
        "    wac = row.get('wac_pu')\n",
        "    avg_ts = row.get('avg_ticket_size', 4000)\n",
        "    tier_1_price = row.get('tier_1_price')\n",
        "    tier_2_price = row.get('tier_2_price')\n",
        "    \n",
        "    orders_per_car = row.get('orders_per_car_by_weight', 15)\n",
        "    if pd.isna(orders_per_car) or orders_per_car <= 0:\n",
        "        orders_per_car = 15\n",
        "    \n",
        "    car_cost_per_order = WS_CAR_COST / orders_per_car\n",
        "    \n",
        "    if pd.isna(avg_ts) or avg_ts <= 0:\n",
        "        avg_ts = 4000\n",
        "    \n",
        "    # Validation\n",
        "    if pd.isna(current_price) or pd.isna(wac) or current_price <= 0 or wac <= 0:\n",
        "        return pd.Series({\n",
        "            'ws_qty': np.nan, 'ws_price': np.nan, 'ws_discount_pct': np.nan,\n",
        "            'ws_margin': np.nan, 'ws_multiplier': np.nan, 'ws_savings_pct': np.nan\n",
        "        })\n",
        "    \n",
        "    # Determine comparison price\n",
        "    if pd.notna(tier_2_price) and tier_2_price > 0:\n",
        "        compare_price = tier_2_price\n",
        "    elif pd.notna(tier_1_price) and tier_1_price > 0:\n",
        "        compare_price = tier_1_price\n",
        "    else:\n",
        "        compare_price = None\n",
        "    \n",
        "    min_ws_price_legacy = wac / (1 - WS_MIN_MARGIN)\n",
        "    min_acceptable_price = min_ws_price_legacy\n",
        "    \n",
        "    best_scenario = None\n",
        "    best_savings_pct = 0\n",
        "    \n",
        "    # V2: Start from WS_MIN_MULTIPLIER (2x instead of 3x)\n",
        "    for multiplier in range(WS_MIN_MULTIPLIER, int(orders_per_car) + 1):\n",
        "        order_value = avg_ts * multiplier\n",
        "        deliveries_saved = multiplier - 1\n",
        "        total_savings = deliveries_saved * car_cost_per_order\n",
        "        qty_at_current_price = order_value / current_price\n",
        "        \n",
        "        if qty_at_current_price <= 0:\n",
        "            continue\n",
        "        \n",
        "        discount_per_unit = total_savings / qty_at_current_price\n",
        "        new_price = current_price - discount_per_unit\n",
        "        \n",
        "        price_ok = compare_price is None or new_price < compare_price\n",
        "        if new_price >= min_acceptable_price and order_value <= WS_MAX_TICKET_SIZE and price_ok:\n",
        "            margin = (new_price - wac) / new_price\n",
        "            savings_pct = (discount_per_unit / current_price) * 100\n",
        "            \n",
        "            if savings_pct > best_savings_pct:\n",
        "                best_savings_pct = savings_pct\n",
        "                best_scenario = {\n",
        "                    'ws_qty': round(qty_at_current_price, 0),\n",
        "                    'ws_price': round(new_price, 2),\n",
        "                    'ws_discount_pct': round((current_price - new_price) / current_price * 100, 2),\n",
        "                    'ws_margin': round(margin, 4),\n",
        "                    'ws_multiplier': multiplier,\n",
        "                    'ws_savings_pct': round(savings_pct, 2)\n",
        "                }\n",
        "    \n",
        "    # V2 FALLBACK: If no valid scenario from delivery calc, use margin-based\n",
        "    if best_scenario is None and current_price > wac:\n",
        "        current_margin = (current_price - wac) / current_price\n",
        "        \n",
        "        if current_margin > MIN_DISCOUNT_PCT / 100:\n",
        "            # T3: Keep 60% of margin (give up 40%)\n",
        "            t3_margin = current_margin * FALLBACK_T3_MARGIN_KEEP\n",
        "            fallback_price = wac / (1 - t3_margin)\n",
        "            \n",
        "            # Set qty as T2 qty * 2 or T1 qty * 3\n",
        "            tier_2_qty = row.get('tier_2_qty')\n",
        "            tier_1_qty = row.get('tier_1_qty')\n",
        "            \n",
        "            if pd.notna(tier_2_qty) and tier_2_qty > 0:\n",
        "                ws_qty = tier_2_qty * 2\n",
        "            elif pd.notna(tier_1_qty) and tier_1_qty > 0:\n",
        "                ws_qty = tier_1_qty * 3\n",
        "            else:\n",
        "                ws_qty = 50  # Default\n",
        "            \n",
        "            # Validate price is below T2/T1\n",
        "            price_ok = compare_price is None or fallback_price < compare_price\n",
        "            \n",
        "            if price_ok and fallback_price > wac:\n",
        "                margin = (fallback_price - wac) / fallback_price\n",
        "                discount_pct = (current_price - fallback_price) / current_price * 100\n",
        "                \n",
        "                if MIN_DISCOUNT_PCT <= discount_pct <= MAX_DISCOUNT_PCT:\n",
        "                    best_scenario = {\n",
        "                        'ws_qty': int(ws_qty),\n",
        "                        'ws_price': round(fallback_price, 2),\n",
        "                        'ws_discount_pct': round(discount_pct, 2),\n",
        "                        'ws_margin': round(margin, 4),\n",
        "                        'ws_multiplier': 0,  # Indicates fallback\n",
        "                        'ws_savings_pct': round(discount_pct, 2)\n",
        "                    }\n",
        "    \n",
        "    if best_scenario:\n",
        "        return pd.Series(best_scenario)\n",
        "    else:\n",
        "        return pd.Series({\n",
        "            'ws_qty': np.nan, 'ws_price': np.nan, 'ws_discount_pct': np.nan,\n",
        "            'ws_margin': np.nan, 'ws_multiplier': np.nan, 'ws_savings_pct': np.nan\n",
        "        })\n",
        "\n",
        "print(\"✓ V2 Tier price calculation functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# V2 PERFORMANCE ADJUSTMENT (Sequential: Qty first, then Discount)\n",
        "# =============================================================================\n",
        "\n",
        "def get_tier_thresholds(tier: int) -> tuple:\n",
        "    \"\"\"Get LOW and HIGH conversion thresholds for a tier.\"\"\"\n",
        "    if tier == 1:\n",
        "        return T1_LOW_CONVERSION_THRESHOLD, T1_HIGH_CONVERSION_THRESHOLD\n",
        "    elif tier == 2:\n",
        "        return T2_LOW_CONVERSION_THRESHOLD, T2_HIGH_CONVERSION_THRESHOLD\n",
        "    else:  # tier == 3\n",
        "        return T3_LOW_CONVERSION_THRESHOLD, T3_HIGH_CONVERSION_THRESHOLD\n",
        "\n",
        "\n",
        "def adjust_tier_by_performance(row, tier: int, \n",
        "                               qty_col: str, disc_col: str, contribution_col: str,\n",
        "                               current_price_col: str = 'packing_unit_price') -> dict:\n",
        "    \"\"\"\n",
        "    Adjust a single tier's qty/discount based on its conversion performance.\n",
        "    \n",
        "    V2 SEQUENTIAL LOGIC:\n",
        "    1. If conversion LOW/ZERO and qty_lower_count < 2: Lower qty by 10%\n",
        "    2. If conversion LOW/ZERO and qty exhausted and disc_boost_count < 3: Boost discount\n",
        "    3. If conversion HIGH: Reduce discount by 20%\n",
        "    \n",
        "    Args:\n",
        "        row: DataFrame row with tier data and performance columns\n",
        "        tier: Tier number (1, 2, or 3)\n",
        "        qty_col: Column name for tier quantity\n",
        "        disc_col: Column name for tier discount percentage\n",
        "        contribution_col: Column name for tier contribution (conversion)\n",
        "        current_price_col: Column name for current price\n",
        "        \n",
        "    Returns:\n",
        "        dict with adjusted qty, disc, and updated counts\n",
        "    \"\"\"\n",
        "    qty = row.get(qty_col)\n",
        "    disc_pct = row.get(disc_col)\n",
        "    contribution = row.get(contribution_col, 0) or 0\n",
        "    current_price = row.get(current_price_col)\n",
        "    wac = row.get('wac_pu')\n",
        "    has_active_qd = row.get('has_active_qd', False)\n",
        "    \n",
        "    # Get tracking counts (default 0 for new QDs)\n",
        "    qty_lower_count = row.get(f't{tier}_qty_lower_count', 0) or 0\n",
        "    disc_boost_count = row.get(f't{tier}_disc_boost_count', 0) or 0\n",
        "    \n",
        "    # Get thresholds\n",
        "    low_threshold, high_threshold = get_tier_thresholds(tier)\n",
        "    \n",
        "    result = {\n",
        "        'qty': qty,\n",
        "        'disc_pct': disc_pct,\n",
        "        'qty_lower_count': qty_lower_count,\n",
        "        'disc_boost_count': disc_boost_count,\n",
        "        'action': 'none'\n",
        "    }\n",
        "    \n",
        "    # Skip if tier not valid\n",
        "    if pd.isna(qty) or pd.isna(disc_pct) or qty <= 0 or disc_pct <= 0:\n",
        "        result['action'] = 'invalid_tier'\n",
        "        return result\n",
        "    \n",
        "    # Skip adjustment if no existing QD (use calculated values)\n",
        "    if not has_active_qd:\n",
        "        result['action'] = 'new_qd'\n",
        "        return result\n",
        "    \n",
        "    # ==========================================================================\n",
        "    # PERFORMANCE-BASED ADJUSTMENT\n",
        "    # ==========================================================================\n",
        "    \n",
        "    # CASE 1: HIGH CONVERSION - Reduce discount to save margin\n",
        "    if contribution >= high_threshold:\n",
        "        new_disc = disc_pct * HIGH_CONVERSION_MULTIPLIER\n",
        "        \n",
        "        # Ensure discount stays above minimum\n",
        "        if new_disc >= MIN_DISCOUNT_PCT:\n",
        "            result['disc_pct'] = round(new_disc, 2)\n",
        "            result['action'] = 'high_conversion_reduce'\n",
        "        else:\n",
        "            result['action'] = 'high_conversion_skip_below_min'\n",
        "        \n",
        "        return result\n",
        "    \n",
        "    # CASE 2: LOW/ZERO CONVERSION - Sequential adjustment\n",
        "    if contribution < low_threshold:\n",
        "        \n",
        "        # STEP 1: Try lowering quantity first (max 2 times)\n",
        "        if qty_lower_count < MAX_QTY_LOWER_COUNT:\n",
        "            reduction = max(QTY_REDUCTION_MIN, int(qty * QTY_REDUCTION_PCT))\n",
        "            new_qty = qty - reduction\n",
        "            \n",
        "            if new_qty >= 2:  # Minimum qty of 2\n",
        "                result['qty'] = int(new_qty)\n",
        "                result['qty_lower_count'] = qty_lower_count + 1\n",
        "                result['action'] = f'low_conv_lower_qty_{qty_lower_count + 1}'\n",
        "                return result\n",
        "        \n",
        "        # STEP 2: If qty exhausted, try boosting discount (max 3 times)\n",
        "        if disc_boost_count < MAX_DISC_BOOST_COUNT:\n",
        "            # Use different multiplier for zero vs low\n",
        "            if contribution == 0:\n",
        "                multiplier = ZERO_CONVERSION_MULTIPLIER\n",
        "            else:\n",
        "                multiplier = LOW_CONVERSION_MULTIPLIER\n",
        "            \n",
        "            new_disc = disc_pct * multiplier\n",
        "            \n",
        "            # Cap at maximum discount\n",
        "            if new_disc <= MAX_DISCOUNT_PCT:\n",
        "                result['disc_pct'] = round(new_disc, 2)\n",
        "                result['disc_boost_count'] = disc_boost_count + 1\n",
        "                result['action'] = f'low_conv_boost_disc_{disc_boost_count + 1}'\n",
        "            else:\n",
        "                # Cap at max and still increment\n",
        "                result['disc_pct'] = MAX_DISCOUNT_PCT\n",
        "                result['disc_boost_count'] = disc_boost_count + 1\n",
        "                result['action'] = f'low_conv_boost_disc_{disc_boost_count + 1}_capped'\n",
        "            \n",
        "            return result\n",
        "        \n",
        "        # STEP 3: Both exhausted - hold\n",
        "        result['action'] = 'low_conv_exhausted'\n",
        "        return result\n",
        "    \n",
        "    # CASE 3: NORMAL CONVERSION - Keep as-is\n",
        "    result['action'] = 'normal_keep'\n",
        "    return result\n",
        "\n",
        "\n",
        "def apply_performance_adjustments(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Apply performance adjustments to all tiers for all SKUs.\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame with tier data and contribution columns\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame with adjusted tiers\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    # Initialize tracking columns if not present\n",
        "    for tier in [1, 2, 3]:\n",
        "        if f't{tier}_qty_lower_count' not in df.columns:\n",
        "            df[f't{tier}_qty_lower_count'] = 0\n",
        "        if f't{tier}_disc_boost_count' not in df.columns:\n",
        "            df[f't{tier}_disc_boost_count'] = 0\n",
        "    \n",
        "    adjustment_summary = {1: {}, 2: {}, 3: {}}\n",
        "    \n",
        "    # Tier 1 adjustments\n",
        "    print(\"\\n  Adjusting Tier 1...\")\n",
        "    t1_adjustments = df.apply(\n",
        "        lambda row: adjust_tier_by_performance(\n",
        "            row, tier=1, \n",
        "            qty_col='tier_1_qty', \n",
        "            disc_col='tier_1_disc_pct',\n",
        "            contribution_col='t1_cntrb_uth'\n",
        "        ), axis=1, result_type='expand'\n",
        "    )\n",
        "    \n",
        "    df['tier_1_qty'] = t1_adjustments['qty']\n",
        "    df['tier_1_disc_pct'] = t1_adjustments['disc_pct']\n",
        "    df['t1_qty_lower_count'] = t1_adjustments['qty_lower_count']\n",
        "    df['t1_disc_boost_count'] = t1_adjustments['disc_boost_count']\n",
        "    df['t1_action'] = t1_adjustments['action']\n",
        "    \n",
        "    # Tier 2 adjustments\n",
        "    print(\"  Adjusting Tier 2...\")\n",
        "    t2_adjustments = df.apply(\n",
        "        lambda row: adjust_tier_by_performance(\n",
        "            row, tier=2, \n",
        "            qty_col='tier_2_qty', \n",
        "            disc_col='tier_2_disc_pct',\n",
        "            contribution_col='t2_cntrb_uth'\n",
        "        ), axis=1, result_type='expand'\n",
        "    )\n",
        "    \n",
        "    df['tier_2_qty'] = t2_adjustments['qty']\n",
        "    df['tier_2_disc_pct'] = t2_adjustments['disc_pct']\n",
        "    df['t2_qty_lower_count'] = t2_adjustments['qty_lower_count']\n",
        "    df['t2_disc_boost_count'] = t2_adjustments['disc_boost_count']\n",
        "    df['t2_action'] = t2_adjustments['action']\n",
        "    \n",
        "    # Tier 3 adjustments\n",
        "    print(\"  Adjusting Tier 3...\")\n",
        "    t3_adjustments = df.apply(\n",
        "        lambda row: adjust_tier_by_performance(\n",
        "            row, tier=3, \n",
        "            qty_col='ws_qty', \n",
        "            disc_col='ws_discount_pct',\n",
        "            contribution_col='t3_cntrb_uth'\n",
        "        ), axis=1, result_type='expand'\n",
        "    )\n",
        "    \n",
        "    df['ws_qty'] = t3_adjustments['qty']\n",
        "    df['ws_discount_pct'] = t3_adjustments['disc_pct']\n",
        "    df['t3_qty_lower_count'] = t3_adjustments['qty_lower_count']\n",
        "    df['t3_disc_boost_count'] = t3_adjustments['disc_boost_count']\n",
        "    df['t3_action'] = t3_adjustments['action']\n",
        "    \n",
        "    # Summary\n",
        "    for tier, col in [(1, 't1_action'), (2, 't2_action'), (3, 't3_action')]:\n",
        "        actions = df[col].value_counts()\n",
        "        print(f\"\\n  Tier {tier} adjustment summary:\")\n",
        "        for action, count in actions.items():\n",
        "            print(f\"    {action}: {count}\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "def recalculate_prices_from_discounts(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Recalculate tier prices from adjusted discount percentages.\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame with packing_unit_price and tier disc_pct columns\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame with recalculated prices\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    # Recalculate T1 price\n",
        "    mask_t1 = df['tier_1_disc_pct'].notna() & (df['tier_1_disc_pct'] > 0)\n",
        "    df.loc[mask_t1, 'tier_1_price'] = (\n",
        "        df.loc[mask_t1, 'packing_unit_price'] * \n",
        "        (1 - df.loc[mask_t1, 'tier_1_disc_pct'] / 100)\n",
        "    ).round(2)\n",
        "    \n",
        "    # Recalculate T2 price\n",
        "    mask_t2 = df['tier_2_disc_pct'].notna() & (df['tier_2_disc_pct'] > 0)\n",
        "    df.loc[mask_t2, 'tier_2_price'] = (\n",
        "        df.loc[mask_t2, 'packing_unit_price'] * \n",
        "        (1 - df.loc[mask_t2, 'tier_2_disc_pct'] / 100)\n",
        "    ).round(2)\n",
        "    \n",
        "    # Recalculate WS price\n",
        "    mask_ws = df['ws_discount_pct'].notna() & (df['ws_discount_pct'] > 0)\n",
        "    df.loc[mask_ws, 'ws_price'] = (\n",
        "        df.loc[mask_ws, 'packing_unit_price'] * \n",
        "        (1 - df.loc[mask_ws, 'ws_discount_pct'] / 100)\n",
        "    ).round(2)\n",
        "    \n",
        "    return df\n",
        "\n",
        "print(\"✓ V2 Performance adjustment functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# HELPER FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "def parse_keep_qd_tiers(value):\n",
        "    \"\"\"Parse keep_qd_tiers from string or list.\"\"\"\n",
        "    if isinstance(value, list):\n",
        "        return value\n",
        "    if value is None:\n",
        "        return []\n",
        "    try:\n",
        "        if pd.isna(value):\n",
        "            return []\n",
        "    except (ValueError, TypeError):\n",
        "        pass\n",
        "    if isinstance(value, str):\n",
        "        try:\n",
        "            return ast.literal_eval(value)\n",
        "        except:\n",
        "            return []\n",
        "    return []\n",
        "\n",
        "\n",
        "def validate_tier_ordering(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Validate and enforce: discount_T1 < discount_T2 < discount_T3\n",
        "    \n",
        "    If violated, invalidate the higher quantity tier.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    print(\"\\n  Validating discount ordering (T1 < T2 < T3)...\")\n",
        "    \n",
        "    # If T1 >= T2 discount, invalidate T2\n",
        "    t1_gte_t2 = (df['tier_1_disc_pct'].notna() & \n",
        "                 df['tier_2_disc_pct'].notna() & \n",
        "                 (df['tier_1_disc_pct'] >= df['tier_2_disc_pct']))\n",
        "    if t1_gte_t2.sum() > 0:\n",
        "        df.loc[t1_gte_t2, ['tier_2_qty', 'tier_2_price', 'tier_2_disc_pct']] = np.nan\n",
        "        print(f\"    Invalidated T2 for {t1_gte_t2.sum()} SKUs (T1 >= T2 discount)\")\n",
        "    \n",
        "    # If T2 >= T3 discount, invalidate T3\n",
        "    t2_gte_t3 = (df['tier_2_disc_pct'].notna() & \n",
        "                 df['ws_discount_pct'].notna() & \n",
        "                 (df['tier_2_disc_pct'] >= df['ws_discount_pct']))\n",
        "    if t2_gte_t3.sum() > 0:\n",
        "        df.loc[t2_gte_t3, ['ws_qty', 'ws_price', 'ws_discount_pct']] = np.nan\n",
        "        print(f\"    Invalidated T3 for {t2_gte_t3.sum()} SKUs (T2 >= T3 discount)\")\n",
        "    \n",
        "    # If T1 >= T3 (when T2 missing)\n",
        "    t1_gte_t3 = (df['tier_1_disc_pct'].notna() & \n",
        "                 df['ws_discount_pct'].notna() & \n",
        "                 (df['tier_1_disc_pct'] >= df['ws_discount_pct']))\n",
        "    if t1_gte_t3.sum() > 0:\n",
        "        df.loc[t1_gte_t3, ['ws_qty', 'ws_price', 'ws_discount_pct']] = np.nan\n",
        "        print(f\"    Invalidated T3 for {t1_gte_t3.sum()} SKUs (T1 >= T3 discount)\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "def validate_elasticity_t1_t2(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Validate elasticity constraint for T1/T2 only (T3 exempt).\n",
        "    \n",
        "    Constraint: qty_ratio < discount_ratio (elasticity > 1)\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    print(\"\\n  Validating T1/T2 elasticity...\")\n",
        "    \n",
        "    mask = (df['tier_1_qty'].notna() & df['tier_2_qty'].notna() &\n",
        "            df['tier_1_disc_pct'].notna() & df['tier_2_disc_pct'].notna() &\n",
        "            (df['tier_1_qty'] > 0) & (df['tier_1_disc_pct'] > 0))\n",
        "    \n",
        "    violations = 0\n",
        "    for idx in df[mask].index:\n",
        "        qty_ratio = df.loc[idx, 'tier_2_qty'] / df.loc[idx, 'tier_1_qty']\n",
        "        disc_ratio = df.loc[idx, 'tier_2_disc_pct'] / df.loc[idx, 'tier_1_disc_pct']\n",
        "        \n",
        "        # Constraint: qty_ratio < disc_ratio\n",
        "        if disc_ratio <= qty_ratio:\n",
        "            # Try to adjust T2 discount\n",
        "            target_disc_ratio = qty_ratio * MIN_RATIO\n",
        "            new_t2_disc = df.loc[idx, 'tier_1_disc_pct'] * target_disc_ratio\n",
        "            \n",
        "            if new_t2_disc <= MAX_DISCOUNT_PCT:\n",
        "                df.loc[idx, 'tier_2_disc_pct'] = round(new_t2_disc, 2)\n",
        "                df.loc[idx, 'tier_2_price'] = round(\n",
        "                    df.loc[idx, 'packing_unit_price'] * (1 - new_t2_disc / 100), 2\n",
        "                )\n",
        "            else:\n",
        "                # Invalidate T2\n",
        "                df.loc[idx, ['tier_2_qty', 'tier_2_price', 'tier_2_disc_pct']] = np.nan\n",
        "                violations += 1\n",
        "    \n",
        "    if violations > 0:\n",
        "        print(f\"    Invalidated T2 for {violations} SKUs (elasticity violation)\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "def apply_tier_flags(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Apply keep_qd_tiers filter and calculate tier flags.\"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    def get_tier_flags(row):\n",
        "        keep_tiers = parse_keep_qd_tiers(row.get('keep_qd_tiers'))\n",
        "        if not keep_tiers:\n",
        "            keep_tiers = ['T1', 'T2', 'T3']\n",
        "        \n",
        "        t1_valid = ('T1' in keep_tiers and \n",
        "                    pd.notna(row.get('tier_1_qty')) and \n",
        "                    pd.notna(row.get('tier_1_disc_pct')) and \n",
        "                    row.get('tier_1_disc_pct', 0) > 0)\n",
        "        \n",
        "        t2_valid = ('T2' in keep_tiers and \n",
        "                    pd.notna(row.get('tier_2_qty')) and \n",
        "                    pd.notna(row.get('tier_2_disc_pct')) and \n",
        "                    row.get('tier_2_disc_pct', 0) > 0)\n",
        "        \n",
        "        t3_valid = ('T3' in keep_tiers and \n",
        "                    pd.notna(row.get('ws_qty')) and \n",
        "                    pd.notna(row.get('ws_discount_pct')) and \n",
        "                    row.get('ws_discount_pct', 0) > 0)\n",
        "        \n",
        "        return pd.Series({\n",
        "            't1_f': int(t1_valid),\n",
        "            't2_f': int(t2_valid),\n",
        "            't3_f': int(t3_valid)\n",
        "        })\n",
        "    \n",
        "    tier_flags = df.apply(get_tier_flags, axis=1)\n",
        "    df = pd.concat([df, tier_flags], axis=1)\n",
        "    \n",
        "    # Set invalid tier values to null\n",
        "    df.loc[df['t1_f'] == 0, ['tier_1_qty', 'tier_1_price', 'tier_1_disc_pct']] = np.nan\n",
        "    df.loc[df['t2_f'] == 0, ['tier_2_qty', 'tier_2_price', 'tier_2_disc_pct']] = np.nan\n",
        "    df.loc[df['t3_f'] == 0, ['ws_qty', 'ws_price', 'ws_discount_pct']] = np.nan\n",
        "    \n",
        "    df['all_f'] = df['t1_f'] + df['t2_f'] + df['t3_f']\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "def priority_selection_v2(df: pd.DataFrame, limit: int = TOP_TIERS_PER_WAREHOUSE) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    V2 Priority-based selection:\n",
        "    1. High DOH SKUs FIRST (DOH > 30 AND inventory_value > 10K)\n",
        "    2. Then by mtd_qty * effective_price\n",
        "    3. Total limit: 400 tier entries per warehouse\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    print(f\"\\n  Applying V2 priority selection (High DOH first, {limit} total)...\")\n",
        "    \n",
        "    # Calculate inventory value if not present\n",
        "    if 'inventory_value' not in df.columns:\n",
        "        if 'stocks' in df.columns and 'effective_price' in df.columns:\n",
        "            df['inventory_value'] = df['stocks'] * df['effective_price']\n",
        "        else:\n",
        "            df['inventory_value'] = 0\n",
        "    \n",
        "    # Mark High DOH priority\n",
        "    df['is_high_doh'] = (\n",
        "        (df.get('responsive_doh', df.get('doh', 0)) > HIGH_DOH_THRESHOLD) &\n",
        "        (df['inventory_value'] >= HIGH_DOH_INVENTORY_THRESHOLD)\n",
        "    ).astype(int)\n",
        "    \n",
        "    high_doh_count = df['is_high_doh'].sum()\n",
        "    print(f\"    High DOH SKUs (DOH>{HIGH_DOH_THRESHOLD}, inv>={HIGH_DOH_INVENTORY_THRESHOLD}): {high_doh_count}\")\n",
        "    \n",
        "    # Calculate ranking score\n",
        "    df['mtd_qty'] = df['mtd_qty'].fillna(0)\n",
        "    df['ranking_score'] = df['mtd_qty'] * df['effective_price']\n",
        "    \n",
        "    # Sort: High DOH first, then by ranking score\n",
        "    df = df.sort_values(\n",
        "        ['warehouse_id', 'is_high_doh', 'ranking_score'], \n",
        "        ascending=[True, False, False]\n",
        "    )\n",
        "    \n",
        "    # Calculate cumulative tier count and filter\n",
        "    df['cumsum'] = df.groupby('warehouse_id')['all_f'].cumsum()\n",
        "    df_selected = df[df['cumsum'] <= limit].copy()\n",
        "    \n",
        "    # Summary\n",
        "    high_doh_selected = df_selected['is_high_doh'].sum()\n",
        "    print(f\"    Selected: {len(df_selected)} SKUs ({df_selected['all_f'].sum()} tiers)\")\n",
        "    print(f\"    High DOH selected: {high_doh_selected}\")\n",
        "    print(f\"    Regular selected: {len(df_selected) - high_doh_selected}\")\n",
        "    \n",
        "    return df_selected\n",
        "\n",
        "print(\"✓ Helper functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# MAIN FUNCTION: process_qd_v2\n",
        "# =============================================================================\n",
        "\n",
        "def process_qd_v2(df_qd: pd.DataFrame, dry_run: bool = True) -> dict:\n",
        "    \"\"\"\n",
        "    V2 Main function to process Quantity Discounts.\n",
        "    \n",
        "    V2 IMPROVEMENTS:\n",
        "    - Priority-based selection (High DOH first)\n",
        "    - Sequential performance adjustment (qty first, then discount)\n",
        "    - Tier-specific conversion thresholds\n",
        "    - Margin-based fallbacks\n",
        "    - Elasticity validation (T1/T2 only)\n",
        "    \n",
        "    Args:\n",
        "        df_qd: DataFrame with columns from Module 3\n",
        "        dry_run: If True, only log what would be done (default: True)\n",
        "        \n",
        "    Returns:\n",
        "        dict with processing results\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"QD HANDLER V2: PROCESSING QUANTITY DISCOUNTS\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Mode: {'DRY RUN (testing)' if dry_run else 'LIVE'}\")\n",
        "    print(f\"Timestamp: {CAIRO_NOW.strftime('%Y-%m-%d %H:%M')} Cairo Time\")\n",
        "    print(f\"Input SKUs: {len(df_qd)}\")\n",
        "    \n",
        "    if len(df_qd) == 0:\n",
        "        print(\"\\nNo SKUs to process. Exiting.\")\n",
        "        return {\n",
        "            'mode': 'testing' if dry_run else 'live',\n",
        "            'total_input': 0,\n",
        "            'processed': 0,\n",
        "            'failed': 0,\n",
        "            'deactivate_result': {'total_active': 0, 'deactivated': [], 'failed': []},\n",
        "            'create_result': {'created_count': 0, 'failed_count': 0, 'errors': []}\n",
        "        }\n",
        "    \n",
        "    print(f\"\\nUnique warehouses: {df_qd['warehouse_id'].nunique()}\")\n",
        "    \n",
        "    # =========================================================================\n",
        "    # STEP 1: DEACTIVATE ALL EXISTING QUANTITY DISCOUNTS\n",
        "    # =========================================================================\n",
        "    print(\"\\n\" + \"-\"*60)\n",
        "    print(\"STEP 1: Deactivating existing Quantity Discounts...\")\n",
        "    print(\"-\"*60)\n",
        "    \n",
        "    deactivate_result = deactivate_active_qd(dry_run=dry_run)\n",
        "    \n",
        "    # =========================================================================\n",
        "    # STEP 2: GET PACKING UNITS\n",
        "    # =========================================================================\n",
        "    print(\"\\n\" + \"-\"*60)\n",
        "    print(\"STEP 2: Getting top-selling packing units...\")\n",
        "    print(\"-\"*60)\n",
        "    \n",
        "    product_warehouse_list = df_qd[['product_id', 'warehouse_id']].drop_duplicates().values.tolist()\n",
        "    df_packing_units = get_top_selling_packing_units(product_warehouse_list)\n",
        "    \n",
        "    if len(df_packing_units) == 0:\n",
        "        print(\"  No packing units found!\")\n",
        "        return {\n",
        "            'mode': 'testing' if dry_run else 'live',\n",
        "            'total_input': len(df_qd),\n",
        "            'processed': 0,\n",
        "            'failed': len(df_qd),\n",
        "            'deactivate_result': deactivate_result,\n",
        "            'create_result': {'created_count': 0, 'failed_count': 0, 'errors': [{'error': 'No packing units found'}]}\n",
        "        }\n",
        "    \n",
        "    df_work = df_qd.merge(df_packing_units, on=['product_id', 'warehouse_id'], how='inner')\n",
        "    print(f\"  Matched {len(df_work)} SKUs with packing units\")\n",
        "    \n",
        "    # Use new_price if available\n",
        "    df_work['effective_price'] = df_work['new_price'].fillna(df_work['current_price'])\n",
        "    df_work['wac_pu'] = df_work['wac_p'] * df_work['basic_unit_count']\n",
        "    df_work['packing_unit_price'] = df_work['effective_price'] * df_work['basic_unit_count']\n",
        "    \n",
        "    # =========================================================================\n",
        "    # STEP 3: GET WAREHOUSE TICKET STATISTICS\n",
        "    # =========================================================================\n",
        "    print(\"\\n\" + \"-\"*60)\n",
        "    print(\"STEP 3: Getting warehouse ticket statistics...\")\n",
        "    print(\"-\"*60)\n",
        "    \n",
        "    df_warehouse_stats = get_warehouse_ticket_stats()\n",
        "    \n",
        "    if len(df_warehouse_stats) > 0:\n",
        "        df_work = df_work.merge(\n",
        "            df_warehouse_stats[['warehouse_id', 'avg_ticket_size', 'orders_per_car_by_weight']],\n",
        "            on='warehouse_id', how='left'\n",
        "        )\n",
        "    else:\n",
        "        df_work['avg_ticket_size'] = 4000\n",
        "        df_work['orders_per_car_by_weight'] = 15\n",
        "    \n",
        "    # =========================================================================\n",
        "    # STEP 4: CALCULATE TIER QUANTITIES\n",
        "    # =========================================================================\n",
        "    print(\"\\n\" + \"-\"*60)\n",
        "    print(\"STEP 4: Calculating tier quantities...\")\n",
        "    print(\"-\"*60)\n",
        "    \n",
        "    product_warehouse_pu_list = df_work[['warehouse_id', 'product_id', 'packing_unit_id']].drop_duplicates().values.tolist()\n",
        "    df_tier_qty = get_tier_quantities(product_warehouse_pu_list)\n",
        "    \n",
        "    if len(df_tier_qty) > 0:\n",
        "        df_work = df_work.merge(\n",
        "            df_tier_qty[['warehouse_id', 'product_id', 'packing_unit_id', 'tier_1_qty', 'tier_2_qty']],\n",
        "            on=['warehouse_id', 'product_id', 'packing_unit_id'], how='left'\n",
        "        )\n",
        "        print(f\"  {df_work['tier_1_qty'].notna().sum()} SKUs have tier quantities\")\n",
        "    \n",
        "    # =========================================================================\n",
        "    # STEP 5: CALCULATE T1 & T2 PRICES (V2 with fallback)\n",
        "    # =========================================================================\n",
        "    print(\"\\n\" + \"-\"*60)\n",
        "    print(\"STEP 5: Calculating T1 & T2 prices (V2 with fallback)...\")\n",
        "    print(\"-\"*60)\n",
        "    \n",
        "    price_results = df_work.apply(calculate_tier_prices_v2, axis=1)\n",
        "    df_work = pd.concat([df_work, price_results], axis=1)\n",
        "    \n",
        "    valid_t1_t2 = df_work['tier_1_price'].notna() & df_work['tier_2_price'].notna()\n",
        "    print(f\"  Valid T1 & T2 prices: {valid_t1_t2.sum()} / {len(df_work)}\")\n",
        "    \n",
        "    if 'price_source' in df_work.columns:\n",
        "        print(\"\\n  Price source distribution:\")\n",
        "        for source, count in df_work['price_source'].value_counts().head(5).items():\n",
        "            print(f\"    {source}: {count}\")\n",
        "    \n",
        "    # =========================================================================\n",
        "    # STEP 6: CALCULATE T3 (WHOLESALE) PRICES (V2 with 2x min)\n",
        "    # =========================================================================\n",
        "    print(\"\\n\" + \"-\"*60)\n",
        "    print(\"STEP 6: Calculating T3 (wholesale) prices (V2: 2x min)...\")\n",
        "    print(\"-\"*60)\n",
        "    \n",
        "    ws_results = df_work.apply(calculate_wholesale_tier_v2, axis=1)\n",
        "    df_work = pd.concat([df_work, ws_results], axis=1)\n",
        "    \n",
        "    valid_t3 = df_work['ws_price'].notna()\n",
        "    print(f\"  Valid T3 prices: {valid_t3.sum()} / {len(df_work)}\")\n",
        "    \n",
        "    # =========================================================================\n",
        "    # STEP 7: CALCULATE DISCOUNT PERCENTAGES\n",
        "    # =========================================================================\n",
        "    print(\"\\n\" + \"-\"*60)\n",
        "    print(\"STEP 7: Calculating discount percentages...\")\n",
        "    print(\"-\"*60)\n",
        "    \n",
        "    df_work['tier_1_disc_pct'] = ((df_work['packing_unit_price'] - df_work['tier_1_price']) / \n",
        "                                  df_work['packing_unit_price'] * 100).round(2)\n",
        "    df_work['tier_2_disc_pct'] = ((df_work['packing_unit_price'] - df_work['tier_2_price']) / \n",
        "                                  df_work['packing_unit_price'] * 100).round(2)\n",
        "    \n",
        "    # =========================================================================\n",
        "    # STEP 8: V2 PERFORMANCE ADJUSTMENTS\n",
        "    # =========================================================================\n",
        "    print(\"\\n\" + \"-\"*60)\n",
        "    print(\"STEP 8: Applying V2 performance adjustments...\")\n",
        "    print(\"-\"*60)\n",
        "    \n",
        "    # Initialize performance columns if not present\n",
        "    for col in ['t1_cntrb_uth', 't2_cntrb_uth', 't3_cntrb_uth', 'has_active_qd']:\n",
        "        if col not in df_work.columns:\n",
        "            df_work[col] = 0\n",
        "    \n",
        "    df_work = apply_performance_adjustments(df_work)\n",
        "    df_work = recalculate_prices_from_discounts(df_work)\n",
        "    \n",
        "    # =========================================================================\n",
        "    # STEP 9: VALIDATE TIER ORDERING AND ELASTICITY\n",
        "    # =========================================================================\n",
        "    print(\"\\n\" + \"-\"*60)\n",
        "    print(\"STEP 9: Validating tier ordering and elasticity...\")\n",
        "    print(\"-\"*60)\n",
        "    \n",
        "    df_work = validate_tier_ordering(df_work)\n",
        "    df_work = validate_elasticity_t1_t2(df_work)\n",
        "    \n",
        "    # =========================================================================\n",
        "    # STEP 10: APPLY TIER FLAGS AND FILTER\n",
        "    # =========================================================================\n",
        "    print(\"\\n\" + \"-\"*60)\n",
        "    print(\"STEP 10: Applying tier flags and filtering...\")\n",
        "    print(\"-\"*60)\n",
        "    \n",
        "    df_work = apply_tier_flags(df_work)\n",
        "    \n",
        "    # Only keep SKUs with at least 2 valid tiers\n",
        "    df_work = df_work[df_work['all_f'] >= 2].copy()\n",
        "    \n",
        "    print(f\"  SKUs with valid tiers after filtering: {len(df_work)}\")\n",
        "    print(f\"  Total tier entries: {df_work['all_f'].sum()}\")\n",
        "    print(f\"    T1 valid: {df_work['t1_f'].sum()}\")\n",
        "    print(f\"    T2 valid: {df_work['t2_f'].sum()}\")\n",
        "    print(f\"    T3 valid: {df_work['t3_f'].sum()}\")\n",
        "    \n",
        "    # =========================================================================\n",
        "    # STEP 11: V2 PRIORITY SELECTION (High DOH first)\n",
        "    # =========================================================================\n",
        "    print(\"\\n\" + \"-\"*60)\n",
        "    print(\"STEP 11: V2 Priority selection (High DOH first)...\")\n",
        "    print(\"-\"*60)\n",
        "    \n",
        "    df_top = priority_selection_v2(df_work)\n",
        "    \n",
        "    print(f\"\\n  Tier entries per warehouse:\")\n",
        "    for wh in df_top['warehouse_id'].unique():\n",
        "        wh_data = df_top[df_top['warehouse_id'] == wh]\n",
        "        high_doh = wh_data['is_high_doh'].sum()\n",
        "        print(f\"    WH {wh}: {len(wh_data)} SKUs, {wh_data['all_f'].sum()} tiers (High DOH: {high_doh})\")\n",
        "    \n",
        "    # =========================================================================\n",
        "    # STEP 12: BUILD QD CONFIGURATIONS\n",
        "    # =========================================================================\n",
        "    print(\"\\n\" + \"-\"*60)\n",
        "    print(\"STEP 12: Building QD configurations...\")\n",
        "    print(\"-\"*60)\n",
        "    \n",
        "    qd_configs = []\n",
        "    \n",
        "    for _, row in df_top.iterrows():\n",
        "        tiers = []\n",
        "        \n",
        "        if row['t1_f'] == 1:\n",
        "            tiers.append({\n",
        "                \"tier\": 1,\n",
        "                \"quantity\": int(row['tier_1_qty']),\n",
        "                \"discount_pct\": float(row['tier_1_disc_pct'])\n",
        "            })\n",
        "        \n",
        "        if row['t2_f'] == 1:\n",
        "            tiers.append({\n",
        "                \"tier\": 2,\n",
        "                \"quantity\": int(row['tier_2_qty']),\n",
        "                \"discount_pct\": float(row['tier_2_disc_pct'])\n",
        "            })\n",
        "        \n",
        "        if row['t3_f'] == 1:\n",
        "            tiers.append({\n",
        "                \"tier\": 3,\n",
        "                \"quantity\": int(row['ws_qty']),\n",
        "                \"discount_pct\": float(row['ws_discount_pct'])\n",
        "            })\n",
        "        \n",
        "        qd_configs.append({\n",
        "            'product_id': int(row['product_id']),\n",
        "            'warehouse_id': int(row['warehouse_id']),\n",
        "            'cohort_id': int(row.get('cohort_id', 0)),\n",
        "            'packing_unit_id': int(row['packing_unit_id']),\n",
        "            'tiers': tiers,\n",
        "            'sku': row.get('sku', 'N/A'),\n",
        "            'is_high_doh': int(row.get('is_high_doh', 0)),\n",
        "            'ranking_score': row.get('ranking_score', 0)\n",
        "        })\n",
        "    \n",
        "    print(f\"  Valid QD configs: {len(qd_configs)}\")\n",
        "    \n",
        "    tier_counts = {1: 0, 2: 0, 3: 0}\n",
        "    for config in qd_configs:\n",
        "        for t in config['tiers']:\n",
        "            tier_counts[t['tier']] += 1\n",
        "    print(f\"\\n  Tier distribution:\")\n",
        "    print(f\"    T1: {tier_counts[1]}, T2: {tier_counts[2]}, T3: {tier_counts[3]}\")\n",
        "    \n",
        "    # =========================================================================\n",
        "    # STEP 13: SAVE REVIEW FILE\n",
        "    # =========================================================================\n",
        "    print(\"\\n\" + \"-\"*60)\n",
        "    print(\"STEP 13: Saving review file...\")\n",
        "    print(\"-\"*60)\n",
        "    \n",
        "    review_columns = [\n",
        "        'warehouse_id', 'product_id', 'packing_unit_id', 'sku', 'brand', 'cat',\n",
        "        'effective_price', 'packing_unit_price', 'wac_p', 'wac_pu',\n",
        "        'tier_1_qty', 'tier_1_price', 'tier_1_disc_pct', 't1_f', 't1_action',\n",
        "        'tier_2_qty', 'tier_2_price', 'tier_2_disc_pct', 't2_f', 't2_action',\n",
        "        'ws_qty', 'ws_price', 'ws_discount_pct', 't3_f', 't3_action',\n",
        "        'all_f', 'is_high_doh', 'ranking_score', 'price_source'\n",
        "    ]\n",
        "    review_columns = [c for c in review_columns if c in df_top.columns]\n",
        "    df_review = df_top[review_columns].copy()\n",
        "    \n",
        "    SLACK_CHANNEL_ID = 'C0AAWK97Z3Q'\n",
        "    review_file_name = f'QD_V2_review_{CAIRO_NOW.strftime(\"%Y%m%d_%H%M\")}.xlsx'\n",
        "    send_file_slack(\n",
        "        df_review, \n",
        "        f'QD V2 Review: {len(df_review)} SKUs ready for processing', \n",
        "        SLACK_CHANNEL_ID,\n",
        "        filename=review_file_name\n",
        "    )\n",
        "    print(f\"  Sent review file to Slack ({len(df_review)} rows)\")\n",
        "    \n",
        "    # =========================================================================\n",
        "    # STEP 14: CREATE NEW QUANTITY DISCOUNTS\n",
        "    # =========================================================================\n",
        "    print(\"\\n\" + \"-\"*60)\n",
        "    print(\"STEP 14: Creating new Quantity Discounts...\")\n",
        "    print(\"-\"*60)\n",
        "    \n",
        "    if len(qd_configs) == 0:\n",
        "        print(\"  No Quantity Discounts to create.\")\n",
        "        create_result = {\"success\": True, \"created_count\": 0, \"failed_count\": 0, \"errors\": []}\n",
        "    else:\n",
        "        print(f\"  Creating {len(qd_configs)} Quantity Discounts...\")\n",
        "        create_result = bulk_create_qd(qd_configs, df_top, dry_run=dry_run)\n",
        "        print(f\"\\n  Creation Result:\")\n",
        "        print(f\"    Created: {create_result['created_count']}\")\n",
        "        print(f\"    Failed: {create_result['failed_count']}\")\n",
        "    \n",
        "    # =========================================================================\n",
        "    # STEP 15: UPDATE CART RULES\n",
        "    # =========================================================================\n",
        "    print(\"\\n\" + \"-\"*60)\n",
        "    print(\"STEP 15: Updating cart rules...\")\n",
        "    print(\"-\"*60)\n",
        "    \n",
        "    cart_rules_update = prepare_cart_rules_update(df_top, df_qd)\n",
        "    \n",
        "    if len(cart_rules_update) == 0:\n",
        "        print(\"  No cart rules need updating.\")\n",
        "        cart_rules_result = {'success': [], 'failed': []}\n",
        "    else:\n",
        "        print(f\"  Uploading cart rules...\")\n",
        "        cart_rules_result = upload_cart_rules(cart_rules_update, dry_run=dry_run)\n",
        "        print(f\"\\n  Cart Rules Result:\")\n",
        "        print(f\"    Cohorts updated: {len(cart_rules_result['success'])}\")\n",
        "        print(f\"    Cohorts failed: {len(cart_rules_result['failed'])}\")\n",
        "    \n",
        "    # =========================================================================\n",
        "    # SUMMARY\n",
        "    # =========================================================================\n",
        "    total_tiers = sum(tier_counts.values())\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"QD HANDLER V2 - SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Mode: {'DRY RUN (testing)' if dry_run else 'LIVE'}\")\n",
        "    print(f\"Total SKUs in input: {len(df_qd)}\")\n",
        "    print(f\"SKUs with valid T1 & T2 prices: {valid_t1_t2.sum()}\")\n",
        "    print(f\"SKUs with valid T3 prices: {valid_t3.sum()}\")\n",
        "    print(f\"SKUs after filtering & selection: {len(df_top)}\")\n",
        "    print(f\"  High DOH: {df_top['is_high_doh'].sum()}\")\n",
        "    print(f\"  Regular: {len(df_top) - df_top['is_high_doh'].sum()}\")\n",
        "    print(f\"Total tier entries: {total_tiers}\")\n",
        "    print(f\"QD found active: {deactivate_result['total_active']}\")\n",
        "    print(f\"QD deactivated: {len(deactivate_result['deactivated'])}\")\n",
        "    print(f\"QD created: {create_result['created_count']}\")\n",
        "    print(f\"Cart rules updated: {len(cart_rules_update)} products\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    return {\n",
        "        'mode': 'testing' if dry_run else 'live',\n",
        "        'total_input': len(df_qd),\n",
        "        'processed': create_result['created_count'],\n",
        "        'failed': create_result['failed_count'],\n",
        "        'total_tiers': total_tiers,\n",
        "        'high_doh_count': int(df_top['is_high_doh'].sum()),\n",
        "        'deactivate_result': deactivate_result,\n",
        "        'create_result': create_result,\n",
        "        'cart_rules_result': cart_rules_result,\n",
        "        'cart_rules_update': cart_rules_update,\n",
        "        'qd_configs': qd_configs,\n",
        "        'df_work': df_top,\n",
        "        'review_file': review_file_name\n",
        "    }\n",
        "\n",
        "print(\"✓ process_qd_v2() function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# API FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "def deactivate_active_qd(dry_run: bool = True) -> dict:\n",
        "    \"\"\"Deactivate ALL active Quantity Discounts.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"DEACTIVATING ACTIVE QUANTITY DISCOUNTS\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Mode: {'DRY RUN' if dry_run else 'LIVE'}\")\n",
        "    \n",
        "    print(\"\\nStep 1: Querying active Quantity Discounts from Snowflake...\")\n",
        "    df_active = get_active_qd_now()\n",
        "    \n",
        "    if len(df_active) == 0:\n",
        "        print(\"  No active Quantity Discounts found.\")\n",
        "        return {'success': True, 'deactivated': [], 'failed': [], 'total_active': 0}\n",
        "    \n",
        "    discount_ids = df_active['discount_id'].tolist()\n",
        "    print(f\"  Found {len(discount_ids)} active Quantity Discounts\")\n",
        "    \n",
        "    print(f\"\\nStep 2: Deactivating {len(discount_ids)} discounts...\")\n",
        "    \n",
        "    results = {'deactivated': [], 'failed': []}\n",
        "    \n",
        "    if not dry_run:\n",
        "        auth_token = _get_api_token()\n",
        "        headers = {\n",
        "            'Authorization': f'Bearer {auth_token}',\n",
        "            'Content-Type': 'application/json'\n",
        "        }\n",
        "    \n",
        "    for idx, discount_id in enumerate(discount_ids):\n",
        "        if dry_run:\n",
        "            print(f\"  [{idx+1}/{len(discount_ids)}] [DRY RUN] Would deactivate: {discount_id}\")\n",
        "            results['deactivated'].append(discount_id)\n",
        "            continue\n",
        "        \n",
        "        url = f\"{QD_API_URL}{discount_id}/activation?status=false\"\n",
        "        \n",
        "        try:\n",
        "            response = requests.put(url, headers=headers, json={'status': False})\n",
        "            \n",
        "            if response.status_code in [200, 204]:\n",
        "                print(f\"  [{idx+1}/{len(discount_ids)}] [OK] Deactivated: {discount_id}\")\n",
        "                results['deactivated'].append(discount_id)\n",
        "            else:\n",
        "                print(f\"  [{idx+1}/{len(discount_ids)}] [ERROR] {discount_id}: {response.status_code}\")\n",
        "                results['failed'].append({'id': discount_id, 'error': f\"{response.status_code}\"})\n",
        "        except Exception as e:\n",
        "            print(f\"  [{idx+1}/{len(discount_ids)}] [EXCEPTION] {discount_id}: {e}\")\n",
        "            results['failed'].append({'id': discount_id, 'error': str(e)})\n",
        "        \n",
        "        time.sleep(0.5)\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"DEACTIVATION SUMMARY\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Total active found: {len(discount_ids)}\")\n",
        "    print(f\"Successfully deactivated: {len(results['deactivated'])}\")\n",
        "    print(f\"Failed: {len(results['failed'])}\")\n",
        "    \n",
        "    return {\n",
        "        'success': len(results['failed']) == 0,\n",
        "        'deactivated': results['deactivated'],\n",
        "        'failed': results['failed'],\n",
        "        'total_active': len(discount_ids)\n",
        "    }\n",
        "\n",
        "\n",
        "def create_upload_format(df_configs: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Create upload format DataFrame from QD configurations.\"\"\"\n",
        "    final_quantity_discount = pd.DataFrame(columns=['warehouse_id', 'Discounts Group 1', 'Discounts Group 2', 'Description'])\n",
        "    \n",
        "    for wh_id in df_configs['warehouse_id'].unique():\n",
        "        warehouse_data = df_configs[df_configs['warehouse_id'] == wh_id]\n",
        "        warehouse_id = int(wh_id)\n",
        "        \n",
        "        tier_1_items = []\n",
        "        tier_2_items = []\n",
        "        ws_items = []\n",
        "        \n",
        "        for _, r in warehouse_data.iterrows():\n",
        "            product_id = int(r['product_id'])\n",
        "            packing_unit_id = int(r['packing_unit_id'])\n",
        "            \n",
        "            if r.get('t1_f', 0) == 1 and pd.notna(r.get('tier_1_qty')) and pd.notna(r.get('tier_1_disc_pct')):\n",
        "                q_1 = int(r['tier_1_qty'])\n",
        "                d_1 = min(round(r['tier_1_disc_pct'], 2), MAX_DISCOUNT_CAP_T1)\n",
        "                tier_1_items.append([product_id, packing_unit_id, q_1, d_1])\n",
        "            \n",
        "            if r.get('t2_f', 0) == 1 and pd.notna(r.get('tier_2_qty')) and pd.notna(r.get('tier_2_disc_pct')):\n",
        "                q_2 = int(r['tier_2_qty'])\n",
        "                d_2 = min(round(r['tier_2_disc_pct'], 2), MAX_DISCOUNT_CAP_T2)\n",
        "                tier_2_items.append([product_id, packing_unit_id, q_2, d_2])\n",
        "            \n",
        "            if r.get('t3_f', 0) == 1 and pd.notna(r.get('ws_qty')) and pd.notna(r.get('ws_discount_pct')):\n",
        "                q_ws = int(r['ws_qty'])\n",
        "                d_ws = min(round(r['ws_discount_pct'], 2), MAX_DISCOUNT_CAP_WS)\n",
        "                ws_items.append([product_id, packing_unit_id, q_ws, d_ws])\n",
        "        \n",
        "        group_1_items = tier_1_items + ws_items\n",
        "        \n",
        "        if len(group_1_items) > MAX_GROUP_SIZE:\n",
        "            overflow = group_1_items[MAX_GROUP_SIZE:]\n",
        "            group_1_items = group_1_items[:MAX_GROUP_SIZE]\n",
        "            group_2_items = tier_2_items + overflow\n",
        "        else:\n",
        "            group_2_items = tier_2_items\n",
        "        \n",
        "        new_row = {\n",
        "            'warehouse_id': warehouse_id,\n",
        "            'Discounts Group 1': group_1_items,\n",
        "            'Discounts Group 2': group_2_items,\n",
        "            'Description': f'{warehouse_id}QD'\n",
        "        }\n",
        "        final_quantity_discount = pd.concat([final_quantity_discount, pd.DataFrame([new_row])], ignore_index=True)\n",
        "    \n",
        "    return final_quantity_discount\n",
        "\n",
        "\n",
        "def prepare_upload_file(df_upload: pd.DataFrame, dry_run: bool = True) -> tuple:\n",
        "    \"\"\"Prepare the final upload file with tag IDs and date/time.\"\"\"\n",
        "    df_mapping = pd.DataFrame([\n",
        "        {'warehouse_id': wh_id, 'warehouse_name': info['name'], 'tag_id': info['tag_id']}\n",
        "        for wh_id, info in WAREHOUSE_TAG_MAPPING.items()\n",
        "    ])\n",
        "    \n",
        "    to_upload = df_upload.merge(df_mapping, on='warehouse_id', how='left')\n",
        "    \n",
        "    to_upload['Description'] = (\n",
        "        to_upload['warehouse_name'].astype(str)\n",
        "        .str.replace(' ', '')\n",
        "        .str.replace('-', '')\n",
        "        + \"QD\"\n",
        "    )\n",
        "    \n",
        "    cairo_now = datetime.now(CAIRO_TZ)\n",
        "    start_date = cairo_now + timedelta(minutes=10)\n",
        "    end_date = cairo_now + timedelta(hours=QD_DURATION_HOURS)\n",
        "    \n",
        "    to_upload['Start Date/Time'] = start_date.strftime('%d/%m/%Y %H:%M')\n",
        "    to_upload['End Date/Time'] = end_date.strftime('%d/%m/%Y %H:%M')\n",
        "    to_upload = to_upload.rename(columns={'tag_id': 'Tag ID'})\n",
        "    \n",
        "    to_upload = to_upload[['Tag ID', 'Description', 'Start Date/Time', 'End Date/Time', 'Discounts Group 1', 'Discounts Group 2']]\n",
        "    to_upload = to_upload[to_upload['Tag ID'].notna()]\n",
        "    \n",
        "    filename = f'{QD_OUTPUT_DIR}/QD_V2_upload_{cairo_now.strftime(\"%Y%m%d_%H%M\")}.xlsx'\n",
        "    \n",
        "    if not dry_run:\n",
        "        to_upload.to_excel(filename, index=False)\n",
        "        print(f\"  Saved upload file: {filename} ({len(to_upload)} warehouses)\")\n",
        "    \n",
        "    return to_upload, filename\n",
        "\n",
        "\n",
        "def post_QD(filename: str) -> requests.Response:\n",
        "    \"\"\"Upload Quantity Discount file to MaxAB API.\"\"\"\n",
        "    token = get_access_token(\n",
        "        'https://sso.maxab.info/auth/realms/maxab/protocol/openid-connect/token',\n",
        "        'main-system-externals',\n",
        "        API_SECRET\n",
        "    )\n",
        "    \n",
        "    url = \"https://api.maxab.info/commerce/api/admins/v1/quantity-discounts\"\n",
        "    \n",
        "    files = [\n",
        "        ('file', (filename, open(filename, 'rb'), \n",
        "                  'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'))\n",
        "    ]\n",
        "    headers = {'Authorization': f'bearer {token}'}\n",
        "    \n",
        "    response = requests.request(\"POST\", url, headers=headers, data={}, files=files)\n",
        "    return response\n",
        "\n",
        "\n",
        "def bulk_create_qd(qd_configs: list, df_work: pd.DataFrame, dry_run: bool = True) -> dict:\n",
        "    \"\"\"Bulk create Quantity Discounts using file upload method.\"\"\"\n",
        "    print(\"\\n  Creating upload format...\")\n",
        "    \n",
        "    df_upload = create_upload_format(df_work)\n",
        "    \n",
        "    print(f\"  Upload format created: {len(df_upload)} warehouse rows\")\n",
        "    print(f\"\\n  Per warehouse breakdown:\")\n",
        "    for _, row in df_upload.iterrows():\n",
        "        wh = row['warehouse_id']\n",
        "        g1_count = len(row['Discounts Group 1'])\n",
        "        g2_count = len(row['Discounts Group 2'])\n",
        "        print(f\"    WH {wh}: Group 1 = {g1_count} items, Group 2 = {g2_count} items\")\n",
        "    \n",
        "    print(\"\\n  Preparing upload file...\")\n",
        "    to_upload, filename = prepare_upload_file(df_upload, dry_run=dry_run)\n",
        "    \n",
        "    if dry_run:\n",
        "        print(f\"\\n  [DRY RUN] Would upload {len(to_upload)} warehouses\")\n",
        "        return {\n",
        "            \"success\": True,\n",
        "            \"created_count\": len(qd_configs),\n",
        "            \"failed_count\": 0,\n",
        "            \"errors\": [],\n",
        "            \"upload_df\": to_upload\n",
        "        }\n",
        "    \n",
        "    print(f\"\\n  Uploading QD file to API...\")\n",
        "    response = post_QD(filename)\n",
        "    \n",
        "    if response.ok:\n",
        "        print(f\"  Upload succeeded (status: {response.status_code})\")\n",
        "        return {\n",
        "            \"success\": True,\n",
        "            \"created_count\": len(qd_configs),\n",
        "            \"failed_count\": 0,\n",
        "            \"errors\": [],\n",
        "            \"upload_df\": to_upload\n",
        "        }\n",
        "    else:\n",
        "        print(f\"  Upload failed (status: {response.status_code})\")\n",
        "        print(f\"  Response: {response.content[:500]}\")\n",
        "        return {\n",
        "            \"success\": False,\n",
        "            \"created_count\": 0,\n",
        "            \"failed_count\": len(qd_configs),\n",
        "            \"errors\": [{\"error\": f\"API upload failed: {response.status_code}\"}],\n",
        "            \"upload_df\": to_upload\n",
        "        }\n",
        "\n",
        "\n",
        "def prepare_cart_rules_update(df_work: pd.DataFrame, df_qd_input: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Prepare cart rules update based on QD tier quantities.\"\"\"\n",
        "    cart_cols = ['product_id', 'warehouse_id']\n",
        "    if 'current_cart_rule' in df_qd_input.columns:\n",
        "        cart_cols.append('current_cart_rule')\n",
        "    if 'new_cart_rule' in df_qd_input.columns:\n",
        "        cart_cols.append('new_cart_rule')\n",
        "    \n",
        "    df_cart_merge = df_qd_input[cart_cols].drop_duplicates()\n",
        "    df_work_cart = df_work.merge(df_cart_merge, on=['product_id', 'warehouse_id'], how='left')\n",
        "    \n",
        "    if 'new_cart_rule' in df_work_cart.columns:\n",
        "        df_work_cart['effective_cart_rule'] = df_work_cart['new_cart_rule'].fillna(\n",
        "            df_work_cart.get('current_cart_rule', 0)\n",
        "        )\n",
        "    else:\n",
        "        df_work_cart['effective_cart_rule'] = df_work_cart.get('current_cart_rule', 0)\n",
        "    \n",
        "    df_work_cart['effective_cart_rule'] = df_work_cart['effective_cart_rule'].fillna(0)\n",
        "    \n",
        "    tier_cols = ['tier_1_qty', 'tier_2_qty', 'ws_qty']\n",
        "    tier_cols = [c for c in tier_cols if c in df_work_cart.columns]\n",
        "    df_work_cart['max_tier_qty'] = df_work_cart[tier_cols].max(axis=1, skipna=True)\n",
        "    \n",
        "    needs_update = df_work_cart['max_tier_qty'] > df_work_cart['effective_cart_rule']\n",
        "    cart_rules_update = df_work_cart[needs_update][['cohort_id', 'product_id', 'packing_unit_id', 'max_tier_qty']].copy()\n",
        "    cart_rules_update = cart_rules_update.rename(columns={'max_tier_qty': 'new_cart_rule'})\n",
        "    \n",
        "    cart_rules_update['new_cart_rule'] = cart_rules_update['new_cart_rule'].round().astype(int)\n",
        "    cart_rules_update = cart_rules_update.groupby(['cohort_id', 'product_id', 'packing_unit_id'])['new_cart_rule'].max().reset_index()\n",
        "    \n",
        "    return cart_rules_update\n",
        "\n",
        "\n",
        "def post_cart_rules(cohort_id: int, filename: str) -> requests.Response:\n",
        "    \"\"\"Upload Cart Rules file for a specific cohort.\"\"\"\n",
        "    token = get_access_token(\n",
        "        'https://sso.maxab.info/auth/realms/maxab/protocol/openid-connect/token',\n",
        "        'main-system-externals',\n",
        "        API_SECRET\n",
        "    )\n",
        "    \n",
        "    url = f\"https://api.maxab.info/main-system/api/admin-portal/cohorts/{cohort_id}/cart-rules\"\n",
        "    \n",
        "    files = [\n",
        "        ('sheet', (filename, open(filename, 'rb'),\n",
        "                   'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'))\n",
        "    ]\n",
        "    headers = {'Authorization': f'bearer {token}'}\n",
        "    \n",
        "    response = requests.request(\"POST\", url, headers=headers, data={}, files=files)\n",
        "    return response\n",
        "\n",
        "\n",
        "def upload_cart_rules(cart_rules_update: pd.DataFrame, dry_run: bool = True) -> dict:\n",
        "    \"\"\"Upload cart rules updates by cohort.\"\"\"\n",
        "    results = {'success': [], 'failed': []}\n",
        "    \n",
        "    print(f\"\\n  Cart rules to update: {len(cart_rules_update)} products across {cart_rules_update['cohort_id'].nunique()} cohorts\")\n",
        "    \n",
        "    for cohort in cart_rules_update['cohort_id'].unique():\n",
        "        req_data = cart_rules_update[cart_rules_update['cohort_id'] == cohort].copy()\n",
        "        \n",
        "        if len(req_data) > 0:\n",
        "            req_data = req_data[['product_id', 'packing_unit_id', 'new_cart_rule']]\n",
        "            req_data.columns = ['Product ID', 'Packing Unit ID', 'Cart Rules']\n",
        "            \n",
        "            filename = f'{QD_OUTPUT_DIR}/qd_v2_cart_rules_{cohort}.xlsx'\n",
        "            \n",
        "            if dry_run:\n",
        "                print(f\"    [DRY RUN] Cohort {cohort}: Would upload {len(req_data)} rules\")\n",
        "                results['success'].append(cohort)\n",
        "                continue\n",
        "            \n",
        "            req_data.to_excel(filename, index=False)\n",
        "            \n",
        "            time.sleep(2)\n",
        "            response = post_cart_rules(cohort, filename)\n",
        "            \n",
        "            if response.ok:\n",
        "                print(f\"    Cohort {cohort}: {len(req_data)} rules uploaded\")\n",
        "                results['success'].append(cohort)\n",
        "            else:\n",
        "                print(f\"    Cohort {cohort}: Upload failed ({response.status_code})\")\n",
        "                results['failed'].append({'cohort_id': cohort, 'error': response.content[:200]})\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"✓ API functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# QD HANDLER V2 READY\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"QD HANDLER V2 READY TO USE\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nAvailable functions:\")\n",
        "print(\"  - process_qd_v2(df_qd, dry_run=True)      : Main V2 function\")\n",
        "print(\"  - deactivate_active_qd(dry_run=True)      : Deactivate all active QDs\")\n",
        "print(\"  - apply_performance_adjustments(df)       : Apply conversion-based adjustments\")\n",
        "print(\"  - priority_selection_v2(df, limit=400)    : High DOH first selection\")\n",
        "print(\"\\nV2 Key Features:\")\n",
        "print(\"  - Sequential adjustment: Lower qty first (2x), then boost discount (3x)\")\n",
        "print(\"  - Tier-specific thresholds: T1=3%, T2=7%, T3=5%\")\n",
        "print(\"  - High DOH priority: DOH>30 AND inv>10K ranked first\")\n",
        "print(\"  - Margin-based fallback for SKUs without market data\")\n",
        "print(\"  - Elasticity validation for T1/T2 only (T3 exempt)\")\n",
        "print(\"  - Wholesale min multiplier: 2x (was 3x)\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TEST CELL - Run this to test the V2 handler with sample data\n",
        "# =============================================================================\n",
        "\n",
        "# Create sample test data with all required columns\n",
        "sample_df = pd.DataFrame({\n",
        "    # Identifiers\n",
        "    'product_id': [12345, 67890, 11111, 22222, 33333],\n",
        "    'warehouse_id': [1, 1, 236, 236, 337],\n",
        "    'cohort_id': [700, 700, 701, 701, 702],\n",
        "    'sku': ['Test SKU A', 'Test SKU B', 'Test SKU C', 'Test SKU D', 'Test SKU E'],\n",
        "    'brand': ['Brand A', 'Brand B', 'Brand A', 'Brand C', 'Brand B'],\n",
        "    'cat': ['Cat 1', 'Cat 1', 'Cat 2', 'Cat 2', 'Cat 1'],\n",
        "    \n",
        "    # Pricing data\n",
        "    'wac_p': [10.0, 20.0, 15.0, 25.0, 30.0],\n",
        "    'current_price': [15.0, 30.0, 22.0, 38.0, 45.0],\n",
        "    'new_price': [None, None, None, None, None],\n",
        "    'target_margin': [0.33, 0.33, 0.32, 0.34, 0.33],\n",
        "    'min_boundary': [11.0, 22.0, 16.0, 27.0, 32.0],\n",
        "    \n",
        "    # Market margins (converted to prices internally)\n",
        "    'below_market': [0.25, 0.28, 0.26, 0.30, 0.27],\n",
        "    'market_min': [0.28, 0.30, 0.28, 0.32, 0.29],\n",
        "    'market_25': [0.30, 0.32, 0.30, 0.34, 0.31],\n",
        "    'market_50': [0.32, 0.34, 0.32, 0.36, 0.33],\n",
        "    'market_75': [0.34, 0.36, 0.34, 0.38, 0.35],\n",
        "    'market_max': [0.36, 0.38, 0.36, 0.40, 0.37],\n",
        "    'above_market': [0.38, 0.40, 0.38, 0.42, 0.39],\n",
        "    \n",
        "    # Margin tiers\n",
        "    'margin_tier_1': [0.30, 0.32, 0.30, 0.34, 0.31],\n",
        "    'margin_tier_2': [0.28, 0.30, 0.28, 0.32, 0.29],\n",
        "    'margin_tier_3': [0.26, 0.28, 0.26, 0.30, 0.27],\n",
        "    'margin_tier_4': [0.24, 0.26, 0.24, 0.28, 0.25],\n",
        "    'margin_tier_5': [0.22, 0.24, 0.22, 0.26, 0.23],\n",
        "    'margin_tier_above_1': [0.32, 0.34, 0.32, 0.36, 0.33],\n",
        "    'margin_tier_above_2': [0.34, 0.36, 0.34, 0.38, 0.35],\n",
        "    \n",
        "    # Performance data (V2 NEW)\n",
        "    'responsive_doh': [15, 45, 10, 60, 25],  # SKU B and D are high DOH\n",
        "    'stocks': [100, 500, 50, 800, 150],\n",
        "    'doh': [15, 45, 10, 60, 25],\n",
        "    't1_cntrb_uth': [5.0, 1.0, 8.0, 0.0, 3.0],  # Tier 1 contribution\n",
        "    't2_cntrb_uth': [3.0, 0.5, 6.0, 0.0, 2.0],  # Tier 2 contribution\n",
        "    't3_cntrb_uth': [1.0, 0.0, 3.0, 0.0, 1.0],  # Tier 3 contribution\n",
        "    'has_active_qd': [True, True, False, True, False],\n",
        "    'mtd_qty': [200, 150, 300, 100, 250],\n",
        "    \n",
        "    # QD configuration\n",
        "    'keep_qd_tiers': [['T1', 'T2', 'T3'], ['T1', 'T2'], ['T1', 'T2', 'T3'], ['T1', 'T2', 'T3'], ['T1', 'T2']],\n",
        "    \n",
        "    # Cart rules\n",
        "    'current_cart_rule': [5, 10, 5, 10, 5],\n",
        "    'new_cart_rule': [None, None, None, None, None],\n",
        "})\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"SAMPLE TEST DATA\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nTotal SKUs: {len(sample_df)}\")\n",
        "print(f\"Warehouses: {sample_df['warehouse_id'].unique().tolist()}\")\n",
        "print(f\"\\nHigh DOH SKUs (DOH>{HIGH_DOH_THRESHOLD}):\")\n",
        "high_doh = sample_df[sample_df['responsive_doh'] > HIGH_DOH_THRESHOLD]\n",
        "for _, row in high_doh.iterrows():\n",
        "    inv_value = row['stocks'] * row['current_price']\n",
        "    print(f\"  - {row['sku']}: DOH={row['responsive_doh']}, inv_value={inv_value:.0f} EGP\")\n",
        "\n",
        "print(f\"\\nSKUs with existing QD (has_active_qd=True):\")\n",
        "for _, row in sample_df[sample_df['has_active_qd'] == True].iterrows():\n",
        "    print(f\"  - {row['sku']}: T1={row['t1_cntrb_uth']}%, T2={row['t2_cntrb_uth']}%, T3={row['t3_cntrb_uth']}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Running process_qd_v2 with dry_run=True...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Run the V2 handler\n",
        "result = process_qd_v2(sample_df, dry_run=True)\n",
        "\n",
        "# Display results\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"RESULT INSPECTION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nProcessing mode: {result['mode']}\")\n",
        "print(f\"Total input: {result['total_input']}\")\n",
        "print(f\"Processed: {result['processed']}\")\n",
        "print(f\"Failed: {result['failed']}\")\n",
        "print(f\"Total tiers: {result.get('total_tiers', 0)}\")\n",
        "print(f\"High DOH count: {result.get('high_doh_count', 0)}\")\n",
        "\n",
        "if 'df_work' in result and len(result['df_work']) > 0:\n",
        "    df_result = result['df_work']\n",
        "    print(f\"\\n--- Selected SKUs ({len(df_result)}) ---\")\n",
        "    display_cols = ['sku', 'warehouse_id', 'is_high_doh', \n",
        "                    'tier_1_qty', 'tier_1_disc_pct', 't1_action',\n",
        "                    'tier_2_qty', 'tier_2_disc_pct', 't2_action',\n",
        "                    'ws_qty', 'ws_discount_pct', 't3_action']\n",
        "    display_cols = [c for c in display_cols if c in df_result.columns]\n",
        "    print(df_result[display_cols].to_string(index=False))\n",
        "\n",
        "if 'qd_configs' in result and len(result['qd_configs']) > 0:\n",
        "    print(f\"\\n--- QD Configs ({len(result['qd_configs'])}) ---\")\n",
        "    for config in result['qd_configs'][:3]:\n",
        "        tiers_str = \", \".join([f\"T{t['tier']}:qty={t['quantity']},disc={t['discount_pct']:.1f}%\" for t in config['tiers']])\n",
        "        print(f\"  {config['sku']}: [{tiers_str}] (High DOH: {config.get('is_high_doh', 0)})\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
