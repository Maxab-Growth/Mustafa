{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantity Discount (QD) Handler Module\n",
    "\n",
    "Handles calculation, creation, activation, and deactivation of Quantity Discounts.\n",
    "\n",
    "## Usage\n",
    "This module is called from `module_3_periodic_actions.ipynb` with a DataFrame.\n",
    "\n",
    "```python\n",
    "%run qd_handler.ipynb\n",
    "\n",
    "# Process QD with DataFrame from Module 3\n",
    "result = process_qd(df_qd, dry_run=True)\n",
    "```\n",
    "\n",
    "## Input Requirements (DataFrame columns from Module 3)\n",
    "**Identifiers:**\n",
    "- `product_id`, `warehouse_id`, `cohort_id`, `sku`, `brand`, `cat`\n",
    "\n",
    "**Pricing Data:**\n",
    "- `wac_p` - Weighted Average Cost (per basic unit)\n",
    "- `current_price` - Current price (per basic unit)\n",
    "- `target_margin`, `min_boundary`\n",
    "\n",
    "**Cart Rules:**\n",
    "- `current_cart_rule`, `new_cart_rule`\n",
    "\n",
    "**Market Margins (will be converted to prices):**\n",
    "- `below_market`, `market_min`, `market_25`, `market_50`, `market_75`, `market_max`, `above_market`\n",
    "\n",
    "**Margin Tiers (will be converted to prices):**\n",
    "- `margin_tier_1` through `margin_tier_above_2`\n",
    "\n",
    "**QD Configuration:**\n",
    "- `keep_qd_tiers` - List of tiers to keep, e.g., `['T1', 'T2']`\n",
    "\n",
    "## Workflow\n",
    "1. **Deactivate ALL existing Quantity Discounts (FIRST!)**\n",
    "2. Get top-selling packing unit per product per warehouse (last 90 days)\n",
    "3. Get warehouse ticket statistics for wholesale calculations\n",
    "4. Calculate tier quantities (T1, T2) using historical order data\n",
    "5. Calculate T1 & T2 prices using market margins and margin tiers\n",
    "6. Calculate T3 (wholesale) prices based on delivery cost savings\n",
    "7. Validate T3 constraints (T3 qty > T2 qty, T3 price < T2 price)\n",
    "7.5. Adjust T2 qty if ratio is too low (check_qty < 1.3 and elasticity_ratio > 3)\n",
    "8. Apply keep_qd_tiers filter from Module 3 & calculate tier flags\n",
    "9. Select top 400 **tier entries** per warehouse (sorted by mtd_qty × effective_price)\n",
    "10. Build QD configurations & upload format (Group 1: T1+WS, Group 2: T2)\n",
    "11. Upload QD file to API\n",
    "12. Update cart rules (cart_rule >= max tier quantity)\n",
    "\n",
    "## Important Notes\n",
    "- Uses `new_price` from Module 3 if available, otherwise falls back to `current_price`\n",
    "- The 400 limit is per warehouse for **tier entries**, not SKUs (API limitation)\n",
    "- If a SKU has 3 tiers, it counts as 3 towards the 400 limit\n",
    "- Upload format: Group 1 = T1 + Wholesale (max 200), Group 2 = T2 + overflow\n",
    "- Start time = now + 10 mins, End time = now + 12 hours (Cairo time)\n",
    "- Cart rules are updated to be >= max(T1 qty, T2 qty, WS qty)\n",
    "\n",
    "## Wholesale (T3) Logic\n",
    "- Car cost savings passed to retailer for bulk orders\n",
    "- Multiplier range: 3x to orders_per_car_by_weight\n",
    "- Constraints:\n",
    "  - Minimum margin: max(40% of current margin, 1.5%)\n",
    "  - Max ticket size: 35,000 EGP\n",
    "  - T3 price must be < T2 price\n",
    "\n",
    "## Output\n",
    "Returns dict with processing results, configs, and working DataFrame and QD configurations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS & CONFIGURATION\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "import os\n",
    "import ast\n",
    "import json\n",
    "import time\n",
    "import base64\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import snowflake.connector\n",
    "import sys\n",
    "\n",
    "%run queries_module.ipynb\n",
    "# Add parent directory for imports\n",
    "sys.path.insert(0, '..')\n",
    "import setup_environment_2\n",
    "\n",
    "# Initialize environment variables (loads Snowflake credentials)\n",
    "setup_environment_2.initialize_env()\n",
    "\n",
    "# Cairo Timezone\n",
    "CAIRO_TZ = pytz.timezone('Africa/Cairo')\n",
    "CAIRO_NOW = datetime.now(CAIRO_TZ)\n",
    "TODAY = CAIRO_NOW.date()\n",
    "\n",
    "# =============================================================================\n",
    "# SNOWFLAKE CONNECTION\n",
    "# =============================================================================\n",
    "def query_snowflake(query):\n",
    "    \"\"\"Execute a query on Snowflake and return results as DataFrame.\"\"\"\n",
    "    con = snowflake.connector.connect(\n",
    "        user=os.environ[\"SNOWFLAKE_USERNAME\"],\n",
    "        account=os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "        password=os.environ[\"SNOWFLAKE_PASSWORD\"],\n",
    "        database=os.environ[\"SNOWFLAKE_DATABASE\"]\n",
    "    )\n",
    "    try:\n",
    "        cur = con.cursor()\n",
    "        cur.execute(\"USE WAREHOUSE COMPUTE_WH\")\n",
    "        cur.execute(query)\n",
    "        data = cur.fetchall()\n",
    "        columns = [desc[0].lower() for desc in cur.description]\n",
    "        return pd.DataFrame(data, columns=columns)\n",
    "    finally:\n",
    "        con.close()\n",
    "\n",
    "def get_snowflake_timezone():\n",
    "    result = query_snowflake(\"SHOW PARAMETERS LIKE 'TIMEZONE'\")\n",
    "    return result['value'].iloc[0] if len(result) > 0 else \"UTC\"\n",
    "\n",
    "TIMEZONE = get_snowflake_timezone()\n",
    "\n",
    "# =============================================================================\n",
    "# AWS & API FUNCTIONS\n",
    "# =============================================================================\n",
    "def get_secret(secret_name: str) -> str:\n",
    "    \"\"\"Retrieve a secret from AWS Secrets Manager.\"\"\"\n",
    "    region_name = \"us-east-1\"\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(service_name='secretsmanager', region_name=region_name)\n",
    "\n",
    "    try:\n",
    "        response = client.get_secret_value(SecretId=secret_name)\n",
    "    except ClientError as e:\n",
    "        print(f\"AWS Error: {e}\")\n",
    "        raise e\n",
    "    \n",
    "    if 'SecretString' in response:\n",
    "        return response['SecretString']\n",
    "    return base64.b64decode(response['SecretBinary'])\n",
    "\n",
    "\n",
    "def get_access_token(url: str, client_id: str, client_secret: str) -> str:\n",
    "    \"\"\"Get OAuth2 access token for MaxAB API authentication.\"\"\"\n",
    "    response = requests.post(\n",
    "        url,\n",
    "        data={\n",
    "            'grant_type': 'password',\n",
    "            'client_id': client_id,\n",
    "            'client_secret': client_secret,\n",
    "            'username': API_USERNAME,\n",
    "            'password': API_PASSWORD\n",
    "        }\n",
    "    )\n",
    "    return response.json()['access_token']\n",
    "\n",
    "\n",
    "def _get_api_token() -> str:\n",
    "    \"\"\"Get a fresh API token for MaxAB API requests.\"\"\"\n",
    "    return get_access_token(\n",
    "        'https://sso.maxab.info/auth/realms/maxab/protocol/openid-connect/token',\n",
    "        'main-system-externals',\n",
    "        API_SECRET\n",
    "    )\n",
    "\n",
    "# =============================================================================\n",
    "# API CREDENTIALS INITIALIZATION\n",
    "# =============================================================================\n",
    "pricing_api_secret = json.loads(get_secret(\"prod/pricing/api/\"))\n",
    "API_USERNAME = pricing_api_secret[\"egypt_username\"]\n",
    "API_PASSWORD = pricing_api_secret[\"egypt_password\"]\n",
    "API_SECRET = pricing_api_secret[\"egypt_secret\"]\n",
    "\n",
    "# =============================================================================\n",
    "# API CONFIGURATION\n",
    "# =============================================================================\n",
    "QD_API_URL = 'https://api.maxab.app/commerce/api/admins/v1/quantity-discounts/'\n",
    "\n",
    "# Default QD settings\n",
    "DEFAULT_QD_DURATION_HOURS = 12  # QD valid until next run\n",
    "\n",
    "print(\"✓ QD Handler initialized\")\n",
    "print(f\"  Timezone: {TIMEZONE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# QD CALCULATION CONFIGURATION\n",
    "# =============================================================================\n",
    "import numpy as np\n",
    "\n",
    "# Discount bounds (same as original)\n",
    "MAX_DISCOUNT_PCT = 5.0    # Maximum allowed discount from current price\n",
    "MIN_DISCOUNT_PCT = 0.35   # Minimum required discount from current price\n",
    "\n",
    "# Ratio constraints: discount_2/discount_1 should be between MIN_RATIO and MAX_RATIO times qty_2/qty_1\n",
    "MIN_RATIO = 1.1  # Minimum elasticity ratio\n",
    "MAX_RATIO = 3.0  # Maximum elasticity ratio\n",
    "\n",
    "# Minimum gap between tier prices\n",
    "MIN_GAP_PCT = 0.25\n",
    "\n",
    "# =============================================================================\n",
    "# WHOLESALE (TIER 3) CONFIGURATION\n",
    "# =============================================================================\n",
    "WS_CAR_COST = 1400           # Cost per delivery (EGP)\n",
    "WS_CAR_CAPACITY_TONS = 1.8   # Max car capacity in tons\n",
    "WS_MAX_TICKET_SIZE = 35000   # Maximum ticket size (EGP)\n",
    "WS_MIN_MARGIN = 0.015        # Minimum margin (1.5%) above WAC\n",
    "\n",
    "# Top SKU selection\n",
    "TOP_SKUS_PER_WAREHOUSE = 400  # Number of top tier entries to select per warehouse\n",
    "\n",
    "# =============================================================================\n",
    "# UPLOAD FORMAT CONFIGURATION\n",
    "# =============================================================================\n",
    "MAX_GROUP_SIZE = 200         # Max items per discount group in API\n",
    "MAX_DISCOUNT_CAP_T1 = 4.0    # Maximum discount for Tier 1\n",
    "MAX_DISCOUNT_CAP_T2 = 5.0    # Maximum discount for Tier 2\n",
    "MAX_DISCOUNT_CAP_WS = 6.0    # Maximum discount for Wholesale\n",
    "\n",
    "# QD Duration\n",
    "QD_DURATION_HOURS = 12       # QD valid for 12 hours\n",
    "\n",
    "# =============================================================================\n",
    "# WAREHOUSE TO TAG ID MAPPING\n",
    "# =============================================================================\n",
    "WAREHOUSE_TAG_MAPPING = {\n",
    "    501: {'name': 'Assiut FC', 'tag_id': 3301},\n",
    "    401: {'name': 'Bani sweif', 'tag_id': 3302},\n",
    "    236: {'name': 'Barageel', 'tag_id': 3303},\n",
    "    337: {'name': 'El-Mahala', 'tag_id': 3304},\n",
    "    797: {'name': 'Khorshed Alex', 'tag_id': 3305},\n",
    "    339: {'name': 'Mansoura FC', 'tag_id': 3306},\n",
    "    703: {'name': 'Menya Samalot', 'tag_id': 3307},\n",
    "    1: {'name': 'Mostorod', 'tag_id': 3308},\n",
    "    962: {'name': 'Sakkarah', 'tag_id': 3309},\n",
    "    170: {'name': 'Sharqya', 'tag_id': 3310},\n",
    "    632: {'name': 'Sohag', 'tag_id': 3311},\n",
    "    8: {'name': 'Tanta', 'tag_id': 3312},\n",
    "    38: {'name': 'El-Marg', 'tag_id': 3313},  # Added if missing\n",
    "}\n",
    "\n",
    "print(\"✓ QD calculation parameters:\")\n",
    "print(f\"  MAX_DISCOUNT_PCT: {MAX_DISCOUNT_PCT}%\")\n",
    "print(f\"  MIN_DISCOUNT_PCT: {MIN_DISCOUNT_PCT}%\")\n",
    "print(f\"  RATIO RANGE: [{MIN_RATIO}, {MAX_RATIO}]\")\n",
    "print(f\"\\n✓ Wholesale (T3) parameters:\")\n",
    "print(f\"  WS_CAR_COST: {WS_CAR_COST} EGP\")\n",
    "print(f\"  WS_MAX_TICKET_SIZE: {WS_MAX_TICKET_SIZE} EGP\")\n",
    "print(f\"  WS_MIN_MARGIN: {WS_MIN_MARGIN*100}%\")\n",
    "print(f\"  TOP_SKUS_PER_WAREHOUSE: {TOP_SKUS_PER_WAREHOUSE}\")\n",
    "print(f\"\\n✓ Upload parameters:\")\n",
    "print(f\"  MAX_GROUP_SIZE: {MAX_GROUP_SIZE}\")\n",
    "print(f\"  QD_DURATION_HOURS: {QD_DURATION_HOURS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA FETCHING: PACKING UNITS & TIER QUANTITIES\n",
    "# =============================================================================\n",
    "\n",
    "def get_top_selling_packing_units(product_warehouse_list: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get the top-selling packing unit per product per warehouse (last 90 days).\n",
    "    \n",
    "    Args:\n",
    "        product_warehouse_list: List of (product_id, warehouse_id) tuples\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with product_id, warehouse_id, packing_unit_id, basic_unit_count\n",
    "    \"\"\"\n",
    "    if not product_warehouse_list:\n",
    "        return pd.DataFrame(columns=['product_id', 'warehouse_id', 'packing_unit_id', 'basic_unit_count'])\n",
    "    \n",
    "    # Build tuples string for SQL\n",
    "    tuples_str = ','.join([f\"({int(p)}, {int(w)})\" for p, w in product_warehouse_list])\n",
    "    \n",
    "    query = f'''\n",
    "    WITH input_products AS (\n",
    "        SELECT product_id, warehouse_id\n",
    "        FROM (VALUES {tuples_str}) AS x(product_id, warehouse_id)\n",
    "    ),\n",
    "    \n",
    "    sales_by_pu AS (\n",
    "        SELECT \n",
    "            pso.product_id,\n",
    "            so.warehouse_id,\n",
    "            pso.packing_unit_id,\n",
    "            SUM(pso.total_price) as nmv\n",
    "        FROM product_sales_order pso\n",
    "        JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "        JOIN input_products ip ON ip.product_id = pso.product_id AND ip.warehouse_id = so.warehouse_id\n",
    "        WHERE so.created_at >= CURRENT_DATE - 90\n",
    "            AND so.sales_order_status_id NOT IN (7, 12)\n",
    "            AND so.channel IN ('telesales', 'retailer')\n",
    "            AND pso.purchased_item_count <> 0\n",
    "        GROUP BY 1, 2, 3\n",
    "    ),\n",
    "    \n",
    "    ranked_pu AS (\n",
    "        SELECT \n",
    "            s.product_id, \n",
    "            s.warehouse_id, \n",
    "            s.packing_unit_id,\n",
    "            pup.basic_unit_count,\n",
    "            s.nmv,\n",
    "            ROW_NUMBER() OVER (PARTITION BY s.product_id, s.warehouse_id ORDER BY s.nmv DESC) as rnk\n",
    "        FROM sales_by_pu s\n",
    "        JOIN packing_unit_products pup \n",
    "            ON pup.product_id = s.product_id \n",
    "            AND pup.packing_unit_id = s.packing_unit_id\n",
    "        WHERE pup.deleted_at IS NULL\n",
    "    )\n",
    "    \n",
    "    SELECT product_id, warehouse_id, packing_unit_id, basic_unit_count\n",
    "    FROM ranked_pu\n",
    "    WHERE rnk = 1\n",
    "    '''\n",
    "    \n",
    "    print(\"  Fetching top-selling packing units (last 90 days)...\")\n",
    "    df = query_snowflake(query)\n",
    "    \n",
    "    # Convert to numeric\n",
    "    for col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='ignore')\n",
    "    \n",
    "    print(f\"    Found packing units for {len(df)} product-warehouse combinations\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_tier_quantities(product_warehouse_pu_list: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate tier quantities based on historical order data.\n",
    "    \n",
    "    Args:\n",
    "        product_warehouse_pu_list: List of (warehouse_id, product_id, packing_unit_id) tuples\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with tier_1_qty, tier_2_qty per product-warehouse\n",
    "    \"\"\"\n",
    "    if not product_warehouse_pu_list:\n",
    "        return pd.DataFrame(columns=['warehouse_id', 'product_id', 'packing_unit_id', 'tier_1_qty', 'tier_2_qty'])\n",
    "    \n",
    "    # Build tuples string for SQL\n",
    "    tuples_str = ','.join([f\"({int(w)}, {int(p)}, {int(pu)})\" for w, p, pu in product_warehouse_pu_list])\n",
    "    \n",
    "    query = f'''\n",
    "    WITH selected_products AS (\n",
    "        SELECT warehouse_id, product_id, packing_unit_id\n",
    "        FROM (VALUES {tuples_str}) AS x(warehouse_id, product_id, packing_unit_id)\n",
    "    ),\n",
    "    \n",
    "    -- Retailers in QD cohorts\n",
    "    base AS (\n",
    "        SELECT *, ROW_NUMBER() OVER (PARTITION BY retailer_id ORDER BY priority) as rnk \n",
    "        FROM (\n",
    "            SELECT x.*, TAGGABLE_ID as retailer_id \n",
    "            FROM (\n",
    "                SELECT id as cohort_id, name as cohort_name, priority, dynamic_tag_id \n",
    "                FROM cohorts \n",
    "                WHERE is_active = 'true'\n",
    "                    AND id IN (700,701,702,703,704,1123,1124,1125,1126)\n",
    "            ) x \n",
    "            JOIN DYNAMIC_TAGgables dt ON x.dynamic_tag_id = dt.dynamic_tag_id\n",
    "            WHERE dt.taggable_id not IN (\n",
    "                SELECT taggable_id FROM DYNAMIC_TAGgables \n",
    "                WHERE dynamic_tag_id IN (2807, 2808, 2809, 2810, 2811, 2812)\n",
    "            )\n",
    "        )\n",
    "        QUALIFY rnk = 1 \n",
    "    ),\n",
    "    \n",
    "    -- Warehouse mapping\n",
    "    warehouse_mapping AS (\n",
    "        SELECT * FROM (VALUES\n",
    "            ('Cairo', 'Mostorod', 1),\n",
    "            ('Giza', 'Barageel', 236),\n",
    "            ('Giza', 'Sakkarah', 962),\n",
    "            ('Delta West', 'El-Mahala', 337),\n",
    "            ('Delta West', 'Tanta', 8),\n",
    "            ('Delta East', 'Mansoura FC', 339),\n",
    "            ('Delta East', 'Sharqya', 170),\n",
    "            ('Upper Egypt', 'Assiut FC', 501),\n",
    "            ('Upper Egypt', 'Bani sweif', 401),\n",
    "            ('Upper Egypt', 'Menya Samalot', 703),\n",
    "            ('Upper Egypt', 'Sohag', 632),\n",
    "            ('Alexandria', 'Khorshed Alex', 797)\n",
    "        ) x(region_name, wh, warehouse_id)\n",
    "    ),\n",
    "    \n",
    "    raw_order_quantities AS (\n",
    "        SELECT \n",
    "            whs.warehouse_id,\n",
    "            pso.product_id,\n",
    "            pso.packing_unit_id,\n",
    "            so.parent_sales_order_id,\n",
    "            so.retailer_id,\n",
    "            so.created_at::date as order_date,\n",
    "            SUM(pso.purchased_item_count) as order_qty,\n",
    "            EXP(-0.02 * DATEDIFF('day', so.created_at::date, CURRENT_DATE)) as recency_weight\n",
    "            \n",
    "        FROM product_sales_order pso\n",
    "        JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "        JOIN base ON base.retailer_id = so.retailer_id\n",
    "        JOIN materialized_views.retailer_polygon ON materialized_views.retailer_polygon.retailer_id = so.retailer_id\n",
    "        JOIN districts ON districts.id = materialized_views.retailer_polygon.district_id\n",
    "        JOIN cities ON cities.id = districts.city_id\n",
    "        JOIN states ON states.id = cities.state_id\n",
    "        JOIN regions ON regions.id = states.region_id\n",
    "        JOIN warehouse_mapping whs ON whs.region_name = CASE WHEN regions.id = 2 THEN states.name_en ELSE regions.name_en END\n",
    "        JOIN selected_products sp ON sp.warehouse_id = whs.warehouse_id \n",
    "            AND sp.product_id = pso.product_id\n",
    "            AND sp.packing_unit_id = pso.packing_unit_id\n",
    "        \n",
    "        WHERE TRUE\n",
    "            AND so.created_at::date BETWEEN DATE_TRUNC('month', CURRENT_DATE - INTERVAL '4 months') AND CURRENT_DATE - 1\n",
    "            AND so.sales_order_status_id NOT IN (7, 12)\n",
    "            AND so.channel IN ('telesales', 'retailer')\n",
    "            AND pso.purchased_item_count <> 0\n",
    "        \n",
    "        GROUP BY 1, 2, 3, 4, 5, 6\n",
    "    ),\n",
    "    \n",
    "    retailer_frequency AS (\n",
    "        SELECT \n",
    "            warehouse_id, product_id, packing_unit_id, retailer_id,\n",
    "            COUNT(DISTINCT parent_sales_order_id) as order_count,\n",
    "            COUNT(DISTINCT DATE_TRUNC('week', order_date)) as weeks_ordered\n",
    "        FROM raw_order_quantities\n",
    "        GROUP BY 1, 2, 3, 4\n",
    "    ),\n",
    "    \n",
    "    frequent_buyers AS (\n",
    "        SELECT warehouse_id, product_id, packing_unit_id, retailer_id\n",
    "        FROM retailer_frequency\n",
    "        WHERE order_count >= 2 OR weeks_ordered >= 2\n",
    "    ),\n",
    "    \n",
    "    filtered_orders AS (\n",
    "        SELECT roq.*\n",
    "        FROM raw_order_quantities roq\n",
    "        JOIN frequent_buyers fb ON fb.warehouse_id = roq.warehouse_id\n",
    "            AND fb.product_id = roq.product_id\n",
    "            AND fb.packing_unit_id = roq.packing_unit_id\n",
    "            AND fb.retailer_id = roq.retailer_id\n",
    "    ),\n",
    "    \n",
    "    initial_stats AS (\n",
    "        SELECT \n",
    "            warehouse_id, product_id, packing_unit_id,\n",
    "            PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY order_qty) as q1,\n",
    "            PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY order_qty) as q3,\n",
    "            MEDIAN(order_qty) as median_qty,\n",
    "            STDDEV_POP(order_qty) as stddev_qty,\n",
    "            AVG(order_qty) as avg_qty\n",
    "        FROM filtered_orders\n",
    "        GROUP BY 1, 2, 3\n",
    "    ),\n",
    "    \n",
    "    cleaned_orders AS (\n",
    "        SELECT fo.*\n",
    "        FROM filtered_orders fo\n",
    "        JOIN initial_stats ist ON ist.warehouse_id = fo.warehouse_id\n",
    "            AND ist.product_id = fo.product_id\n",
    "            AND ist.packing_unit_id = fo.packing_unit_id\n",
    "        WHERE fo.order_qty >= ist.q1 - 1.5 * (ist.q3 - ist.q1)\n",
    "            AND fo.order_qty <= ist.q3 + 1.5 * (ist.q3 - ist.q1)\n",
    "            AND (ist.stddev_qty = 0 OR ABS(fo.order_qty - ist.avg_qty) <= 3 * ist.stddev_qty)\n",
    "    ),\n",
    "    \n",
    "    recent_trends AS (\n",
    "        SELECT \n",
    "            warehouse_id, product_id, packing_unit_id,\n",
    "            SUM(order_qty * recency_weight) / NULLIF(SUM(recency_weight), 0) as weighted_avg_qty,\n",
    "            AVG(CASE WHEN order_date >= CURRENT_DATE - 15 THEN order_qty END) as last_15d_avg,\n",
    "            MEDIAN(CASE WHEN order_date >= CURRENT_DATE - 15 THEN order_qty END) as last_15d_median,\n",
    "            MAX(CASE WHEN order_date >= CURRENT_DATE - 15 THEN order_qty END) as last_15d_max,\n",
    "            COUNT(CASE WHEN order_date >= CURRENT_DATE - 15 THEN 1 END) as last_15d_orders\n",
    "        FROM cleaned_orders\n",
    "        GROUP BY 1, 2, 3\n",
    "    ),\n",
    "    \n",
    "    quantity_stats AS (\n",
    "        SELECT \n",
    "            warehouse_id, product_id, packing_unit_id,\n",
    "            COUNT(DISTINCT parent_sales_order_id) as total_orders,\n",
    "            MEDIAN(order_qty) as median_qty,\n",
    "            STDDEV_POP(order_qty) as stddev_qty,\n",
    "            PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY order_qty) as q3_qty,\n",
    "            PERCENTILE_CONT(0.85) WITHIN GROUP (ORDER BY order_qty) as p85_qty,\n",
    "            PERCENTILE_CONT(0.90) WITHIN GROUP (ORDER BY order_qty) as p90_qty,\n",
    "            PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY order_qty) as p95_qty\n",
    "        FROM cleaned_orders\n",
    "        GROUP BY 1, 2, 3\n",
    "    ),\n",
    "    \n",
    "    most_frequent_qty AS (\n",
    "        SELECT warehouse_id, product_id, packing_unit_id, order_qty as mode_qty\n",
    "        FROM (\n",
    "            SELECT warehouse_id, product_id, packing_unit_id, order_qty,\n",
    "                   COUNT(*) as freq,\n",
    "                   ROW_NUMBER() OVER (PARTITION BY warehouse_id, product_id, packing_unit_id ORDER BY COUNT(*) DESC, order_qty DESC) as rn\n",
    "            FROM cleaned_orders\n",
    "            GROUP BY 1, 2, 3, 4\n",
    "        )\n",
    "        WHERE rn = 1\n",
    "    ),\n",
    "    \n",
    "    tier_calculations AS (\n",
    "        SELECT \n",
    "            qs.warehouse_id, qs.product_id, qs.packing_unit_id,\n",
    "            qs.median_qty, qs.stddev_qty, qs.q3_qty, qs.p85_qty, qs.p90_qty, qs.p95_qty,\n",
    "            COALESCE(mf.mode_qty, qs.median_qty) as mode_qty,\n",
    "            rt.weighted_avg_qty, rt.last_15d_median, rt.last_15d_max, rt.last_15d_orders,\n",
    "            \n",
    "            -- Tier 1 calculation\n",
    "            CEIL(GREATEST(\n",
    "                (0.7 * qs.median_qty + 0.3 * COALESCE(rt.weighted_avg_qty, qs.median_qty)) + 1.0 * COALESCE(qs.stddev_qty, 1),\n",
    "                qs.q3_qty,\n",
    "                COALESCE(mf.mode_qty, qs.median_qty) + GREATEST(3, qs.median_qty * 0.3),\n",
    "                CASE \n",
    "                    WHEN rt.last_15d_orders >= 2 AND rt.last_15d_median > qs.median_qty \n",
    "                    THEN rt.last_15d_median * 1.2\n",
    "                    ELSE qs.median_qty * 1.3\n",
    "                END,\n",
    "                qs.median_qty + 2\n",
    "            )) as tier_1_qty,\n",
    "            \n",
    "            -- Tier 2 base calculation\n",
    "            CEIL(GREATEST(\n",
    "                qs.q3_qty + 1.5 * COALESCE(qs.stddev_qty, 1),\n",
    "                qs.p85_qty + 1.0 * COALESCE(qs.stddev_qty, 1),\n",
    "                qs.p90_qty + 0.5 * COALESCE(qs.stddev_qty, 1),\n",
    "                qs.p95_qty,\n",
    "                (0.6 * qs.median_qty + 0.4 * COALESCE(rt.weighted_avg_qty, qs.median_qty)) * 2.0,\n",
    "                CASE \n",
    "                    WHEN rt.last_15d_orders >= 2 AND rt.last_15d_max > qs.p90_qty \n",
    "                    THEN rt.last_15d_max * 1.1\n",
    "                    ELSE qs.median_qty * 1.6\n",
    "                END\n",
    "            )) as tier_2_qty_base\n",
    "            \n",
    "        FROM quantity_stats qs\n",
    "        LEFT JOIN most_frequent_qty mf ON mf.warehouse_id = qs.warehouse_id \n",
    "            AND mf.product_id = qs.product_id AND mf.packing_unit_id = qs.packing_unit_id\n",
    "        LEFT JOIN recent_trends rt ON rt.warehouse_id = qs.warehouse_id\n",
    "            AND rt.product_id = qs.product_id AND rt.packing_unit_id = qs.packing_unit_id\n",
    "    )\n",
    "    \n",
    "    SELECT \n",
    "        warehouse_id, product_id, packing_unit_id,\n",
    "        tier_1_qty,\n",
    "        LEAST(\n",
    "            CEIL(GREATEST(tier_2_qty_base, tier_1_qty * 1.6)),\n",
    "            GREATEST(tier_1_qty * 3.5, tier_1_qty + 20)\n",
    "        ) as tier_2_qty,\n",
    "        median_qty, stddev_qty\n",
    "    FROM tier_calculations\n",
    "    '''\n",
    "    \n",
    "    print(\"  Calculating tier quantities from order history...\")\n",
    "    df = query_snowflake(query)\n",
    "    \n",
    "    # Convert to numeric\n",
    "    for col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='ignore')\n",
    "    \n",
    "    print(f\"    Calculated tiers for {len(df)} product-warehouse combinations\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_warehouse_ticket_stats() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get warehouse-level ticket size statistics for wholesale calculations.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with warehouse_id, avg_ticket_size, orders_per_car_by_weight\n",
    "    \"\"\"\n",
    "    query = f'''\n",
    "    WITH base AS (\n",
    "        SELECT *, ROW_NUMBER() OVER (PARTITION BY retailer_id ORDER BY priority) as rnk \n",
    "        FROM (\n",
    "            SELECT x.*, TAGGABLE_ID as retailer_id \n",
    "            FROM (\n",
    "                SELECT id as cohort_id, name as cohort_name, priority, dynamic_tag_id \n",
    "                FROM cohorts \n",
    "                WHERE is_active = 'true'\n",
    "                    AND id IN (700,701,702,703,704,1123,1124,1125,1126)\n",
    "            ) x \n",
    "            JOIN DYNAMIC_TAGgables dt ON x.dynamic_tag_id = dt.dynamic_tag_id\n",
    "            WHERE dt.taggable_id not IN (\n",
    "                SELECT taggable_id FROM DYNAMIC_TAGgables \n",
    "                WHERE dynamic_tag_id IN (2807, 2808, 2809, 2810, 2811, 2812)\n",
    "            )\n",
    "        )\n",
    "        QUALIFY rnk = 1 \n",
    "    ),\n",
    "\n",
    "    -- Map regions to warehouses\n",
    "    whs AS (\n",
    "        SELECT * FROM (VALUES\n",
    "            ('Cairo', 'El-Marg', 38),\n",
    "            ('Cairo', 'Mostorod', 1),\n",
    "            ('Giza', 'Barageel', 236),\n",
    "            ('Giza', 'Sakkarah', 962),\n",
    "            ('Delta West', 'El-Mahala', 337),\n",
    "            ('Delta West', 'Tanta', 8),\n",
    "            ('Delta East', 'Mansoura FC', 339),\n",
    "            ('Delta East', 'Sharqya', 170),\n",
    "            ('Upper Egypt', 'Assiut FC', 501),\n",
    "            ('Upper Egypt', 'Bani sweif', 401),\n",
    "            ('Upper Egypt', 'Menya Samalot', 703),\n",
    "            ('Upper Egypt', 'Sohag', 632),\n",
    "            ('Alexandria', 'Khorshed Alex', 797)\n",
    "        ) x(region_name, wh, warehouse_id)\n",
    "    ),\n",
    "\n",
    "    -- Get ticket sizes (order values) for last 4 months\n",
    "    ticket_sizes AS (\n",
    "        SELECT \n",
    "            whs.warehouse_id,\n",
    "            whs.wh as warehouse_name,\n",
    "            so.parent_sales_order_id,\n",
    "            so.retailer_id,\n",
    "            SUM(pso.total_price) as ticket_size,\n",
    "            SUM(pso.purchased_item_count * pup.weight / 1000) as order_weight_kg\n",
    "        FROM product_sales_order pso\n",
    "        JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "        JOIN base ON base.retailer_id = so.retailer_id\n",
    "        JOIN packing_unit_products pup ON pup.product_id = pso.product_id \n",
    "            AND pup.packing_unit_id = pso.packing_unit_id\n",
    "        JOIN materialized_views.retailer_polygon rp ON rp.retailer_id = so.retailer_id\n",
    "        JOIN districts ON districts.id = rp.district_id\n",
    "        JOIN cities ON cities.id = districts.city_id\n",
    "        JOIN states ON states.id = cities.state_id\n",
    "        JOIN regions ON regions.id = states.region_id\n",
    "        JOIN whs ON whs.region_name = CASE WHEN regions.id = 2 THEN states.name_en ELSE regions.name_en END\n",
    "        WHERE so.created_at::date BETWEEN DATE_TRUNC('month', CURRENT_DATE - INTERVAL '4 months') AND CURRENT_DATE - 1\n",
    "            AND so.sales_order_status_id NOT IN (7, 12)\n",
    "            AND so.channel IN ('telesales', 'retailer')\n",
    "            AND pso.purchased_item_count > 0\n",
    "        GROUP BY whs.warehouse_id, whs.wh, so.parent_sales_order_id, so.retailer_id\n",
    "    ),\n",
    "\n",
    "    -- Calculate warehouse-level statistics\n",
    "    warehouse_stats AS (\n",
    "        SELECT \n",
    "            warehouse_id,\n",
    "            warehouse_name,\n",
    "            COUNT(DISTINCT parent_sales_order_id) as total_orders,\n",
    "            COUNT(DISTINCT retailer_id) as total_retailers,\n",
    "            AVG(ticket_size) as avg_ticket_size,\n",
    "            MEDIAN(ticket_size) as median_ticket_size,\n",
    "            AVG(order_weight_kg) as avg_order_weight_kg\n",
    "        FROM ticket_sizes\n",
    "        WHERE ticket_size > 0\n",
    "        GROUP BY warehouse_id, warehouse_name\n",
    "    )\n",
    "\n",
    "    SELECT \n",
    "        warehouse_id,\n",
    "        warehouse_name,\n",
    "        ROUND(avg_ticket_size, 2) as avg_ticket_size,\n",
    "        ROUND(median_ticket_size, 2) as median_ticket_size,\n",
    "        ROUND(avg_order_weight_kg, 2) as avg_order_weight_kg,\n",
    "        -- Calculate how many orders fit in one car based on weight\n",
    "        ROUND({WS_CAR_CAPACITY_TONS * 1000} / NULLIF(avg_order_weight_kg, 0), 1) as orders_per_car_by_weight\n",
    "    FROM warehouse_stats\n",
    "    ORDER BY warehouse_id\n",
    "    '''\n",
    "    \n",
    "    print(\"  Fetching warehouse ticket statistics...\")\n",
    "    df = query_snowflake(query)\n",
    "    \n",
    "    # Convert to numeric\n",
    "    for col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='ignore')\n",
    "    \n",
    "    print(f\"    Got stats for {len(df)} warehouses\")\n",
    "    return df\n",
    "\n",
    "print(\"✓ Data fetching functions defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TIER PRICE CALCULATION\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_tier_prices(row):\n",
    "    \"\"\"\n",
    "    Calculate tier 1 and tier 2 prices for a single row.\n",
    "    \n",
    "    Uses market margins and margin tiers (converted to prices) to find\n",
    "    the best two prices within discount bounds.\n",
    "    \n",
    "    Constraints:\n",
    "    - Ensure: WAC < Tier 2 < Tier 1 < Current Price\n",
    "    - Ensure: BOTH tiers must be valid or BOTH are None\n",
    "    - Enforce ratio: discount_2/discount_1 / (qty_2/qty_1) between MIN_RATIO and MAX_RATIO\n",
    "    \n",
    "    Args:\n",
    "        row: DataFrame row with wac_p, current_price, market margins, margin tiers, tier quantities\n",
    "        \n",
    "    Returns:\n",
    "        Series with tier_1_price, tier_2_price, price_source\n",
    "    \"\"\"\n",
    "    current_price = row.get('packing_unit_price')  # Price for packing unit\n",
    "    wac = row.get('wac_pu')  # WAC for packing unit\n",
    "    tier_1_qty = row.get('tier_1_qty')\n",
    "    tier_2_qty = row.get('tier_2_qty')\n",
    "    \n",
    "    # Validation\n",
    "    if pd.isna(current_price) or current_price <= 0:\n",
    "        return pd.Series({'tier_1_price': np.nan, 'tier_2_price': np.nan, 'price_source': 'invalid_current_price'})\n",
    "    \n",
    "    if pd.isna(wac) or wac <= 0:\n",
    "        return pd.Series({'tier_1_price': np.nan, 'tier_2_price': np.nan, 'price_source': 'invalid_wac'})\n",
    "    \n",
    "    if current_price <= wac:\n",
    "        return pd.Series({'tier_1_price': np.nan, 'tier_2_price': np.nan, 'price_source': 'current_price_below_wac'})\n",
    "    \n",
    "    # Calculate discount bounds\n",
    "    max_discount_price = current_price * (1 - MAX_DISCOUNT_PCT / 100)  # Minimum allowed price (max discount)\n",
    "    min_discount_price = current_price * (1 - MIN_DISCOUNT_PCT / 100)  # Maximum allowed price (min discount)\n",
    "    \n",
    "    # Collect candidate prices from market margins (convert margin to price)\n",
    "    candidate_prices = []\n",
    "    \n",
    "    # Market margin columns (these are margins, convert to prices)\n",
    "    market_margin_cols = ['below_market', 'market_min', 'market_25', 'market_50', \n",
    "                          'market_75', 'market_max', 'above_market']\n",
    "    \n",
    "    for col in market_margin_cols:\n",
    "        margin = row.get(col)\n",
    "        if pd.notna(margin) and 0 < margin < 1:\n",
    "            price = wac / (1 - margin)\n",
    "            if max_discount_price <= price <= min_discount_price and price > wac:\n",
    "                candidate_prices.append(('market', col, price))\n",
    "    \n",
    "    # Margin tier columns (these are margins, convert to prices)\n",
    "    margin_tier_cols = ['margin_tier_1', 'margin_tier_2', 'margin_tier_3', 'margin_tier_4',\n",
    "                        'margin_tier_5', 'margin_tier_above_1', 'margin_tier_above_2']\n",
    "    \n",
    "    for col in margin_tier_cols:\n",
    "        margin = row.get(col)\n",
    "        if pd.notna(margin) and 0 < margin < 1:\n",
    "            price = wac / (1 - margin)\n",
    "            if max_discount_price <= price <= min_discount_price and price > wac:\n",
    "                candidate_prices.append(('margin_tier', col, price))\n",
    "    \n",
    "    # Remove duplicates and sort by price descending\n",
    "    unique_prices = {}\n",
    "    for source_type, source_col, price in candidate_prices:\n",
    "        price_rounded = round(price, 2)\n",
    "        if price_rounded not in unique_prices:\n",
    "            unique_prices[price_rounded] = (source_type, source_col)\n",
    "    \n",
    "    valid_prices = sorted(unique_prices.keys(), reverse=True)\n",
    "    \n",
    "    # Need at least 2 prices\n",
    "    if len(valid_prices) < 2:\n",
    "        return pd.Series({'tier_1_price': np.nan, 'tier_2_price': np.nan, 'price_source': 'insufficient_valid_prices'})\n",
    "    \n",
    "    tier_1 = None\n",
    "    tier_2 = None\n",
    "    source = ''\n",
    "    \n",
    "    # Strategy: Find two prices with minimum gap\n",
    "    for i, p1 in enumerate(valid_prices):\n",
    "        for p2 in valid_prices[i+1:]:\n",
    "            # Ensure minimum gap between tiers\n",
    "            if p2 < p1 * (1 - MIN_GAP_PCT / 100):\n",
    "                tier_1 = p1\n",
    "                tier_2 = p2\n",
    "                source = f\"{unique_prices[p1][0]}_{unique_prices[p2][0]}\"\n",
    "                break\n",
    "        if tier_1 is not None:\n",
    "            break\n",
    "    \n",
    "    # If no pair with minimum gap, take top two\n",
    "    if tier_1 is None and len(valid_prices) >= 2:\n",
    "        tier_1 = valid_prices[0]\n",
    "        tier_2 = valid_prices[1]\n",
    "        source = 'top_two_prices'\n",
    "    \n",
    "    # Final validation\n",
    "    if tier_1 is None or tier_2 is None:\n",
    "        return pd.Series({'tier_1_price': np.nan, 'tier_2_price': np.nan, 'price_source': 'no_valid_pair'})\n",
    "    \n",
    "    # Ensure correct ordering\n",
    "    if tier_2 >= tier_1:\n",
    "        tier_1, tier_2 = max(tier_1, tier_2), min(tier_1, tier_2)\n",
    "    \n",
    "    # Check basic constraints\n",
    "    if not (wac < tier_2 < tier_1 < current_price):\n",
    "        return pd.Series({'tier_1_price': np.nan, 'tier_2_price': np.nan, 'price_source': 'invalid_ordering'})\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # RATIO ADJUSTMENT\n",
    "    # Ensure: discount_2/discount_1 / (qty_2/qty_1) is between MIN_RATIO and MAX_RATIO\n",
    "    # ==========================================================================\n",
    "    if pd.notna(tier_1_qty) and pd.notna(tier_2_qty) and tier_1_qty > 0:\n",
    "        tier_1_discount = current_price - tier_1\n",
    "        tier_2_discount = current_price - tier_2\n",
    "        \n",
    "        if tier_1_discount > 0:\n",
    "            qty_ratio = tier_2_qty / tier_1_qty\n",
    "            discount_ratio = tier_2_discount / tier_1_discount\n",
    "            \n",
    "            if qty_ratio > 0:\n",
    "                elasticity_ratio = discount_ratio / qty_ratio\n",
    "                \n",
    "                # If ratio too high, reduce T2 discount (increase T2 price)\n",
    "                if elasticity_ratio > MAX_RATIO:\n",
    "                    target_discount_ratio = MAX_RATIO * qty_ratio\n",
    "                    target_tier_2_discount = target_discount_ratio * tier_1_discount\n",
    "                    adjusted_tier_2 = current_price - target_tier_2_discount\n",
    "                    \n",
    "                    if adjusted_tier_2 > wac and adjusted_tier_2 < tier_1:\n",
    "                        tier_2 = round(adjusted_tier_2, 2)\n",
    "                        source += '_ratio_down'\n",
    "                    else:\n",
    "                        return pd.Series({'tier_1_price': np.nan, 'tier_2_price': np.nan, \n",
    "                                         'price_source': f'cannot_adjust_ratio_{elasticity_ratio:.2f}_max'})\n",
    "                \n",
    "                # If ratio too low, increase T2 discount (decrease T2 price)\n",
    "                elif elasticity_ratio < MIN_RATIO:\n",
    "                    target_discount_ratio = MIN_RATIO * qty_ratio\n",
    "                    target_tier_2_discount = target_discount_ratio * tier_1_discount\n",
    "                    adjusted_tier_2 = current_price - target_tier_2_discount\n",
    "                    \n",
    "                    if adjusted_tier_2 > wac and adjusted_tier_2 < tier_1:\n",
    "                        tier_2 = round(adjusted_tier_2, 2)\n",
    "                        source += '_ratio_up'\n",
    "                    else:\n",
    "                        return pd.Series({'tier_1_price': np.nan, 'tier_2_price': np.nan, \n",
    "                                         'price_source': f'cannot_adjust_ratio_{elasticity_ratio:.2f}_min'})\n",
    "    \n",
    "    # Final rounding\n",
    "    tier_1 = round(tier_1, 2)\n",
    "    tier_2 = round(tier_2, 2)\n",
    "    \n",
    "    return pd.Series({\n",
    "        'tier_1_price': tier_1,\n",
    "        'tier_2_price': tier_2,\n",
    "        'price_source': source\n",
    "    })\n",
    "\n",
    "\n",
    "def calculate_wholesale_tier(row):\n",
    "    \"\"\"\n",
    "    Calculate wholesale (Tier 3) pricing based on delivery cost savings.\n",
    "    \n",
    "    Logic:\n",
    "    - Car cost per order = WS_CAR_COST / orders_per_car\n",
    "    - If retailer consolidates orders, they save delivery costs\n",
    "    - Savings = deliveries_saved * car_cost_per_order\n",
    "    - Calculate scenarios from 3x to orders_per_car multiplier\n",
    "    \n",
    "    Constraints:\n",
    "    - new_price >= max(wac/(1 - 0.4*current_margin), wac/(1 - WS_MIN_MARGIN))\n",
    "    - order_value <= WS_MAX_TICKET_SIZE\n",
    "    - new_price < tier_2_price (T3 price must be lower than T2)\n",
    "    \n",
    "    Args:\n",
    "        row: DataFrame row with packing_unit_price, wac_pu, tier_2_price, \n",
    "             avg_ticket_size, orders_per_car_by_weight\n",
    "             \n",
    "    Returns:\n",
    "        Series with ws_qty, ws_price, ws_discount_pct, ws_margin, etc.\n",
    "    \"\"\"\n",
    "    current_price = row.get('packing_unit_price')\n",
    "    wac = row.get('wac_pu')\n",
    "    avg_ts = row.get('avg_ticket_size', 4000)\n",
    "    tier_2_price = row.get('tier_2_price')\n",
    "    \n",
    "    # Get orders per car (how many orders fit in one car trip based on weight)\n",
    "    orders_per_car = row.get('orders_per_car_by_weight', 15)\n",
    "    if pd.isna(orders_per_car) or orders_per_car <= 0:\n",
    "        orders_per_car = 15\n",
    "    \n",
    "    # Calculate car cost per order\n",
    "    car_cost_per_order = WS_CAR_COST / orders_per_car\n",
    "    \n",
    "    if pd.isna(avg_ts) or avg_ts <= 0:\n",
    "        avg_ts = 4000\n",
    "    \n",
    "    # Validation\n",
    "    if pd.isna(current_price) or pd.isna(wac) or current_price <= 0 or wac <= 0:\n",
    "        return pd.Series({\n",
    "            'ws_qty': np.nan, 'ws_price': np.nan, 'ws_discount_pct': np.nan,\n",
    "            'ws_margin': np.nan, 'ws_multiplier': np.nan, 'ws_savings_pct': np.nan\n",
    "        })\n",
    "    \n",
    "    if pd.isna(tier_2_price) or tier_2_price <= 0:\n",
    "        return pd.Series({\n",
    "            'ws_qty': np.nan, 'ws_price': np.nan, 'ws_discount_pct': np.nan,\n",
    "            'ws_margin': np.nan, 'ws_multiplier': np.nan, 'ws_savings_pct': np.nan\n",
    "        })\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # MARGIN-BASED MINIMUM PRICE FOR WHOLESALE\n",
    "    # Wholesale: minimum margin = 40% of current margin\n",
    "    # ==========================================================================\n",
    "    current_margin = (current_price - wac) / current_price\n",
    "    min_ws_margin = 0.4 * current_margin\n",
    "    min_ws_price_margin_based = wac / (1 - min_ws_margin) if min_ws_margin < 1 else current_price\n",
    "    \n",
    "    # Also keep legacy minimum (WAC + WS_MIN_MARGIN)\n",
    "    min_ws_price_legacy = wac / (1 - WS_MIN_MARGIN)\n",
    "    \n",
    "    # Use the HIGHER of the two constraints\n",
    "    min_acceptable_price = max(min_ws_price_margin_based, min_ws_price_legacy)\n",
    "    \n",
    "    best_scenario = None\n",
    "    best_savings_pct = 0\n",
    "    \n",
    "    # Test scenarios from 3x to orders_per_car\n",
    "    for multiplier in range(3, int(orders_per_car) + 1):\n",
    "        # Order value at this multiplier\n",
    "        order_value = avg_ts * multiplier\n",
    "        \n",
    "        # Deliveries saved = multiplier - 1 (consolidating multiple orders into one)\n",
    "        deliveries_saved = multiplier - 1\n",
    "        \n",
    "        # Total savings = deliveries_saved * car_cost_per_order\n",
    "        total_savings = deliveries_saved * car_cost_per_order\n",
    "        \n",
    "        # How many units of this SKU fit in this order value?\n",
    "        qty_at_current_price = order_value / current_price\n",
    "        \n",
    "        if qty_at_current_price <= 0:\n",
    "            continue\n",
    "        \n",
    "        # Discount per unit from car cost savings\n",
    "        discount_per_unit = total_savings / qty_at_current_price\n",
    "        \n",
    "        # New price after passing car cost savings\n",
    "        new_price = current_price - discount_per_unit\n",
    "        \n",
    "        # Check all constraints:\n",
    "        # 1. Price above minimum margin\n",
    "        # 2. Order value within max ticket size\n",
    "        # 3. Price below tier_2_price (T3 must be cheaper than T2)\n",
    "        if new_price >= min_acceptable_price and order_value <= WS_MAX_TICKET_SIZE and new_price < tier_2_price:\n",
    "            # Calculate margin at new price\n",
    "            margin = (new_price - wac) / new_price\n",
    "            \n",
    "            # Savings percentage for retailer\n",
    "            savings_pct = (discount_per_unit / current_price) * 100\n",
    "            \n",
    "            # Keep track of best scenario (highest savings while valid)\n",
    "            if savings_pct > best_savings_pct:\n",
    "                best_savings_pct = savings_pct\n",
    "                best_scenario = {\n",
    "                    'ws_qty': round(qty_at_current_price, 0),\n",
    "                    'ws_price': round(new_price, 2),\n",
    "                    'ws_discount_pct': round((current_price - new_price) / current_price * 100, 2),\n",
    "                    'ws_margin': round(margin, 4),\n",
    "                    'ws_multiplier': multiplier,\n",
    "                    'ws_savings_pct': round(savings_pct, 2)\n",
    "                }\n",
    "    \n",
    "    if best_scenario:\n",
    "        return pd.Series(best_scenario)\n",
    "    else:\n",
    "        return pd.Series({\n",
    "            'ws_qty': np.nan, 'ws_price': np.nan, 'ws_discount_pct': np.nan,\n",
    "            'ws_margin': np.nan, 'ws_multiplier': np.nan, 'ws_savings_pct': np.nan\n",
    "        })\n",
    "\n",
    "print(\"✓ Tier price calculation function defined\")\n",
    "print(\"✓ Wholesale tier calculation function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MAIN FUNCTION: process_qd\n",
    "# =============================================================================\n",
    "def process_qd(df_qd: pd.DataFrame, dry_run: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    Main function to process Quantity Discounts.\n",
    "    Called from module_3_periodic_actions.ipynb with a filtered DataFrame.\n",
    "    \n",
    "    This function:\n",
    "    1. Deactivates ALL currently active Quantity Discounts (FIRST!)\n",
    "    2. Gets packing units for each product-warehouse\n",
    "    3. Gets warehouse ticket statistics for wholesale calculations\n",
    "    4. Calculates tier quantities from order history\n",
    "    5. Calculates T1 & T2 prices using market margins and margin tiers\n",
    "    6. Calculates T3 (wholesale) prices based on delivery cost savings\n",
    "    7. Selects top N SKUs per warehouse based on mtd_qty * current_price\n",
    "    8. Filters tiers based on keep_qd_tiers from Module 3\n",
    "    9. Creates new QDs with calculated tiers\n",
    "    \n",
    "    Args:\n",
    "        df_qd: DataFrame with columns from Module 3 (see documentation)\n",
    "        dry_run: If True, only log what would be done (default: True)\n",
    "        \n",
    "    Returns:\n",
    "        dict with processing results\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"QD HANDLER: PROCESSING QUANTITY DISCOUNTS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Mode: {'DRY RUN (testing)' if dry_run else 'LIVE'}\")\n",
    "    print(f\"Timestamp: {CAIRO_NOW.strftime('%Y-%m-%d %H:%M')} Cairo Time\")\n",
    "    print(f\"Input SKUs: {len(df_qd)}\")\n",
    "    \n",
    "    if len(df_qd) == 0:\n",
    "        print(\"\\nNo SKUs to process. Exiting.\")\n",
    "        return {\n",
    "            'mode': 'testing' if dry_run else 'live',\n",
    "            'total_input': 0,\n",
    "            'processed': 0,\n",
    "            'failed': 0,\n",
    "            'deactivate_result': {'total_active': 0, 'deactivated': [], 'failed': []},\n",
    "            'create_result': {'created_count': 0, 'failed_count': 0, 'errors': []}\n",
    "        }\n",
    "    \n",
    "    # Preview input\n",
    "    print(f\"\\nUnique warehouses: {df_qd['warehouse_id'].nunique()}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 1: DEACTIVATE ALL EXISTING QUANTITY DISCOUNTS (FIRST!)\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"STEP 1: Deactivating existing Quantity Discounts...\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    deactivate_result = deactivate_active_qd(dry_run=dry_run)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 2: GET PACKING UNITS\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"STEP 2: Getting top-selling packing units...\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    # Create list of (product_id, warehouse_id) tuples\n",
    "    product_warehouse_list = df_qd[['product_id', 'warehouse_id']].drop_duplicates().values.tolist()\n",
    "    \n",
    "    df_packing_units = get_top_selling_packing_units(product_warehouse_list)\n",
    "    \n",
    "    if len(df_packing_units) == 0:\n",
    "        print(\"  ⚠ No packing units found!\")\n",
    "        return {\n",
    "            'mode': 'testing' if dry_run else 'live',\n",
    "            'total_input': len(df_qd),\n",
    "            'processed': 0,\n",
    "            'failed': len(df_qd),\n",
    "            'deactivate_result': deactivate_result,\n",
    "            'create_result': {'created_count': 0, 'failed_count': 0, 'errors': [{'error': 'No packing units found'}]}\n",
    "        }\n",
    "    \n",
    "    # Merge packing units with input data\n",
    "    df_work = df_qd.merge(df_packing_units, on=['product_id', 'warehouse_id'], how='inner')\n",
    "    print(f\"  Matched {len(df_work)} SKUs with packing units\")\n",
    "    \n",
    "    # Use new_price if available, otherwise fallback to current_price\n",
    "    df_work['effective_price'] = df_work['new_price'].fillna(df_work['current_price'])\n",
    "    print(f\"  Using new_price: {df_work['new_price'].notna().sum()} SKUs\")\n",
    "    print(f\"  Using current_price (fallback): {df_work['new_price'].isna().sum()} SKUs\")\n",
    "    \n",
    "    # Calculate packing unit prices (multiply by basic_unit_count)\n",
    "    df_work['wac_pu'] = df_work['wac_p'] * df_work['basic_unit_count']\n",
    "    df_work['packing_unit_price'] = df_work['effective_price'] * df_work['basic_unit_count']\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 3: GET WAREHOUSE TICKET STATISTICS (for wholesale)\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"STEP 3: Getting warehouse ticket statistics...\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    df_warehouse_stats = get_warehouse_ticket_stats()\n",
    "    \n",
    "    if len(df_warehouse_stats) > 0:\n",
    "        df_work = df_work.merge(\n",
    "            df_warehouse_stats[['warehouse_id', 'avg_ticket_size', 'orders_per_car_by_weight']],\n",
    "            on='warehouse_id',\n",
    "            how='left'\n",
    "        )\n",
    "        print(f\"  Merged ticket stats for {df_work['avg_ticket_size'].notna().sum()} SKUs\")\n",
    "    else:\n",
    "        print(\"  ⚠ No warehouse stats found, using defaults for wholesale\")\n",
    "        df_work['avg_ticket_size'] = 4000\n",
    "        df_work['orders_per_car_by_weight'] = 15\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 4: CALCULATE TIER QUANTITIES\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"STEP 4: Calculating tier quantities...\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    # Create list of (warehouse_id, product_id, packing_unit_id) tuples\n",
    "    product_warehouse_pu_list = df_work[['warehouse_id', 'product_id', 'packing_unit_id']].drop_duplicates().values.tolist()\n",
    "    \n",
    "    df_tier_qty = get_tier_quantities(product_warehouse_pu_list)\n",
    "    \n",
    "    if len(df_tier_qty) == 0:\n",
    "        print(\"  ⚠ No tier quantities calculated!\")\n",
    "    else:\n",
    "        # Merge tier quantities\n",
    "        df_work = df_work.merge(\n",
    "            df_tier_qty[['warehouse_id', 'product_id', 'packing_unit_id', 'tier_1_qty', 'tier_2_qty']],\n",
    "            on=['warehouse_id', 'product_id', 'packing_unit_id'],\n",
    "            how='left'\n",
    "        )\n",
    "        print(f\"  {df_work['tier_1_qty'].notna().sum()} SKUs have tier quantities\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 5: CALCULATE T1 & T2 PRICES\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"STEP 5: Calculating T1 & T2 prices...\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    # Apply price calculation to each row\n",
    "    price_results = df_work.apply(calculate_tier_prices, axis=1)\n",
    "    df_work = pd.concat([df_work, price_results], axis=1)\n",
    "    \n",
    "    valid_t1_t2 = df_work['tier_1_price'].notna() & df_work['tier_2_price'].notna()\n",
    "    print(f\"  Valid T1 & T2 prices: {valid_t1_t2.sum()} / {len(df_work)}\")\n",
    "    \n",
    "    # Show price source distribution\n",
    "    if 'price_source' in df_work.columns:\n",
    "        print(\"\\n  Price source distribution:\")\n",
    "        for source, count in df_work['price_source'].value_counts().head(5).items():\n",
    "            print(f\"    {source}: {count}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 6: CALCULATE T3 (WHOLESALE) PRICES\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"STEP 6: Calculating T3 (wholesale) prices...\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    # Calculate wholesale tier for rows with valid T2 prices\n",
    "    ws_results = df_work.apply(calculate_wholesale_tier, axis=1)\n",
    "    df_work = pd.concat([df_work, ws_results], axis=1)\n",
    "    \n",
    "    valid_t3 = df_work['ws_price'].notna()\n",
    "    print(f\"  Valid T3 prices: {valid_t3.sum()} / {len(df_work)}\")\n",
    "    \n",
    "    if valid_t3.sum() > 0:\n",
    "        print(f\"\\n  T3 Statistics:\")\n",
    "        print(f\"    Average multiplier: {df_work.loc[valid_t3, 'ws_multiplier'].mean():.1f}x\")\n",
    "        print(f\"    Average discount: {df_work.loc[valid_t3, 'ws_discount_pct'].mean():.2f}%\")\n",
    "        print(f\"    Average margin: {df_work.loc[valid_t3, 'ws_margin'].mean()*100:.2f}%\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 7: VALIDATE T3 CONSTRAINTS\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"STEP 7: Validating T3 constraints...\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    # Constraint 1: T3 qty must be > T2 qty\n",
    "    invalid_t3_qty = (df_work['ws_qty'].notna() & \n",
    "                      df_work['tier_2_qty'].notna() & \n",
    "                      (df_work['ws_qty'] <= df_work['tier_2_qty']))\n",
    "    if invalid_t3_qty.sum() > 0:\n",
    "        # Fix: Set T3 qty = T2 qty * 10\n",
    "        df_work.loc[invalid_t3_qty, 'ws_qty'] = (\n",
    "            df_work.loc[invalid_t3_qty, 'tier_2_qty'] * 10\n",
    "        ).astype(int)\n",
    "        print(f\"  Fixed {invalid_t3_qty.sum()} SKUs where T3 qty <= T2 qty\")\n",
    "    \n",
    "    # Constraint 2: T3 price must be < T2 price (T3 discount > T2 discount)\n",
    "    # This is already enforced in calculate_wholesale_tier, but double-check\n",
    "    invalid_t3_price = (df_work['ws_price'].notna() & \n",
    "                        df_work['tier_2_price'].notna() & \n",
    "                        (df_work['ws_price'] >= df_work['tier_2_price']))\n",
    "    if invalid_t3_price.sum() > 0:\n",
    "        # Invalidate T3 for these rows\n",
    "        df_work.loc[invalid_t3_price, 'ws_qty'] = np.nan\n",
    "        df_work.loc[invalid_t3_price, 'ws_price'] = np.nan\n",
    "        df_work.loc[invalid_t3_price, 'ws_discount_pct'] = np.nan\n",
    "        print(f\"  Invalidated {invalid_t3_price.sum()} SKUs where T3 price >= T2 price\")\n",
    "    \n",
    "    print(f\"  Final valid T3 count: {df_work['ws_price'].notna().sum()}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 7.5: ADJUST TIER 2 QUANTITY IF RATIO IS TOO LOW\n",
    "    # =========================================================================\n",
    "    # If tier_2_qty/tier_1_qty < 1.3 and elasticity_ratio > 3, adjust tier_2_qty\n",
    "    print(\"\\n  Checking tier quantity ratios...\")\n",
    "    \n",
    "    # Calculate discount ratios\n",
    "    df_work['discount_1_pct'] = df_work['tier_1_disc_pct'] if 'tier_1_disc_pct' in df_work.columns else \\\n",
    "        ((df_work['packing_unit_price'] - df_work['tier_1_price']) / df_work['packing_unit_price'] * 100)\n",
    "    df_work['discount_2_pct'] = df_work['tier_2_disc_pct'] if 'tier_2_disc_pct' in df_work.columns else \\\n",
    "        ((df_work['packing_unit_price'] - df_work['tier_2_price']) / df_work['packing_unit_price'] * 100)\n",
    "    \n",
    "    # Calculate ratios\n",
    "    df_work['check_qty'] = df_work['tier_2_qty'] / df_work['tier_1_qty']\n",
    "    df_work['discount_ratio'] = df_work['discount_2_pct'] / df_work['discount_1_pct'].replace(0, np.nan)\n",
    "    df_work['elasticity_ratio'] = df_work['discount_ratio'] / df_work['check_qty'].replace(0, np.nan)\n",
    "    df_work['target_qty_ratio'] = df_work['discount_ratio'] / 2\n",
    "    df_work['target_tier_2_q'] = np.round(df_work['target_qty_ratio'] * df_work['tier_1_qty'])\n",
    "    \n",
    "    # Adjust tier_2_qty where check_qty < 1.3 and elasticity_ratio > 3\n",
    "    adjustment_mask = (df_work['check_qty'] < 1.3) & (df_work['elasticity_ratio'] > 3)\n",
    "    if adjustment_mask.sum() > 0:\n",
    "        df_work.loc[adjustment_mask, 'tier_2_qty'] = df_work.loc[adjustment_mask, 'target_tier_2_q']\n",
    "        print(f\"  Adjusted tier_2_qty for {adjustment_mask.sum()} SKUs (low qty ratio + high elasticity)\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 8: APPLY KEEP_QD_TIERS FILTER & CALCULATE TIER FLAGS\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"STEP 8: Applying keep_qd_tiers filter and calculating tier flags...\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    # Calculate discount percentages for T1 & T2\n",
    "    df_work['tier_1_disc_pct'] = ((df_work['packing_unit_price'] - df_work['tier_1_price']) / df_work['packing_unit_price'] * 100).round(2)\n",
    "    df_work['tier_2_disc_pct'] = ((df_work['packing_unit_price'] - df_work['tier_2_price']) / df_work['packing_unit_price'] * 100).round(2)\n",
    "    \n",
    "    # Apply keep_qd_tiers filter and calculate tier flags\n",
    "    def apply_tier_filter(row):\n",
    "        \"\"\"Apply keep_qd_tiers filter and return tier flags.\"\"\"\n",
    "        keep_tiers = parse_keep_qd_tiers(row.get('keep_qd_tiers'))\n",
    "        \n",
    "        # If no tiers specified, default to all valid tiers\n",
    "        if not keep_tiers:\n",
    "            keep_tiers = ['T1', 'T2', 'T3']\n",
    "        \n",
    "        # Determine which tiers are valid after filtering\n",
    "        t1_valid = ('T1' in keep_tiers and \n",
    "                    pd.notna(row.get('tier_1_qty')) and \n",
    "                    pd.notna(row.get('tier_1_disc_pct')) and \n",
    "                    row.get('tier_1_disc_pct', 0) > 0)\n",
    "        \n",
    "        t2_valid = ('T2' in keep_tiers and \n",
    "                    pd.notna(row.get('tier_2_qty')) and \n",
    "                    pd.notna(row.get('tier_2_disc_pct')) and \n",
    "                    row.get('tier_2_disc_pct', 0) > 0)\n",
    "        \n",
    "        t3_valid = ('T3' in keep_tiers and \n",
    "                    pd.notna(row.get('ws_qty')) and \n",
    "                    pd.notna(row.get('ws_discount_pct')) and \n",
    "                    row.get('ws_discount_pct', 0) > 0)\n",
    "        \n",
    "        return pd.Series({\n",
    "            't1_f': int(t1_valid),\n",
    "            't2_f': int(t2_valid),\n",
    "            't3_f': int(t3_valid)\n",
    "        })\n",
    "    \n",
    "    tier_flags = df_work.apply(apply_tier_filter, axis=1)\n",
    "    df_work = pd.concat([df_work, tier_flags], axis=1)\n",
    "    \n",
    "    # Set invalid tier values to null\n",
    "    # T1: if t1_f == 0, set tier_1_qty, tier_1_price, tier_1_disc_pct to null\n",
    "    df_work.loc[df_work['t1_f'] == 0, 'tier_1_qty'] = np.nan\n",
    "    df_work.loc[df_work['t1_f'] == 0, 'tier_1_price'] = np.nan\n",
    "    df_work.loc[df_work['t1_f'] == 0, 'tier_1_disc_pct'] = np.nan\n",
    "    \n",
    "    # T2: if t2_f == 0, set tier_2_qty, tier_2_price, tier_2_disc_pct to null\n",
    "    df_work.loc[df_work['t2_f'] == 0, 'tier_2_qty'] = np.nan\n",
    "    df_work.loc[df_work['t2_f'] == 0, 'tier_2_price'] = np.nan\n",
    "    df_work.loc[df_work['t2_f'] == 0, 'tier_2_disc_pct'] = np.nan\n",
    "    \n",
    "    # T3: if t3_f == 0, set ws_qty, ws_price, ws_discount_pct to null\n",
    "    df_work.loc[df_work['t3_f'] == 0, 'ws_qty'] = np.nan\n",
    "    df_work.loc[df_work['t3_f'] == 0, 'ws_price'] = np.nan\n",
    "    df_work.loc[df_work['t3_f'] == 0, 'ws_discount_pct'] = np.nan\n",
    "    \n",
    "    # Calculate total tiers per SKU\n",
    "    df_work['all_f'] = df_work['t1_f'] + df_work['t2_f'] + df_work['t3_f']\n",
    "    \n",
    "    # Only keep SKUs with at least 2 valid tiers\n",
    "    df_work = df_work[df_work['all_f'] >= 2].copy()\n",
    "    \n",
    "    print(f\"  SKUs with valid tiers after filtering: {len(df_work)}\")\n",
    "    print(f\"  Total tier entries: {df_work['all_f'].sum()}\")\n",
    "    print(f\"    T1 valid: {df_work['t1_f'].sum()}\")\n",
    "    print(f\"    T2 valid: {df_work['t2_f'].sum()}\")\n",
    "    print(f\"    T3 valid: {df_work['t3_f'].sum()}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 9: SELECT TOP TIERS PER WAREHOUSE (MAX 400 TIER ENTRIES)\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(f\"STEP 9: Selecting top {TOP_SKUS_PER_WAREHOUSE} tier entries per warehouse...\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    # Calculate ranking score: mtd_qty * effective_price (higher is better)\n",
    "    df_work['mtd_qty'] = df_work['mtd_qty'].fillna(0)\n",
    "    df_work['ranking_score'] = df_work['mtd_qty'] * df_work['effective_price']\n",
    "    \n",
    "    # Sort by warehouse and ranking score (descending)\n",
    "    df_work = df_work.sort_values(['warehouse_id', 'ranking_score'], ascending=[True, False])\n",
    "    \n",
    "    # Calculate cumulative sum of tier entries per warehouse\n",
    "    df_work['cumsum'] = df_work.groupby('warehouse_id')['all_f'].cumsum()\n",
    "    \n",
    "    # Filter to keep cumsum <= 400 (max 400 tier entries per warehouse)\n",
    "    df_top = df_work[df_work['cumsum'] <= TOP_SKUS_PER_WAREHOUSE].copy()\n",
    "    \n",
    "    print(f\"  Before filtering: {len(df_work)} SKUs ({df_work['all_f'].sum()} tier entries)\")\n",
    "    print(f\"  After top {TOP_SKUS_PER_WAREHOUSE} limit: {len(df_top)} SKUs ({df_top['all_f'].sum()} tier entries)\")\n",
    "    print(f\"\\n  Tier entries per warehouse:\")\n",
    "    for wh in df_top['warehouse_id'].unique():\n",
    "        wh_data = df_top[df_top['warehouse_id'] == wh]\n",
    "        print(f\"    Warehouse {wh}: {len(wh_data)} SKUs, {wh_data['all_f'].sum()} tiers\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 10: BUILD QD CONFIGURATIONS\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"STEP 10: Building QD configurations...\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    qd_configs = []\n",
    "    \n",
    "    for _, row in df_top.iterrows():\n",
    "        # Build tiers based on tier flags\n",
    "        tiers = []\n",
    "        \n",
    "        # Tier 1\n",
    "        if row['t1_f'] == 1:\n",
    "            tiers.append({\n",
    "                \"tier\": 1,\n",
    "                \"quantity\": int(row['tier_1_qty']),\n",
    "                \"discount_pct\": float(row['tier_1_disc_pct'])\n",
    "            })\n",
    "        \n",
    "        # Tier 2\n",
    "        if row['t2_f'] == 1:\n",
    "            tiers.append({\n",
    "                \"tier\": 2,\n",
    "                \"quantity\": int(row['tier_2_qty']),\n",
    "                \"discount_pct\": float(row['tier_2_disc_pct'])\n",
    "            })\n",
    "        \n",
    "        # Tier 3 (Wholesale)\n",
    "        if row['t3_f'] == 1:\n",
    "            tiers.append({\n",
    "                \"tier\": 3,\n",
    "                \"quantity\": int(row['ws_qty']),\n",
    "                \"discount_pct\": float(row['ws_discount_pct'])\n",
    "            })\n",
    "        \n",
    "        qd_configs.append({\n",
    "            'product_id': int(row['product_id']),\n",
    "            'warehouse_id': int(row['warehouse_id']),\n",
    "            'cohort_id': int(row.get('cohort_id', 0)),\n",
    "            'packing_unit_id': int(row['packing_unit_id']),\n",
    "            'tiers': tiers,\n",
    "            'sku': row.get('sku', 'N/A'),\n",
    "            'packing_unit_price': row['packing_unit_price'],\n",
    "            'tier_1_price': row.get('tier_1_price'),\n",
    "            'tier_2_price': row.get('tier_2_price'),\n",
    "            'ws_price': row.get('ws_price'),\n",
    "            'ranking_score': row.get('ranking_score', 0)\n",
    "        })\n",
    "    \n",
    "    print(f\"  Valid QD configs: {len(qd_configs)}\")\n",
    "    \n",
    "    # Count tiers distribution\n",
    "    tier_counts = {1: 0, 2: 0, 3: 0}\n",
    "    for config in qd_configs:\n",
    "        for t in config['tiers']:\n",
    "            tier_counts[t['tier']] += 1\n",
    "    print(f\"\\n  Tier distribution in configs:\")\n",
    "    print(f\"    T1: {tier_counts[1]} configs\")\n",
    "    print(f\"    T2: {tier_counts[2]} configs\")\n",
    "    print(f\"    T3 (wholesale): {tier_counts[3]} configs\")\n",
    "    print(f\"    Total tier entries: {sum(tier_counts.values())}\")\n",
    "    \n",
    "    # Preview configs\n",
    "    if qd_configs:\n",
    "        print(\"\\n  Sample QD configs:\")\n",
    "        for config in qd_configs[:5]:\n",
    "            tier_str = \", \".join([f\"T{t['tier']}:qty={t['quantity']},disc={t['discount_pct']:.2f}%\" for t in config['tiers']])\n",
    "            print(f\"    {config['sku'][:30]}: [{tier_str}]\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 10.5: SAVE DATA FOR REVIEW BEFORE PUSH\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"STEP 10.5: Saving data for review...\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    # Save detailed data to Excel for review\n",
    "    review_filename = f'QD_detailed_review_{CAIRO_NOW.strftime(\"%Y%m%d_%H%M\")}.xlsx'\n",
    "    \n",
    "    # Select important columns for review\n",
    "    review_columns = [\n",
    "        'warehouse_id', 'product_id', 'packing_unit_id', 'sku', 'brand', 'cat',\n",
    "        'effective_price', 'packing_unit_price', 'wac_p', 'wac_pu', 'basic_unit_count',\n",
    "        'tier_1_qty', 'tier_1_price', 'tier_1_disc_pct', 't1_f',\n",
    "        'tier_2_qty', 'tier_2_price', 'tier_2_disc_pct', 't2_f',\n",
    "        'ws_qty', 'ws_price', 'ws_discount_pct', 't3_f',\n",
    "        'all_f', 'ranking_score', 'mtd_qty', 'keep_qd_tiers'\n",
    "    ]\n",
    "    # Filter to columns that exist\n",
    "    review_columns = [c for c in review_columns if c in df_top.columns]\n",
    "    \n",
    "    df_review = df_top[review_columns].copy()\n",
    "    df_review.to_excel(review_filename, index=False)\n",
    "    print(f\"  ✓ Saved review file: {review_filename}\")\n",
    "    print(f\"    Total SKUs: {len(df_review)}\")\n",
    "    print(f\"    Columns: {len(review_columns)}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 11: CREATE NEW QUANTITY DISCOUNTS\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"STEP 11: Creating new Quantity Discounts...\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    if len(qd_configs) == 0:\n",
    "        print(\"  No Quantity Discounts to create.\")\n",
    "        create_result = {\"success\": True, \"created_count\": 0, \"failed_count\": 0, \"errors\": []}\n",
    "    else:\n",
    "        print(f\"  Creating {len(qd_configs)} Quantity Discounts...\")\n",
    "        create_result = bulk_create_qd(qd_configs, df_top, dry_run=dry_run)\n",
    "        \n",
    "        print(f\"\\n  Creation Result:\")\n",
    "        print(f\"    Created: {create_result['created_count']}\")\n",
    "        print(f\"    Failed: {create_result['failed_count']}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 12: UPDATE CART RULES\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"STEP 12: Updating cart rules...\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    # Prepare cart rules update - cart rule should be >= max tier quantity\n",
    "    cart_rules_update = prepare_cart_rules_update(df_top, df_qd)\n",
    "    \n",
    "    if len(cart_rules_update) == 0:\n",
    "        print(\"  No cart rules need updating.\")\n",
    "        cart_rules_result = {'success': [], 'failed': []}\n",
    "    else:\n",
    "        print(f\"  Uploading cart rules...\")\n",
    "        cart_rules_result = upload_cart_rules(cart_rules_update, dry_run=dry_run)\n",
    "        \n",
    "        print(f\"\\n  Cart Rules Result:\")\n",
    "        print(f\"    Cohorts updated: {len(cart_rules_result['success'])}\")\n",
    "        print(f\"    Cohorts failed: {len(cart_rules_result['failed'])}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # SUMMARY\n",
    "    # =========================================================================\n",
    "    total_tiers = sum(tier_counts.values())\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"QD HANDLER - SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Mode: {'DRY RUN (testing)' if dry_run else 'LIVE'}\")\n",
    "    print(f\"Total SKUs in input: {len(df_qd)}\")\n",
    "    print(f\"SKUs with valid T1 & T2 prices: {valid_t1_t2.sum()}\")\n",
    "    print(f\"SKUs with valid T3 prices: {valid_t3.sum()}\")\n",
    "    print(f\"SKUs after keep_qd_tiers & {TOP_SKUS_PER_WAREHOUSE} tier limit: {len(df_top)}\")\n",
    "    print(f\"Total tier entries: {total_tiers}\")\n",
    "    print(f\"Valid QD configs: {len(qd_configs)}\")\n",
    "    print(f\"QD found active: {deactivate_result['total_active']}\")\n",
    "    print(f\"QD deactivated: {len(deactivate_result['deactivated'])}\")\n",
    "    print(f\"QD created: {create_result['created_count']}\")\n",
    "    print(f\"QD creation failed: {create_result['failed_count']}\")\n",
    "    print(f\"Cart rules updated: {len(cart_rules_update)} products\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return {\n",
    "        'mode': 'testing' if dry_run else 'live',\n",
    "        'total_input': len(df_qd),\n",
    "        'processed': create_result['created_count'],\n",
    "        'failed': create_result['failed_count'],\n",
    "        'total_tiers': total_tiers,\n",
    "        'deactivate_result': deactivate_result,\n",
    "        'create_result': create_result,\n",
    "        'cart_rules_result': cart_rules_result,\n",
    "        'cart_rules_update': cart_rules_update,\n",
    "        'qd_configs': qd_configs,  # Return configs for inspection\n",
    "        'df_work': df_top,  # Return working DataFrame for debugging\n",
    "        'review_file': review_filename  # File saved for review before push\n",
    "    }\n",
    "\n",
    "print(\"✓ process_qd() function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def parse_keep_qd_tiers(value):\n",
    "    \"\"\"Parse keep_qd_tiers from string or list.\"\"\"\n",
    "    # Handle list first (before pd.isna which fails on lists)\n",
    "    if isinstance(value, list):\n",
    "        return value\n",
    "    # Now safe to check for None/NaN\n",
    "    if value is None:\n",
    "        return []\n",
    "    try:\n",
    "        if pd.isna(value):\n",
    "            return []\n",
    "    except (ValueError, TypeError):\n",
    "        pass  # pd.isna fails on some types, continue\n",
    "    if isinstance(value, str):\n",
    "        try:\n",
    "            return ast.literal_eval(value)\n",
    "        except:\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "\n",
    "def build_tiers_from_row(row, keep_tiers: list) -> list:\n",
    "    \"\"\"\n",
    "    Build tier configuration from row data.\n",
    "    \n",
    "    Args:\n",
    "        row: DataFrame row with qd_tier_X_qty and qd_tier_X_disc_pct columns\n",
    "        keep_tiers: List of tiers to include, e.g., ['T1', 'T2']\n",
    "        \n",
    "    Returns:\n",
    "        List of tier configs for create_qd()\n",
    "    \"\"\"\n",
    "    tiers = []\n",
    "    \n",
    "    tier_map = {\n",
    "        'T1': (1, 'qd_tier_1_qty', 'qd_tier_1_disc_pct'),\n",
    "        'T2': (2, 'qd_tier_2_qty', 'qd_tier_2_disc_pct'),\n",
    "        'T3': (3, 'qd_tier_3_qty', 'qd_tier_3_disc_pct')\n",
    "    }\n",
    "    \n",
    "    for tier_name in keep_tiers:\n",
    "        if tier_name in tier_map:\n",
    "            tier_num, qty_col, disc_col = tier_map[tier_name]\n",
    "            qty = row.get(qty_col, 0)\n",
    "            disc = row.get(disc_col, 0)\n",
    "            \n",
    "            # Only include if both qty and discount are valid\n",
    "            if qty and qty > 0 and disc and disc > 0:\n",
    "                tiers.append({\n",
    "                    \"tier\": tier_num,\n",
    "                    \"quantity\": int(qty),\n",
    "                    \"discount_pct\": float(disc)\n",
    "                })\n",
    "    \n",
    "    return tiers\n",
    "\n",
    "print(\"Helper functions defined ✓\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# API FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def deactivate_active_qd(dry_run: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    Deactivate ALL active Quantity Discounts.\n",
    "    \n",
    "    This function:\n",
    "    1. Queries Snowflake to get all currently active QD IDs\n",
    "    2. Calls the API to deactivate each one\n",
    "    \n",
    "    Args:\n",
    "        dry_run: If True, only log what would be done without making API calls\n",
    "        \n",
    "    Returns:\n",
    "        dict with 'success', 'deactivated', 'failed', 'total_active'\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DEACTIVATING ACTIVE QUANTITY DISCOUNTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Mode: {'DRY RUN' if dry_run else 'LIVE'}\")\n",
    "    \n",
    "    # Step 1: Query Snowflake to get all active QD IDs\n",
    "    \n",
    "    print(\"\\nStep 1: Querying active Quantity Discounts from Snowflake...\")\n",
    "    df_active =get_active_qd_now()\n",
    "    \n",
    "    if len(df_active) == 0:\n",
    "        print(\"  No active Quantity Discounts found.\")\n",
    "        return {\n",
    "            'success': True,\n",
    "            'deactivated': [],\n",
    "            'failed': [],\n",
    "            'total_active': 0\n",
    "        }\n",
    "    \n",
    "    discount_ids = df_active['discount_id'].tolist()\n",
    "    print(f\"  Found {len(discount_ids)} active Quantity Discounts\")\n",
    "    \n",
    "    # Step 2: Deactivate each QD via API\n",
    "    print(f\"\\nStep 2: Deactivating {len(discount_ids)} discounts...\")\n",
    "    \n",
    "    results = {'deactivated': [], 'failed': []}\n",
    "    \n",
    "    # Get fresh API token\n",
    "    if not dry_run:\n",
    "        auth_token = _get_api_token()\n",
    "        headers = {\n",
    "            'Authorization': f'Bearer {auth_token}',\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "    \n",
    "    for idx, discount_id in enumerate(discount_ids):\n",
    "        if dry_run:\n",
    "            print(f\"  [{idx+1}/{len(discount_ids)}] [DRY RUN] Would deactivate: {discount_id}\")\n",
    "            results['deactivated'].append(discount_id)\n",
    "            continue\n",
    "        \n",
    "        url = f\"{QD_API_URL}{discount_id}/deactivate\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.patch(url, headers=headers, json={'active': False})\n",
    "            \n",
    "            if response.status_code in [200, 204]:\n",
    "                print(f\"  [{idx+1}/{len(discount_ids)}] [OK] Deactivated: {discount_id}\")\n",
    "                results['deactivated'].append(discount_id)\n",
    "            else:\n",
    "                print(f\"  [{idx+1}/{len(discount_ids)}] [ERROR] {discount_id}: {response.status_code} - {response.text[:100]}\")\n",
    "                results['failed'].append({\n",
    "                    'id': discount_id, \n",
    "                    'error': f\"{response.status_code}: {response.text[:200]}\"\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"  [{idx+1}/{len(discount_ids)}] [EXCEPTION] {discount_id}: {e}\")\n",
    "            results['failed'].append({'id': discount_id, 'error': str(e)})\n",
    "        \n",
    "        # Rate limiting - 0.5 second delay between requests\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"DEACTIVATION SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total active found: {len(discount_ids)}\")\n",
    "    print(f\"Successfully deactivated: {len(results['deactivated'])}\")\n",
    "    print(f\"Failed: {len(results['failed'])}\")\n",
    "    \n",
    "    if results['failed']:\n",
    "        print(\"\\nFailed IDs:\")\n",
    "        for item in results['failed'][:10]:  # Show first 10\n",
    "            print(f\"  - {item['id']}: {item['error']}\")\n",
    "        if len(results['failed']) > 10:\n",
    "            print(f\"  ... and {len(results['failed']) - 10} more\")\n",
    "    \n",
    "    return {\n",
    "        'success': len(results['failed']) == 0,\n",
    "        'deactivated': results['deactivated'],\n",
    "        'failed': results['failed'],\n",
    "        'total_active': len(discount_ids)\n",
    "    }\n",
    "\n",
    "\n",
    "def create_upload_format(df_configs: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create upload format DataFrame from QD configurations.\n",
    "    \n",
    "    Format: ONE row per warehouse_id with:\n",
    "    - Discounts Group 1: List of [tier 1 items + wholesale items] (max 200, overflow goes to Group 2)\n",
    "    - Discounts Group 2: List of [tier 2 items + overflow from Group 1]\n",
    "    - Each item format: [product_id, packing_unit_id, quantity, discount_pct]\n",
    "    \n",
    "    Args:\n",
    "        df_configs: DataFrame with columns: warehouse_id, product_id, packing_unit_id,\n",
    "                   tier_1_qty, tier_1_disc_pct, tier_2_qty, tier_2_disc_pct,\n",
    "                   ws_qty, ws_discount_pct, packing_unit_price, t1_f, t2_f, t3_f\n",
    "                   \n",
    "    Returns:\n",
    "        DataFrame with upload format\n",
    "    \"\"\"\n",
    "    final_quantity_discount = pd.DataFrame(columns=['warehouse_id', 'Discounts Group 1', 'Discounts Group 2', 'Description'])\n",
    "    \n",
    "    for wh_id in df_configs['warehouse_id'].unique():\n",
    "        warehouse_data = df_configs[df_configs['warehouse_id'] == wh_id]\n",
    "        warehouse_id = int(wh_id)\n",
    "        \n",
    "        # Collect all tier 1 items\n",
    "        tier_1_items = []\n",
    "        # Collect all tier 2 items\n",
    "        tier_2_items = []\n",
    "        # Collect all wholesale items\n",
    "        ws_items = []\n",
    "        \n",
    "        for _, r in warehouse_data.iterrows():\n",
    "            product_id = int(r['product_id'])\n",
    "            packing_unit_id = int(r['packing_unit_id'])\n",
    "            current_price = r['packing_unit_price']\n",
    "            \n",
    "            # Tier 1 (cap discount at MAX_DISCOUNT_CAP_T1)\n",
    "            if r.get('t1_f', 0) == 1 and pd.notna(r.get('tier_1_qty')) and pd.notna(r.get('tier_1_disc_pct')):\n",
    "                q_1 = int(r['tier_1_qty'])\n",
    "                d_1 = min(round(r['tier_1_disc_pct'], 2), MAX_DISCOUNT_CAP_T1)\n",
    "                tier_1_items.append([product_id, packing_unit_id, q_1, d_1])\n",
    "            \n",
    "            # Tier 2 (cap discount at MAX_DISCOUNT_CAP_T2)\n",
    "            if r.get('t2_f', 0) == 1 and pd.notna(r.get('tier_2_qty')) and pd.notna(r.get('tier_2_disc_pct')):\n",
    "                q_2 = int(r['tier_2_qty'])\n",
    "                d_2 = min(round(r['tier_2_disc_pct'], 2), MAX_DISCOUNT_CAP_T2)\n",
    "                tier_2_items.append([product_id, packing_unit_id, q_2, d_2])\n",
    "            \n",
    "            # Wholesale (cap discount at MAX_DISCOUNT_CAP_WS)\n",
    "            if r.get('t3_f', 0) == 1 and pd.notna(r.get('ws_qty')) and pd.notna(r.get('ws_discount_pct')):\n",
    "                q_ws = int(r['ws_qty'])\n",
    "                d_ws = min(round(r['ws_discount_pct'], 2), MAX_DISCOUNT_CAP_WS)\n",
    "                ws_items.append([product_id, packing_unit_id, q_ws, d_ws])\n",
    "        \n",
    "        # Group 1: Tier 1 + Wholesale (max 200)\n",
    "        group_1_items = tier_1_items + ws_items\n",
    "        \n",
    "        # Group 2: Tier 2 + overflow from Group 1\n",
    "        if len(group_1_items) > MAX_GROUP_SIZE:\n",
    "            # Overflow goes to Group 2\n",
    "            overflow = group_1_items[MAX_GROUP_SIZE:]\n",
    "            group_1_items = group_1_items[:MAX_GROUP_SIZE]\n",
    "            group_2_items = tier_2_items + overflow\n",
    "        else:\n",
    "            group_2_items = tier_2_items\n",
    "        \n",
    "        new_row = {\n",
    "            'warehouse_id': warehouse_id,\n",
    "            'Discounts Group 1': group_1_items,\n",
    "            'Discounts Group 2': group_2_items,\n",
    "            'Description': f'{warehouse_id}QD'\n",
    "        }\n",
    "        final_quantity_discount = pd.concat([final_quantity_discount, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    \n",
    "    return final_quantity_discount\n",
    "\n",
    "\n",
    "def prepare_upload_file(df_upload: pd.DataFrame, dry_run: bool = True) -> tuple:\n",
    "    \"\"\"\n",
    "    Prepare the final upload file with tag IDs and date/time.\n",
    "    \n",
    "    Args:\n",
    "        df_upload: DataFrame from create_upload_format()\n",
    "        dry_run: If True, only prepare but don't save\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (prepared_df, filename)\n",
    "    \"\"\"\n",
    "    # Merge with warehouse mapping\n",
    "    df_mapping = pd.DataFrame([\n",
    "        {'warehouse_id': wh_id, 'warehouse_name': info['name'], 'tag_id': info['tag_id']}\n",
    "        for wh_id, info in WAREHOUSE_TAG_MAPPING.items()\n",
    "    ])\n",
    "    \n",
    "    to_upload = df_upload.merge(df_mapping, on='warehouse_id', how='left')\n",
    "    \n",
    "    # Set description\n",
    "    to_upload['Description'] = (\n",
    "        to_upload['warehouse_name'].astype(str)\n",
    "        .str.replace(' ', '')\n",
    "        .str.replace('-', '')\n",
    "        + \"QD\"\n",
    "    )\n",
    "    \n",
    "    # Set start and end dates\n",
    "    cairo_now = datetime.now(CAIRO_TZ)\n",
    "    start_date = cairo_now + timedelta(minutes=10)\n",
    "    end_date = cairo_now + timedelta(hours=QD_DURATION_HOURS)\n",
    "    \n",
    "    start_date_str = start_date.strftime('%d/%m/%Y %H:%M')\n",
    "    end_date_str = end_date.strftime('%d/%m/%Y %H:%M')\n",
    "    \n",
    "    to_upload['Start Date/Time'] = start_date_str\n",
    "    to_upload['End Date/Time'] = end_date_str\n",
    "    to_upload = to_upload.rename(columns={'tag_id': 'Tag ID'})\n",
    "    \n",
    "    # Select final columns\n",
    "    to_upload = to_upload[['Tag ID', 'Description', 'Start Date/Time', 'End Date/Time', 'Discounts Group 1', 'Discounts Group 2']]\n",
    "    \n",
    "    # Remove rows without Tag ID\n",
    "    to_upload = to_upload[to_upload['Tag ID'].notna()]\n",
    "    \n",
    "    filename = 'QD_upload.xlsx'\n",
    "    \n",
    "    if not dry_run:\n",
    "        to_upload.to_excel(filename, index=False)\n",
    "        print(f\"  ✓ Saved upload file: {filename} ({len(to_upload)} warehouses)\")\n",
    "    \n",
    "    return to_upload, filename\n",
    "\n",
    "\n",
    "def post_QD(filename: str) -> requests.Response:\n",
    "    \"\"\"\n",
    "    Upload QD file to API.\n",
    "    \n",
    "    Args:\n",
    "        filename: Path to the Excel file to upload\n",
    "        \n",
    "    Returns:\n",
    "        Response object from the API\n",
    "    \"\"\"\n",
    "    auth_token = _get_api_token()\n",
    "    \n",
    "    url = 'https://api.maxab.app/commerce/api/admins/v1/quantity-discounts/upload'\n",
    "    \n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {auth_token}'\n",
    "    }\n",
    "    \n",
    "    with open(filename, 'rb') as f:\n",
    "        files = {'file': (filename, f, 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet')}\n",
    "        response = requests.post(url, headers=headers, files=files)\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "def bulk_create_qd(qd_configs: list, df_work: pd.DataFrame, dry_run: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    Bulk create Quantity Discounts using file upload method.\n",
    "    \n",
    "    Args:\n",
    "        qd_configs: List of QD configuration dicts (for logging)\n",
    "        df_work: Working DataFrame with all tier data\n",
    "        dry_run: If True, only log what would be done\n",
    "            \n",
    "    Returns:\n",
    "        dict with 'success', 'created_count', 'failed_count', 'errors'\n",
    "    \"\"\"\n",
    "    print(\"\\n  Creating upload format...\")\n",
    "    \n",
    "    # Create upload format\n",
    "    df_upload = create_upload_format(df_work)\n",
    "    \n",
    "    print(f\"  Upload format created: {len(df_upload)} warehouse rows\")\n",
    "    print(f\"\\n  Per warehouse breakdown:\")\n",
    "    for _, row in df_upload.iterrows():\n",
    "        wh = row['warehouse_id']\n",
    "        g1_count = len(row['Discounts Group 1'])\n",
    "        g2_count = len(row['Discounts Group 2'])\n",
    "        print(f\"    WH {wh}: Group 1 = {g1_count} items, Group 2 = {g2_count} items\")\n",
    "    \n",
    "    # Prepare upload file\n",
    "    print(\"\\n  Preparing upload file...\")\n",
    "    to_upload, filename = prepare_upload_file(df_upload, dry_run=dry_run)\n",
    "    \n",
    "    if dry_run:\n",
    "        print(f\"\\n  [DRY RUN] Would upload {len(to_upload)} warehouses\")\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"created_count\": len(qd_configs),\n",
    "            \"failed_count\": 0,\n",
    "            \"errors\": [],\n",
    "            \"upload_df\": to_upload\n",
    "        }\n",
    "    \n",
    "    # Upload to API\n",
    "    print(f\"\\n  Uploading QD file to API...\")\n",
    "    response = post_QD(filename)\n",
    "    \n",
    "    if response.ok:\n",
    "        print(f\"  ✓ Upload succeeded (status: {response.status_code})\")\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"created_count\": len(qd_configs),\n",
    "            \"failed_count\": 0,\n",
    "            \"errors\": [],\n",
    "            \"upload_df\": to_upload\n",
    "        }\n",
    "    else:\n",
    "        print(f\"  ❌ Upload failed (status: {response.status_code})\")\n",
    "        print(f\"  Response: {response.content[:500]}\")\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"created_count\": 0,\n",
    "            \"failed_count\": len(qd_configs),\n",
    "            \"errors\": [{\"error\": f\"API upload failed: {response.status_code}\"}],\n",
    "            \"upload_df\": to_upload\n",
    "        }\n",
    "\n",
    "\n",
    "def prepare_cart_rules_update(df_work: pd.DataFrame, df_qd_input: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare cart rules update based on QD tier quantities.\n",
    "    \n",
    "    Cart rule should be >= max(tier_1_qty, tier_2_qty, ws_qty) for each SKU.\n",
    "    \n",
    "    Args:\n",
    "        df_work: Working DataFrame with tier quantities\n",
    "        df_qd_input: Original input DataFrame with current_cart_rule and new_cart_rule\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with cart rules to update\n",
    "    \"\"\"\n",
    "    # Merge cart rules from original input to working df using product_id and warehouse_id\n",
    "    cart_cols = ['product_id', 'warehouse_id']\n",
    "    if 'current_cart_rule' in df_qd_input.columns:\n",
    "        cart_cols.append('current_cart_rule')\n",
    "    if 'new_cart_rule' in df_qd_input.columns:\n",
    "        cart_cols.append('new_cart_rule')\n",
    "    \n",
    "    df_cart_merge = df_qd_input[cart_cols].drop_duplicates()\n",
    "    df_work_cart = df_work.merge(df_cart_merge, on=['product_id', 'warehouse_id'], how='left')\n",
    "    \n",
    "    # Use new_cart_rule if available, otherwise current_cart_rule\n",
    "    if 'new_cart_rule' in df_work_cart.columns:\n",
    "        df_work_cart['effective_cart_rule'] = df_work_cart['new_cart_rule'].fillna(\n",
    "            df_work_cart.get('current_cart_rule', 0)\n",
    "        )\n",
    "    else:\n",
    "        df_work_cart['effective_cart_rule'] = df_work_cart.get('current_cart_rule', 0)\n",
    "    \n",
    "    df_work_cart['effective_cart_rule'] = df_work_cart['effective_cart_rule'].fillna(0)\n",
    "    \n",
    "    # Calculate max tier quantity for each SKU\n",
    "    tier_cols = ['tier_1_qty', 'tier_2_qty', 'ws_qty']\n",
    "    tier_cols = [c for c in tier_cols if c in df_work_cart.columns]\n",
    "    df_work_cart['max_tier_qty'] = df_work_cart[tier_cols].max(axis=1, skipna=True)\n",
    "    \n",
    "    # Only update cart rules that need to increase\n",
    "    needs_update = df_work_cart['max_tier_qty'] > df_work_cart['effective_cart_rule']\n",
    "    cart_rules_update = df_work_cart[needs_update][['cohort_id', 'product_id', 'packing_unit_id', 'max_tier_qty']].copy()\n",
    "    cart_rules_update = cart_rules_update.rename(columns={'max_tier_qty': 'new_cart_rule'})\n",
    "    \n",
    "    # Round cart rules and ensure they're integers\n",
    "    cart_rules_update['new_cart_rule'] = cart_rules_update['new_cart_rule'].round().astype(int)\n",
    "    \n",
    "    # Deduplicate by taking max per cohort/product/packing_unit\n",
    "    cart_rules_update = cart_rules_update.groupby(['cohort_id', 'product_id', 'packing_unit_id'])['new_cart_rule'].max().reset_index()\n",
    "    cart_rules_update = cart_rules_update.drop_duplicates()\n",
    "    \n",
    "    return cart_rules_update\n",
    "\n",
    "\n",
    "def post_cart_rules(cohort_id: int, filename: str) -> requests.Response:\n",
    "    \"\"\"\n",
    "    Upload cart rules file to API for a specific cohort.\n",
    "    \n",
    "    Args:\n",
    "        cohort_id: The cohort ID to update\n",
    "        filename: Path to the Excel file to upload\n",
    "        \n",
    "    Returns:\n",
    "        Response object from the API\n",
    "    \"\"\"\n",
    "    auth_token = _get_api_token()\n",
    "    \n",
    "    url = f'https://api.maxab.app/commerce/api/admins/v1/cohorts/{cohort_id}/cart-rules/upload'\n",
    "    \n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {auth_token}'\n",
    "    }\n",
    "    \n",
    "    with open(filename, 'rb') as f:\n",
    "        files = {'file': (filename, f, 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet')}\n",
    "        response = requests.post(url, headers=headers, files=files)\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "def upload_cart_rules(cart_rules_update: pd.DataFrame, dry_run: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    Upload cart rules updates by cohort.\n",
    "    \n",
    "    Args:\n",
    "        cart_rules_update: DataFrame with cohort_id, product_id, packing_unit_id, new_cart_rule\n",
    "        dry_run: If True, only log what would be done\n",
    "        \n",
    "    Returns:\n",
    "        dict with upload results\n",
    "    \"\"\"\n",
    "    results = {'success': [], 'failed': []}\n",
    "    \n",
    "    print(f\"\\n  Cart rules to update: {len(cart_rules_update)} products across {cart_rules_update['cohort_id'].nunique()} cohorts\")\n",
    "    \n",
    "    for cohort in cart_rules_update['cohort_id'].unique():\n",
    "        req_data = cart_rules_update[cart_rules_update['cohort_id'] == cohort].copy()\n",
    "        \n",
    "        if len(req_data) > 0:\n",
    "            # Prepare data for upload\n",
    "            req_data = req_data[['product_id', 'packing_unit_id', 'new_cart_rule']]\n",
    "            req_data.columns = ['Product ID', 'Packing Unit ID', 'Cart Rules']\n",
    "            \n",
    "            filename = f'CartRules_{cohort}.xlsx'\n",
    "            \n",
    "            if dry_run:\n",
    "                print(f\"    [DRY RUN] Cohort {cohort}: Would upload {len(req_data)} rules\")\n",
    "                results['success'].append(cohort)\n",
    "                continue\n",
    "            \n",
    "            # Save and upload\n",
    "            req_data.to_excel(filename, index=False, engine='openpyxl')\n",
    "            \n",
    "            time.sleep(2)  # Rate limiting\n",
    "            response = post_cart_rules(cohort, filename)\n",
    "            \n",
    "            if response.ok:\n",
    "                print(f\"    ✓ Cohort {cohort}: {len(req_data)} rules uploaded\")\n",
    "                results['success'].append(cohort)\n",
    "            else:\n",
    "                print(f\"    ❌ Cohort {cohort}: Upload failed ({response.status_code})\")\n",
    "                results['failed'].append({'cohort_id': cohort, 'error': response.content[:200]})\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"✓ API functions defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STANDALONE FUNCTIONS (can be called independently)\n",
    "# =============================================================================\n",
    "# Use deactivate_active_qd() directly if you only need to deactivate QDs\n",
    "# without creating new ones.\n",
    "#\n",
    "# Example:\n",
    "#   result = deactivate_active_qd(dry_run=True)\n",
    "#   print(f\"Deactivated: {len(result['deactivated'])} QDs\")\n",
    "\n",
    "print(\"✓ QD Handler ready to use\")\n",
    "print(\"\\nAvailable functions:\")\n",
    "print(\"  - process_qd(df_qd, dry_run=True)      : Main function to process QDs from Module 3\")\n",
    "print(\"  - deactivate_active_qd(dry_run=True)   : Deactivate all active QDs\")\n",
    "print(\"  - create_upload_format(df_configs)     : Create upload format DataFrame\")\n",
    "print(\"  - prepare_upload_file(df_upload, ...)  : Prepare final upload file with tag IDs\")\n",
    "print(\"  - post_QD(filename)                    : Upload QD file to API\")\n",
    "print(\"  - prepare_cart_rules_update(df_work, df_qd) : Prepare cart rules update\")\n",
    "print(\"  - upload_cart_rules(cart_rules, ...)   : Upload cart rules by cohort\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# USAGE EXAMPLE (for testing)\n",
    "# =============================================================================\n",
    "# Uncomment below to test with sample data:\n",
    "#\n",
    "# sample_df = pd.DataFrame({\n",
    "#     'product_id': [12345, 67890],\n",
    "#     'warehouse_id': [9, 625],\n",
    "#     'cohort_id': [3304, 3305],\n",
    "#     'sku': ['Test Product 1', 'Test Product 2'],\n",
    "#     'keep_qd_tiers': [['T1', 'T2'], ['T1']]\n",
    "# })\n",
    "# \n",
    "# result = process_qd(sample_df, dry_run=True)\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell kept for potential future use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell kept for potential future use\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
