{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3: Periodic Action Module (UTH-Based Adjustments)\n",
    "\n",
    "## Purpose\n",
    "This module runs at 12 PM, 3 PM, 6 PM, 9 PM, and 12 AM Cairo time to:\n",
    "1. Adjust prices based on Up-Till-Hour (UTH) performance vs benchmarks\n",
    "2. Manage SKU discounts and Quantity Discounts based on performance\n",
    "3. Adjust cart rules dynamically\n",
    "\n",
    "## UTH Benchmarks\n",
    "- Calculate historical qty from start of day till current hour over the last 4 months\n",
    "- Multiply by P80 all-time-high quantity and P70 retailers\n",
    "\n",
    "## Action Logic\n",
    "- **On Track (±10%)**: No action\n",
    "- **Growing (>110%)**: Deactivate discounts or increase price, reduce cart if too open\n",
    "- **Dropping (<90%)**: Reduce price, increase cart by 20%\n",
    "- **Zero Demand (qty=0 today)**: Market min + SKU discount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Upgrade pip\n",
    "!pip install --upgrade pip\n",
    "# Connectivity\n",
    "!pip install psycopg2-binary\n",
    "!pip install snowflake-connector-python==3.15.0\n",
    "!pip install snowflake-sqlalchemy\n",
    "!pip install warnings\n",
    "!pip install keyring==23.11.0\n",
    "!pip install sqlalchemy==1.4.46\n",
    "!pip install requests\n",
    "!pip install boto3\n",
    "!pip install oauth2client\n",
    "!pip install gspread==5.9.0\n",
    "!pip install gspread_dataframe\n",
    "!pip install google.cloud\n",
    "# Data manipulation and analysis\n",
    "!pip install polars\n",
    "!pip install pandas==2.2.1\n",
    "!pip install numpy\n",
    "!pip install openpyxl\n",
    "!pip install xlsxwriter\n",
    "# Date and time handling\n",
    "!pip install --upgrade datetime\n",
    "!pip install python-time\n",
    "!pip install --upgrade pytz\n",
    "# Progress bar\n",
    "!pip install tqdm\n",
    "# Database data types\n",
    "!pip install db-dtypes\n",
    "# Modeling\n",
    "!pip install statsmodels\n",
    "!pip install scikit-learn\n",
    "!pip install import-ipynb\n",
    "# Plotting\n",
    "!pip install matplotlib\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.12/site-packages/snowflake/connector/options.py:104: UserWarning: You have an incompatible version of 'pyarrow' installed (20.0.0), please install a version that adheres to: 'pyarrow<19.0.0; extra == \"pandas\"'\n",
      "  warn_incompatible_dep(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.Renviron\n",
      "/home/ec2-user/service_account_key.json\n",
      "Queries Module | Timezone: America/Los_Angeles\n",
      "✅ UTH and Last Hour functions defined\n",
      "\n",
      "==================================================\n",
      "QUERIES MODULE READY\n",
      "==================================================\n",
      "\n",
      "Live Data Functions:\n",
      "  • get_current_stocks()\n",
      "  • get_packing_units()\n",
      "  • get_current_prices()\n",
      "  • get_current_wac()\n",
      "  • get_current_cart_rules()\n",
      "\n",
      "UTH Performance Functions:\n",
      "  • get_uth_performance()         - UTH qty/retailers (Snowflake)\n",
      "  • get_hourly_distribution()     - Historical hour contributions (Snowflake)\n",
      "  • get_last_hour_performance()   - Last hour qty/retailers (DWH)\n",
      "\n",
      "Note: Market prices use MODULE_1_INPUT data\n",
      "Retailer Selection Queries defined ✓\n",
      "  - get_churned_dropped_retailers()\n",
      "  - get_category_not_product_retailers()\n",
      "  - get_out_of_cycle_retailers()\n",
      "  - get_view_no_orders_retailers()\n",
      "  - get_excluded_retailers()\n",
      "  - get_retailers_with_quantity_discount()\n",
      "  - get_retailer_main_warehouse()\n",
      "/home/ec2-user/.Renviron\n",
      "/home/ec2-user/service_account_key.json\n",
      "Market Data Module loaded at 2026-02-10 12:07:33 Cairo time\n",
      "Snowflake timezone: America/Los_Angeles\n",
      "All queries defined ✓\n",
      "Helper functions defined ✓\n",
      "get_market_data() function defined ✓\n",
      "get_margin_tiers() function defined ✓\n",
      "\n",
      "======================================================================\n",
      "MARKET DATA MODULE READY\n",
      "======================================================================\n",
      "\n",
      "Available functions (NO INPUT REQUIRED):\n",
      "  - get_market_data()   : Fetch and process all market prices\n",
      "  - get_margin_tiers()  : Fetch and calculate margin tiers\n",
      "\n",
      "Usage:\n",
      "  %run market_data_module.ipynb\n",
      "  df_market = get_market_data()\n",
      "  df_tiers = get_margin_tiers()\n",
      "======================================================================\n",
      "Module 3: Periodic Actions\n",
      "Run Time (Cairo): 2026-02-10 12:07:33\n",
      "Current Hour (Cairo): 12\n",
      "Input: MATERIALIZED_VIEWS.Pricing_data_extraction (today's data)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS AND SETUP\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "# Run queries_module - this:\n",
    "# 1. Initializes Snowflake credentials (setup_environment_2.initialize_env())\n",
    "# 2. Provides query_snowflake() function\n",
    "# 3. Provides TIMEZONE from Snowflake\n",
    "# 4. Provides get_current_stocks(), get_current_prices(), get_current_wac(), get_current_cart_rules()\n",
    "%run queries_module.ipynb\n",
    "\n",
    "# Run market_data_module - this:\n",
    "# 1. Provides get_market_data() for fetching fresh market prices (NO INPUT REQUIRED)\n",
    "# 2. Provides get_margin_tiers() for fetching margin tiers (NO INPUT REQUIRED)\n",
    "# 3. Fetches Ben Soliman, Marketplace, and Scrapped prices\n",
    "# 4. Fills missing prices from group-level data\n",
    "# 5. Calculates market price percentiles and margin tiers\n",
    "%run market_data_module.ipynb\n",
    "\n",
    "# Cairo timezone\n",
    "CAIRO_TZ = pytz.timezone('Africa/Cairo')\n",
    "CAIRO_NOW = datetime.now(CAIRO_TZ)\n",
    "TODAY = CAIRO_NOW.date()\n",
    "CURRENT_HOUR = CAIRO_NOW.hour\n",
    "\n",
    "# Configuration\n",
    "UTH_GROWING_THRESHOLD = 1.10    # >110% = Growing\n",
    "UTH_DROPPING_THRESHOLD = 0.90   # <90% = Dropping\n",
    "LOW_STOCK_DOH_THRESHOLD = 2     # SKUs with DOH <= this are protected from price reduction\n",
    "CART_INCREASE_PCT = 0.20        # 20% cart increase\n",
    "CART_DECREASE_PCT = 0.20        # 20% cart decrease\n",
    "MIN_CART_RULE = 5\n",
    "MAX_CART_RULE = 150\n",
    "MIN_PRICE_CHANGE_EGP = 0.25     # Minimum 0.25 EGP for any price change\n",
    "CONTRIBUTION_THRESHOLD = 50     # 50% contribution threshold\n",
    "MAX_PRICE_REDUCTIONS_PER_DAY = 2  # Max price reductions per day\n",
    "# SKU discount percentage will be decided in sku_discount_handler\n",
    "\n",
    "# Input/Output configuration\n",
    "# Data is now loaded from Snowflake instead of Excel\n",
    "INPUT_TABLE = 'MATERIALIZED_VIEWS.Pricing_data_extraction'\n",
    "PREVIOUS_OUTPUT_TABLE = 'MATERIALIZED_VIEWS.pricing_periodic_push'\n",
    "OUTPUT_FILE = f'module_3_output_{CAIRO_NOW.strftime(\"%Y%m%d_%H%M\")}.xlsx'\n",
    "\n",
    "print(f\"Module 3: Periodic Actions\")\n",
    "print(f\"Run Time (Cairo): {CAIRO_NOW.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Current Hour (Cairo): {CURRENT_HOUR}\")\n",
    "print(f\"Input: {INPUT_TABLE} (today's data)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading previous actions from today...\n",
      "No previous Module 3 outputs found for today. This is the first run.\n",
      "Previous actions loaded: 0 records\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD PREVIOUS ACTIONS (Track price reductions per day)\n",
    "# Now loads from Snowflake instead of local Excel files\n",
    "# =============================================================================\n",
    "\n",
    "def load_previous_actions():\n",
    "    \"\"\"Load previous Module 3 outputs from today (from Snowflake) to track price reductions.\"\"\"\n",
    "    try:\n",
    "        # Query today's previous actions from Snowflake\n",
    "        query = f\"\"\"\n",
    "        SELECT * FROM {PREVIOUS_OUTPUT_TABLE}\n",
    "        WHERE DATE(created_at) = '{TODAY}'\n",
    "        ORDER BY created_at\n",
    "        \"\"\"\n",
    "        df = query_snowflake(query)\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            print(\"No previous Module 3 outputs found for today. This is the first run.\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        print(f\"Loaded {len(df)} previous action records from Snowflake\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading previous actions from Snowflake: {e}\")\n",
    "        print(\"This may be the first run or table doesn't exist yet.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def count_price_reductions_today(product_id, warehouse_id, previous_df):\n",
    "    \"\"\"Count how many price reductions this SKU has had today.\"\"\"\n",
    "    if previous_df.empty:\n",
    "        return 0\n",
    "    \n",
    "    mask = (\n",
    "        (previous_df['product_id'] == product_id) & \n",
    "        (previous_df['warehouse_id'] == warehouse_id) &\n",
    "        (previous_df['price_action'].str.contains('decrease', na=False))\n",
    "    )\n",
    "    return mask.sum()\n",
    "def count_price_increase_today(product_id, warehouse_id, previous_df):\n",
    "    \"\"\"Count how many price increase this SKU has had today.\"\"\"\n",
    "    if previous_df.empty:\n",
    "        return 0\n",
    "    \n",
    "    mask = (\n",
    "        (previous_df['product_id'] == product_id) & \n",
    "        (previous_df['warehouse_id'] == warehouse_id) &\n",
    "        (previous_df['price_action'].str.contains('increase', na=False))\n",
    "    )\n",
    "    return mask.sum()\n",
    "    \n",
    "\n",
    "print(\"Loading previous actions from today...\")\n",
    "df_previous_actions = load_previous_actions()\n",
    "print(f\"Previous actions loaded: {len(df_previous_actions)} records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    prev_inc = (\n",
    "        df_previous_actions.assign(\n",
    "            inc_flag=df_previous_actions['price_action'].str.contains('increase', case=False, na=False)\n",
    "        )\n",
    "        .groupby(['product_id', 'warehouse_id'])['inc_flag']\n",
    "        .sum()\n",
    "        .reset_index(name='increase_count')\n",
    "    )\n",
    "except:\n",
    "    prev_inc = pd.DataFrame(columns=['product_id', 'warehouse_id','increase_count'])\n",
    "try:    \n",
    "    prev_red = (\n",
    "    df_previous_actions.assign(\n",
    "        red_flag=df_previous_actions['price_action'].str.contains('decrease', case=False, na=False)\n",
    "    )\n",
    "    .groupby(['product_id', 'warehouse_id'])['red_flag']\n",
    "    .sum()\n",
    "    .reset_index(name='reduced_count') \n",
    "    )\n",
    "except:\n",
    "    prev_red = pd.DataFrame(columns=['product_id', 'warehouse_id','reduced_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snowflake connection ready\n",
      "Timezone: America/Los_Angeles\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SNOWFLAKE CONNECTION\n",
    "# =============================================================================\n",
    "# query_snowflake() and TIMEZONE are provided by queries_module.ipynb\n",
    "# (which also initializes Snowflake credentials from setup_environment_2)\n",
    "print(f\"Snowflake connection ready\")\n",
    "print(f\"Timezone: {TIMEZONE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading today's UTH performance with discount contributions...\n",
      "Loaded 6426 UTH records\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# QUERY 1: TODAY'S UTH PERFORMANCE\n",
    "# =============================================================================\n",
    "UTH_LIVE_QUERY = f'''\n",
    "WITH params AS (\n",
    "    SELECT\n",
    "        CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE AS today,\n",
    "        HOUR(CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())) AS current_hour\n",
    "),\n",
    "\n",
    "-- Map dynamic tags to warehouse IDs using name matching\n",
    "qd_det AS (\n",
    "    SELECT DISTINCT \n",
    "        dt.id AS tag_id, \n",
    "        dt.name AS tag_name,\n",
    "        REPLACE(w.name, ' ', '') AS warehouse_name,\n",
    "        w.id AS warehouse_id,\n",
    "        warehouse_name ILIKE '%' || CASE \n",
    "            WHEN SPLIT_PART(tag_name, '_', 1) = 'El' THEN SPLIT_PART(tag_name, '_', 2) \n",
    "            ELSE SPLIT_PART(tag_name, '_', 1) \n",
    "        END || '%' AS contains_flag\n",
    "    FROM dynamic_tags dt\n",
    "    JOIN dynamic_taggables dta ON dt.id = dta.dynamic_tag_id \n",
    "    CROSS JOIN warehouses w \n",
    "    WHERE dt.id > 3000\n",
    "        AND dt.name LIKE '%QD_rets%'\n",
    "        AND w.id IN (1, 236, 337, 8, 339, 170, 501, 401, 703, 632, 797, 962)\n",
    "        AND contains_flag = 'true'\n",
    "),\n",
    "\n",
    "-- Get current active QD configurations\n",
    "qd_config AS (\n",
    "    SELECT * \n",
    "    FROM (\n",
    "        SELECT \n",
    "            product_id,\n",
    "            start_at,\n",
    "            end_at,\n",
    "            packing_unit_id,\n",
    "            id AS qd_id,\n",
    "            qd.warehouse_id,\n",
    "            MAX(CASE WHEN tier = 1 THEN quantity END) AS tier_1_qty,\n",
    "            MAX(CASE WHEN tier = 1 THEN discount_percentage END) AS tier_1_discount_pct,\n",
    "            MAX(CASE WHEN tier = 2 THEN quantity END) AS tier_2_qty,\n",
    "            MAX(CASE WHEN tier = 2 THEN discount_percentage END) AS tier_2_discount_pct,\n",
    "            MAX(CASE WHEN tier = 3 THEN quantity END) AS tier_3_qty,\n",
    "            MAX(CASE WHEN tier = 3 THEN discount_percentage END) AS tier_3_discount_pct\n",
    "        FROM (\n",
    "            SELECT \n",
    "                qd.id,\n",
    "                qdv.product_id,\n",
    "                qdv.packing_unit_id,\n",
    "                qdv.quantity,\n",
    "                qdv.discount_percentage,\n",
    "                qd.dynamic_tag_id,\n",
    "                qd.start_at,\n",
    "                qd.end_at,\n",
    "                ROW_NUMBER() OVER (\n",
    "                    PARTITION BY qdv.product_id, qdv.packing_unit_id, qd.id \n",
    "                    ORDER BY qdv.quantity\n",
    "                ) AS tier\n",
    "            FROM quantity_discounts qd \n",
    "            JOIN quantity_discount_values qdv ON qdv.quantity_discount_id = qd.id\n",
    "            WHERE active = 'true'\n",
    "        ) qd_tiers\n",
    "        JOIN qd_det qd ON qd.tag_id = qd_tiers.dynamic_tag_id\n",
    "        GROUP BY ALL\n",
    "    )\n",
    "    QUALIFY ROW_NUMBER() OVER (PARTITION BY product_id, packing_unit_id, warehouse_id ORDER BY start_at DESC) = 1\n",
    "),\n",
    "\n",
    "-- Today's sales up-till-hour with discount breakdown\n",
    "today_uth_sales AS (\n",
    "    SELECT \n",
    "        pso.warehouse_id,\n",
    "        pso.product_id,\n",
    "        so.retailer_id,\n",
    "        pso.packing_unit_id,\n",
    "        pso.purchased_item_count AS qty,\n",
    "        pso.total_price AS nmv,\n",
    "        pso.ITEM_DISCOUNT_VALUE AS sku_discount_per_unit,\n",
    "        pso.ITEM_QUANTITY_DISCOUNT_VALUE AS qty_discount_per_unit,\n",
    "        qd.tier_1_qty,\n",
    "        qd.tier_2_qty,\n",
    "        qd.tier_3_qty,\n",
    "        -- Determine tier used\n",
    "        CASE \n",
    "            WHEN pso.ITEM_QUANTITY_DISCOUNT_VALUE = 0 OR qd.tier_1_qty IS NULL THEN 'Base'\n",
    "            WHEN qd.tier_3_qty IS NOT NULL AND pso.purchased_item_count >= qd.tier_3_qty THEN 'Tier 3'\n",
    "            WHEN qd.tier_2_qty IS NOT NULL AND pso.purchased_item_count >= qd.tier_2_qty THEN 'Tier 2'\n",
    "            WHEN qd.tier_1_qty IS NOT NULL AND pso.purchased_item_count >= qd.tier_1_qty THEN 'Tier 1'\n",
    "            ELSE 'Base'\n",
    "        END AS tier_used\n",
    "    FROM product_sales_order pso\n",
    "    JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "    LEFT JOIN qd_config qd \n",
    "        ON qd.product_id = pso.product_id \n",
    "        AND qd.packing_unit_id = pso.packing_unit_id\n",
    "        AND qd.warehouse_id = so.warehouse_id\n",
    "    CROSS JOIN params p\n",
    "    WHERE so.created_at::DATE = p.today\n",
    "        AND HOUR(so.created_at) < p.current_hour\n",
    "        AND so.sales_order_status_id NOT IN (7, 12)\n",
    "        AND so.channel IN ('telesales', 'retailer')\n",
    "        AND pso.purchased_item_count <> 0\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    warehouse_id,\n",
    "    product_id,\n",
    "    SUM(qty) AS uth_qty,\n",
    "    SUM(nmv) AS uth_nmv,\n",
    "    COUNT(DISTINCT retailer_id) AS uth_retailers,\n",
    "    -- SKU discount NMV and contribution\n",
    "    SUM(CASE WHEN sku_discount_per_unit > 0 THEN nmv ELSE 0 END) AS sku_discount_nmv_uth,\n",
    "    ROUND(SUM(CASE WHEN sku_discount_per_unit > 0 THEN nmv ELSE 0 END) * 100.0 / NULLIF(SUM(nmv), 0), 2) AS sku_disc_cntrb_uth,\n",
    "    -- Quantity discount NMV and contribution\n",
    "    SUM(CASE WHEN qty_discount_per_unit > 0 THEN nmv ELSE 0 END) AS qty_discount_nmv_uth,\n",
    "    ROUND(SUM(CASE WHEN qty_discount_per_unit > 0 THEN nmv ELSE 0 END) * 100.0 / NULLIF(SUM(nmv), 0), 2) AS qty_disc_cntrb_uth,\n",
    "    -- Tier-level NMV\n",
    "    SUM(CASE WHEN tier_used = 'Tier 1' THEN nmv ELSE 0 END) AS t1_nmv_uth,\n",
    "    SUM(CASE WHEN tier_used = 'Tier 2' THEN nmv ELSE 0 END) AS t2_nmv_uth,\n",
    "    SUM(CASE WHEN tier_used = 'Tier 3' THEN nmv ELSE 0 END) AS t3_nmv_uth,\n",
    "    -- Tier-level contributions\n",
    "    ROUND(SUM(CASE WHEN tier_used = 'Tier 1' THEN nmv ELSE 0 END) * 100.0 / NULLIF(SUM(nmv), 0), 2) AS t1_cntrb_uth,\n",
    "    ROUND(SUM(CASE WHEN tier_used = 'Tier 2' THEN nmv ELSE 0 END) * 100.0 / NULLIF(SUM(nmv), 0), 2) AS t2_cntrb_uth,\n",
    "    ROUND(SUM(CASE WHEN tier_used = 'Tier 3' THEN nmv ELSE 0 END) * 100.0 / NULLIF(SUM(nmv), 0), 2) AS t3_cntrb_uth\n",
    "FROM today_uth_sales\n",
    "GROUP BY warehouse_id, product_id\n",
    "HAVING SUM(nmv) > 0\n",
    "'''\n",
    "\n",
    "print(\"Loading today's UTH performance with discount contributions...\")\n",
    "df_uth_today = query_snowflake(UTH_LIVE_QUERY)\n",
    "print(f\"Loaded {len(df_uth_today)} UTH records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching hourly distribution from Snowflake...\n",
      "  Loaded 770 hourly distribution records\n",
      "Using avg_uth_pct_qty as avg_uth_pct for Module 3 compatibility\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# QUERY 2: HISTORICAL HOURLY DISTRIBUTION (Last 4 Months) - By Category & Warehouse\n",
    "# =============================================================================\n",
    "# Uses get_hourly_distribution() from queries_module\n",
    "\n",
    "df_hourly_dist = get_hourly_distribution()\n",
    "\n",
    "# Rename column for backwards compatibility with rest of Module 3\n",
    "df_hourly_dist['avg_uth_pct'] = df_hourly_dist['avg_uth_pct_qty']\n",
    "print(f\"Using avg_uth_pct_qty as avg_uth_pct for Module 3 compatibility\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading active SKU discounts...\n",
      "Loaded 4642 active SKU discount records\n",
      "Loading active Quantity discounts...\n",
      "Loaded 278 active QD records\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# QUERY 3 & 4: ACTIVE DISCOUNTS\n",
    "# =============================================================================\n",
    "\n",
    "# SKU Discounts query (from data_extraction.ipynb)\n",
    "ACTIVE_SKU_DISCOUNTS_QUERY = f'''\n",
    "WITH active_sku_discount AS ( \n",
    "    SELECT \n",
    "        x.id AS sku_discount_id,\n",
    "        retailer_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        DISCOUNT_PERCENTAGE,\n",
    "        start_at,\n",
    "        end_at \n",
    "    FROM (\n",
    "        SELECT \n",
    "            sd.*,\n",
    "            f.value::INT AS retailer_id \n",
    "        FROM SKU_DISCOUNTS sd,\n",
    "        LATERAL FLATTEN(\n",
    "            input => SPLIT(\n",
    "                REPLACE(REPLACE(REPLACE(sd.retailer_ids, '{{', ''), '}}', ''), '\"', ''), \n",
    "                ','\n",
    "            )\n",
    "        ) f\n",
    "        WHERE start_at::DATE <= CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE\n",
    "        and end_at::DATE >= CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE\n",
    "            AND active = 'true'\n",
    "    ) x \n",
    "    JOIN SKU_DISCOUNT_VALUES sdv ON x.id = sdv.sku_discount_id\n",
    "    WHERE name_en = 'Special Discounts'\n",
    "    QUALIFY MAX(start_at) OVER (PARTITION BY retailer_id, product_id, packing_unit_id) = start_at \n",
    ")\n",
    "\n",
    "SELECT \n",
    "    product_id, \n",
    "    warehouse_id,\n",
    "    AVG(DISCOUNT_PERCENTAGE) AS active_sku_disc_pct,\n",
    "    1 AS has_active_sku_discount\n",
    "FROM (\n",
    "    SELECT \n",
    "        asd.*,\n",
    "        warehouse_id \n",
    "    FROM active_sku_discount asd \n",
    "    JOIN materialized_views.retailer_polygon rp ON rp.retailer_id = asd.retailer_id\n",
    "    JOIN WAREHOUSE_DISPATCHING_RULES wdr ON wdr.product_id = asd.product_id\n",
    "    JOIN DISPATCHING_POLYGONS dp ON dp.id = wdr.DISPATCHING_POLYGON_ID AND dp.district_id = rp.district_id\n",
    ")\n",
    "GROUP BY ALL\n",
    "'''\n",
    "\n",
    "# Active QD Query - Reuses the same CTE structure from UTH_LIVE_QUERY\n",
    "ACTIVE_QD_QUERY = f'''\n",
    "WITH qd_det AS (\n",
    "    SELECT DISTINCT \n",
    "        dt.id AS tag_id, \n",
    "        dt.name AS tag_name,\n",
    "        REPLACE(w.name, ' ', '') AS warehouse_name,\n",
    "        w.id AS warehouse_id,\n",
    "        warehouse_name ILIKE '%' || CASE \n",
    "            WHEN SPLIT_PART(tag_name, '_', 1) = 'El' THEN SPLIT_PART(tag_name, '_', 2) \n",
    "            ELSE SPLIT_PART(tag_name, '_', 1) \n",
    "        END || '%' AS contains_flag\n",
    "    FROM dynamic_tags dt\n",
    "    JOIN dynamic_taggables dta ON dt.id = dta.dynamic_tag_id \n",
    "    CROSS JOIN warehouses w \n",
    "    WHERE dt.id > 3000\n",
    "        AND dt.name LIKE '%QD_rets%'\n",
    "        AND w.id IN (1, 236, 337, 8, 339, 170, 501, 401, 703, 632, 797, 962)\n",
    "        AND contains_flag = 'true'\n",
    "),\n",
    "\n",
    "qd_config AS (\n",
    "    SELECT * \n",
    "    FROM (\n",
    "        SELECT \n",
    "            product_id,\n",
    "            packing_unit_id,\n",
    "            qd.warehouse_id,\n",
    "            MAX(CASE WHEN tier = 1 THEN quantity END) AS qd_tier_1_qty,\n",
    "            MAX(CASE WHEN tier = 1 THEN discount_percentage END) AS qd_tier_1_disc_pct,\n",
    "            MAX(CASE WHEN tier = 2 THEN quantity END) AS qd_tier_2_qty,\n",
    "            MAX(CASE WHEN tier = 2 THEN discount_percentage END) AS qd_tier_2_disc_pct,\n",
    "            MAX(CASE WHEN tier = 3 THEN quantity END) AS qd_tier_3_qty,\n",
    "            MAX(CASE WHEN tier = 3 THEN discount_percentage END) AS qd_tier_3_disc_pct\n",
    "        FROM (\n",
    "            SELECT \n",
    "                qd.id,\n",
    "                qdv.product_id,\n",
    "                qdv.packing_unit_id,\n",
    "                qdv.quantity,\n",
    "                qdv.discount_percentage,\n",
    "                qd.dynamic_tag_id,\n",
    "                qd.start_at,\n",
    "                ROW_NUMBER() OVER (\n",
    "                    PARTITION BY qdv.product_id, qdv.packing_unit_id, qd.id \n",
    "                    ORDER BY qdv.quantity\n",
    "                ) AS tier\n",
    "            FROM quantity_discounts qd \n",
    "            JOIN quantity_discount_values qdv ON qdv.quantity_discount_id = qd.id\n",
    "            WHERE  active = TRUE\n",
    "        ) qd_tiers\n",
    "        JOIN qd_det qd ON qd.tag_id = qd_tiers.dynamic_tag_id\n",
    "        GROUP BY ALL\n",
    "    )\n",
    "    QUALIFY ROW_NUMBER() OVER (PARTITION BY product_id, packing_unit_id, warehouse_id ORDER BY qd_tier_1_qty DESC) = 1\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    product_id,\n",
    "    warehouse_id,\n",
    "    qd_tier_1_qty,\n",
    "    qd_tier_1_disc_pct,\n",
    "    qd_tier_2_qty,\n",
    "    qd_tier_2_disc_pct,\n",
    "    qd_tier_3_qty,\n",
    "    qd_tier_3_disc_pct,\n",
    "    1 AS has_active_qd\n",
    "FROM qd_config\n",
    "'''\n",
    "\n",
    "print(\"Loading active SKU discounts...\")\n",
    "df_active_sku_disc = query_snowflake(ACTIVE_SKU_DISCOUNTS_QUERY)\n",
    "print(f\"Loaded {len(df_active_sku_disc)} active SKU discount records\")\n",
    "\n",
    "print(\"Loading active Quantity discounts...\")\n",
    "df_active_qd = query_snowflake(ACTIVE_QD_QUERY)\n",
    "print(f\"Loaded {len(df_active_qd)} active QD records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from Snowflake...\n",
      "Loaded 28116 records from Snowflake\n",
      "\n",
      "Refreshing live data...\n",
      "Fetching current stocks...\n",
      "  Loaded 1865878 records\n",
      "Fetching current prices...\n",
      "  Loaded 116402 records\n",
      "Fetching current WAC...\n",
      "  Loaded 8192 records\n",
      "Fetching current cart rules...\n",
      "  Loaded 72968 records\n",
      "Live data refreshed: stocks, prices, WAC, cart rules\n",
      "\n",
      "Refreshing market prices and margin tiers...\n",
      "\n",
      "======================================================================\n",
      "FETCHING MARKET DATA\n",
      "======================================================================\n",
      "Timestamp: 2026-02-10 12:15:58 Cairo time\n",
      "\n",
      "Step 1: Fetching raw price data...\n",
      "  1.1 Ben Soliman prices...\n",
      "      Loaded 1542 records\n",
      "  1.2 Marketplace prices...\n",
      "      Loaded 10718 records\n",
      "  1.3 Scrapped prices...\n",
      "      Loaded 3085 records\n",
      "  1.4 Product groups...\n",
      "      Loaded 1580 records\n",
      "  1.5 Sales data (for NMV weighting)...\n",
      "      Loaded 20913 records\n",
      "  1.6 Margin stats...\n",
      "      Loaded 28970 records\n",
      "  1.7 Target margins...\n",
      "      Loaded 469 records\n",
      "  1.8 Product base (WAC)...\n",
      "      Loaded 65358 records\n",
      "\n",
      "Step 2: Joining all market price sources (outer join)...\n",
      "    Market prices base: 15257 records\n",
      "\n",
      "Step 3: Adding cohort IDs and supporting data...\n",
      "    Records after adding cohorts: 22856\n",
      "\n",
      "Step 4: Processing group-level prices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1396/3245917641.py:139: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: pd.Series({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Records after group processing: 24347\n",
      "\n",
      "Step 5: Adding WAC and margin data...\n",
      "    Records with WAC: 23951\n",
      "\n",
      "Step 6: Filtering by price coverage...\n",
      "    Records after price coverage filter: 12066\n",
      "\n",
      "Step 7: Calculating price percentiles...\n",
      "    Records after price analysis: 11547\n",
      "\n",
      "Step 8: Converting prices to margins...\n",
      "\n",
      "======================================================================\n",
      "MARKET DATA COMPLETE\n",
      "======================================================================\n",
      "Total records: 11547\n",
      "  - With marketplace prices: 11352\n",
      "  - With scrapped prices: 3774\n",
      "  - With Ben Soliman prices: 8010\n",
      "  Fetched 11547 market data records\n",
      "\n",
      "======================================================================\n",
      "FETCHING MARGIN TIERS\n",
      "======================================================================\n",
      "Timestamp: 2026-02-10 12:17:20 Cairo time\n",
      "\n",
      "Step 1: Fetching margin boundaries from PRODUCT_STATISTICS...\n",
      "    Loaded 18058 records\n",
      "\n",
      "Step 2: Adding cohort IDs...\n",
      "    Records with cohorts: 24919\n",
      "\n",
      "Step 3: Calculating margin tiers...\n",
      "\n",
      "======================================================================\n",
      "MARGIN TIERS COMPLETE\n",
      "======================================================================\n",
      "Total records: 24919\n",
      "\n",
      "Margin Tier Structure:\n",
      "  margin_tier_below:   effective_min - step (1 below)\n",
      "  margin_tier_1:       effective_min_margin\n",
      "  margin_tier_2:       effective_min + 1*step\n",
      "  margin_tier_3:       effective_min + 2*step\n",
      "  margin_tier_4:       effective_min + 3*step\n",
      "  margin_tier_5:       max_boundary\n",
      "  margin_tier_above_1: max_boundary + 1*step\n",
      "  margin_tier_above_2: max_boundary + 2*step\n",
      "  Fetched 24919 margin tier records\n",
      "Market data refreshed\n",
      "Data merged. Total records: 28124\n",
      "  SKUs with active SKU discount: 4600\n",
      "  SKUs with active QD: 278\n",
      "  SKUs with high DOH (>30): 18717\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD DATA FROM SNOWFLAKE (Instead of Excel file)\n",
    "# =============================================================================\n",
    "print(\"Loading data from Snowflake...\")\n",
    "\n",
    "# Query to get today's data from Pricing_data_extraction\n",
    "LOAD_QUERY = f\"\"\"\n",
    "SELECT * FROM {INPUT_TABLE}\n",
    "WHERE created_at = '{datetime.now(CAIRO_TZ).date()}'\n",
    "\"\"\"\n",
    "\n",
    "df = query_snowflake(LOAD_QUERY)\n",
    "print(f\"Loaded {len(df)} records from Snowflake\")\n",
    "\n",
    "# Refresh live data using queries_module\n",
    "print(\"\\nRefreshing live data...\")\n",
    "\n",
    "# Refresh stocks\n",
    "df_fresh_stocks = get_current_stocks()\n",
    "df = df.drop(columns=['stocks'], errors='ignore')\n",
    "df = df.merge(df_fresh_stocks, on=['warehouse_id', 'product_id'], how='left')\n",
    "df['stocks'] = df['stocks'].fillna(0)\n",
    "\n",
    "# Refresh current prices\n",
    "df_fresh_prices = get_current_prices()\n",
    "df = df.drop(columns=['current_price'], errors='ignore')\n",
    "df = df.merge(df_fresh_prices[['cohort_id', 'product_id', 'current_price']], \n",
    "              on=['cohort_id', 'product_id'], how='left')\n",
    "\n",
    "# Refresh WAC\n",
    "df_fresh_wac = get_current_wac()\n",
    "df = df.drop(columns=['wac_p'], errors='ignore')\n",
    "df = df.merge(df_fresh_wac, on='product_id', how='left')\n",
    "\n",
    "# Refresh cart rules\n",
    "df_fresh_cart = get_current_cart_rules()\n",
    "df = df.drop(columns=['current_cart_rule'], errors='ignore')\n",
    "df = df.merge(df_fresh_cart, on=['cohort_id', 'product_id'], how='left')\n",
    "\n",
    "print(f\"Live data refreshed: stocks, prices, WAC, cart rules\")\n",
    "\n",
    "# Refresh market prices and margin tiers using new standalone functions\n",
    "print(\"\\nRefreshing market prices and margin tiers...\")\n",
    "\n",
    "# Get fresh market data (no input required)\n",
    "df_fresh_market = get_market_data()\n",
    "print(f\"  Fetched {len(df_fresh_market)} market data records\")\n",
    "\n",
    "# Get fresh margin tiers (no input required)\n",
    "df_fresh_tiers = get_margin_tiers()\n",
    "print(f\"  Fetched {len(df_fresh_tiers)} margin tier records\")\n",
    "\n",
    "# Drop old market columns and merge fresh data\n",
    "market_cols_to_drop = [\n",
    "    'below_market', 'market_min', 'market_25', 'market_50', \n",
    "    'market_75', 'market_max', 'above_market',\n",
    "    'minimum', 'percentile_25', 'percentile_50', 'percentile_75', 'maximum',\n",
    "    'ben_soliman_price', 'final_min_price', 'final_max_price', 'final_mod_price',\n",
    "    'final_true_min', 'final_true_max', 'min_scrapped', 'scrapped25', \n",
    "    'scrapped50', 'scrapped75', 'max_scrapped'\n",
    "]\n",
    "df = df.drop(columns=[c for c in market_cols_to_drop if c in df.columns], errors='ignore')\n",
    "\n",
    "# Merge fresh market data\n",
    "df = df.merge(\n",
    "    df_fresh_market, \n",
    "    on=['cohort_id', 'product_id','region'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Drop old margin tier columns and merge fresh data\n",
    "margin_tier_cols_to_drop = [\n",
    "    'margin_tier_below', 'margin_tier_1', 'margin_tier_2', 'margin_tier_3',\n",
    "    'margin_tier_4', 'margin_tier_5', 'margin_tier_above_1', 'margin_tier_above_2',\n",
    "    'optimal_bm', 'min_boundary', 'max_boundary', 'median_bm',\n",
    "    'effective_min_margin', 'margin_step'\n",
    "]\n",
    "df = df.drop(columns=[c for c in margin_tier_cols_to_drop if c in df.columns], errors='ignore')\n",
    "\n",
    "# Merge fresh margin tiers\n",
    "df = df.merge(\n",
    "    df_fresh_tiers, \n",
    "    on=['cohort_id', 'product_id','region'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"Market data refreshed\")\n",
    "\n",
    "# Merge UTH today data - drop old columns first\n",
    "uth_cols = ['uth_qty', 'uth_nmv', 'uth_retailers', 'sku_discount_nmv_uth', 'sku_disc_cntrb_uth',\n",
    "            'qty_discount_nmv_uth', 'qty_disc_cntrb_uth', 't1_nmv_uth', 't2_nmv_uth', 't3_nmv_uth',\n",
    "            't1_cntrb_uth', 't2_cntrb_uth', 't3_cntrb_uth']\n",
    "df = df.drop(columns=[c for c in uth_cols if c in df.columns], errors='ignore')\n",
    "\n",
    "if len(df_uth_today) > 0:\n",
    "    df = df.merge(df_uth_today, on=['warehouse_id', 'product_id'], how='left')\n",
    "else:\n",
    "    for col in uth_cols:\n",
    "        df[col] = 0\n",
    "\n",
    "# Merge hourly distribution - drop old column first (now by warehouse_id + cat)\n",
    "df = df.drop(columns=['avg_uth_pct'], errors='ignore')\n",
    "if len(df_hourly_dist) > 0:\n",
    "    df = df.merge(df_hourly_dist, on=['warehouse_id', 'cat'], how='left')\n",
    "else:\n",
    "    df['avg_uth_pct'] = 0.5  # Default 50%\n",
    "\n",
    "# Merge active SKU discounts - drop old columns first\n",
    "sku_disc_cols = ['has_active_sku_discount', 'active_sku_disc_pct', 'active_sku_discount_value']\n",
    "df = df.drop(columns=[c for c in sku_disc_cols if c in df.columns], errors='ignore')\n",
    "\n",
    "if len(df_active_sku_disc) > 0:\n",
    "    df = df.merge(df_active_sku_disc, on=['warehouse_id', 'product_id'], how='left')\n",
    "else:\n",
    "    df['has_active_sku_discount'] = 0\n",
    "    df['active_sku_disc_pct'] = 0\n",
    "\n",
    "# Merge active QD - drop old columns first\n",
    "qd_cols = ['has_active_qd', 'qd_tier_1_qty', 'qd_tier_1_disc_pct', \n",
    "           'qd_tier_2_qty', 'qd_tier_2_disc_pct', 'qd_tier_3_qty', 'qd_tier_3_disc_pct']\n",
    "df = df.drop(columns=[c for c in qd_cols if c in df.columns], errors='ignore')\n",
    "\n",
    "if len(df_active_qd) > 0:\n",
    "    df = df.merge(df_active_qd, on=['warehouse_id', 'product_id'], how='left')\n",
    "else:\n",
    "    df['has_active_qd'] = 0\n",
    "    df['qd_tier_1_qty'] = 0\n",
    "    df['qd_tier_1_disc_pct'] = 0\n",
    "    df['qd_tier_2_qty'] = 0\n",
    "    df['qd_tier_2_disc_pct'] = 0\n",
    "    df['qd_tier_3_qty'] = 0\n",
    "    df['qd_tier_3_disc_pct'] = 0\n",
    "\n",
    "# Fill NaN\n",
    "df['uth_qty'] = df['uth_qty'].fillna(0)\n",
    "df['uth_retailers'] = df['uth_retailers'].fillna(0)\n",
    "df['avg_uth_pct'] = df['avg_uth_pct'].fillna(0.5)\n",
    "df['has_active_sku_discount'] = df['has_active_sku_discount'].fillna(0)\n",
    "df['active_sku_discount_value'] = df.get('active_sku_discount_value', pd.Series([0]*len(df))).fillna(0)\n",
    "df['has_active_qd'] = df['has_active_qd'].fillna(0)\n",
    "df['qd_tier_1_qty'] = df['qd_tier_1_qty'].fillna(0)\n",
    "df['qd_tier_1_disc_pct'] = df['qd_tier_1_disc_pct'].fillna(0)\n",
    "df['qd_tier_2_qty'] = df['qd_tier_2_qty'].fillna(0)\n",
    "df['qd_tier_2_disc_pct'] = df['qd_tier_2_disc_pct'].fillna(0)\n",
    "df['qd_tier_3_qty'] = df['qd_tier_3_qty'].fillna(0)\n",
    "df['qd_tier_3_disc_pct'] = df['qd_tier_3_disc_pct'].fillna(0)\n",
    "\n",
    "# =============================================================================\n",
    "# TURNOVER-BASED DOH: Calculate responsive_doh and min_induced_price (vectorized)\n",
    "# =============================================================================\n",
    "# responsive_doh = stocks / yesterday_qty (yesterday_qty comes from INPUT_TABLE)\n",
    "df['yesterday_qty'] = pd.to_numeric(df.get('yesterday_qty', 0), errors='coerce').fillna(0)\n",
    "df['responsive_doh'] = np.where(\n",
    "    df['yesterday_qty'] > 0,\n",
    "    df['stocks'] / df['yesterday_qty'],\n",
    "    999  # No sales yesterday = infinite DOH\n",
    ")\n",
    "\n",
    "# min_induced_price = wac_p * (0.9 + target_margin * 0.5)\n",
    "# This is the floor price for induced pricing when DOH > 30\n",
    "df['target_margin'] = pd.to_numeric(df.get('target_margin', 0), errors='coerce').fillna(0)\n",
    "df['min_induced_price'] = df['wac_p'] * (0.9)\n",
    "\n",
    "print(f\"Data merged. Total records: {len(df)}\")\n",
    "print(f\"  SKUs with active SKU discount: {(df['has_active_sku_discount'] == 1).sum()}\")\n",
    "print(f\"  SKUs with active QD: {(df['has_active_qd'] == 1).sum()}\")\n",
    "print(f\"  SKUs with high DOH (>30): {(df['responsive_doh'] > 30).sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions loaded.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_margin(price, wac):\n",
    "    if pd.isna(price) or pd.isna(wac) or price == 0:\n",
    "        return None\n",
    "    return (price - wac) / price\n",
    "\n",
    "def get_market_tiers(row):\n",
    "    \"\"\"Get sorted list of market price tiers.\"\"\"\n",
    "    tiers = []\n",
    "    for col in ['minimum', 'percentile_25', 'percentile_50', 'percentile_75', 'maximum']:\n",
    "        val = row.get(col)\n",
    "        if pd.notna(val) and val > 0:\n",
    "            tiers.append(val)\n",
    "    return sorted(set(tiers))\n",
    "\n",
    "def get_margin_tiers(row):\n",
    "    \"\"\"Get sorted list of margin-based price tiers (converted to prices).\"\"\"\n",
    "    tiers = []\n",
    "    wac = row.get('wac_p', 0)\n",
    "    if wac <= 0:\n",
    "        return tiers\n",
    "    \n",
    "    for tier_col in ['margin_tier_below','margin_tier_1', 'margin_tier_2', 'margin_tier_3', \n",
    "                     'margin_tier_4', 'margin_tier_5', 'margin_tier_above_1', 'margin_tier_above_2']:\n",
    "        margin = row.get(tier_col)\n",
    "        if pd.notna(margin) and 0 < margin < 1:\n",
    "            price = wac / (1 - margin)\n",
    "            tiers.append(round(price, 2))\n",
    "    return sorted(set(tiers))\n",
    "\n",
    "def find_next_price_above(current_price, row):\n",
    "    \"\"\"\n",
    "    Find the first price tier ABOVE current_price by at least MIN_PRICE_CHANGE_EGP.\n",
    "    Market first, then margin. Skips tiers less than 0.25 EGP above.\n",
    "    \"\"\"\n",
    "    current_price = float(current_price) if current_price else 0\n",
    "    if pd.isna(current_price) or current_price <= 0:\n",
    "        return current_price\n",
    "    \n",
    "    for tier in get_market_tiers(row):\n",
    "        if tier > current_price + MIN_PRICE_CHANGE_EGP:\n",
    "            return round(tier, 2)\n",
    "    \n",
    "    for tier in get_margin_tiers(row):\n",
    "        if tier > current_price + MIN_PRICE_CHANGE_EGP:\n",
    "            return round(tier, 2)\n",
    "    \n",
    "    return current_price\n",
    "\n",
    "def find_next_price_below(current_price, row):\n",
    "    \"\"\"\n",
    "    Find the first price tier BELOW current_price by at least MIN_PRICE_CHANGE_EGP.\n",
    "    Market first, then margin. Skips tiers less than 0.25 EGP below.\n",
    "    \"\"\"\n",
    "    current_price = float(current_price) if current_price else 0\n",
    "    if pd.isna(current_price) or current_price <= 0:\n",
    "        return current_price\n",
    "    \n",
    "    for tier in reversed(get_market_tiers(row)):\n",
    "        if tier < current_price - MIN_PRICE_CHANGE_EGP:\n",
    "            return round(tier, 2)\n",
    "    \n",
    "    for tier in reversed(get_margin_tiers(row)):\n",
    "        if tier < current_price - MIN_PRICE_CHANGE_EGP:\n",
    "            return round(tier, 2)\n",
    "    \n",
    "    return current_price\n",
    "\n",
    "def find_price_n_steps_below(current_price, n_steps, row):\n",
    "    \"\"\"Find price N steps below current (iteratively find next tier below).\"\"\"\n",
    "    price = current_price\n",
    "    for _ in range(n_steps):\n",
    "        next_price = find_next_price_below(price, row)\n",
    "        if next_price >= price:  # No tier below found\n",
    "            break\n",
    "        price = next_price\n",
    "    return price\n",
    "\n",
    "def is_cart_too_open(row):\n",
    "    \"\"\"Check if cart rule is too open: > normal_refill + 10*std\"\"\"\n",
    "    normal_refill = float(row.get('normal_refill', 5) or 5)\n",
    "    stddev = float(row.get('refill_stddev', 2) or 2)\n",
    "    current_cart = float(row.get('cart_rule', normal_refill) or normal_refill)\n",
    "    threshold = normal_refill + (10 * stddev)\n",
    "    return current_cart > threshold\n",
    "\n",
    "def adjust_cart_rule(current_cart, direction, row):\n",
    "    \"\"\"Adjust cart rule by 20% up or down.\"\"\"\n",
    "    normal_refill = float(row.get('normal_refill', 5) or 5)\n",
    "    stddev = float(row.get('refill_stddev', 2) or 2)\n",
    "    current_cart = float(current_cart or 5)\n",
    "    \n",
    "    if direction == 'increase':\n",
    "        new_cart = current_cart * (1 + CART_INCREASE_PCT)\n",
    "        new_cart = min(new_cart, MAX_CART_RULE)\n",
    "    else:  # decrease\n",
    "        # Formula: max(0.8 * cart, normal_refill + 3*std)\n",
    "        new_cart = current_cart * (1 - CART_DECREASE_PCT)\n",
    "        min_floor = normal_refill + (3 * stddev)\n",
    "        new_cart = max(new_cart, min_floor, MIN_CART_RULE)\n",
    "    \n",
    "    return int(new_cart)\n",
    "\n",
    "def get_highest_qd_tier_contribution(row):\n",
    "    \"\"\"Find which QD tier has highest contribution.\"\"\"\n",
    "    t1 = row.get('yesterday_t1_cntrb', 0) or 0\n",
    "    t2 = row.get('yesterday_t2_cntrb', 0) or 0\n",
    "    t3 = row.get('yesterday_t3_cntrb', 0) or 0\n",
    "    \n",
    "    if t1 >= t2 and t1 >= t3 and t1 > 0:\n",
    "        return 'T1', t1\n",
    "    elif t2 >= t1 and t2 >= t3 and t2 > 0:\n",
    "        return 'T2', t2\n",
    "    elif t3 > 0:\n",
    "        return 'T3', t3\n",
    "    return None, 0\n",
    "\n",
    "print(\"Helper functions loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main engine function loaded.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# HELPER: Calculate margin step from existing tier prices\n",
    "# =============================================================================\n",
    "def calculate_margin_step(row):\n",
    "    \"\"\"\n",
    "    Calculate the margin step size from existing margin tiers.\n",
    "    Used to induce prices below available tiers when DOH > 30.\n",
    "    \n",
    "    Returns:\n",
    "        Average step size between consecutive tiers, or 0.015 (1.5%) as default\n",
    "    \"\"\"\n",
    "    tier_cols = ['margin_tier_1', 'margin_tier_2', 'margin_tier_3', \n",
    "                 'margin_tier_4', 'margin_tier_5']\n",
    "    tiers = [row.get(col) for col in tier_cols]\n",
    "    valid_tiers = [t for t in tiers if pd.notna(t) and t is not None]\n",
    "    \n",
    "    if len(valid_tiers) >= 2:\n",
    "        # Calculate steps between consecutive tiers\n",
    "        steps = [abs(valid_tiers[i+1] - valid_tiers[i]) for i in range(len(valid_tiers)-1)]\n",
    "        return np.mean(steps) if steps else 0.01\n",
    "    \n",
    "    # Fallback: use market margins if available\n",
    "    market_cols = ['market_min', 'market_25', 'market_50', 'market_75', 'market_max']\n",
    "    markets = [row.get(col) for col in market_cols]\n",
    "    valid_markets = [m for m in markets if pd.notna(m) and m is not None]\n",
    "    \n",
    "    if len(valid_markets) >= 2:\n",
    "        steps = [abs(valid_markets[i+1] - valid_markets[i]) for i in range(len(valid_markets)-1)]\n",
    "        return np.mean(steps) if steps else 0.01\n",
    "    \n",
    "    return 0.01 # Default 1% step\n",
    "\n",
    "def calculate_induced_price(row, current_price):\n",
    "    \"\"\"\n",
    "    Calculate induced price by reducing margin by one step.\n",
    "    Used for Zero Demand and High DOH scenarios.\n",
    "    \n",
    "    Returns:\n",
    "        Induced price if valid and lower than current, else None\n",
    "    \"\"\"\n",
    "    wac_p = float(row.get('wac_p', 0) or 0)\n",
    "    if wac_p <= 0 or current_price <= 0:\n",
    "        return None\n",
    "    \n",
    "    current_margin = (current_price - wac_p) / current_price\n",
    "    margin_step = calculate_margin_step(row)\n",
    "    new_margin = current_margin - margin_step\n",
    "    \n",
    "    if new_margin >= 1:\n",
    "        return None\n",
    "    \n",
    "    induced_price = wac_p / (1 - new_margin)\n",
    "    induced_price = round(induced_price * 4) / 4  # Round to 0.25\n",
    "    \n",
    "    # Apply floors: min_induced_price and commercial_min_price\n",
    "    min_induced = float(row.get('min_induced_price', 0) or 0)\n",
    "    commercial_min = float(row.get('commercial_min_price', 0) or 0)\n",
    "    floor_price = max(min_induced, commercial_min) if commercial_min > 0 else min_induced\n",
    "    \n",
    "    if induced_price < floor_price:\n",
    "        return None  # Can't reduce further\n",
    "    \n",
    "    return induced_price if induced_price < current_price else None\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN ENGINE: GENERATE PERIODIC ACTION\n",
    "# =============================================================================\n",
    "\n",
    "def generate_periodic_action(row, previous_df):\n",
    "    \"\"\"\n",
    "    Generate periodic action based on UTH performance.\n",
    "    \n",
    "    Logic:\n",
    "    - Zero Demand: 1 step below current + SKU discount\n",
    "    - On Track: No action\n",
    "    - Growing: Deactivate discounts or increase price, reduce cart if too open\n",
    "    - Dropping: Based on qty_ratio vs retailer_ratio:\n",
    "        - qty OK, retailers dropping: SKU discount (then price if already has)\n",
    "        - qty dropping, retailers OK: QD (then price if already has)\n",
    "        - both dropping: SKU discount (then price if already has)\n",
    "    - Price reduction max 2x per day\n",
    "    \"\"\"\n",
    "    product_id = row.get('product_id')\n",
    "    warehouse_id = row.get('warehouse_id')\n",
    "    \n",
    "    result = {\n",
    "        'product_id': product_id,\n",
    "        'warehouse_id': warehouse_id,\n",
    "        'cohort_id': row.get('cohort_id'),\n",
    "        'sku': row.get('sku'),\n",
    "        'brand': row.get('brand'),\n",
    "        'cat': row.get('cat'),\n",
    "        'stocks': row.get('stocks', 0),\n",
    "        'current_price': row.get('current_price'),\n",
    "        'wac_p': row.get('wac_p'),\n",
    "        'uth_qty': row.get('uth_qty', 0),\n",
    "        'uth_retailers': row.get('uth_retailers', 0),\n",
    "        'p80_daily_240d': row.get('p80_daily_240d', 1),\n",
    "        'p70_daily_retailers_240d': row.get('p70_daily_retailers_240d', 1),\n",
    "        'avg_uth_pct': row.get('avg_uth_pct', 0.5),\n",
    "        # Today's UTH discount contributions\n",
    "        'sku_disc_cntrb_uth': row.get('sku_disc_cntrb_uth', 0) or 0,\n",
    "        't1_cntrb_uth': row.get('t1_cntrb_uth', 0) or 0,\n",
    "        't2_cntrb_uth': row.get('t2_cntrb_uth', 0) or 0,\n",
    "        't3_cntrb_uth': row.get('t3_cntrb_uth', 0) or 0,\n",
    "        'uth_status': None,\n",
    "        'qty_ratio': None,\n",
    "        'retailer_ratio': None,\n",
    "        'new_price': None,\n",
    "        'price_action': None,\n",
    "        'current_cart_rule':row.get('current_cart_rule'),\n",
    "        'new_cart_rule': None,\n",
    "        'activate_sku_discount': False,  # True = SKU should have discount after this run\n",
    "        'activate_qd': False,             # True = SKU should have QD after this run\n",
    "        'keep_qd_tiers': None,            # List of QD tiers to keep (e.g., ['T1', 'T2'])\n",
    "        # QD tier configuration (passed to qd_handler)\n",
    "        'qd_tier_1_qty': row.get('qd_tier_1_qty', 0) or 0,\n",
    "        'qd_tier_1_disc_pct': row.get('qd_tier_1_disc_pct', 0) or 0,\n",
    "        'qd_tier_2_qty': row.get('qd_tier_2_qty', 0) or 0,\n",
    "        'qd_tier_2_disc_pct': row.get('qd_tier_2_disc_pct', 0) or 0,\n",
    "        'qd_tier_3_qty': row.get('qd_tier_3_qty', 0) or 0,\n",
    "        'qd_tier_3_disc_pct': row.get('qd_tier_3_disc_pct', 0) or 0,\n",
    "        'removed_discount': None,         # Which discount was removed (for Growing)\n",
    "        'removed_discount_cntrb': 0,      # Contribution of removed discount\n",
    "        'price_reductions_today': row.get('reduced_count', 0) or 0,\n",
    "        'action_reason': None,\n",
    "        # =====================================================================\n",
    "        # ADDITIONAL COLUMNS FOR QD AND SKU DISCOUNT HANDLERS\n",
    "        # =====================================================================\n",
    "        # Pricing and margin data\n",
    "        'target_margin': row.get('target_margin'),\n",
    "        'min_boundary': row.get('min_boundary'),\n",
    "        'doh': row.get('doh', 0),  # Days on hand - for SKU discount handler\n",
    "        'mtd_qty': row.get('mtd_qty', 0),  # MTD quantity - for QD ranking\n",
    "        # Active SKU discount info - for SKU discount handler\n",
    "        'active_sku_disc_pct': row.get('active_sku_disc_pct', 0),\n",
    "        'has_active_sku_discount': row.get('has_active_sku_discount', 0),\n",
    "        'has_active_qd': row.get('has_active_qd', 0),\n",
    "        # Market margins (converted to prices in handlers)\n",
    "        'below_market': row.get('below_market'),\n",
    "        'market_min': row.get('market_min'),\n",
    "        'market_25': row.get('market_25'),\n",
    "        'market_50': row.get('market_50'),\n",
    "        'market_75': row.get('market_75'),\n",
    "        'market_max': row.get('market_max'),\n",
    "        'above_market': row.get('above_market'),\n",
    "        # Margin tiers (converted to prices in handlers)\n",
    "        'margin_tier_below': row.get('margin_tier_below'),\n",
    "        'margin_tier_1': row.get('margin_tier_1'),\n",
    "        'margin_tier_2': row.get('margin_tier_2'),\n",
    "        'margin_tier_3': row.get('margin_tier_3'),\n",
    "        'margin_tier_4': row.get('margin_tier_4'),\n",
    "        'margin_tier_5': row.get('margin_tier_5'),\n",
    "        'margin_tier_above_1': row.get('margin_tier_above_1'),\n",
    "        'margin_tier_above_2': row.get('margin_tier_above_2'),\n",
    "    }\n",
    "    \n",
    "    # Skip if OOS (price only in Module 2)\n",
    "    if row.get('stocks', 0) <= 0:\n",
    "        result['action_reason'] = 'OOS - skip (price only in Module 2)'\n",
    "        return result\n",
    "    \n",
    "    # Skip if below minimum stock (stock < min selling unit qty)\n",
    "    if row.get('below_min_stock_flag', 0) == 1:\n",
    "        result['action_reason'] = 'Below min stock - skip (cannot sell)'\n",
    "        return result\n",
    "    \n",
    "    # Count previous price reductions today\n",
    "    price_reductions_today = row.get('reduced_count', 0)\n",
    "    can_reduce_price = price_reductions_today < MAX_PRICE_REDUCTIONS_PER_DAY\n",
    "\n",
    "    # Count previous price increase today\n",
    "    price_increase_today = row.get('increase_count', 0)\n",
    "    can_increase_price = price_increase_today < MAX_PRICE_REDUCTIONS_PER_DAY    \n",
    "    \n",
    "    # Calculate UTH benchmark: historical_pct * P80_qty\n",
    "    # Convert to float to handle decimal.Decimal from Snowflake\n",
    "    p80_qty = float(row.get('p80_daily_240d', 1) or 1)\n",
    "    p70_retailers = float(row.get('p70_daily_retailers_240d', 1) or 1)\n",
    "    avg_uth_pct = float(row.get('avg_uth_pct', 0.5) or 0.5)\n",
    "    \n",
    "    uth_qty_target = p80_qty * avg_uth_pct\n",
    "    uth_retailer_target = p70_retailers * avg_uth_pct\n",
    "    \n",
    "    uth_qty = float(row.get('uth_qty', 0) or 0)\n",
    "    uth_retailers = float(row.get('uth_retailers', 0) or 0)\n",
    "    \n",
    "    # Calculate UTH ratios\n",
    "    qty_ratio = uth_qty / uth_qty_target if uth_qty_target > 0 else 0\n",
    "    retailer_ratio = uth_retailers / uth_retailer_target if uth_retailer_target > 0 else 0\n",
    "    \n",
    "    result['uth_qty_target'] = round(uth_qty_target, 2)\n",
    "    result['uth_retailer_target'] = round(uth_retailer_target, 2)\n",
    "    result['qty_ratio'] = round(qty_ratio, 2)\n",
    "    result['retailer_ratio'] = round(retailer_ratio, 2)\n",
    "    \n",
    "    current_price = float(row.get('current_price') or 0)\n",
    "    current_cart = float(row.get('current_cart_rule', row.get('normal_refill', 10)) or 10)\n",
    "    has_sku_disc = row.get('has_active_sku_discount', 0) == 1\n",
    "    has_qd = row.get('has_active_qd', 0) == 1\n",
    "    \n",
    "    # Determine if qty/retailers are dropping (below threshold)\n",
    "    qty_dropping = qty_ratio < UTH_DROPPING_THRESHOLD\n",
    "    qty_ok = qty_ratio >= UTH_DROPPING_THRESHOLD\n",
    "    retailer_dropping = retailer_ratio < UTH_DROPPING_THRESHOLD\n",
    "    retailer_ok = retailer_ratio >= UTH_DROPPING_THRESHOLD\n",
    "    \n",
    "    # =========================================================================\n",
    "    # CASE 1: Zero demand today\n",
    "    # - If NO existing SKU discount: Add SKU discount ONLY (wait for next day)\n",
    "    # - If HAS existing SKU discount: Keep discount + INDUCED price reduction\n",
    "    # - Open cart if tight (both cases)\n",
    "    # =========================================================================\n",
    "    if uth_qty == 0:\n",
    "        result['uth_status'] = 'Zero Demand'\n",
    "        result['activate_sku_discount'] = True\n",
    "        \n",
    "        # Check if cart rule is tight (< normal_refill + 10*std) and increase if so\n",
    "        normal_refill = float(row.get('normal_refill', 5) or 5)\n",
    "        stddev = float(row.get('refill_stddev', 2) or 2)\n",
    "        cart_threshold = normal_refill + (10 * stddev)\n",
    "        \n",
    "        if current_cart < cart_threshold:\n",
    "            new_cart = min(cart_threshold, MAX_CART_RULE)\n",
    "            new_cart = max(new_cart, MIN_CART_RULE)\n",
    "            result['new_cart_rule'] = int(new_cart)\n",
    "            cart_action = f' + open cart to {int(new_cart)}'\n",
    "        else:\n",
    "            cart_action = ''\n",
    "        \n",
    "        if not has_sku_disc:\n",
    "            # First occurrence: Add SKU discount only - wait for next day\n",
    "            result['price_action'] = 'add_sku_disc'\n",
    "            result['action_reason'] = f'Zero demand - ADD SKU discount (wait for next day){cart_action}'\n",
    "        else:\n",
    "            # Second occurrence: Already has SKU discount but still 0 demand - reduce price\n",
    "            induced_price = calculate_induced_price(row, current_price)\n",
    "            if induced_price:\n",
    "                result['new_price'] = induced_price\n",
    "                result['price_action'] = 'zero_demand_induced_price'\n",
    "                result['action_reason'] = f'Zero demand + existing discount - INDUCED price ({current_price:.2f} -> {induced_price:.2f}){cart_action}'\n",
    "            else:\n",
    "                result['price_action'] = 'keep_sku_disc'\n",
    "                result['action_reason'] = f'Zero demand + existing discount - no lower price available{cart_action}'\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    # =========================================================================\n",
    "    # CASE 1.5: HIGH DOH (responsive_doh > 30) - Two-step approach\n",
    "    # - If NO existing SKU discount: Add SKU discount ONLY (wait for next day)\n",
    "    # - If HAS existing SKU discount and qty_ratio >= 0.9 (\"grew\"): Keep discount only\n",
    "    # - If HAS existing SKU discount and qty_ratio < 0.9 (\"didn't grow\"): Keep discount + induced price\n",
    "    # Only applies if inventory value (stocks * price) > 10,000 EGP\n",
    "    # Skip SKUs that were out of stock yesterday (oos_yesterday = 1)\n",
    "    # =========================================================================\n",
    "    DOH_HIGH_TURNOVER_THRESHOLD = 30\n",
    "    HIGH_INVENTORY_VALUE_THRESHOLD = 10000\n",
    "    responsive_doh = float(row.get('responsive_doh', 999) or 999)\n",
    "    stocks = float(row.get('stocks', 0) or 0)\n",
    "    inventory_value = stocks * current_price\n",
    "    oos_yesterday = int(row.get('oos_yesterday', 0) or 0)\n",
    "    \n",
    "    if responsive_doh > DOH_HIGH_TURNOVER_THRESHOLD and inventory_value > HIGH_INVENTORY_VALUE_THRESHOLD and oos_yesterday != 1:\n",
    "        result['uth_status'] = 'High DOH'\n",
    "        result['activate_sku_discount'] = True\n",
    "        result['activate_qd'] = True  # Add QD for bulk purchase incentive to move inventory faster\n",
    "        \n",
    "        if not has_sku_disc:\n",
    "            # First occurrence: Add SKU discount only - wait for next day\n",
    "            result['price_action'] = 'add_sku_disc_doh'\n",
    "            result['action_reason'] = f'High DOH ({responsive_doh:.1f} days) - ADD SKU discount (wait for next day)'\n",
    "            return result\n",
    "        \n",
    "        else:\n",
    "            # Has existing SKU discount - check if \"grew\" (qty_ratio >= 0.9)\n",
    "            if qty_ratio >= 0.9:\n",
    "                # SKU \"grew\" - keep discount but don't reduce price\n",
    "                result['price_action'] = 'keep_sku_disc'\n",
    "                result['action_reason'] = f'High DOH ({responsive_doh:.1f} days) + grew (qty={qty_ratio:.2f}) - KEEP SKU discount only'\n",
    "                return result\n",
    "            else:\n",
    "                # SKU \"didn't grow\" - keep discount + reduce price with induced logic\n",
    "                if can_reduce_price:\n",
    "                    induced_price = calculate_induced_price(row, current_price)\n",
    "                    if induced_price:\n",
    "                        result['new_price'] = induced_price\n",
    "                        result['price_action'] = 'induced_doh_reduction'\n",
    "                        result['action_reason'] = f'High DOH ({responsive_doh:.1f} days) + didn\\'t grow (qty={qty_ratio:.2f}) - INDUCED price ({current_price:.2f} -> {induced_price:.2f})'\n",
    "                        return result\n",
    "                    else:\n",
    "                        result['price_action'] = 'keep_sku_disc'\n",
    "                        result['action_reason'] = f'High DOH ({responsive_doh:.1f} days) - no lower price available'\n",
    "                        return result\n",
    "                else:\n",
    "                    result['price_action'] = 'keep_sku_disc'\n",
    "                    result['action_reason'] = f'High DOH ({responsive_doh:.1f} days) - price reduction limit reached'\n",
    "                    return result\n",
    "    \n",
    "    # =========================================================================\n",
    "    # CASE 1.6: LOW STOCK PROTECTION (DOH <= 2 with demand)\n",
    "    # Protect inventory until next receiving - no price reduction, cap cart at normal_refill\n",
    "    # But still allow price INCREASE if growing\n",
    "    # =========================================================================\n",
    "    normal_refill = float(row.get('normal_refill', 5) or 5)\n",
    "    is_low_stock = responsive_doh <= LOW_STOCK_DOH_THRESHOLD and uth_qty > 0\n",
    "    \n",
    "    if is_low_stock:\n",
    "        result['uth_status'] = 'Low Stock Protected'\n",
    "        result['price_action'] = 'hold_low_stock'\n",
    "        \n",
    "        # Cap cart rule at normal_refill (don't open cart wide for low stock)\n",
    "        if current_cart > normal_refill:\n",
    "            result['new_cart_rule'] = max(int(normal_refill),5)\n",
    "            result['action_reason'] = f'Low stock (DOH={responsive_doh:.1f}) - hold price, cap cart to {int(normal_refill)}'\n",
    "        else:\n",
    "            result['action_reason'] = f'Low stock (DOH={responsive_doh:.1f}) - hold price'\n",
    "        \n",
    "        # Still allow price INCREASE if growing\n",
    "        if qty_ratio > UTH_GROWING_THRESHOLD and can_increase_price:\n",
    "            new_price = find_next_price_above(current_price, row)\n",
    "            if pd.notna(new_price) and new_price > current_price:\n",
    "                result['new_price'] = new_price\n",
    "                result['price_action'] = 'low_stock_increase'\n",
    "                result['action_reason'] += f' + increase price ({current_price:.2f} -> {new_price:.2f})'\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    # =========================================================================\n",
    "    # CASE 2: On Track (both qty and retailers ±10%)\n",
    "    # If has existing discounts, keep them (they'll be deactivated otherwise)\n",
    "    # =========================================================================\n",
    "    if (UTH_DROPPING_THRESHOLD <= qty_ratio <= UTH_GROWING_THRESHOLD and\n",
    "        UTH_DROPPING_THRESHOLD <= retailer_ratio <= UTH_GROWING_THRESHOLD):\n",
    "        result['uth_status'] = 'On Track'\n",
    "        result['price_action'] = 'hold'\n",
    "        \n",
    "        # Preserve existing discounts (all discounts are deactivated at start of each run)\n",
    "        if has_sku_disc:\n",
    "            result['activate_sku_discount'] = True\n",
    "            result['action_reason'] = f'On Track (qty={qty_ratio:.2f}, ret={retailer_ratio:.2f}) - keep existing SKU discount'\n",
    "        elif has_qd:\n",
    "            result['activate_qd'] = True\n",
    "            result['action_reason'] = f'On Track (qty={qty_ratio:.2f}, ret={retailer_ratio:.2f}) - keep existing QD'\n",
    "        else:\n",
    "            result['action_reason'] = f'On Track (qty={qty_ratio:.2f}, ret={retailer_ratio:.2f}) - no action'\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    # =========================================================================\n",
    "    # CASE 2.5: Retailers Growing but Qty On Track\n",
    "    # Action: Increase price 1 step (high retailer demand, normal qty = opportunity)\n",
    "    # =========================================================================\n",
    "    if (UTH_DROPPING_THRESHOLD <= qty_ratio <= UTH_GROWING_THRESHOLD and\n",
    "        retailer_ratio > UTH_GROWING_THRESHOLD):\n",
    "        result['uth_status'] = 'Retailers Growing'\n",
    "        if can_increase_price:\n",
    "            new_price = find_next_price_above(current_price, row)\n",
    "        else:\n",
    "            new_price = np.nan\n",
    "        if new_price > current_price:\n",
    "            result['new_price'] = new_price\n",
    "            result['price_action'] = 'retailers_growing_increase'\n",
    "            result['action_reason'] = f'Retailers growing (qty={qty_ratio:.2f}, ret={retailer_ratio:.2f}) - increase price ({current_price:.2f} -> {new_price:.2f})'\n",
    "        else:\n",
    "            result['price_action'] = 'hold'\n",
    "            result['action_reason'] = f'Retailers growing (qty={qty_ratio:.2f}, ret={retailer_ratio:.2f}) - no tier above, hold'\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    # =========================================================================\n",
    "    # CASE 3: Growing (qty > 110%)\n",
    "    # Find discount with HIGHEST contribution (from TODAY's UTH) and remove it\n",
    "    # Keep (re-activate) the others\n",
    "    # If no discounts -> increase price\n",
    "    # =========================================================================\n",
    "    if qty_ratio > UTH_GROWING_THRESHOLD:\n",
    "        result['uth_status'] = 'Growing'\n",
    "        \n",
    "        # Get TODAY's UTH discount contributions (not yesterday's)\n",
    "        sku_disc_cntrb = row.get('sku_disc_cntrb_uth', 0) or 0\n",
    "        t1_cntrb = row.get('t1_cntrb_uth', 0) or 0\n",
    "        t2_cntrb = row.get('t2_cntrb_uth', 0) or 0\n",
    "        t3_cntrb = row.get('t3_cntrb_uth', 0) or 0\n",
    "        \n",
    "        # Build list of EXISTING discounts with their contributions\n",
    "        # Note: We check if tiers EXIST (qty > 0), not just if they had sales today\n",
    "        # A tier can exist but have 0 contribution if no orders used it yet today\n",
    "        active_discounts = []\n",
    "        \n",
    "        # SKU discount: check if it exists (has_sku_disc from active discount query)\n",
    "        if has_sku_disc:\n",
    "            active_discounts.append(('sku_disc', sku_disc_cntrb))  # Include even if cntrb=0\n",
    "        \n",
    "        # QD tiers: check if each tier EXISTS (qty > 0 means the tier is configured)\n",
    "        if has_qd:\n",
    "            qd_t1_qty = row.get('qd_tier_1_qty', 0) or 0\n",
    "            qd_t2_qty = row.get('qd_tier_2_qty', 0) or 0\n",
    "            qd_t3_qty = row.get('qd_tier_3_qty', 0) or 0\n",
    "            \n",
    "            if qd_t1_qty > 0:  # Tier 1 exists\n",
    "                active_discounts.append(('qd_t1', t1_cntrb))  # Include even if cntrb=0\n",
    "            if qd_t2_qty > 0:  # Tier 2 exists\n",
    "                active_discounts.append(('qd_t2', t2_cntrb))  # Include even if cntrb=0\n",
    "            if qd_t3_qty > 0:  # Tier 3 exists\n",
    "                active_discounts.append(('qd_t3', t3_cntrb))  # Include even if cntrb=0\n",
    "        \n",
    "        if active_discounts:\n",
    "            # Sort by contribution descending - remove the highest\n",
    "            active_discounts.sort(key=lambda x: x[1], reverse=True)\n",
    "            highest_disc, highest_cntrb = active_discounts[0]\n",
    "            remaining_discounts = [d[0] for d in active_discounts[1:]]\n",
    "            \n",
    "            # Determine what to keep (re-activate)\n",
    "            keep_sku_disc = 'sku_disc' in remaining_discounts\n",
    "            keep_qd_t1 = 'qd_t1' in remaining_discounts\n",
    "            keep_qd_t2 = 'qd_t2' in remaining_discounts\n",
    "            keep_qd_t3 = 'qd_t3' in remaining_discounts\n",
    "            keep_any_qd = keep_qd_t1 or keep_qd_t2 or keep_qd_t3\n",
    "            \n",
    "            # Set activation flags\n",
    "            if keep_sku_disc:\n",
    "                result['activate_sku_discount'] = True\n",
    "            \n",
    "            if keep_any_qd:\n",
    "                result['activate_qd'] = True\n",
    "                result['keep_qd_tiers'] = [t for t in ['T1', 'T2', 'T3'] \n",
    "                                           if (t == 'T1' and keep_qd_t1) or \n",
    "                                              (t == 'T2' and keep_qd_t2) or \n",
    "                                              (t == 'T3' and keep_qd_t3)]\n",
    "            \n",
    "            result['removed_discount'] = highest_disc\n",
    "            result['removed_discount_cntrb'] = highest_cntrb\n",
    "            result['price_action'] = f'remove_{highest_disc}'\n",
    "            result['action_reason'] = f'Growing (qty={qty_ratio:.2f}) - remove {highest_disc} (cntrb={highest_cntrb}%)'\n",
    "            \n",
    "            if remaining_discounts:\n",
    "                result['action_reason'] += f', keep {remaining_discounts}'\n",
    "        \n",
    "        elif has_sku_disc or has_qd:\n",
    "            # Has discounts but no contribution data - remove all\n",
    "            result['price_action'] = 'remove_all_disc'\n",
    "            result['action_reason'] = f'Growing (qty={qty_ratio:.2f}) - remove all discounts (no contribution data)'\n",
    "        \n",
    "        else:\n",
    "            # No discounts\n",
    "            result['price_action'] = 'no_discount_growing'\n",
    "            result['action_reason'] = f'Growing (qty={qty_ratio:.2f}) - no discounts'\n",
    "        \n",
    "        # ALWAYS increase price 1 step (regardless of discounts)\n",
    "        if can_increase_price:\n",
    "            new_price = find_next_price_above(current_price, row)\n",
    "            if pd.notna(new_price) and new_price > current_price:\n",
    "                result['new_price'] = new_price\n",
    "                result['action_reason'] += f' + increase price ({current_price:.2f} -> {new_price:.2f})'\n",
    "            else:\n",
    "                result['action_reason'] += ' + no tier above for price increase'\n",
    "        else:\n",
    "            result['action_reason'] += ' + price increase limit reached'\n",
    "        \n",
    "        # ALWAYS reduce cart rule (not just when too open)\n",
    "        result['new_cart_rule'] = adjust_cart_rule(current_cart, 'decrease', row)\n",
    "        result['action_reason'] += ' + reduce cart'\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    # =========================================================================\n",
    "    # CASE 4: Dropping - Different actions based on qty vs retailer ratios\n",
    "    # =========================================================================\n",
    "    result['uth_status'] = 'Dropping'\n",
    "    \n",
    "    def apply_price_reduction():\n",
    "        \"\"\"Helper to apply price reduction if allowed.\"\"\"\n",
    "        if not can_reduce_price:\n",
    "            return None, f'Price reduction limit reached ({price_reductions_today}/{MAX_PRICE_REDUCTIONS_PER_DAY} today)'\n",
    "        \n",
    "        new_price = find_next_price_below(current_price, row)\n",
    "        if new_price < current_price:\n",
    "            commercial_min = float(row.get('commercial_min_price', row.get('minimum', 0)) or 0)\n",
    "            if pd.notna(commercial_min) and commercial_min > 0:\n",
    "                new_price = max(new_price, commercial_min)\n",
    "            return new_price, f'decrease ({current_price:.2f} -> {new_price:.2f})'\n",
    "        return None, 'no tier below'\n",
    "    \n",
    "    # CASE 4A: qty OK (≥90%) but retailers dropping (<90%)\n",
    "    # Action: SKU discount (add new OR keep existing), then price if already has\n",
    "    if qty_ok and retailer_dropping:\n",
    "        # Always set activate_sku_discount = True (either adding new or keeping existing)\n",
    "        result['activate_sku_discount'] = True\n",
    "        \n",
    "        if not has_sku_disc:\n",
    "            # Adding new SKU discount\n",
    "            result['price_action'] = 'add_sku_disc'\n",
    "            result['action_reason'] = f'Retailers dropping (ret={retailer_ratio:.2f}, qty OK) - ADD new SKU discount'\n",
    "        else:\n",
    "            # Keeping existing SKU discount + reduce price\n",
    "            new_price, reason = apply_price_reduction()\n",
    "            if new_price:\n",
    "                #result['new_price'] = new_price\n",
    "                result['price_action'] = 'keep_sku_disc_and_decrease'\n",
    "                result['action_reason'] = f'Retailers dropping - KEEP SKU disc + {reason}'\n",
    "            else:\n",
    "                result['price_action'] = 'keep_sku_disc'\n",
    "                result['action_reason'] = f'Retailers dropping - KEEP SKU disc ({reason})'\n",
    "    \n",
    "    # CASE 4B: qty dropping (<90%) but retailers OK (≥90%)\n",
    "    # Action: QD (add new OR keep existing), then price if already has\n",
    "    elif qty_dropping and retailer_ok:\n",
    "        # Always set activate_qd = True (either adding new or keeping existing)\n",
    "        result['activate_qd'] = True\n",
    "        \n",
    "        if not has_qd:\n",
    "            # Adding new QD\n",
    "            result['price_action'] = 'add_qd'\n",
    "            result['action_reason'] = f'Qty dropping (qty={qty_ratio:.2f}, ret OK) - ADD new QD'\n",
    "        else:\n",
    "            # Keeping existing QD + reduce price\n",
    "            new_price, reason = apply_price_reduction()\n",
    "            if new_price:\n",
    "                result['new_price'] = new_price\n",
    "                result['price_action'] = 'keep_qd_and_decrease'\n",
    "                result['action_reason'] = f'Qty dropping - KEEP QD + {reason}'\n",
    "            else:\n",
    "                result['price_action'] = 'keep_qd'\n",
    "                result['action_reason'] = f'Qty dropping - KEEP QD ({reason})'\n",
    "    \n",
    "    # CASE 4C: Both dropping (<90%)\n",
    "    # Action: SKU discount (add new OR keep existing), then price if already has\n",
    "    elif qty_dropping and retailer_dropping:\n",
    "        # Always set activate_sku_discount = True (either adding new or keeping existing)\n",
    "        result['activate_sku_discount'] = True\n",
    "        \n",
    "        if not has_sku_disc:\n",
    "            # Adding new SKU discount\n",
    "            result['price_action'] = 'add_sku_disc'\n",
    "            result['action_reason'] = f'Both dropping (qty={qty_ratio:.2f}, ret={retailer_ratio:.2f}) - ADD new SKU discount'\n",
    "        else:\n",
    "            # Keeping existing SKU discount + reduce price\n",
    "            new_price, reason = apply_price_reduction()\n",
    "            if new_price:\n",
    "                result['new_price'] = new_price\n",
    "                result['price_action'] = 'keep_sku_disc_and_decrease'\n",
    "                result['action_reason'] = f'Both dropping - KEEP SKU disc + {reason}'\n",
    "            else:\n",
    "                result['price_action'] = 'keep_sku_disc'\n",
    "                result['action_reason'] = f'Both dropping - KEEP SKU disc ({reason})'\n",
    "    \n",
    "    else:\n",
    "        result['price_action'] = 'hold'\n",
    "        result['action_reason'] = f'Unexpected state (qty={qty_ratio:.2f}, ret={retailer_ratio:.2f})'\n",
    "    \n",
    "    # Increase cart for dropping SKUs\n",
    "    result['new_cart_rule'] = adjust_cart_rule(current_cart, 'increase', row)\n",
    "    result['action_reason'] += ' + increase cart 20%'\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"Main engine function loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1396/2181086998.py:3: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['increase_count'] = df['increase_count'].fillna(0)\n",
      "/tmp/ipykernel_1396/2181086998.py:4: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['reduced_count'] = df['reduced_count'].fillna(0)\n"
     ]
    }
   ],
   "source": [
    "df = df.merge(prev_inc,on=['product_id','warehouse_id'],how='left')\n",
    "df = df.merge(prev_red,on=['product_id','warehouse_id'],how='left')\n",
    "df['increase_count'] = df['increase_count'].fillna(0)\n",
    "df['reduced_count'] = df['reduced_count'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 28124 SKUs...\n",
      "============================================================\n",
      "Processed 10000/28124 SKUs...\n",
      "Processed 20000/28124 SKUs...\n",
      "\n",
      "✅ Processed 28124 SKUs\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXECUTE MODULE 3\n",
    "# =============================================================================\n",
    "print(f\"Processing {len(df)} SKUs...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = []\n",
    "for idx, row in df.iterrows():\n",
    "    result = generate_periodic_action(row, df_previous_actions)\n",
    "    results.append(result)\n",
    "    \n",
    "    if (idx + 1) % 10000 == 0:\n",
    "        print(f\"Processed {idx + 1}/{len(df)} SKUs...\")\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(f\"\\n✅ Processed {len(df_results)} SKUs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = df_results.drop_duplicates(subset=['product_id', 'warehouse_id'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODULE 3 SUMMARY\n",
      "============================================================\n",
      "\n",
      "Total SKUs: 28116\n",
      "\n",
      "By UTH Status:\n",
      "uth_status\n",
      "Zero Demand            13327\n",
      "None                    9037\n",
      "Dropping                2568\n",
      "Growing                 1512\n",
      "Low Stock Protected     1163\n",
      "High DOH                 262\n",
      "Retailers Growing        127\n",
      "On Track                 120\n",
      "\n",
      "Actions:\n",
      "  Price changes: 4182\n",
      "  Cart rule changes: 17580\n",
      "  SKU discounts to activate: 15681\n",
      "  QD to activate: 849\n",
      "  Discounts removed (Growing SKUs): 724\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(\"=\"*60)\n",
    "print(\"MODULE 3 SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nTotal SKUs: {len(df_results)}\")\n",
    "\n",
    "print(f\"\\nBy UTH Status:\")\n",
    "print(df_results['uth_status'].value_counts(dropna=False).to_string())\n",
    "\n",
    "# Actions breakdown\n",
    "price_changes = df_results[df_results['new_price'].notna()]\n",
    "cart_changes = df_results[df_results['new_cart_rule'].notna()]\n",
    "sku_disc_activate = df_results[df_results['activate_sku_discount'] == True]\n",
    "qd_activate = df_results[df_results['activate_qd'] == True]\n",
    "discounts_removed = df_results[df_results['removed_discount'].notna()]\n",
    "\n",
    "print(f\"\\nActions:\")\n",
    "print(f\"  Price changes: {len(price_changes)}\")\n",
    "print(f\"  Cart rule changes: {len(cart_changes)}\")\n",
    "print(f\"  SKU discounts to activate: {len(sku_disc_activate)}\")\n",
    "print(f\"  QD to activate: {len(qd_activate)}\")\n",
    "print(f\"  Discounts removed (Growing SKUs): {len(discounts_removed)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Saved 28116 rows for Slack upload\n",
      "Total records: 28116 (after removing 0 duplicates)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXPORT RESULTS\n",
    "# =============================================================================\n",
    "output_cols = [\n",
    "    # Identifiers\n",
    "    'product_id', 'warehouse_id', 'cohort_id', 'sku', 'brand', 'cat', 'stocks',\n",
    "    # Pricing data\n",
    "    'current_price', 'wac_p', 'new_price',\n",
    "    'target_margin', 'min_boundary',\n",
    "    # Performance data\n",
    "    'uth_qty', 'uth_retailers',\n",
    "    'p80_daily_240d', 'p70_daily_retailers_240d', 'avg_uth_pct',\n",
    "    'sku_disc_cntrb_uth', 't1_cntrb_uth', 't2_cntrb_uth', 't3_cntrb_uth',\n",
    "    'uth_qty_target', 'uth_retailer_target', 'qty_ratio', 'retailer_ratio', 'uth_status',\n",
    "    'doh', 'mtd_qty',\n",
    "    # Cart rules\n",
    "    'price_action', 'current_cart_rule', 'new_cart_rule',\n",
    "    # SKU Discount fields\n",
    "    'activate_sku_discount', 'active_sku_disc_pct', 'has_active_sku_discount',\n",
    "    # QD fields (for qd_handler)\n",
    "    'activate_qd', 'keep_qd_tiers', 'has_active_qd',\n",
    "    'qd_tier_1_qty', 'qd_tier_1_disc_pct',\n",
    "    'qd_tier_2_qty', 'qd_tier_2_disc_pct',\n",
    "    'qd_tier_3_qty', 'qd_tier_3_disc_pct',\n",
    "    # Market margins (for handlers to convert to prices)\n",
    "    'below_market', 'market_min', 'market_25', 'market_50',\n",
    "    'market_75', 'market_max', 'above_market',\n",
    "    # Margin tiers (for handlers to convert to prices)\n",
    "    'margin_tier_below', 'margin_tier_1', 'margin_tier_2', 'margin_tier_3', 'margin_tier_4',\n",
    "    'margin_tier_5', 'margin_tier_above_1', 'margin_tier_above_2',\n",
    "    # Action tracking\n",
    "    'removed_discount', 'removed_discount_cntrb',\n",
    "    'price_reductions_today', 'action_reason'\n",
    "]\n",
    "\n",
    "# Filter to existing columns\n",
    "output_cols = [c for c in output_cols if c in df_results.columns]\n",
    "\n",
    "# Drop duplicates before saving\n",
    "df_output = df_results[output_cols].drop_duplicates(subset=['product_id', 'warehouse_id'], keep='first')\n",
    "# Save df_output state before any manipulation for Slack upload later\n",
    "temp_df_for_slack = df_output.copy()\n",
    "print(f\"\\n✅ Saved {len(temp_df_for_slack)} rows for Slack upload\")\n",
    "print(f\"Total records: {len(df_output)} (after removing {len(df_results) - len(df_output)} duplicates)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Push Cart Rules Handler loaded at 2026-02-10 12:22:40 Cairo time\n",
      "✓ API credentials loaded successfully\n",
      "Push Prices Handler loaded at 2026-02-10 12:22:40 Cairo time\n",
      "✓ API credentials loaded successfully\n",
      "✓ Google Sheets client initialized\n",
      "Fetching packing_units ...\n",
      "  Loaded 35126 records\n",
      "\n",
      "======================================================================\n",
      "STEP 1: PUSHING CART RULES\n",
      "======================================================================\n",
      "\n",
      "🚀 MODE: LIVE\n",
      "   Files will be prepared AND uploaded to API\n",
      "\n",
      "============================================================\n",
      "PUSH CART RULES - Source: module_3\n",
      "============================================================\n",
      "Total received: 28116\n",
      "Cart rule changes to push: 13891\n",
      "Skipped (no change): 14225\n",
      "\n",
      "Cart rule changes summary:\n",
      "  Increases: 12289\n",
      "  Decreases: 1602\n",
      "\n",
      "📋 Prepared 16746 packing unit cart rules\n",
      "\n",
      "Sample cart rule adjustments (showing products with multiple PUs):\n",
      " product_id  basic_unit_count  final_cart_rule  final_pu_cart_rule\n",
      "          3                 1               40                  40\n",
      "          3                 1                1                   2\n",
      "          3                 1               13                  13\n",
      "          3                 1                1                   2\n",
      "          3                 1               26                  26\n",
      "          3                 1               51                  51\n",
      "          3                 1               55                  55\n",
      "          3                 1               21                  21\n",
      "          9                 1                6                   6\n",
      "          9                 1               10                  10\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 700\n",
      "==================================================\n",
      "  Saved: uploads/module_3_cart_rules_700.xlsx (2037 rows)\n",
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Saving chunks: 100%|██████████| 1/1 [00:00<00:00, 16.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 702\n",
      "==================================================\n",
      "  Saved: uploads/module_3_cart_rules_702.xlsx (1505 rows)\n",
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Saving chunks: 100%|██████████| 1/1 [00:00<00:00, 21.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 703\n",
      "==================================================\n",
      "  Saved: uploads/module_3_cart_rules_703.xlsx (2729 rows)\n",
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Saving chunks: 100%|██████████| 1/1 [00:00<00:00,  4.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n",
      "    ✓ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 704\n",
      "==================================================\n",
      "  Saved: uploads/module_3_cart_rules_704.xlsx (2715 rows)\n",
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Saving chunks: 100%|██████████| 1/1 [00:00<00:00, 12.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1123\n",
      "==================================================\n",
      "  Saved: uploads/module_3_cart_rules_1123.xlsx (1093 rows)\n",
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Saving chunks: 100%|██████████| 1/1 [00:00<00:00, 27.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1124\n",
      "==================================================\n",
      "  Saved: uploads/module_3_cart_rules_1124.xlsx (1181 rows)\n",
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Saving chunks: 100%|██████████| 1/1 [00:00<00:00, 24.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1125\n",
      "==================================================\n",
      "  Saved: uploads/module_3_cart_rules_1125.xlsx (1032 rows)\n",
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Saving chunks: 100%|██████████| 1/1 [00:00<00:00, 29.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1126\n",
      "==================================================\n",
      "  Saved: uploads/module_3_cart_rules_1126.xlsx (1210 rows)\n",
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Saving chunks: 100%|██████████| 1/1 [00:00<00:00, 25.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 701\n",
      "==================================================\n",
      "  Saved: uploads/module_3_cart_rules_701.xlsx (3244 rows)\n",
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Saving chunks: 100%|██████████| 1/1 [00:00<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n",
      "    ✓ Chunk 1 uploaded successfully\n",
      "\n",
      "============================================================\n",
      "🚀 UPLOAD COMPLETE\n",
      "============================================================\n",
      "Mode: live\n",
      "Total prepared: 16746\n",
      "Total failed: 0\n",
      "\n",
      "============================================================\n",
      "CART RULES RESULT\n",
      "============================================================\n",
      "Mode: live\n",
      "Cart rule changes: 13891\n",
      "Pushed: 16746\n",
      "Failed: 0\n",
      "\n",
      "======================================================================\n",
      "STEP 2: PUSHING PRICES\n",
      "======================================================================\n",
      "\n",
      "🚀 MODE: LIVE\n",
      "   Files will be prepared AND uploaded to API\n",
      "Loading disable_pu_visibility from Google Sheets...\n",
      "  ✓ Loaded 88 products to disable min PU visibility\n",
      "\n",
      "============================================================\n",
      "PUSH PRICES - Source: module_3\n",
      "============================================================\n",
      "Total received: 28116\n",
      "Price changes to push: 4137\n",
      "Skipped (no change): 23979\n",
      "\n",
      "Price changes summary:\n",
      "  Increases: 1934\n",
      "  Decreases: 2203\n",
      "\n",
      "📋 Prepared 5246 packing unit prices\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 703\n",
      "==================================================\n",
      "  Saved: uploads/module_3_703.xlsx (928 rows)\n",
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Saving chunks: 100%|██████████| 1/1 [00:00<00:00, 15.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 704\n",
      "==================================================\n",
      "  Saved: uploads/module_3_704.xlsx (887 rows)\n",
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Saving chunks: 100%|██████████| 1/1 [00:00<00:00, 16.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 702\n",
      "==================================================\n",
      "  Saved: uploads/module_3_702.xlsx (338 rows)\n",
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Saving chunks: 100%|██████████| 1/1 [00:00<00:00, 39.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1124\n",
      "==================================================\n",
      "  Saved: uploads/module_3_1124.xlsx (248 rows)\n",
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Saving chunks: 100%|██████████| 1/1 [00:00<00:00, 50.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1123\n",
      "==================================================\n",
      "  Saved: uploads/module_3_1123.xlsx (283 rows)\n",
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Saving chunks: 100%|██████████| 1/1 [00:00<00:00, 44.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 700\n",
      "==================================================\n",
      "  Saved: uploads/module_3_700.xlsx (791 rows)\n",
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Saving chunks: 100%|██████████| 1/1 [00:00<00:00, 18.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 701\n",
      "==================================================\n",
      "  Saved: uploads/module_3_701.xlsx (1291 rows)\n",
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Saving chunks: 100%|██████████| 1/1 [00:00<00:00, 11.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1125\n",
      "==================================================\n",
      "  Saved: uploads/module_3_1125.xlsx (222 rows)\n",
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Saving chunks: 100%|██████████| 1/1 [00:00<00:00, 55.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1126\n",
      "==================================================\n",
      "  Saved: uploads/module_3_1126.xlsx (258 rows)\n",
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Saving chunks: 100%|██████████| 1/1 [00:00<00:00, 49.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Chunk 1 uploaded successfully\n",
      "\n",
      "============================================================\n",
      "🚀 UPLOAD COMPLETE\n",
      "============================================================\n",
      "Mode: live\n",
      "Total prepared: 5246\n",
      "Total failed: 0\n",
      "\n",
      "============================================================\n",
      "PRICES RESULT\n",
      "============================================================\n",
      "Mode: live\n",
      "Source: module_3\n",
      "Timestamp: 2026-02-10 12:23:30\n",
      "Total received: 28116\n",
      "Price changes: 4137\n",
      "Pushed: 5246\n",
      "Skipped: 23979\n",
      "Failed: 0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PUSH CART RULES & PRICES\n",
    "# =============================================================================\n",
    "# Push cart rules FIRST, then prices\n",
    "# If cart rules fail for certain cohorts, skip those cohorts for prices\n",
    "\n",
    "%run push_cart_rules_handler.ipynb\n",
    "%run push_prices_handler.ipynb\n",
    "pus = get_packing_units()\n",
    "\n",
    "# ⚠️ MODE CONFIGURATION:\n",
    "# - 'testing' (default): Prepare files but DON'T upload to API\n",
    "# - 'live': Prepare files AND upload to MaxAB API\n",
    "PUSH_MODE = 'live'  # Change to 'live' when ready to push\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: Push Cart Rules First\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 1: PUSHING CART RULES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "cart_result = push_cart_rules(df_output, pus, source_module='module_3', mode=PUSH_MODE)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CART RULES RESULT\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Mode: {cart_result['mode']}\")\n",
    "print(f\"Cart rule changes: {cart_result['cart_rule_changes']}\")\n",
    "print(f\"Pushed: {cart_result['pushed']}\")\n",
    "print(f\"Failed: {cart_result['failed']}\")\n",
    "if cart_result['failed_cohorts']:\n",
    "    print(f\"⚠️ Failed cohorts: {cart_result['failed_cohorts']}\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: Push Prices (skip failed cohorts)\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: PUSHING PRICES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get failed cohorts from cart rules to skip in price push\n",
    "failed_cohorts = cart_result.get('failed_cohorts', [])\n",
    "\n",
    "# Call push_prices with the results, skipping failed cohorts\n",
    "push_result = push_prices(df_output, pus, source_module='module_3', mode=PUSH_MODE, skip_cohorts=failed_cohorts)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PRICES RESULT\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Mode: {push_result['mode']}\")\n",
    "print(f\"Source: {push_result['source_module']}\")\n",
    "print(f\"Timestamp: {push_result['timestamp']}\")\n",
    "print(f\"Total received: {push_result['total_received']}\")\n",
    "print(f\"Price changes: {push_result['price_changes']}\")\n",
    "print(f\"Pushed: {push_result['pushed']}\")\n",
    "print(f\"Skipped: {push_result['skipped']}\")\n",
    "print(f\"Failed: {push_result['failed']}\")\n",
    "if push_result.get('skipped_cohorts'):\n",
    "    print(f\"⚠️ Skipped cohorts (cart rules failed): {push_result['skipped_cohorts']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 3: PROCESSING SKU DISCOUNTS\n",
      "======================================================================\n",
      "/home/ec2-user/.Renviron\n",
      "/home/ec2-user/service_account_key.json\n",
      "SKU Discount Handler loaded at 2026-02-10 12:25:44 Cairo time\n",
      "Excluded categories: ['كروت شحن']\n",
      "Excluded brands: ['فيوري', 'العروسة']\n",
      "AWS & API functions defined ✓\n",
      "✓ API credentials loaded successfully\n",
      "Snowflake timezone: America/Los_Angeles\n",
      "Function 1: deactivate_active_sku_discounts() defined ✓\n",
      "/home/ec2-user/.Renviron\n",
      "/home/ec2-user/service_account_key.json\n",
      "Queries Module | Timezone: America/Los_Angeles\n",
      "✅ UTH and Last Hour functions defined\n",
      "\n",
      "==================================================\n",
      "QUERIES MODULE READY\n",
      "==================================================\n",
      "\n",
      "Live Data Functions:\n",
      "  • get_current_stocks()\n",
      "  • get_packing_units()\n",
      "  • get_current_prices()\n",
      "  • get_current_wac()\n",
      "  • get_current_cart_rules()\n",
      "\n",
      "UTH Performance Functions:\n",
      "  • get_uth_performance()         - UTH qty/retailers (Snowflake)\n",
      "  • get_hourly_distribution()     - Historical hour contributions (Snowflake)\n",
      "  • get_last_hour_performance()   - Last hour qty/retailers (DWH)\n",
      "\n",
      "Note: Market prices use MODULE_1_INPUT data\n",
      "Retailer Selection Queries defined ✓\n",
      "  - get_churned_dropped_retailers()\n",
      "  - get_category_not_product_retailers()\n",
      "  - get_out_of_cycle_retailers()\n",
      "  - get_view_no_orders_retailers()\n",
      "  - get_excluded_retailers()\n",
      "  - get_retailers_with_quantity_discount()\n",
      "  - get_retailer_main_warehouse()\n",
      "Function 2: select_target_retailers() defined ✓\n",
      "  - Queries 4 retailer sources (churned, category, cycle, view)\n",
      "  - Applies exclusions (failed orders, inactive, wholesale)\n",
      "  - Removes retailers with existing quantity discounts\n",
      "Function 3: Price selection & discount calculation defined ✓\n",
      "  - margin_to_price()\n",
      "  - build_candidate_prices()\n",
      "  - select_target_price()\n",
      "  - calculate_discount_for_row()\n",
      "  - calculate_discounts_batch()\n",
      "Function 4: structure_sku_discount_dataframe() defined ✓\n",
      "  - clear_output_folder()\n",
      "  - structure_sku_discount_dataframe()\n",
      "  - save_sku_discount_files()\n",
      "Function 5: push_sku_discount() defined ✓\n",
      "  - _get_presigned_url()\n",
      "  - _upload_file_to_s3()\n",
      "  - _validate_sku_discount()\n",
      "  - _proceed_sku_discount()\n",
      "  - _upload_single_file()\n",
      "  - push_sku_discount()\n",
      "Main function: process_sku_discounts() defined ✓\n",
      "\n",
      "============================================================\n",
      "SKU DISCOUNT HANDLER MODULE READY\n",
      "============================================================\n",
      "\n",
      "Required input columns from Module 3:\n",
      "  - product_id, warehouse_id, sku, cohort_id, brand, cat\n",
      "  - activate_sku_discount (bool)\n",
      "  - current_price, new_price, wac_p\n",
      "  - doh, uth_qty, uth_status, active_sku_disc_pct\n",
      "  - target_margin, min_boundary\n",
      "\n",
      "Required market margin columns (prices derived via wac/(1-margin)):\n",
      "  - below_market\n",
      "  - market_min\n",
      "  - market_25\n",
      "  - market_50\n",
      "  - market_75\n",
      "  - market_max\n",
      "  - above_market\n",
      "\n",
      "Required margin tier columns:\n",
      "  - margin_tier_below\n",
      "  - margin_tier_1\n",
      "  - margin_tier_2\n",
      "  - margin_tier_3\n",
      "  - margin_tier_4\n",
      "  - margin_tier_5\n",
      "  - margin_tier_above_1\n",
      "  - margin_tier_above_2\n",
      "\n",
      "Retailer Selection: Queries 4 sources, applies exclusions, removes QD overlap\n",
      "\n",
      "Usage: result = process_sku_discounts(df_output, mode='testing')\n",
      "SKUs needing SKU discount: 15681\n",
      "  Merged market margins and margin tiers from df\n",
      "\n",
      "======================================================================\n",
      "SKU DISCOUNT HANDLER\n",
      "======================================================================\n",
      "Mode: live\n",
      "Input records: 15681\n",
      "\n",
      "--------------------------------------------------\n",
      "STEP 1: Deactivating existing SKU discounts\n",
      "--------------------------------------------------\n",
      "🚀 Deactivating SKU discounts (mode: live)\n",
      "  Querying active SKU discounts from Snowflake...\n",
      "  Found 44860 active SKU discounts to deactivate\n",
      "  Deactivating in 4486 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deactivating SKU Discounts: 100%|██████████| 4486/4486 [09:38<00:00,  7.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ✓ Completed! Deactivated: 44860, Failed: 0\n",
      "\n",
      "--------------------------------------------------\n",
      "STEP 2: Filtering SKUs for discount\n",
      "--------------------------------------------------\n",
      "SKUs flagged for discount: 15681\n",
      "\n",
      "  Applying exclusions...\n",
      "    - Excluded by category: 1\n",
      "    - Excluded by brand: 18\n",
      "\n",
      "  Final SKUs to activate: 15662\n",
      "\n",
      "--------------------------------------------------\n",
      "STEP 3: Calculating discount percentages\n",
      "--------------------------------------------------\n",
      "Calculating discounts for 15662 SKUs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating discounts: 100%|██████████| 15662/15662 [00:04<00:00, 3192.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ✓ Discounts calculated:\n",
      "    - Valid discounts: 3192\n",
      "    - Avg discount: 1.74%\n",
      "    - Discount sources: {'no_lower_prices': 3373, 'below_min_threshold': 2781, 'zero_demand': 2590, 'overstock_no_valid_price': 2167, 'low_stock_protected': 1367, 'dropping_2_below': 1317, 'dropping_below_old': 627, 'dropping_lowest': 545, 'overstock_2_below': 401, 'no_candidates': 242, 'no_reduction_needed': 191, 'on_track_keep_old': 34, 'default_valid': 17, 'growing_above_old': 7, 'growing_keep_old': 3}\n",
      "\n",
      "  SKUs with valid discounts (>0%): 3192\n",
      "\n",
      "--------------------------------------------------\n",
      "STEP 4: Selecting target retailers\n",
      "--------------------------------------------------\n",
      "\n",
      "  Selecting target retailers...\n",
      "    SKUs with valid discounts: 3192\n",
      "    Created tuple string for 3192 unique product-warehouse combinations\n",
      "\n",
      "    Querying retailer sources...\n",
      "  Fetching churned/dropped retailers...\n",
      "    Found 8932 churned/dropped retailer-product combinations\n",
      "  Fetching category-not-product retailers...\n",
      "    Found 3977694 category-not-product retailer-product combinations\n",
      "  Fetching out-of-cycle retailers...\n",
      "    Found 2449 out-of-cycle retailer-product combinations\n",
      "  Fetching view-no-orders retailers...\n",
      "    Found 548076 view-no-orders retailer-product combinations\n",
      "\n",
      "    Combining retailer sources...\n",
      "    Total retailer-product combinations before filtering: 4198599\n",
      "\n",
      "    Getting retailer main warehouses...\n",
      "  Fetching retailer main warehouses...\n",
      "    Found 113128 retailer-warehouse mappings\n",
      "    Retailers after warehouse filter: 4143782\n",
      "\n",
      "    Applying exclusions...\n",
      "  Fetching excluded retailers...\n",
      "    Found 128935 retailers to exclude\n",
      "    Excluded 925102 retailers (failed orders, inactive, wholesale, existing discounts)\n",
      "\n",
      "    Removing retailers with existing quantity discounts...\n",
      "  Fetching retailers with quantity discounts...\n",
      "    Found 1954026 retailer-product combinations with quantity discounts\n",
      "    Removed 0 retailer-product combinations with existing QD\n",
      "\n",
      "    ✓ Final retailer-product combinations: 3218680\n",
      "    ✓ Unique retailers: 51702\n",
      "    ✓ Unique products: 1415\n",
      "\n",
      "    ✓ Final output rows: 3218680\n",
      "\n",
      "--------------------------------------------------\n",
      "STEP 5: Structuring data for API\n",
      "--------------------------------------------------\n",
      "Structuring 3218680 SKU discount records for API...\n",
      "  Step 1: Deduplicating...\n",
      "    Records after deduplication: 3218680\n",
      "  Step 2: Merging with packing units...\n",
      "Fetching packing_units ...\n",
      "  Loaded 35126 records\n",
      "    Records after PU merge: 4418417\n",
      "  Step 3: Creating HH_data format...\n",
      "  Step 4: Setting start/end times...\n",
      "    Start: 10/02/2026 12:46\n",
      "    End: 11/02/2026 02:46\n",
      "  Step 5: Grouping by retailer...\n",
      "    Unique retailers: 51702\n",
      "  Step 6: Grouping by discount combinations...\n",
      "    Unique discount combinations: 43595\n",
      "  Step 7: Chunking retailer lists (max 100 per chunk)...\n",
      "    Total chunks: 43596\n",
      "  Step 8: Finalizing columns...\n",
      "  ✓ Structured 43596 records for upload\n",
      "\n",
      "--------------------------------------------------\n",
      "STEP 6: Pushing to API\n",
      "--------------------------------------------------\n",
      "\n",
      "🚀 MODE: LIVE\n",
      "Processing 43596 SKU discount records...\n",
      "\n",
      "  Step 1: Saving files to output folder...\n",
      "\n",
      "Saving SKU discount files...\n",
      "  Clearing output folder...\n",
      "  Cleared 46 files from output folder\n",
      "  Saving 44 files (max 1000 rows each)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving files: 100%|██████████| 44/44 [00:06<00:00,  6.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved 44 files to ../output/sku_discount_sheets\n",
      "\n",
      "  Step 2: Uploading 44 files via S3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:   0%|          | 0/44 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._0.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:   2%|▏         | 1/44 [00:01<01:09,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._1.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:   5%|▍         | 2/44 [00:03<01:06,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._2.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:   7%|▋         | 3/44 [00:04<01:03,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._3.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:   9%|▉         | 4/44 [00:06<01:01,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._4.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  11%|█▏        | 5/44 [00:07<00:58,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._5.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  14%|█▎        | 6/44 [00:08<00:54,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._6.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  16%|█▌        | 7/44 [00:10<00:56,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._7.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  18%|█▊        | 8/44 [00:12<00:56,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._8.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  20%|██        | 9/44 [00:14<00:58,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._9.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  23%|██▎       | 10/44 [00:16<01:03,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._10.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  25%|██▌       | 11/44 [00:17<00:57,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._11.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  27%|██▋       | 12/44 [00:20<01:01,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._12.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  30%|██▉       | 13/44 [00:21<00:53,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._13.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  32%|███▏      | 14/44 [00:22<00:47,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._14.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  34%|███▍      | 15/44 [00:23<00:42,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._15.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  36%|███▋      | 16/44 [00:25<00:38,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._16.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  39%|███▊      | 17/44 [00:27<00:45,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._17.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  41%|████      | 18/44 [00:28<00:41,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._18.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  43%|████▎     | 19/44 [00:30<00:38,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._19.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  45%|████▌     | 20/44 [00:31<00:33,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._20.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  48%|████▊     | 21/44 [00:32<00:30,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._21.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  50%|█████     | 22/44 [00:33<00:27,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._22.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  52%|█████▏    | 23/44 [00:35<00:28,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._23.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  55%|█████▍    | 24/44 [00:36<00:26,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._24.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  57%|█████▋    | 25/44 [00:38<00:26,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._25.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  59%|█████▉    | 26/44 [00:39<00:25,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._26.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  61%|██████▏   | 27/44 [00:41<00:25,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._27.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  64%|██████▎   | 28/44 [00:42<00:22,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._28.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  66%|██████▌   | 29/44 [00:44<00:24,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._29.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  68%|██████▊   | 30/44 [00:46<00:24,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._30.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  70%|███████   | 31/44 [00:47<00:21,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._31.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  73%|███████▎  | 32/44 [00:49<00:18,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._32.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  75%|███████▌  | 33/44 [00:50<00:16,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._33.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  77%|███████▋  | 34/44 [00:51<00:13,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._34.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  80%|███████▉  | 35/44 [00:53<00:12,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._35.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  82%|████████▏ | 36/44 [00:54<00:10,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._36.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  84%|████████▍ | 37/44 [00:55<00:08,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._37.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  86%|████████▋ | 38/44 [00:57<00:09,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._38.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  89%|████████▊ | 39/44 [00:58<00:07,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._39.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  91%|█████████ | 40/44 [00:59<00:05,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._40.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  93%|█████████▎| 41/44 [01:01<00:04,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._41.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  95%|█████████▌| 42/44 [01:02<00:02,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._42.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  98%|█████████▊| 43/44 [01:04<00:01,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-10_NO._43.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files: 100%|██████████| 44/44 [01:05<00:00,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "  ==================================================\n",
      "  UPLOAD SUMMARY\n",
      "  ==================================================\n",
      "  Total files: 44\n",
      "  ✓ Successful: 44\n",
      "  ✗ Failed: 0\n",
      "\n",
      "======================================================================\n",
      "SUMMARY\n",
      "======================================================================\n",
      "Mode: live\n",
      "Total input: 15681\n",
      "Discounts deactivated: 44860\n",
      "SKUs to activate: 15662\n",
      "SKUs with valid discounts: 3192\n",
      "Retailer-product combinations: 3218680\n",
      "Records created/uploaded: 44\n",
      "Records failed: 0\n",
      "Files saved: 44\n",
      "Output folder: ../output/sku_discount_sheets\n",
      "\n",
      "============================================================\n",
      "SKU DISCOUNT RESULT\n",
      "============================================================\n",
      "Mode: live\n",
      "Total input: 15681\n",
      "SKUs to activate: 15662\n",
      "Deactivated: 44860\n",
      "Created: 44\n",
      "Failed: 0\n",
      "\n",
      "======================================================================\n",
      "STEP 4: PROCESSING QUANTITY DISCOUNTS\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.Renviron\n",
      "/home/ec2-user/service_account_key.json\n",
      "Queries Module | Timezone: America/Los_Angeles\n",
      "✅ UTH and Last Hour functions defined\n",
      "\n",
      "==================================================\n",
      "QUERIES MODULE READY\n",
      "==================================================\n",
      "\n",
      "Live Data Functions:\n",
      "  • get_current_stocks()\n",
      "  • get_packing_units()\n",
      "  • get_current_prices()\n",
      "  • get_current_wac()\n",
      "  • get_current_cart_rules()\n",
      "\n",
      "UTH Performance Functions:\n",
      "  • get_uth_performance()         - UTH qty/retailers (Snowflake)\n",
      "  • get_hourly_distribution()     - Historical hour contributions (Snowflake)\n",
      "  • get_last_hour_performance()   - Last hour qty/retailers (DWH)\n",
      "\n",
      "Note: Market prices use MODULE_1_INPUT data\n",
      "Retailer Selection Queries defined ✓\n",
      "  - get_churned_dropped_retailers()\n",
      "  - get_category_not_product_retailers()\n",
      "  - get_out_of_cycle_retailers()\n",
      "  - get_view_no_orders_retailers()\n",
      "  - get_excluded_retailers()\n",
      "  - get_retailers_with_quantity_discount()\n",
      "  - get_retailer_main_warehouse()\n",
      "/home/ec2-user/.Renviron\n",
      "/home/ec2-user/service_account_key.json\n",
      "✓ QD Handler initialized\n",
      "  Timezone: America/Los_Angeles\n",
      "✓ QD calculation parameters:\n",
      "  MAX_DISCOUNT_PCT: 5.0%\n",
      "  MIN_DISCOUNT_PCT: 0.35%\n",
      "  RATIO RANGE: [1.1, 3.0]\n",
      "\n",
      "✓ Wholesale (T3) parameters:\n",
      "  WS_CAR_COST: 1400 EGP\n",
      "  WS_MAX_TICKET_SIZE: 35000 EGP\n",
      "  WS_MIN_MARGIN: -2.0%\n",
      "  TOP_SKUS_PER_WAREHOUSE: 400\n",
      "\n",
      "✓ Upload parameters:\n",
      "  MAX_GROUP_SIZE: 200\n",
      "  QD_DURATION_HOURS: 14\n",
      "\n",
      "✓ Output directory: qd_uploads\n",
      "✓ Data fetching functions defined\n",
      "✓ Tier price calculation function defined\n",
      "✓ Wholesale tier calculation function defined\n",
      "✓ process_qd() function defined\n",
      "Helper functions defined ✓\n",
      "✓ API functions defined\n",
      "✓ QD Handler ready to use\n",
      "\n",
      "Available functions:\n",
      "  - process_qd(df_qd, dry_run=True)      : Main function to process QDs from Module 3\n",
      "  - deactivate_active_qd(dry_run=True)   : Deactivate all active QDs\n",
      "  - create_upload_format(df_configs)     : Create upload format DataFrame\n",
      "  - prepare_upload_file(df_upload, ...)  : Prepare final upload file with tag IDs\n",
      "  - post_QD(filename)                    : Upload QD file to API\n",
      "  - prepare_cart_rules_update(df_work, df_qd) : Prepare cart rules update\n",
      "  - upload_cart_rules(cart_rules, ...)   : Upload cart rules by cohort\n",
      "SKUs needing QD processing: 849\n",
      "\n",
      "======================================================================\n",
      "QD HANDLER: PROCESSING QUANTITY DISCOUNTS\n",
      "======================================================================\n",
      "Mode: LIVE\n",
      "Timestamp: 2026-02-10 12:38 Cairo Time\n",
      "Input SKUs: 849\n",
      "\n",
      "Unique warehouses: 12\n",
      "\n",
      "------------------------------------------------------------\n",
      "STEP 1: Deactivating existing Quantity Discounts...\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "DEACTIVATING ACTIVE QUANTITY DISCOUNTS\n",
      "============================================================\n",
      "Mode: LIVE\n",
      "\n",
      "Step 1: Querying active Quantity Discounts from Snowflake...\n",
      "Fetching  qd ...\n",
      "  Loaded 12 records\n",
      "  Found 12 active Quantity Discounts\n",
      "\n",
      "Step 2: Deactivating 12 discounts...\n",
      "https://api.maxab.info/commerce/api/admins/v1/quantity-discounts/4250/activation?status=false\n",
      "  [1/12] [OK] Deactivated: 4250\n",
      "https://api.maxab.info/commerce/api/admins/v1/quantity-discounts/4252/activation?status=false\n",
      "  [2/12] [OK] Deactivated: 4252\n",
      "https://api.maxab.info/commerce/api/admins/v1/quantity-discounts/4249/activation?status=false\n",
      "  [3/12] [OK] Deactivated: 4249\n",
      "https://api.maxab.info/commerce/api/admins/v1/quantity-discounts/4258/activation?status=false\n",
      "  [4/12] [OK] Deactivated: 4258\n",
      "https://api.maxab.info/commerce/api/admins/v1/quantity-discounts/4257/activation?status=false\n",
      "  [5/12] [OK] Deactivated: 4257\n",
      "https://api.maxab.info/commerce/api/admins/v1/quantity-discounts/4260/activation?status=false\n",
      "  [6/12] [OK] Deactivated: 4260\n",
      "https://api.maxab.info/commerce/api/admins/v1/quantity-discounts/4253/activation?status=false\n",
      "  [7/12] [OK] Deactivated: 4253\n",
      "https://api.maxab.info/commerce/api/admins/v1/quantity-discounts/4251/activation?status=false\n",
      "  [8/12] [OK] Deactivated: 4251\n",
      "https://api.maxab.info/commerce/api/admins/v1/quantity-discounts/4259/activation?status=false\n",
      "  [9/12] [OK] Deactivated: 4259\n",
      "https://api.maxab.info/commerce/api/admins/v1/quantity-discounts/4255/activation?status=false\n",
      "  [10/12] [OK] Deactivated: 4255\n",
      "https://api.maxab.info/commerce/api/admins/v1/quantity-discounts/4256/activation?status=false\n",
      "  [11/12] [OK] Deactivated: 4256\n",
      "https://api.maxab.info/commerce/api/admins/v1/quantity-discounts/4254/activation?status=false\n",
      "  [12/12] [OK] Deactivated: 4254\n",
      "\n",
      "============================================================\n",
      "DEACTIVATION SUMMARY\n",
      "============================================================\n",
      "Total active found: 12\n",
      "Successfully deactivated: 12\n",
      "Failed: 0\n",
      "\n",
      "------------------------------------------------------------\n",
      "STEP 2: Getting top-selling packing units...\n",
      "------------------------------------------------------------\n",
      "  Fetching top-selling packing units (last 90 days)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1396/4053721457.py:73: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Found packing units for 849 product-warehouse combinations\n",
      "  Matched 849 SKUs with packing units\n",
      "  Using new_price: 129 SKUs\n",
      "  Using current_price (fallback): 720 SKUs\n",
      "\n",
      "------------------------------------------------------------\n",
      "STEP 3: Getting warehouse ticket statistics...\n",
      "------------------------------------------------------------\n",
      "  Fetching warehouse ticket statistics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1396/4053721457.py:425: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Got stats for 13 warehouses\n",
      "  Merged ticket stats for 849 SKUs\n",
      "\n",
      "------------------------------------------------------------\n",
      "STEP 4: Calculating tier quantities...\n",
      "------------------------------------------------------------\n",
      "  Calculating tier quantities from order history...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1396/4053721457.py:314: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Calculated tiers for 819 product-warehouse combinations\n",
      "  819 SKUs have tier quantities\n",
      "\n",
      "------------------------------------------------------------\n",
      "STEP 5: Calculating T1 & T2 prices...\n",
      "------------------------------------------------------------\n",
      "  Valid T1 & T2 prices: 285 / 849\n",
      "\n",
      "  Price source distribution:\n",
      "    insufficient_valid_prices: 507\n",
      "    margin_tier_margin_tier: 98\n",
      "    current_price_below_wac: 53\n",
      "    margin_tier_margin_tier_ratio_up: 40\n",
      "    margin_tier_market_ratio_up: 35\n",
      "\n",
      "------------------------------------------------------------\n",
      "STEP 6: Calculating T3 (wholesale) prices...\n",
      "------------------------------------------------------------\n",
      "  Valid T3 prices: 624 / 849\n",
      "\n",
      "  T3 Statistics:\n",
      "    Average multiplier: 7.3x\n",
      "    Average discount: 1.58%\n",
      "    Average margin: 2.11%\n",
      "\n",
      "------------------------------------------------------------\n",
      "STEP 7: Validating T3 constraints...\n",
      "------------------------------------------------------------\n",
      "  Fixed 2 SKUs where T3 qty <= T2 qty\n",
      "  Final valid T3 count: 624\n",
      "\n",
      "  Checking tier quantity ratios...\n",
      "\n",
      "------------------------------------------------------------\n",
      "STEP 8: Applying keep_qd_tiers filter and calculating tier flags...\n",
      "------------------------------------------------------------\n",
      "\n",
      "  Validating unique discount ordering (T1 < T2 < T3)...\n",
      "    Invalidated T3 for 1 SKUs (T2 discount >= T3 discount)\n",
      "  SKUs with valid tiers after filtering: 240\n",
      "  Total tier entries: 553\n",
      "    T1 valid: 234\n",
      "    T2 valid: 236\n",
      "    T3 valid: 83\n",
      "\n",
      "------------------------------------------------------------\n",
      "STEP 9: Selecting top 400 tier entries per warehouse...\n",
      "------------------------------------------------------------\n",
      "  Before filtering: 240 SKUs (553 tier entries)\n",
      "  After top 400 limit: 240 SKUs (553 tier entries)\n",
      "\n",
      "  Tier entries per warehouse:\n",
      "    Warehouse 1: 42 SKUs, 93 tiers\n",
      "    Warehouse 8: 16 SKUs, 38 tiers\n",
      "    Warehouse 170: 11 SKUs, 24 tiers\n",
      "    Warehouse 236: 56 SKUs, 128 tiers\n",
      "    Warehouse 337: 11 SKUs, 26 tiers\n",
      "    Warehouse 339: 21 SKUs, 51 tiers\n",
      "    Warehouse 401: 9 SKUs, 23 tiers\n",
      "    Warehouse 501: 12 SKUs, 31 tiers\n",
      "    Warehouse 632: 7 SKUs, 16 tiers\n",
      "    Warehouse 703: 12 SKUs, 28 tiers\n",
      "    Warehouse 797: 10 SKUs, 23 tiers\n",
      "    Warehouse 962: 33 SKUs, 72 tiers\n",
      "\n",
      "------------------------------------------------------------\n",
      "STEP 10: Building QD configurations...\n",
      "------------------------------------------------------------\n",
      "  Valid QD configs: 240\n",
      "\n",
      "  Tier distribution in configs:\n",
      "    T1: 234 configs\n",
      "    T2: 236 configs\n",
      "    T3 (wholesale): 83 configs\n",
      "    Total tier entries: 553\n",
      "\n",
      "  Sample QD configs:\n",
      "    ارز حبوبة رفيع - 1 كجم: [T1:qty=4,disc=0.68%, T2:qty=7,disc=1.34%, T3:qty=127,disc=1.47%]\n",
      "    لبن بخيره - 500 مل: [T1:qty=4,disc=0.41%, T2:qty=7,disc=0.82%, T3:qty=67,disc=1.47%]\n",
      "    عصير بيتى جوافة - 235 مل: [T1:qty=4,disc=0.59%, T2:qty=7,disc=1.13%, T3:qty=172,disc=1.47%]\n",
      "    شاى ليبتون ناعم  - 250 جم: [T1:qty=8,disc=0.44%, T2:qty=17,disc=1.25%, T3:qty=628,disc=1.47%]\n",
      "    زيت كريستال الممتاز خليط - 1 ل: [T1:qty=4,disc=0.68%, T2:qty=7,disc=1.31%, T3:qty=38,disc=1.47%]\n",
      "\n",
      "------------------------------------------------------------\n",
      "STEP 10.5: Saving data for review...\n",
      "------------------------------------------------------------\n",
      "/home/ec2-user/service_account_key.json\n",
      "File QD_review_20260210_1238.xlsx sent to Slack\n",
      "  ✓ Sent review file to Slack\n",
      "    Total SKUs: 240\n",
      "    Columns: 27\n",
      "\n",
      "------------------------------------------------------------\n",
      "STEP 11: Creating new Quantity Discounts...\n",
      "------------------------------------------------------------\n",
      "  Creating 240 Quantity Discounts...\n",
      "\n",
      "  Creating upload format...\n",
      "  Upload format created: 12 warehouse rows\n",
      "\n",
      "  Per warehouse breakdown:\n",
      "    WH 1: Group 1 = 51 items, Group 2 = 42 items\n",
      "    WH 8: Group 1 = 24 items, Group 2 = 14 items\n",
      "    WH 170: Group 1 = 13 items, Group 2 = 11 items\n",
      "    WH 236: Group 1 = 72 items, Group 2 = 56 items\n",
      "    WH 337: Group 1 = 15 items, Group 2 = 11 items\n",
      "    WH 339: Group 1 = 30 items, Group 2 = 21 items\n",
      "    WH 401: Group 1 = 14 items, Group 2 = 9 items\n",
      "    WH 501: Group 1 = 19 items, Group 2 = 12 items\n",
      "    WH 632: Group 1 = 9 items, Group 2 = 7 items\n",
      "    WH 703: Group 1 = 17 items, Group 2 = 11 items\n",
      "    WH 797: Group 1 = 14 items, Group 2 = 9 items\n",
      "    WH 962: Group 1 = 39 items, Group 2 = 33 items\n",
      "\n",
      "  Preparing upload file...\n",
      "  ✓ Saved upload file: qd_uploads/QD_upload_20260210_1238.xlsx (12 warehouses)\n",
      "\n",
      "  Uploading QD file to API...\n",
      "  ✓ Upload succeeded (status: 200)\n",
      "\n",
      "  Creation Result:\n",
      "    Created: 240\n",
      "    Failed: 0\n",
      "\n",
      "------------------------------------------------------------\n",
      "STEP 12: Updating cart rules...\n",
      "------------------------------------------------------------\n",
      "  Uploading cart rules...\n",
      "\n",
      "  Cart rules to update: 229 products across 9 cohorts\n",
      "    ✓ Cohort 700: 42 rules uploaded\n",
      "    ✓ Cohort 701: 82 rules uploaded\n",
      "    ✓ Cohort 702: 10 rules uploaded\n",
      "    ✓ Cohort 703: 25 rules uploaded\n",
      "    ✓ Cohort 704: 30 rules uploaded\n",
      "    ✓ Cohort 1123: 12 rules uploaded\n",
      "    ✓ Cohort 1124: 12 rules uploaded\n",
      "    ✓ Cohort 1125: 7 rules uploaded\n",
      "    ✓ Cohort 1126: 9 rules uploaded\n",
      "\n",
      "  Cart Rules Result:\n",
      "    Cohorts updated: 9\n",
      "    Cohorts failed: 0\n",
      "\n",
      "======================================================================\n",
      "QD HANDLER - SUMMARY\n",
      "======================================================================\n",
      "Mode: LIVE\n",
      "Total SKUs in input: 849\n",
      "SKUs with valid T1 & T2 prices: 285\n",
      "SKUs with valid T3 prices: 624\n",
      "SKUs after keep_qd_tiers & 400 tier limit: 240\n",
      "Total tier entries: 553\n",
      "Valid QD configs: 240\n",
      "QD found active: 12\n",
      "QD deactivated: 12\n",
      "QD created: 240\n",
      "QD creation failed: 0\n",
      "Cart rules updated: 229 products\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'review_file_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 98\u001b[39m\n\u001b[32m     95\u001b[39m df_qd = df_qd[qd_columns].copy()\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df_qd) > \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     qd_result = \u001b[43mprocess_qd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_qd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    101\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mQD PROCESSING RESULT\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/tmp/ipykernel_1396/924446669.py:551\u001b[39m, in \u001b[36mprocess_qd\u001b[39m\u001b[34m(df_qd, dry_run)\u001b[39m\n\u001b[32m    536\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCart rules updated: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(cart_rules_update)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m products\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    537\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m70\u001b[39m)\n\u001b[32m    539\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    540\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmode\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mtesting\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dry_run \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mlive\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    541\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtotal_input\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(df_qd),\n\u001b[32m    542\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mprocessed\u001b[39m\u001b[33m'\u001b[39m: create_result[\u001b[33m'\u001b[39m\u001b[33mcreated_count\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    543\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mfailed\u001b[39m\u001b[33m'\u001b[39m: create_result[\u001b[33m'\u001b[39m\u001b[33mfailed_count\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    544\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtotal_tiers\u001b[39m\u001b[33m'\u001b[39m: total_tiers,\n\u001b[32m    545\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mdeactivate_result\u001b[39m\u001b[33m'\u001b[39m: deactivate_result,\n\u001b[32m    546\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mcreate_result\u001b[39m\u001b[33m'\u001b[39m: create_result,\n\u001b[32m    547\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mcart_rules_result\u001b[39m\u001b[33m'\u001b[39m: cart_rules_result,\n\u001b[32m    548\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mcart_rules_update\u001b[39m\u001b[33m'\u001b[39m: cart_rules_update,\n\u001b[32m    549\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mqd_configs\u001b[39m\u001b[33m'\u001b[39m: qd_configs,  \u001b[38;5;66;03m# Return configs for inspection\u001b[39;00m\n\u001b[32m    550\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mdf_work\u001b[39m\u001b[33m'\u001b[39m: df_top,  \u001b[38;5;66;03m# Return working DataFrame for debugging\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m551\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mreview_file\u001b[39m\u001b[33m'\u001b[39m: \u001b[43mreview_file_name\u001b[49m  \u001b[38;5;66;03m# File sent to Slack for review\u001b[39;00m\n\u001b[32m    552\u001b[39m }\n",
      "\u001b[31mNameError\u001b[39m: name 'review_file_name' is not defined"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 3: PROCESS SKU DISCOUNTS\n",
    "# =============================================================================\n",
    "# This step handles SKU discounts for SKUs that need them based on UTH performance.\n",
    "# Market data has already been refreshed, so we pass the df_output directly.\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 3: PROCESSING SKU DISCOUNTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "%run sku_discount_handler.ipynb\n",
    "\n",
    "# Filter to SKUs that need SKU discount\n",
    "df_sku_discount = df_results[df_results['activate_sku_discount'] == True].copy()\n",
    "print(f\"SKUs needing SKU discount: {len(df_sku_discount)}\")\n",
    "\n",
    "# Merge market margins and margin tiers from df (not in df_results)\n",
    "sku_discount_extra_cols = [\n",
    "    'product_id', 'warehouse_id',\n",
    "    # Market margins\n",
    "    'below_market', 'market_min', 'market_25', 'market_50', \n",
    "    'market_75', 'market_max', 'above_market',\n",
    "    # Margin tiers\n",
    "    'margin_tier_below', 'margin_tier_1', 'margin_tier_2', 'margin_tier_3', \n",
    "    'margin_tier_4', 'margin_tier_5', 'margin_tier_above_1', 'margin_tier_above_2',\n",
    "    # Other needed columns\n",
    "    'doh', 'zero_demand', 'target_margin', 'min_boundary', 'active_sku_disc_pct'\n",
    "]\n",
    "# Filter to columns that exist in df\n",
    "sku_discount_extra_cols = [c for c in sku_discount_extra_cols if c in df.columns]\n",
    "\n",
    "# Merge the extra columns from df\n",
    "df_sku_discount = df_sku_discount.merge(\n",
    "    df[sku_discount_extra_cols].drop_duplicates(subset=['product_id', 'warehouse_id']),\n",
    "    on=['product_id', 'warehouse_id'],\n",
    "    how='left',\n",
    "    suffixes=('', '_from_df')\n",
    ")\n",
    "print(f\"  Merged market margins and margin tiers from df\")\n",
    "\n",
    "if len(df_sku_discount) > 0:\n",
    "    sku_discount_result = process_sku_discounts(df_sku_discount, mode=PUSH_MODE)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SKU DISCOUNT RESULT\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Mode: {sku_discount_result['mode']}\")\n",
    "    print(f\"Total input: {sku_discount_result['total_input']}\")\n",
    "    print(f\"SKUs to activate: {sku_discount_result['to_activate']}\")\n",
    "    print(f\"Deactivated: {sku_discount_result['deactivated']}\")\n",
    "    print(f\"Created: {sku_discount_result['created']}\")\n",
    "    print(f\"Failed: {sku_discount_result['failed']}\")\n",
    "else:\n",
    "    print(\"No SKUs need SKU discounts\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 4: PROCESSING QUANTITY DISCOUNTS (QD)\n",
    "# =============================================================================\n",
    "# This step handles QD adjustments for SKUs flagged by the action engine.\n",
    "# Only processes SKUs where activate_qd=True and uses keep_qd_tiers to determine\n",
    "# which tiers to maintain.\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 4: PROCESSING QUANTITY DISCOUNTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "%run qd_handler.ipynb\n",
    "\n",
    "# Filter to SKUs that need QD processing\n",
    "df_qd = df_results[df_results['activate_qd'] == True].copy()\n",
    "print(f\"SKUs needing QD processing: {len(df_qd)}\")\n",
    "\n",
    "# Required columns for QD handler\n",
    "# Include all data needed for tier quantity and price calculations\n",
    "qd_columns = [\n",
    "    # Identifiers\n",
    "    'product_id', 'warehouse_id', 'cohort_id', 'sku', 'brand', 'cat',\n",
    "    # Pricing data\n",
    "    'wac_p', 'current_price', 'new_price', 'target_margin', 'min_boundary',\n",
    "    # Cart rules\n",
    "    'current_cart_rule', 'new_cart_rule',\n",
    "    # Market margins (to be converted to prices)\n",
    "    'below_market', 'market_min', 'market_25', 'market_50',\n",
    "    'market_75', 'market_max', 'above_market',\n",
    "    # Margin tiers (to be converted to prices)\n",
    "    'margin_tier_1', 'margin_tier_2', 'margin_tier_3', 'margin_tier_4',\n",
    "    'margin_tier_5', 'margin_tier_above_1', 'margin_tier_above_2',\n",
    "    # Performance data (for top SKU selection)\n",
    "    'mtd_qty',\n",
    "    # Stock data (for stock value ranking: stocks * wac_p)\n",
    "    'stocks',\n",
    "    # QD configuration\n",
    "    'keep_qd_tiers'\n",
    "]\n",
    "# Filter to columns that exist in df_results\n",
    "qd_columns = [c for c in qd_columns if c in df_results.columns]\n",
    "df_qd = df_qd[qd_columns].copy()\n",
    "\n",
    "if len(df_qd) > 0:\n",
    "    qd_result = process_qd(df_qd, False)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"QD PROCESSING RESULT\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Mode: {qd_result['mode']}\")\n",
    "    print(f\"Total input: {qd_result['total_input']}\")\n",
    "    print(f\"Processed: {qd_result['processed']}\")\n",
    "    print(f\"Failed: {qd_result['failed']}\")\n",
    "else:\n",
    "    print(\"No SKUs need QD processing\")\n",
    "\n",
    "# =============================================================================\n",
    "# FINAL SUMMARY\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODULE 3 EXECUTION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total SKUs processed: {len(df_output)}\")\n",
    "print(f\"Price changes: {(df_output['new_price'] != df_output['current_price']).sum()}\")\n",
    "print(f\"Cart rule changes: {(df_output['new_cart_rule'] != df_output['current_cart_rule']).sum()}\")\n",
    "print(f\"SKUs with SKU discount: {df_output['activate_sku_discount'].sum()}\")\n",
    "print(f\"SKUs with QD: {df_output['activate_qd'].sum()}\")\n",
    "print(f\"Output saved to: {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "UPLOADING RESULTS TO SNOWFLAKE\n",
      "============================================================\n",
      "/home/ec2-user/service_account_key.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/Pricing Runs/Prediction_Scripts_2/Happy_hour/git/Mustafa/Pricing Logic/modules/../common_functions.py:760: UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)\n",
      "  success, _, _, _ = write_pandas(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.12/site-packages/slack/deprecation.py:14: UserWarning: slack package is deprecated. Please use slack_sdk.web/webhook/rtm package instead. For more info, go to https://docs.slack.dev/tools/python-slack-sdk/v3-migration/\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/service_account_key.json\n",
      "Message Sent\n",
      "✅ Slack notification sent!\n",
      "/home/ec2-user/service_account_key.json\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# UPLOAD RESULTS TO SNOWFLAKE AND SEND SLACK NOTIFICATION\n",
    "# =============================================================================\n",
    "from common_functions import upload_dataframe_to_snowflake, send_text_slack, send_file_slack\n",
    "\n",
    "# Add created_at as TIMESTAMP (module runs multiple times per day)\n",
    "df_output = df_output.drop(columns=['keep_qd_tiers'], errors='ignore')\n",
    "df_output['keep_qd_tiers'] = np.nan\n",
    "df_output['created_at'] = datetime.now(CAIRO_TZ).replace(second=0, microsecond=0)\n",
    "# Upload to Snowflake\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"UPLOADING RESULTS TO SNOWFLAKE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "upload_status = upload_dataframe_to_snowflake(\n",
    "    \"Egypt\", \n",
    "    df_output, \n",
    "    \"MATERIALIZED_VIEWS\", \n",
    "    \"pricing_periodic_push\", \n",
    "    \"append\", \n",
    "    auto_create_table=True, \n",
    "    conn=None\n",
    ")\n",
    "\n",
    "# Prepare status variables\n",
    "prices_pushed = push_result.get('pushed', 0) if 'push_result' in dir() else 0\n",
    "prices_failed = push_result.get('failed', 0) if 'push_result' in dir() else 0\n",
    "cart_rules_pushed = cart_result.get('pushed', 0) if 'cart_result' in dir() else 0\n",
    "cart_rules_failed = cart_result.get('failed', 0) if 'cart_result' in dir() else 0\n",
    "\n",
    "# SKU discount status\n",
    "sku_disc_processed = len(df_sku_discount) if 'df_sku_discount' in dir() else 0\n",
    "\n",
    "# QD status\n",
    "qd_processed = qd_result.get('processed', 0) if 'qd_result' in dir() and qd_result else 0\n",
    "qd_failed = qd_result.get('failed', 0) if 'qd_result' in dir() and qd_result else 0\n",
    "df_output.columns = df_output.columns.str.lower()\n",
    "if upload_status:\n",
    "    slack_message = f\"\"\"✅ *Module 3 - Periodic Actions Completed*\n",
    "\n",
    "📅 Date: {datetime.now(CAIRO_TZ).strftime('%Y-%m-%d')}\n",
    "⏰ Completed at: {datetime.now(CAIRO_TZ).strftime('%H:%M:%S')} Cairo time\n",
    "🔧 Mode: {PUSH_MODE.upper()}\n",
    "\n",
    "📊 *Results:*\n",
    "• Total SKUs processed: {len(df_output):,}\n",
    "• Price changes: {(df_output['new_price'] != df_output['current_price']).sum():,}\n",
    "• Induced DOH prices: {(df_output['price_action'] == 'induced_doh_reduction').sum():,}\n",
    "• Cart rule changes: {(df_output['new_cart_rule'] != df_output['current_cart_rule']).sum():,}\n",
    "\n",
    "📤 *Push Status:*\n",
    "• 💰 Prices: ✅ {prices_pushed} pushed | ❌ {prices_failed} failed\n",
    "• 🛒 Cart Rules: ✅ {cart_rules_pushed} pushed | ❌ {cart_rules_failed} failed\n",
    "• 🏷️ SKU Discounts: {sku_disc_processed} processed\n",
    "• 📦 Quantity Discounts: ✅ {qd_processed} processed | ❌ {qd_failed} failed\n",
    "\n",
    "🗄️ Results uploaded to: MATERIALIZED_VIEWS.pricing_periodic_push\"\"\"\n",
    "    \n",
    "    send_text_slack('new-pricing-logic', slack_message)\n",
    "    print(\"✅ Slack notification sent!\")\n",
    "    \n",
    "    # Send output file to Slack after the text message (using saved copy before manipulation)\n",
    "    SLACK_CHANNEL_ID = 'C0AAWK97Z3Q'\n",
    "    send_file_slack(\n",
    "        temp_df_for_slack, \n",
    "        f'📎 Module 3 Output: {len(temp_df_for_slack)} SKUs processed', \n",
    "        SLACK_CHANNEL_ID,\n",
    "        filename=f'module3_periodic_{datetime.now(CAIRO_TZ).strftime(\"%Y%m%d_%H%M\")}.xlsx'\n",
    "    )\n",
    "    print(\"✅ Output file sent to Slack\")\n",
    "    \n",
    "    print(f\"✅ {len(df_output)} records uploaded to Snowflake\")\n",
    "else:\n",
    "    error_message = f\"\"\"❌ *Module 3 - Periodic Actions Failed*\n",
    "\n",
    "📅 Date: {datetime.now(CAIRO_TZ).strftime('%Y-%m-%d')}\n",
    "⏰ Failed at: {datetime.now(CAIRO_TZ).strftime('%H:%M:%S')} Cairo time\n",
    "⚠️ Upload to Snowflake failed - please check logs\n",
    "\n",
    "📤 *Push Status (before upload failure):*\n",
    "• 💰 Prices: ✅ {prices_pushed} pushed | ❌ {prices_failed} failed\n",
    "• 🛒 Cart Rules: ✅ {cart_rules_pushed} pushed | ❌ {cart_rules_failed} failed\n",
    "• 🏷️ SKU Discounts: {sku_disc_processed} processed\n",
    "• 📦 Quantity Discounts: ✅ {qd_processed} processed | ❌ {qd_failed} failed\"\"\"\n",
    "    \n",
    "    send_text_slack('new-pricing-logic', error_message)\n",
    "    print(\"❌ Error notification sent to Slack!\")\n",
    "    \n",
    "    # Still send output file even on error for debugging (using saved copy before manipulation)\n",
    "    send_file_slack(\n",
    "        temp_df_for_slack, \n",
    "        f'⚠️ Module 3 ERROR: {len(temp_df_for_slack)} SKUs', \n",
    "        SLACK_CHANNEL_ID,\n",
    "        filename=f'module3_periodic_ERROR_{datetime.now(CAIRO_TZ).strftime(\"%Y%m%d_%H%M\")}.xlsx'\n",
    "    )\n",
    "    print(\"✅ Error file sent to Slack\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
