{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3: Periodic Action Module (UTH-Based Adjustments)\n",
    "\n",
    "## Purpose\n",
    "This module runs at 12 PM, 3 PM, 6 PM, 9 PM, and 12 AM Cairo time to:\n",
    "1. Adjust prices based on Up-Till-Hour (UTH) performance vs benchmarks\n",
    "2. Manage SKU discounts and Quantity Discounts based on performance\n",
    "3. Adjust cart rules dynamically\n",
    "\n",
    "## UTH Benchmarks\n",
    "- Calculate historical qty from start of day till current hour over the last 4 months\n",
    "- Multiply by P80 all-time-high quantity and P70 retailers\n",
    "\n",
    "## Action Logic\n",
    "- **On Track (±10%)**: No action\n",
    "- **Growing (>110%)**: Deactivate discounts or increase price, reduce cart if too open\n",
    "- **Dropping (<90%)**: Reduce price, increase cart by 20%\n",
    "- **Zero Demand (qty=0 today)**: Market min + SKU discount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/snowflake/connector/options.py:104: UserWarning: You have an incompatible version of 'pyarrow' installed (22.0.0), please install a version that adheres to: 'pyarrow<19.0.0; extra == \"pandas\"'\n",
      "  warn_incompatible_dep(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.Renviron\n",
      "/home/ec2-user/service_account_key.json\n",
      "Queries Module | Timezone: America/Los_Angeles\n",
      "\n",
      "==================================================\n",
      "QUERIES MODULE READY\n",
      "==================================================\n",
      "Functions:\n",
      "  • get_current_stocks()\n",
      "  • get_packing_units()\n",
      "  • get_current_prices()\n",
      "  • get_current_wac()\n",
      "  • get_current_cart_rules()\n",
      "\n",
      "Note: Market prices use MODULE_1_INPUT data\n",
      "/home/ec2-user/.Renviron\n",
      "/home/ec2-user/service_account_key.json\n",
      "Market Data Module loaded at 2026-01-24 20:47:18 Cairo time\n",
      "Snowflake timezone: America/Los_Angeles\n",
      "All queries defined ✓\n",
      "Helper functions defined ✓\n",
      "get_market_data() function defined ✓\n",
      "get_margin_tiers() function defined ✓\n",
      "\n",
      "======================================================================\n",
      "MARKET DATA MODULE READY\n",
      "======================================================================\n",
      "\n",
      "Available functions (NO INPUT REQUIRED):\n",
      "  - get_market_data()   : Fetch and process all market prices\n",
      "  - get_margin_tiers()  : Fetch and calculate margin tiers\n",
      "\n",
      "Usage:\n",
      "  %run market_data_module.ipynb\n",
      "  df_market = get_market_data()\n",
      "  df_tiers = get_margin_tiers()\n",
      "======================================================================\n",
      "Module 3: Periodic Actions\n",
      "Run Time (Cairo): 2026-01-24 20:47:18\n",
      "Current Hour (Cairo): 20\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS AND SETUP\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "# Run queries_module - this:\n",
    "# 1. Initializes Snowflake credentials (setup_environment_2.initialize_env())\n",
    "# 2. Provides query_snowflake() function\n",
    "# 3. Provides TIMEZONE from Snowflake\n",
    "# 4. Provides get_current_stocks(), get_current_prices(), get_current_wac(), get_current_cart_rules()\n",
    "%run queries_module.ipynb\n",
    "\n",
    "# Run market_data_module - this:\n",
    "# 1. Provides get_market_data() for fetching fresh market prices (NO INPUT REQUIRED)\n",
    "# 2. Provides get_margin_tiers() for fetching margin tiers (NO INPUT REQUIRED)\n",
    "# 3. Fetches Ben Soliman, Marketplace, and Scrapped prices\n",
    "# 4. Fills missing prices from group-level data\n",
    "# 5. Calculates market price percentiles and margin tiers\n",
    "%run market_data_module.ipynb\n",
    "\n",
    "# Cairo timezone\n",
    "CAIRO_TZ = pytz.timezone('Africa/Cairo')\n",
    "CAIRO_NOW = datetime.now(CAIRO_TZ)\n",
    "TODAY = CAIRO_NOW.date()\n",
    "CURRENT_HOUR = CAIRO_NOW.hour\n",
    "\n",
    "# Configuration\n",
    "UTH_GROWING_THRESHOLD = 1.10    # >110% = Growing\n",
    "UTH_DROPPING_THRESHOLD = 0.90   # <90% = Dropping\n",
    "CART_INCREASE_PCT = 0.20        # 20% cart increase\n",
    "CART_DECREASE_PCT = 0.20        # 20% cart decrease\n",
    "MIN_CART_RULE = 2\n",
    "MAX_CART_RULE = 150\n",
    "MIN_PRICE_CHANGE_EGP = 0.25     # Minimum 0.25 EGP for any price change\n",
    "CONTRIBUTION_THRESHOLD = 50     # 50% contribution threshold\n",
    "MAX_PRICE_REDUCTIONS_PER_DAY = 2  # Max price reductions per day\n",
    "# SKU discount percentage will be decided in sku_discount_handler\n",
    "\n",
    "# Input/Output\n",
    "MODULE_1_INPUT = '../pricing_with_discount.xlsx'\n",
    "OUTPUT_FILE = f'module_3_output_{CAIRO_NOW.strftime(\"%Y%m%d_%H%M\")}.xlsx'\n",
    "PREVIOUS_OUTPUT_PATTERN = f'module_3_output_{TODAY.strftime(\"%Y%m%d\")}_*.xlsx'\n",
    "\n",
    "print(f\"Module 3: Periodic Actions\")\n",
    "print(f\"Run Time (Cairo): {CAIRO_NOW.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Current Hour (Cairo): {CURRENT_HOUR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading previous actions from today...\n",
      "No previous Module 3 outputs found for today. This is the first run.\n",
      "Previous actions loaded: 0 records\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD PREVIOUS ACTIONS (Track price reductions per day)\n",
    "# =============================================================================\n",
    "import glob\n",
    "\n",
    "def load_previous_actions():\n",
    "    \"\"\"Load previous Module 3 outputs from today to track price reductions.\"\"\"\n",
    "    # Find all Module 3 outputs from today\n",
    "    pattern = f'module_3_output_{TODAY.strftime(\"%Y%m%d\")}_*.xlsx'\n",
    "    files = glob.glob(pattern)\n",
    "    \n",
    "    if not files:\n",
    "        print(\"No previous Module 3 outputs found for today. This is the first run.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Load all and concatenate\n",
    "    dfs = []\n",
    "    for f in sorted(files):\n",
    "        try:\n",
    "            df = pd.read_excel(f)\n",
    "            df['source_file'] = f\n",
    "            dfs.append(df)\n",
    "            print(f\"  Loaded: {f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error loading {f}: {e}\")\n",
    "    \n",
    "    if dfs:\n",
    "        combined = pd.concat(dfs, ignore_index=True)\n",
    "        print(f\"Loaded {len(combined)} previous action records from {len(dfs)} files\")\n",
    "        return combined\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def count_price_reductions_today(product_id, warehouse_id, previous_df):\n",
    "    \"\"\"Count how many price reductions this SKU has had today.\"\"\"\n",
    "    if previous_df.empty:\n",
    "        return 0\n",
    "    \n",
    "    mask = (\n",
    "        (previous_df['product_id'] == product_id) & \n",
    "        (previous_df['warehouse_id'] == warehouse_id) &\n",
    "        (previous_df['price_action'] == 'decrease')\n",
    "    )\n",
    "    return mask.sum()\n",
    "\n",
    "print(\"Loading previous actions from today...\")\n",
    "df_previous_actions = load_previous_actions()\n",
    "print(f\"Previous actions loaded: {len(df_previous_actions)} records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snowflake connection ready\n",
      "Timezone: America/Los_Angeles\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SNOWFLAKE CONNECTION\n",
    "# =============================================================================\n",
    "# query_snowflake() and TIMEZONE are provided by queries_module.ipynb\n",
    "# (which also initializes Snowflake credentials from setup_environment_2)\n",
    "print(f\"Snowflake connection ready\")\n",
    "print(f\"Timezone: {TIMEZONE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading today's UTH performance with discount contributions...\n",
      "Loaded 8121 UTH records\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# QUERY 1: TODAY'S UTH PERFORMANCE\n",
    "# =============================================================================\n",
    "UTH_LIVE_QUERY = f'''\n",
    "WITH params AS (\n",
    "    SELECT\n",
    "        CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE AS today,\n",
    "        HOUR(CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())) AS current_hour\n",
    "),\n",
    "\n",
    "-- Map dynamic tags to warehouse IDs using name matching\n",
    "qd_det AS (\n",
    "    SELECT DISTINCT \n",
    "        dt.id AS tag_id, \n",
    "        dt.name AS tag_name,\n",
    "        REPLACE(w.name, ' ', '') AS warehouse_name,\n",
    "        w.id AS warehouse_id,\n",
    "        warehouse_name ILIKE '%' || CASE \n",
    "            WHEN SPLIT_PART(tag_name, '_', 1) = 'El' THEN SPLIT_PART(tag_name, '_', 2) \n",
    "            ELSE SPLIT_PART(tag_name, '_', 1) \n",
    "        END || '%' AS contains_flag\n",
    "    FROM dynamic_tags dt\n",
    "    JOIN dynamic_taggables dta ON dt.id = dta.dynamic_tag_id \n",
    "    CROSS JOIN warehouses w \n",
    "    WHERE dt.id > 3000\n",
    "        AND dt.name LIKE '%QD_rets%'\n",
    "        AND w.id IN (1, 236, 337, 8, 339, 170, 501, 401, 703, 632, 797, 962)\n",
    "        AND contains_flag = 'true'\n",
    "),\n",
    "\n",
    "-- Get current active QD configurations\n",
    "qd_config AS (\n",
    "    SELECT * \n",
    "    FROM (\n",
    "        SELECT \n",
    "            product_id,\n",
    "            start_at,\n",
    "            end_at,\n",
    "            packing_unit_id,\n",
    "            id AS qd_id,\n",
    "            qd.warehouse_id,\n",
    "            MAX(CASE WHEN tier = 1 THEN quantity END) AS tier_1_qty,\n",
    "            MAX(CASE WHEN tier = 1 THEN discount_percentage END) AS tier_1_discount_pct,\n",
    "            MAX(CASE WHEN tier = 2 THEN quantity END) AS tier_2_qty,\n",
    "            MAX(CASE WHEN tier = 2 THEN discount_percentage END) AS tier_2_discount_pct,\n",
    "            MAX(CASE WHEN tier = 3 THEN quantity END) AS tier_3_qty,\n",
    "            MAX(CASE WHEN tier = 3 THEN discount_percentage END) AS tier_3_discount_pct\n",
    "        FROM (\n",
    "            SELECT \n",
    "                qd.id,\n",
    "                qdv.product_id,\n",
    "                qdv.packing_unit_id,\n",
    "                qdv.quantity,\n",
    "                qdv.discount_percentage,\n",
    "                qd.dynamic_tag_id,\n",
    "                qd.start_at,\n",
    "                qd.end_at,\n",
    "                ROW_NUMBER() OVER (\n",
    "                    PARTITION BY qdv.product_id, qdv.packing_unit_id, qd.id \n",
    "                    ORDER BY qdv.quantity\n",
    "                ) AS tier\n",
    "            FROM quantity_discounts qd \n",
    "            JOIN quantity_discount_values qdv ON qdv.quantity_discount_id = qd.id\n",
    "            WHERE active = 'true'\n",
    "        ) qd_tiers\n",
    "        JOIN qd_det qd ON qd.tag_id = qd_tiers.dynamic_tag_id\n",
    "        GROUP BY ALL\n",
    "    )\n",
    "    QUALIFY ROW_NUMBER() OVER (PARTITION BY product_id, packing_unit_id, warehouse_id ORDER BY start_at DESC) = 1\n",
    "),\n",
    "\n",
    "-- Today's sales up-till-hour with discount breakdown\n",
    "today_uth_sales AS (\n",
    "    SELECT \n",
    "        pso.warehouse_id,\n",
    "        pso.product_id,\n",
    "        so.retailer_id,\n",
    "        pso.packing_unit_id,\n",
    "        pso.purchased_item_count AS qty,\n",
    "        pso.total_price AS nmv,\n",
    "        pso.ITEM_DISCOUNT_VALUE AS sku_discount_per_unit,\n",
    "        pso.ITEM_QUANTITY_DISCOUNT_VALUE AS qty_discount_per_unit,\n",
    "        qd.tier_1_qty,\n",
    "        qd.tier_2_qty,\n",
    "        qd.tier_3_qty,\n",
    "        -- Determine tier used\n",
    "        CASE \n",
    "            WHEN pso.ITEM_QUANTITY_DISCOUNT_VALUE = 0 OR qd.tier_1_qty IS NULL THEN 'Base'\n",
    "            WHEN qd.tier_3_qty IS NOT NULL AND pso.purchased_item_count >= qd.tier_3_qty THEN 'Tier 3'\n",
    "            WHEN qd.tier_2_qty IS NOT NULL AND pso.purchased_item_count >= qd.tier_2_qty THEN 'Tier 2'\n",
    "            WHEN qd.tier_1_qty IS NOT NULL AND pso.purchased_item_count >= qd.tier_1_qty THEN 'Tier 1'\n",
    "            ELSE 'Base'\n",
    "        END AS tier_used\n",
    "    FROM product_sales_order pso\n",
    "    JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "    LEFT JOIN qd_config qd \n",
    "        ON qd.product_id = pso.product_id \n",
    "        AND qd.packing_unit_id = pso.packing_unit_id\n",
    "        AND qd.warehouse_id = so.warehouse_id\n",
    "    CROSS JOIN params p\n",
    "    WHERE so.created_at::DATE = p.today\n",
    "        AND HOUR(so.created_at) < p.current_hour\n",
    "        AND so.sales_order_status_id NOT IN (7, 12)\n",
    "        AND so.channel IN ('telesales', 'retailer')\n",
    "        AND pso.purchased_item_count <> 0\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    warehouse_id,\n",
    "    product_id,\n",
    "    SUM(qty) AS uth_qty,\n",
    "    SUM(nmv) AS uth_nmv,\n",
    "    COUNT(DISTINCT retailer_id) AS uth_retailers,\n",
    "    -- SKU discount NMV and contribution\n",
    "    SUM(CASE WHEN sku_discount_per_unit > 0 THEN nmv ELSE 0 END) AS sku_discount_nmv_uth,\n",
    "    ROUND(SUM(CASE WHEN sku_discount_per_unit > 0 THEN nmv ELSE 0 END) * 100.0 / NULLIF(SUM(nmv), 0), 2) AS sku_disc_cntrb_uth,\n",
    "    -- Quantity discount NMV and contribution\n",
    "    SUM(CASE WHEN qty_discount_per_unit > 0 THEN nmv ELSE 0 END) AS qty_discount_nmv_uth,\n",
    "    ROUND(SUM(CASE WHEN qty_discount_per_unit > 0 THEN nmv ELSE 0 END) * 100.0 / NULLIF(SUM(nmv), 0), 2) AS qty_disc_cntrb_uth,\n",
    "    -- Tier-level NMV\n",
    "    SUM(CASE WHEN tier_used = 'Tier 1' THEN nmv ELSE 0 END) AS t1_nmv_uth,\n",
    "    SUM(CASE WHEN tier_used = 'Tier 2' THEN nmv ELSE 0 END) AS t2_nmv_uth,\n",
    "    SUM(CASE WHEN tier_used = 'Tier 3' THEN nmv ELSE 0 END) AS t3_nmv_uth,\n",
    "    -- Tier-level contributions\n",
    "    ROUND(SUM(CASE WHEN tier_used = 'Tier 1' THEN nmv ELSE 0 END) * 100.0 / NULLIF(SUM(nmv), 0), 2) AS t1_cntrb_uth,\n",
    "    ROUND(SUM(CASE WHEN tier_used = 'Tier 2' THEN nmv ELSE 0 END) * 100.0 / NULLIF(SUM(nmv), 0), 2) AS t2_cntrb_uth,\n",
    "    ROUND(SUM(CASE WHEN tier_used = 'Tier 3' THEN nmv ELSE 0 END) * 100.0 / NULLIF(SUM(nmv), 0), 2) AS t3_cntrb_uth\n",
    "FROM today_uth_sales\n",
    "GROUP BY warehouse_id, product_id\n",
    "HAVING SUM(nmv) > 0\n",
    "'''\n",
    "\n",
    "print(\"Loading today's UTH performance with discount contributions...\")\n",
    "df_uth_today = query_snowflake(UTH_LIVE_QUERY)\n",
    "print(f\"Loaded {len(df_uth_today)} UTH records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading historical hourly distribution (by category & warehouse)...\n",
      "Loaded 770 hourly distribution records\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# QUERY 2: HISTORICAL HOURLY DISTRIBUTION (Last 4 Months) - By Category & Warehouse\n",
    "# =============================================================================\n",
    "HOURLY_DIST_QUERY = f'''\n",
    "WITH params AS (\n",
    "    SELECT\n",
    "        CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE AS today,\n",
    "        CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE - 120 AS history_start,\n",
    "        HOUR(CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())) AS target_hour\n",
    "),\n",
    "hourly_sales AS (\n",
    "    SELECT\n",
    "        pso.warehouse_id,\n",
    "        c.name_ar AS cat,\n",
    "        so.created_at::DATE AS sale_date,\n",
    "        HOUR(so.created_at) AS sale_hour,\n",
    "        SUM(pso.purchased_item_count * pso.basic_unit_count) AS hourly_qty\n",
    "    FROM product_sales_order pso\n",
    "    JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "    JOIN products pr ON pr.id = pso.product_id\n",
    "    JOIN brands b ON b.id = pr.brand_id\n",
    "    JOIN categories c ON c.id = pr.category_id\n",
    "    CROSS JOIN params p\n",
    "    WHERE so.created_at::DATE >= p.history_start\n",
    "        AND so.created_at::DATE < p.today\n",
    "        AND so.sales_order_status_id NOT IN (7, 12)\n",
    "        AND so.channel IN ('telesales', 'retailer')\n",
    "    GROUP BY ALL\n",
    "),\n",
    "daily_totals AS (\n",
    "    SELECT warehouse_id, cat, sale_date, SUM(hourly_qty) AS day_total\n",
    "    FROM hourly_sales\n",
    "    GROUP BY warehouse_id, cat, sale_date\n",
    "),\n",
    "uth_totals AS (\n",
    "    SELECT hs.warehouse_id, hs.cat, hs.sale_date, SUM(hs.hourly_qty) AS uth_total\n",
    "    FROM hourly_sales hs\n",
    "    CROSS JOIN params p\n",
    "    WHERE hs.sale_hour < p.target_hour\n",
    "    GROUP BY ALL\n",
    ")\n",
    "SELECT\n",
    "    dt.warehouse_id, dt.cat,\n",
    "    AVG(COALESCE(ut.uth_total, 0) / NULLIF(dt.day_total, 0)) AS avg_uth_pct\n",
    "FROM daily_totals dt\n",
    "LEFT JOIN uth_totals ut ON dt.warehouse_id = ut.warehouse_id \n",
    "    AND dt.cat = ut.cat AND dt.sale_date = ut.sale_date\n",
    "WHERE dt.day_total > 0\n",
    "GROUP BY ALL\n",
    "'''\n",
    "\n",
    "print(\"Loading historical hourly distribution (by category & warehouse)...\")\n",
    "df_hourly_dist = query_snowflake(HOURLY_DIST_QUERY)\n",
    "print(f\"Loaded {len(df_hourly_dist)} hourly distribution records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading active SKU discounts...\n",
      "Loaded 14524 active SKU discount records\n",
      "Loading active Quantity discounts...\n",
      "Loaded 2035 active QD records\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# QUERY 3 & 4: ACTIVE DISCOUNTS\n",
    "# =============================================================================\n",
    "\n",
    "# SKU Discounts query (from data_extraction.ipynb)\n",
    "ACTIVE_SKU_DISCOUNTS_QUERY = f'''\n",
    "WITH active_sku_discount AS ( \n",
    "    SELECT \n",
    "        x.id AS sku_discount_id,\n",
    "        retailer_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        DISCOUNT_PERCENTAGE,\n",
    "        start_at,\n",
    "        end_at \n",
    "    FROM (\n",
    "        SELECT \n",
    "            sd.*,\n",
    "            f.value::INT AS retailer_id \n",
    "        FROM SKU_DISCOUNTS sd,\n",
    "        LATERAL FLATTEN(\n",
    "            input => SPLIT(\n",
    "                REPLACE(REPLACE(REPLACE(sd.retailer_ids, '{{', ''), '}}', ''), '\"', ''), \n",
    "                ','\n",
    "            )\n",
    "        ) f\n",
    "        WHERE start_at::DATE <= CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE\n",
    "        and end_at::DATE >= CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE\n",
    "            AND active = 'true'\n",
    "    ) x \n",
    "    JOIN SKU_DISCOUNT_VALUES sdv ON x.id = sdv.sku_discount_id\n",
    "    WHERE name_en = 'Special Discounts'\n",
    "    QUALIFY MAX(start_at) OVER (PARTITION BY retailer_id, product_id, packing_unit_id) = start_at \n",
    ")\n",
    "\n",
    "SELECT \n",
    "    product_id, \n",
    "    warehouse_id,\n",
    "    AVG(DISCOUNT_PERCENTAGE) AS active_sku_disc_pct,\n",
    "    1 AS has_active_sku_discount\n",
    "FROM (\n",
    "    SELECT \n",
    "        asd.*,\n",
    "        warehouse_id \n",
    "    FROM active_sku_discount asd \n",
    "    JOIN materialized_views.retailer_polygon rp ON rp.retailer_id = asd.retailer_id\n",
    "    JOIN WAREHOUSE_DISPATCHING_RULES wdr ON wdr.product_id = asd.product_id\n",
    "    JOIN DISPATCHING_POLYGONS dp ON dp.id = wdr.DISPATCHING_POLYGON_ID AND dp.district_id = rp.district_id\n",
    ")\n",
    "GROUP BY ALL\n",
    "'''\n",
    "\n",
    "# Active QD Query - Reuses the same CTE structure from UTH_LIVE_QUERY\n",
    "ACTIVE_QD_QUERY = f'''\n",
    "WITH qd_det AS (\n",
    "    SELECT DISTINCT \n",
    "        dt.id AS tag_id, \n",
    "        dt.name AS tag_name,\n",
    "        REPLACE(w.name, ' ', '') AS warehouse_name,\n",
    "        w.id AS warehouse_id,\n",
    "        warehouse_name ILIKE '%' || CASE \n",
    "            WHEN SPLIT_PART(tag_name, '_', 1) = 'El' THEN SPLIT_PART(tag_name, '_', 2) \n",
    "            ELSE SPLIT_PART(tag_name, '_', 1) \n",
    "        END || '%' AS contains_flag\n",
    "    FROM dynamic_tags dt\n",
    "    JOIN dynamic_taggables dta ON dt.id = dta.dynamic_tag_id \n",
    "    CROSS JOIN warehouses w \n",
    "    WHERE dt.id > 3000\n",
    "        AND dt.name LIKE '%QD_rets%'\n",
    "        AND w.id IN (1, 236, 337, 8, 339, 170, 501, 401, 703, 632, 797, 962)\n",
    "        AND contains_flag = 'true'\n",
    "),\n",
    "\n",
    "qd_config AS (\n",
    "    SELECT * \n",
    "    FROM (\n",
    "        SELECT \n",
    "            product_id,\n",
    "            packing_unit_id,\n",
    "            qd.warehouse_id,\n",
    "            MAX(CASE WHEN tier = 1 THEN quantity END) AS qd_tier_1_qty,\n",
    "            MAX(CASE WHEN tier = 1 THEN discount_percentage END) AS qd_tier_1_disc_pct,\n",
    "            MAX(CASE WHEN tier = 2 THEN quantity END) AS qd_tier_2_qty,\n",
    "            MAX(CASE WHEN tier = 2 THEN discount_percentage END) AS qd_tier_2_disc_pct,\n",
    "            MAX(CASE WHEN tier = 3 THEN quantity END) AS qd_tier_3_qty,\n",
    "            MAX(CASE WHEN tier = 3 THEN discount_percentage END) AS qd_tier_3_disc_pct\n",
    "        FROM (\n",
    "            SELECT \n",
    "                qd.id,\n",
    "                qdv.product_id,\n",
    "                qdv.packing_unit_id,\n",
    "                qdv.quantity,\n",
    "                qdv.discount_percentage,\n",
    "                qd.dynamic_tag_id,\n",
    "                qd.start_at,\n",
    "                ROW_NUMBER() OVER (\n",
    "                    PARTITION BY qdv.product_id, qdv.packing_unit_id, qd.id \n",
    "                    ORDER BY qdv.quantity\n",
    "                ) AS tier\n",
    "            FROM quantity_discounts qd \n",
    "            JOIN quantity_discount_values qdv ON qdv.quantity_discount_id = qd.id\n",
    "            WHERE  active = TRUE\n",
    "        ) qd_tiers\n",
    "        JOIN qd_det qd ON qd.tag_id = qd_tiers.dynamic_tag_id\n",
    "        GROUP BY ALL\n",
    "    )\n",
    "    QUALIFY ROW_NUMBER() OVER (PARTITION BY product_id, packing_unit_id, warehouse_id ORDER BY qd_tier_1_qty DESC) = 1\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    product_id,\n",
    "    warehouse_id,\n",
    "    qd_tier_1_qty,\n",
    "    qd_tier_1_disc_pct,\n",
    "    qd_tier_2_qty,\n",
    "    qd_tier_2_disc_pct,\n",
    "    qd_tier_3_qty,\n",
    "    qd_tier_3_disc_pct,\n",
    "    1 AS has_active_qd\n",
    "FROM qd_config\n",
    "'''\n",
    "\n",
    "print(\"Loading active SKU discounts...\")\n",
    "df_active_sku_disc = query_snowflake(ACTIVE_SKU_DISCOUNTS_QUERY)\n",
    "print(f\"Loaded {len(df_active_sku_disc)} active SKU discount records\")\n",
    "\n",
    "print(\"Loading active Quantity discounts...\")\n",
    "df_active_qd = query_snowflake(ACTIVE_QD_QUERY)\n",
    "print(f\"Loaded {len(df_active_qd)} active QD records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Module 1 data...\n",
      "Loaded 28377 records from Module 1\n",
      "\n",
      "Refreshing live data...\n",
      "Fetching current stocks...\n",
      "  Loaded 1845703 records\n",
      "Fetching current prices...\n",
      "  Loaded 116390 records\n",
      "Fetching current WAC...\n",
      "  Loaded 8146 records\n",
      "Fetching current cart rules...\n",
      "  Loaded 72968 records\n",
      "Live data refreshed: stocks, prices, WAC, cart rules\n",
      "\n",
      "Refreshing market prices and margin tiers...\n",
      "\n",
      "======================================================================\n",
      "FETCHING MARKET DATA\n",
      "======================================================================\n",
      "Timestamp: 2026-01-24 20:48:04 Cairo time\n",
      "\n",
      "Step 1: Fetching raw price data...\n",
      "  1.1 Ben Soliman prices...\n",
      "      Loaded 1562 records\n",
      "  1.2 Marketplace prices...\n",
      "      Loaded 11457 records\n",
      "  1.3 Scrapped prices...\n",
      "      Loaded 5109 records\n",
      "  1.4 Product groups...\n",
      "      Loaded 1602 records\n",
      "  1.5 Sales data (for NMV weighting)...\n",
      "      Loaded 20874 records\n",
      "  1.6 Margin stats...\n",
      "      Loaded 29426 records\n",
      "  1.7 Target margins...\n",
      "      Loaded 478 records\n",
      "  1.8 Product base (WAC)...\n",
      "      Loaded 65178 records\n",
      "\n",
      "Step 2: Joining all market price sources (outer join)...\n",
      "    Market prices base: 16183 records\n",
      "\n",
      "Step 3: Adding cohort IDs and supporting data...\n",
      "    Records after adding cohorts: 24226\n",
      "\n",
      "Step 4: Processing group-level prices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5298/1916930114.py:138: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: pd.Series({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Records after group processing: 25616\n",
      "\n",
      "Step 5: Adding WAC and margin data...\n",
      "    Records with WAC: 25400\n",
      "\n",
      "Step 6: Filtering by price coverage...\n",
      "    Records after price coverage filter: 13593\n",
      "\n",
      "Step 7: Calculating price percentiles...\n",
      "    Records after price analysis: 12820\n",
      "\n",
      "Step 8: Converting prices to margins...\n",
      "\n",
      "======================================================================\n",
      "MARKET DATA COMPLETE\n",
      "======================================================================\n",
      "Total records: 12820\n",
      "  - With marketplace prices: 12260\n",
      "  - With scrapped prices: 6417\n",
      "  - With Ben Soliman prices: 8750\n",
      "  Fetched 12820 market data records\n",
      "\n",
      "======================================================================\n",
      "FETCHING MARGIN TIERS\n",
      "======================================================================\n",
      "Timestamp: 2026-01-24 20:48:52 Cairo time\n",
      "\n",
      "Step 1: Fetching margin boundaries from PRODUCT_STATISTICS...\n",
      "    Loaded 18102 records\n",
      "\n",
      "Step 2: Adding cohort IDs...\n",
      "    Records with cohorts: 24915\n",
      "\n",
      "Step 3: Calculating margin tiers...\n",
      "\n",
      "======================================================================\n",
      "MARGIN TIERS COMPLETE\n",
      "======================================================================\n",
      "Total records: 24915\n",
      "\n",
      "Margin Tier Structure:\n",
      "  margin_tier_below:   effective_min - step (1 below)\n",
      "  margin_tier_1:       effective_min_margin\n",
      "  margin_tier_2:       effective_min + 1*step\n",
      "  margin_tier_3:       effective_min + 2*step\n",
      "  margin_tier_4:       effective_min + 3*step\n",
      "  margin_tier_5:       max_boundary\n",
      "  margin_tier_above_1: max_boundary + 1*step\n",
      "  margin_tier_above_2: max_boundary + 2*step\n",
      "  Fetched 24915 margin tier records\n",
      "Market data refreshed\n",
      "Data merged. Total records: 28444\n",
      "  SKUs with active SKU discount: 14583\n",
      "  SKUs with active QD: 2035\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD MODULE 1 DATA & REFRESH LIVE COLUMNS\n",
    "# =============================================================================\n",
    "print(\"Loading Module 1 data...\")\n",
    "df = pd.read_excel(MODULE_1_INPUT)\n",
    "print(f\"Loaded {len(df)} records from Module 1\")\n",
    "\n",
    "# Refresh live data using queries_module\n",
    "print(\"\\nRefreshing live data...\")\n",
    "\n",
    "# Refresh stocks\n",
    "df_fresh_stocks = get_current_stocks()\n",
    "df = df.drop(columns=['stocks'], errors='ignore')\n",
    "df = df.merge(df_fresh_stocks, on=['warehouse_id', 'product_id'], how='left')\n",
    "df['stocks'] = df['stocks'].fillna(0)\n",
    "\n",
    "# Refresh current prices\n",
    "df_fresh_prices = get_current_prices()\n",
    "df = df.drop(columns=['current_price'], errors='ignore')\n",
    "df = df.merge(df_fresh_prices[['cohort_id', 'product_id', 'current_price']], \n",
    "              on=['cohort_id', 'product_id'], how='left')\n",
    "\n",
    "# Refresh WAC\n",
    "df_fresh_wac = get_current_wac()\n",
    "df = df.drop(columns=['wac_p'], errors='ignore')\n",
    "df = df.merge(df_fresh_wac, on='product_id', how='left')\n",
    "\n",
    "# Refresh cart rules\n",
    "df_fresh_cart = get_current_cart_rules()\n",
    "df = df.drop(columns=['current_cart_rule'], errors='ignore')\n",
    "df = df.merge(df_fresh_cart, on=['cohort_id', 'product_id'], how='left')\n",
    "\n",
    "print(f\"Live data refreshed: stocks, prices, WAC, cart rules\")\n",
    "\n",
    "# Refresh market prices and margin tiers using new standalone functions\n",
    "print(\"\\nRefreshing market prices and margin tiers...\")\n",
    "\n",
    "# Get fresh market data (no input required)\n",
    "df_fresh_market = get_market_data()\n",
    "print(f\"  Fetched {len(df_fresh_market)} market data records\")\n",
    "\n",
    "# Get fresh margin tiers (no input required)\n",
    "df_fresh_tiers = get_margin_tiers()\n",
    "print(f\"  Fetched {len(df_fresh_tiers)} margin tier records\")\n",
    "\n",
    "# Drop old market columns and merge fresh data\n",
    "market_cols_to_drop = [\n",
    "    'below_market', 'market_min', 'market_25', 'market_50', \n",
    "    'market_75', 'market_max', 'above_market',\n",
    "    'minimum', 'percentile_25', 'percentile_50', 'percentile_75', 'maximum',\n",
    "    'ben_soliman_price', 'final_min_price', 'final_max_price', 'final_mod_price',\n",
    "    'final_true_min', 'final_true_max', 'min_scrapped', 'scrapped25', \n",
    "    'scrapped50', 'scrapped75', 'max_scrapped'\n",
    "]\n",
    "df = df.drop(columns=[c for c in market_cols_to_drop if c in df.columns], errors='ignore')\n",
    "\n",
    "# Merge fresh market data\n",
    "df = df.merge(\n",
    "    df_fresh_market, \n",
    "    on=['cohort_id', 'product_id','region'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Drop old margin tier columns and merge fresh data\n",
    "margin_tier_cols_to_drop = [\n",
    "    'margin_tier_below', 'margin_tier_1', 'margin_tier_2', 'margin_tier_3',\n",
    "    'margin_tier_4', 'margin_tier_5', 'margin_tier_above_1', 'margin_tier_above_2',\n",
    "    'optimal_bm', 'min_boundary', 'max_boundary', 'median_bm',\n",
    "    'effective_min_margin', 'margin_step'\n",
    "]\n",
    "df = df.drop(columns=[c for c in margin_tier_cols_to_drop if c in df.columns], errors='ignore')\n",
    "\n",
    "# Merge fresh margin tiers\n",
    "df = df.merge(\n",
    "    df_fresh_tiers, \n",
    "    on=['cohort_id', 'product_id','region'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"Market data refreshed\")\n",
    "\n",
    "# Merge UTH today data - drop old columns first\n",
    "uth_cols = ['uth_qty', 'uth_nmv', 'uth_retailers', 'sku_discount_nmv_uth', 'sku_disc_cntrb_uth',\n",
    "            'qty_discount_nmv_uth', 'qty_disc_cntrb_uth', 't1_nmv_uth', 't2_nmv_uth', 't3_nmv_uth',\n",
    "            't1_cntrb_uth', 't2_cntrb_uth', 't3_cntrb_uth']\n",
    "df = df.drop(columns=[c for c in uth_cols if c in df.columns], errors='ignore')\n",
    "\n",
    "if len(df_uth_today) > 0:\n",
    "    df = df.merge(df_uth_today, on=['warehouse_id', 'product_id'], how='left')\n",
    "else:\n",
    "    for col in uth_cols:\n",
    "        df[col] = 0\n",
    "\n",
    "# Merge hourly distribution - drop old column first (now by warehouse_id + cat)\n",
    "df = df.drop(columns=['avg_uth_pct'], errors='ignore')\n",
    "if len(df_hourly_dist) > 0:\n",
    "    df = df.merge(df_hourly_dist, on=['warehouse_id', 'cat'], how='left')\n",
    "else:\n",
    "    df['avg_uth_pct'] = 0.5  # Default 50%\n",
    "\n",
    "# Merge active SKU discounts - drop old columns first\n",
    "sku_disc_cols = ['has_active_sku_discount', 'active_sku_disc_pct', 'active_sku_discount_value']\n",
    "df = df.drop(columns=[c for c in sku_disc_cols if c in df.columns], errors='ignore')\n",
    "\n",
    "if len(df_active_sku_disc) > 0:\n",
    "    df = df.merge(df_active_sku_disc, on=['warehouse_id', 'product_id'], how='left')\n",
    "else:\n",
    "    df['has_active_sku_discount'] = 0\n",
    "    df['active_sku_disc_pct'] = 0\n",
    "\n",
    "# Merge active QD - drop old columns first\n",
    "qd_cols = ['has_active_qd', 'qd_tier_1_qty', 'qd_tier_1_disc_pct', \n",
    "           'qd_tier_2_qty', 'qd_tier_2_disc_pct', 'qd_tier_3_qty', 'qd_tier_3_disc_pct']\n",
    "df = df.drop(columns=[c for c in qd_cols if c in df.columns], errors='ignore')\n",
    "\n",
    "if len(df_active_qd) > 0:\n",
    "    df = df.merge(df_active_qd, on=['warehouse_id', 'product_id'], how='left')\n",
    "else:\n",
    "    df['has_active_qd'] = 0\n",
    "    df['qd_tier_1_qty'] = 0\n",
    "    df['qd_tier_1_disc_pct'] = 0\n",
    "    df['qd_tier_2_qty'] = 0\n",
    "    df['qd_tier_2_disc_pct'] = 0\n",
    "    df['qd_tier_3_qty'] = 0\n",
    "    df['qd_tier_3_disc_pct'] = 0\n",
    "\n",
    "# Fill NaN\n",
    "df['uth_qty'] = df['uth_qty'].fillna(0)\n",
    "df['uth_retailers'] = df['uth_retailers'].fillna(0)\n",
    "df['avg_uth_pct'] = df['avg_uth_pct'].fillna(0.5)\n",
    "df['has_active_sku_discount'] = df['has_active_sku_discount'].fillna(0)\n",
    "df['active_sku_discount_value'] = df.get('active_sku_discount_value', pd.Series([0]*len(df))).fillna(0)\n",
    "df['has_active_qd'] = df['has_active_qd'].fillna(0)\n",
    "df['qd_tier_1_qty'] = df['qd_tier_1_qty'].fillna(0)\n",
    "df['qd_tier_1_disc_pct'] = df['qd_tier_1_disc_pct'].fillna(0)\n",
    "df['qd_tier_2_qty'] = df['qd_tier_2_qty'].fillna(0)\n",
    "df['qd_tier_2_disc_pct'] = df['qd_tier_2_disc_pct'].fillna(0)\n",
    "df['qd_tier_3_qty'] = df['qd_tier_3_qty'].fillna(0)\n",
    "df['qd_tier_3_disc_pct'] = df['qd_tier_3_disc_pct'].fillna(0)\n",
    "\n",
    "print(f\"Data merged. Total records: {len(df)}\")\n",
    "print(f\"  SKUs with active SKU discount: {(df['has_active_sku_discount'] == 1).sum()}\")\n",
    "print(f\"  SKUs with active QD: {(df['has_active_qd'] == 1).sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions loaded.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_margin(price, wac):\n",
    "    if pd.isna(price) or pd.isna(wac) or price == 0:\n",
    "        return None\n",
    "    return (price - wac) / price\n",
    "\n",
    "def get_market_tiers(row):\n",
    "    \"\"\"Get sorted list of market price tiers.\"\"\"\n",
    "    tiers = []\n",
    "    for col in ['minimum', 'percentile_25', 'percentile_50', 'percentile_75', 'maximum']:\n",
    "        val = row.get(col)\n",
    "        if pd.notna(val) and val > 0:\n",
    "            tiers.append(val)\n",
    "    return sorted(set(tiers))\n",
    "\n",
    "def get_margin_tiers(row):\n",
    "    \"\"\"Get sorted list of margin-based price tiers (converted to prices).\"\"\"\n",
    "    tiers = []\n",
    "    wac = row.get('wac_p', 0)\n",
    "    if wac <= 0:\n",
    "        return tiers\n",
    "    \n",
    "    for tier_col in ['margin_tier_below','margin_tier_1', 'margin_tier_2', 'margin_tier_3', \n",
    "                     'margin_tier_4', 'margin_tier_5', 'margin_tier_above_1', 'margin_tier_above_2']:\n",
    "        margin = row.get(tier_col)\n",
    "        if pd.notna(margin) and 0 < margin < 1:\n",
    "            price = wac / (1 - margin)\n",
    "            tiers.append(round(price, 2))\n",
    "    return sorted(set(tiers))\n",
    "\n",
    "def find_next_price_above(current_price, row):\n",
    "    \"\"\"\n",
    "    Find the first price tier ABOVE current_price by at least MIN_PRICE_CHANGE_EGP.\n",
    "    Market first, then margin. Skips tiers less than 0.25 EGP above.\n",
    "    \"\"\"\n",
    "    current_price = float(current_price) if current_price else 0\n",
    "    if pd.isna(current_price) or current_price <= 0:\n",
    "        return current_price\n",
    "    \n",
    "    for tier in get_market_tiers(row):\n",
    "        if tier > current_price + MIN_PRICE_CHANGE_EGP:\n",
    "            return round(tier, 2)\n",
    "    \n",
    "    for tier in get_margin_tiers(row):\n",
    "        if tier > current_price + MIN_PRICE_CHANGE_EGP:\n",
    "            return round(tier, 2)\n",
    "    \n",
    "    return current_price\n",
    "\n",
    "def find_next_price_below(current_price, row):\n",
    "    \"\"\"\n",
    "    Find the first price tier BELOW current_price by at least MIN_PRICE_CHANGE_EGP.\n",
    "    Market first, then margin. Skips tiers less than 0.25 EGP below.\n",
    "    \"\"\"\n",
    "    current_price = float(current_price) if current_price else 0\n",
    "    if pd.isna(current_price) or current_price <= 0:\n",
    "        return current_price\n",
    "    \n",
    "    for tier in reversed(get_market_tiers(row)):\n",
    "        if tier < current_price - MIN_PRICE_CHANGE_EGP:\n",
    "            return round(tier, 2)\n",
    "    \n",
    "    for tier in reversed(get_margin_tiers(row)):\n",
    "        if tier < current_price - MIN_PRICE_CHANGE_EGP:\n",
    "            return round(tier, 2)\n",
    "    \n",
    "    return current_price\n",
    "\n",
    "def find_price_n_steps_below(current_price, n_steps, row):\n",
    "    \"\"\"Find price N steps below current (iteratively find next tier below).\"\"\"\n",
    "    price = current_price\n",
    "    for _ in range(n_steps):\n",
    "        next_price = find_next_price_below(price, row)\n",
    "        if next_price >= price:  # No tier below found\n",
    "            break\n",
    "        price = next_price\n",
    "    return price\n",
    "\n",
    "def is_cart_too_open(row):\n",
    "    \"\"\"Check if cart rule is too open: > normal_refill + 10*std\"\"\"\n",
    "    normal_refill = float(row.get('normal_refill', 5) or 5)\n",
    "    stddev = float(row.get('refill_stddev', 2) or 2)\n",
    "    current_cart = float(row.get('cart_rule', normal_refill) or normal_refill)\n",
    "    threshold = normal_refill + (10 * stddev)\n",
    "    return current_cart > threshold\n",
    "\n",
    "def adjust_cart_rule(current_cart, direction, row):\n",
    "    \"\"\"Adjust cart rule by 20% up or down.\"\"\"\n",
    "    normal_refill = float(row.get('normal_refill', 5) or 5)\n",
    "    current_cart = float(current_cart or 5)\n",
    "    \n",
    "    if direction == 'increase':\n",
    "        new_cart = current_cart * (1 + CART_INCREASE_PCT)\n",
    "        new_cart = min(new_cart, MAX_CART_RULE)\n",
    "    else:  # decrease\n",
    "        new_cart = current_cart * (1 - CART_DECREASE_PCT)\n",
    "        new_cart = max(new_cart, normal_refill, MIN_CART_RULE)\n",
    "    \n",
    "    return int(new_cart)\n",
    "\n",
    "def get_highest_qd_tier_contribution(row):\n",
    "    \"\"\"Find which QD tier has highest contribution.\"\"\"\n",
    "    t1 = row.get('yesterday_t1_cntrb', 0) or 0\n",
    "    t2 = row.get('yesterday_t2_cntrb', 0) or 0\n",
    "    t3 = row.get('yesterday_t3_cntrb', 0) or 0\n",
    "    \n",
    "    if t1 >= t2 and t1 >= t3 and t1 > 0:\n",
    "        return 'T1', t1\n",
    "    elif t2 >= t1 and t2 >= t3 and t2 > 0:\n",
    "        return 'T2', t2\n",
    "    elif t3 > 0:\n",
    "        return 'T3', t3\n",
    "    return None, 0\n",
    "\n",
    "print(\"Helper functions loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main engine function loaded.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MAIN ENGINE: GENERATE PERIODIC ACTION\n",
    "# =============================================================================\n",
    "\n",
    "def generate_periodic_action(row, previous_df):\n",
    "    \"\"\"\n",
    "    Generate periodic action based on UTH performance.\n",
    "    \n",
    "    Logic:\n",
    "    - Zero Demand: 1 step below current + SKU discount\n",
    "    - On Track: No action\n",
    "    - Growing: Deactivate discounts or increase price, reduce cart if too open\n",
    "    - Dropping: Based on qty_ratio vs retailer_ratio:\n",
    "        - qty OK, retailers dropping: SKU discount (then price if already has)\n",
    "        - qty dropping, retailers OK: QD (then price if already has)\n",
    "        - both dropping: SKU discount (then price if already has)\n",
    "    - Price reduction max 2x per day\n",
    "    \"\"\"\n",
    "    product_id = row.get('product_id')\n",
    "    warehouse_id = row.get('warehouse_id')\n",
    "    \n",
    "    result = {\n",
    "        'product_id': product_id,\n",
    "        'warehouse_id': warehouse_id,\n",
    "        'cohort_id': row.get('cohort_id'),\n",
    "        'sku': row.get('sku'),\n",
    "        'brand': row.get('brand'),\n",
    "        'cat': row.get('cat'),\n",
    "        'stocks': row.get('stocks', 0),\n",
    "        'current_price': row.get('current_price'),\n",
    "        'wac_p': row.get('wac_p'),\n",
    "        'uth_qty': row.get('uth_qty', 0),\n",
    "        'uth_retailers': row.get('uth_retailers', 0),\n",
    "        'p80_daily_240d': row.get('p80_daily_240d', 1),\n",
    "        'p70_daily_retailers_240d': row.get('p70_daily_retailers_240d', 1),\n",
    "        'avg_uth_pct': row.get('avg_uth_pct', 0.5),\n",
    "        # Today's UTH discount contributions\n",
    "        'sku_disc_cntrb_uth': row.get('sku_disc_cntrb_uth', 0) or 0,\n",
    "        't1_cntrb_uth': row.get('t1_cntrb_uth', 0) or 0,\n",
    "        't2_cntrb_uth': row.get('t2_cntrb_uth', 0) or 0,\n",
    "        't3_cntrb_uth': row.get('t3_cntrb_uth', 0) or 0,\n",
    "        'uth_status': None,\n",
    "        'qty_ratio': None,\n",
    "        'retailer_ratio': None,\n",
    "        'new_price': None,\n",
    "        'price_action': None,\n",
    "        'current_cart_rule':row.get('current_cart_rule'),\n",
    "        'new_cart_rule': None,\n",
    "        'activate_sku_discount': False,  # True = SKU should have discount after this run\n",
    "        'activate_qd': False,             # True = SKU should have QD after this run\n",
    "        'keep_qd_tiers': None,            # List of QD tiers to keep (e.g., ['T1', 'T2'])\n",
    "        # QD tier configuration (passed to qd_handler)\n",
    "        'qd_tier_1_qty': row.get('qd_tier_1_qty', 0) or 0,\n",
    "        'qd_tier_1_disc_pct': row.get('qd_tier_1_disc_pct', 0) or 0,\n",
    "        'qd_tier_2_qty': row.get('qd_tier_2_qty', 0) or 0,\n",
    "        'qd_tier_2_disc_pct': row.get('qd_tier_2_disc_pct', 0) or 0,\n",
    "        'qd_tier_3_qty': row.get('qd_tier_3_qty', 0) or 0,\n",
    "        'qd_tier_3_disc_pct': row.get('qd_tier_3_disc_pct', 0) or 0,\n",
    "        'removed_discount': None,         # Which discount was removed (for Growing)\n",
    "        'removed_discount_cntrb': 0,      # Contribution of removed discount\n",
    "        'price_reductions_today': 0,\n",
    "        'action_reason': None,\n",
    "    }\n",
    "    \n",
    "    # Skip if OOS (price only in Module 2)\n",
    "    if row.get('stocks', 0) <= 0:\n",
    "        result['action_reason'] = 'OOS - skip (price only in Module 2)'\n",
    "        return result\n",
    "    \n",
    "    # Count previous price reductions today\n",
    "    price_reductions_today = count_price_reductions_today(product_id, warehouse_id, previous_df)\n",
    "    result['price_reductions_today'] = price_reductions_today\n",
    "    can_reduce_price = price_reductions_today < MAX_PRICE_REDUCTIONS_PER_DAY\n",
    "    \n",
    "    # Calculate UTH benchmark: historical_pct * P80_qty\n",
    "    # Convert to float to handle decimal.Decimal from Snowflake\n",
    "    p80_qty = float(row.get('p80_daily_240d', 1) or 1)\n",
    "    p70_retailers = float(row.get('p70_daily_retailers_240d', 1) or 1)\n",
    "    avg_uth_pct = float(row.get('avg_uth_pct', 0.5) or 0.5)\n",
    "    \n",
    "    uth_qty_target = p80_qty * avg_uth_pct\n",
    "    uth_retailer_target = p70_retailers * avg_uth_pct\n",
    "    \n",
    "    uth_qty = float(row.get('uth_qty', 0) or 0)\n",
    "    uth_retailers = float(row.get('uth_retailers', 0) or 0)\n",
    "    \n",
    "    # Calculate UTH ratios\n",
    "    qty_ratio = uth_qty / uth_qty_target if uth_qty_target > 0 else 0\n",
    "    retailer_ratio = uth_retailers / uth_retailer_target if uth_retailer_target > 0 else 0\n",
    "    \n",
    "    result['uth_qty_target'] = round(uth_qty_target, 2)\n",
    "    result['uth_retailer_target'] = round(uth_retailer_target, 2)\n",
    "    result['qty_ratio'] = round(qty_ratio, 2)\n",
    "    result['retailer_ratio'] = round(retailer_ratio, 2)\n",
    "    \n",
    "    current_price = float(row.get('current_price') or 0)\n",
    "    current_cart = float(row.get('current_cart_rule', row.get('normal_refill', 10)) or 10)\n",
    "    has_sku_disc = row.get('has_active_sku_discount', 0) == 1\n",
    "    has_qd = row.get('has_active_qd', 0) == 1\n",
    "    \n",
    "    # Determine if qty/retailers are dropping (below threshold)\n",
    "    qty_dropping = qty_ratio < UTH_DROPPING_THRESHOLD\n",
    "    qty_ok = qty_ratio >= UTH_DROPPING_THRESHOLD\n",
    "    retailer_dropping = retailer_ratio < UTH_DROPPING_THRESHOLD\n",
    "    retailer_ok = retailer_ratio >= UTH_DROPPING_THRESHOLD\n",
    "    \n",
    "    # =========================================================================\n",
    "    # CASE 1: Zero demand today - 1 step below + SKU discount + open cart if tight\n",
    "    # =========================================================================\n",
    "    if uth_qty == 0:\n",
    "        new_price = find_next_price_below(current_price, row)\n",
    "        \n",
    "        # Apply commercial minimum floor\n",
    "        commercial_min = float(row.get('commercial_min_price', row.get('minimum', 0)) or 0)\n",
    "        if pd.notna(commercial_min) and commercial_min > 0:\n",
    "            new_price = max(new_price, commercial_min)\n",
    "        \n",
    "        result['new_price'] = new_price\n",
    "        result['activate_sku_discount'] = True\n",
    "        result['uth_status'] = 'Zero Demand'\n",
    "        result['price_action'] = 'zero_demand_decrease'\n",
    "        \n",
    "        # Check if cart rule is tight (< normal_refill + 10*std) and increase if so\n",
    "        normal_refill = float(row.get('normal_refill', 5) or 5)\n",
    "        stddev = float(row.get('refill_stddev', 2) or 2)\n",
    "        cart_threshold = normal_refill + (10 * stddev)\n",
    "        \n",
    "        if current_cart < cart_threshold:\n",
    "            new_cart = min(cart_threshold, MAX_CART_RULE)\n",
    "            new_cart = max(new_cart, MIN_CART_RULE)\n",
    "            result['new_cart_rule'] = int(new_cart)\n",
    "            result['action_reason'] = f'Zero demand - 1 step below ({current_price:.2f} -> {new_price:.2f}) + SKU discount + open cart to {int(new_cart)}'\n",
    "        else:\n",
    "            result['action_reason'] = f'Zero demand - 1 step below ({current_price:.2f} -> {new_price:.2f}) + SKU discount'\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    # =========================================================================\n",
    "    # CASE 2: On Track (both qty and retailers ±10%)\n",
    "    # If has existing discounts, keep them (they'll be deactivated otherwise)\n",
    "    # =========================================================================\n",
    "    if (UTH_DROPPING_THRESHOLD <= qty_ratio <= UTH_GROWING_THRESHOLD and\n",
    "        UTH_DROPPING_THRESHOLD <= retailer_ratio <= UTH_GROWING_THRESHOLD):\n",
    "        result['uth_status'] = 'On Track'\n",
    "        result['price_action'] = 'hold'\n",
    "        \n",
    "        # Preserve existing discounts (all discounts are deactivated at start of each run)\n",
    "        if has_sku_disc:\n",
    "            result['activate_sku_discount'] = True\n",
    "            result['action_reason'] = f'On Track (qty={qty_ratio:.2f}, ret={retailer_ratio:.2f}) - keep existing SKU discount'\n",
    "        elif has_qd:\n",
    "            result['activate_qd'] = True\n",
    "            result['action_reason'] = f'On Track (qty={qty_ratio:.2f}, ret={retailer_ratio:.2f}) - keep existing QD'\n",
    "        else:\n",
    "            result['action_reason'] = f'On Track (qty={qty_ratio:.2f}, ret={retailer_ratio:.2f}) - no action'\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    # =========================================================================\n",
    "    # CASE 3: Growing (qty > 110%)\n",
    "    # Find discount with HIGHEST contribution (from TODAY's UTH) and remove it\n",
    "    # Keep (re-activate) the others\n",
    "    # If no discounts -> increase price\n",
    "    # =========================================================================\n",
    "    if qty_ratio > UTH_GROWING_THRESHOLD:\n",
    "        result['uth_status'] = 'Growing'\n",
    "        \n",
    "        # Get TODAY's UTH discount contributions (not yesterday's)\n",
    "        sku_disc_cntrb = row.get('sku_disc_cntrb_uth', 0) or 0\n",
    "        t1_cntrb = row.get('t1_cntrb_uth', 0) or 0\n",
    "        t2_cntrb = row.get('t2_cntrb_uth', 0) or 0\n",
    "        t3_cntrb = row.get('t3_cntrb_uth', 0) or 0\n",
    "        \n",
    "        # Build list of EXISTING discounts with their contributions\n",
    "        # Note: We check if tiers EXIST (qty > 0), not just if they had sales today\n",
    "        # A tier can exist but have 0 contribution if no orders used it yet today\n",
    "        active_discounts = []\n",
    "        \n",
    "        # SKU discount: check if it exists (has_sku_disc from active discount query)\n",
    "        if has_sku_disc:\n",
    "            active_discounts.append(('sku_disc', sku_disc_cntrb))  # Include even if cntrb=0\n",
    "        \n",
    "        # QD tiers: check if each tier EXISTS (qty > 0 means the tier is configured)\n",
    "        if has_qd:\n",
    "            qd_t1_qty = row.get('qd_tier_1_qty', 0) or 0\n",
    "            qd_t2_qty = row.get('qd_tier_2_qty', 0) or 0\n",
    "            qd_t3_qty = row.get('qd_tier_3_qty', 0) or 0\n",
    "            \n",
    "            if qd_t1_qty > 0:  # Tier 1 exists\n",
    "                active_discounts.append(('qd_t1', t1_cntrb))  # Include even if cntrb=0\n",
    "            if qd_t2_qty > 0:  # Tier 2 exists\n",
    "                active_discounts.append(('qd_t2', t2_cntrb))  # Include even if cntrb=0\n",
    "            if qd_t3_qty > 0:  # Tier 3 exists\n",
    "                active_discounts.append(('qd_t3', t3_cntrb))  # Include even if cntrb=0\n",
    "        \n",
    "        if active_discounts:\n",
    "            # Sort by contribution descending - remove the highest\n",
    "            active_discounts.sort(key=lambda x: x[1], reverse=True)\n",
    "            highest_disc, highest_cntrb = active_discounts[0]\n",
    "            remaining_discounts = [d[0] for d in active_discounts[1:]]\n",
    "            \n",
    "            # Determine what to keep (re-activate)\n",
    "            keep_sku_disc = 'sku_disc' in remaining_discounts\n",
    "            keep_qd_t1 = 'qd_t1' in remaining_discounts\n",
    "            keep_qd_t2 = 'qd_t2' in remaining_discounts\n",
    "            keep_qd_t3 = 'qd_t3' in remaining_discounts\n",
    "            keep_any_qd = keep_qd_t1 or keep_qd_t2 or keep_qd_t3\n",
    "            \n",
    "            # Set activation flags\n",
    "            if keep_sku_disc:\n",
    "                result['activate_sku_discount'] = True\n",
    "            \n",
    "            if keep_any_qd:\n",
    "                result['activate_qd'] = True\n",
    "                result['keep_qd_tiers'] = [t for t in ['T1', 'T2', 'T3'] \n",
    "                                           if (t == 'T1' and keep_qd_t1) or \n",
    "                                              (t == 'T2' and keep_qd_t2) or \n",
    "                                              (t == 'T3' and keep_qd_t3)]\n",
    "            \n",
    "            result['removed_discount'] = highest_disc\n",
    "            result['removed_discount_cntrb'] = highest_cntrb\n",
    "            result['price_action'] = f'remove_{highest_disc}'\n",
    "            result['action_reason'] = f'Growing (qty={qty_ratio:.2f}) - remove {highest_disc} (cntrb={highest_cntrb}%)'\n",
    "            \n",
    "            if remaining_discounts:\n",
    "                result['action_reason'] += f', keep {remaining_discounts}'\n",
    "        \n",
    "        elif has_sku_disc or has_qd:\n",
    "            # Has discounts but no contribution data - remove all\n",
    "            result['price_action'] = 'remove_all_disc'\n",
    "            result['action_reason'] = f'Growing (qty={qty_ratio:.2f}) - remove all discounts (no contribution data)'\n",
    "        \n",
    "        else:\n",
    "            # No discounts - increase price\n",
    "            new_price = find_next_price_above(current_price, row)\n",
    "            if new_price > current_price:\n",
    "                result['new_price'] = new_price\n",
    "                result['price_action'] = 'increase'\n",
    "                result['action_reason'] = f'Growing (qty={qty_ratio:.2f}) - increase ({current_price:.2f} -> {new_price:.2f})'\n",
    "            else:\n",
    "                result['price_action'] = 'hold'\n",
    "                result['action_reason'] = f'Growing (qty={qty_ratio:.2f}) - no tier above, hold'\n",
    "        \n",
    "        # Check if cart too open\n",
    "        if is_cart_too_open(row):\n",
    "            result['new_cart_rule'] = adjust_cart_rule(current_cart, 'decrease', row)\n",
    "            result['action_reason'] += ' + reduce cart 20%'\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    # =========================================================================\n",
    "    # CASE 4: Dropping - Different actions based on qty vs retailer ratios\n",
    "    # =========================================================================\n",
    "    result['uth_status'] = 'Dropping'\n",
    "    \n",
    "    def apply_price_reduction():\n",
    "        \"\"\"Helper to apply price reduction if allowed.\"\"\"\n",
    "        if not can_reduce_price:\n",
    "            return None, f'Price reduction limit reached ({price_reductions_today}/{MAX_PRICE_REDUCTIONS_PER_DAY} today)'\n",
    "        \n",
    "        new_price = find_next_price_below(current_price, row)\n",
    "        if new_price < current_price:\n",
    "            commercial_min = float(row.get('commercial_min_price', row.get('minimum', 0)) or 0)\n",
    "            if pd.notna(commercial_min) and commercial_min > 0:\n",
    "                new_price = max(new_price, commercial_min)\n",
    "            return new_price, f'decrease ({current_price:.2f} -> {new_price:.2f})'\n",
    "        return None, 'no tier below'\n",
    "    \n",
    "    # CASE 4A: qty OK (≥90%) but retailers dropping (<90%)\n",
    "    # Action: SKU discount (add new OR keep existing), then price if already has\n",
    "    if qty_ok and retailer_dropping:\n",
    "        # Always set activate_sku_discount = True (either adding new or keeping existing)\n",
    "        result['activate_sku_discount'] = True\n",
    "        \n",
    "        if not has_sku_disc:\n",
    "            # Adding new SKU discount\n",
    "            result['price_action'] = 'add_sku_disc'\n",
    "            result['action_reason'] = f'Retailers dropping (ret={retailer_ratio:.2f}, qty OK) - ADD new SKU discount'\n",
    "        else:\n",
    "            # Keeping existing SKU discount + reduce price\n",
    "            new_price, reason = apply_price_reduction()\n",
    "            if new_price:\n",
    "                result['new_price'] = new_price\n",
    "                result['price_action'] = 'keep_sku_disc_and_decrease'\n",
    "                result['action_reason'] = f'Retailers dropping - KEEP SKU disc + {reason}'\n",
    "            else:\n",
    "                result['price_action'] = 'keep_sku_disc'\n",
    "                result['action_reason'] = f'Retailers dropping - KEEP SKU disc ({reason})'\n",
    "    \n",
    "    # CASE 4B: qty dropping (<90%) but retailers OK (≥90%)\n",
    "    # Action: QD (add new OR keep existing), then price if already has\n",
    "    elif qty_dropping and retailer_ok:\n",
    "        # Always set activate_qd = True (either adding new or keeping existing)\n",
    "        result['activate_qd'] = True\n",
    "        \n",
    "        if not has_qd:\n",
    "            # Adding new QD\n",
    "            result['price_action'] = 'add_qd'\n",
    "            result['action_reason'] = f'Qty dropping (qty={qty_ratio:.2f}, ret OK) - ADD new QD'\n",
    "        else:\n",
    "            # Keeping existing QD + reduce price\n",
    "            new_price, reason = apply_price_reduction()\n",
    "            if new_price:\n",
    "                result['new_price'] = new_price\n",
    "                result['price_action'] = 'keep_qd_and_decrease'\n",
    "                result['action_reason'] = f'Qty dropping - KEEP QD + {reason}'\n",
    "            else:\n",
    "                result['price_action'] = 'keep_qd'\n",
    "                result['action_reason'] = f'Qty dropping - KEEP QD ({reason})'\n",
    "    \n",
    "    # CASE 4C: Both dropping (<90%)\n",
    "    # Action: SKU discount (add new OR keep existing), then price if already has\n",
    "    elif qty_dropping and retailer_dropping:\n",
    "        # Always set activate_sku_discount = True (either adding new or keeping existing)\n",
    "        result['activate_sku_discount'] = True\n",
    "        \n",
    "        if not has_sku_disc:\n",
    "            # Adding new SKU discount\n",
    "            result['price_action'] = 'add_sku_disc'\n",
    "            result['action_reason'] = f'Both dropping (qty={qty_ratio:.2f}, ret={retailer_ratio:.2f}) - ADD new SKU discount'\n",
    "        else:\n",
    "            # Keeping existing SKU discount + reduce price\n",
    "            new_price, reason = apply_price_reduction()\n",
    "            if new_price:\n",
    "                result['new_price'] = new_price\n",
    "                result['price_action'] = 'keep_sku_disc_and_decrease'\n",
    "                result['action_reason'] = f'Both dropping - KEEP SKU disc + {reason}'\n",
    "            else:\n",
    "                result['price_action'] = 'keep_sku_disc'\n",
    "                result['action_reason'] = f'Both dropping - KEEP SKU disc ({reason})'\n",
    "    \n",
    "    else:\n",
    "        result['price_action'] = 'hold'\n",
    "        result['action_reason'] = f'Unexpected state (qty={qty_ratio:.2f}, ret={retailer_ratio:.2f})'\n",
    "    \n",
    "    # Increase cart for dropping SKUs\n",
    "    result['new_cart_rule'] = adjust_cart_rule(current_cart, 'increase', row)\n",
    "    result['action_reason'] += ' + increase cart 20%'\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"Main engine function loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 28444 SKUs...\n",
      "============================================================\n",
      "Processed 10000/28444 SKUs...\n",
      "Processed 20000/28444 SKUs...\n",
      "\n",
      "✅ Processed 28444 SKUs\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXECUTE MODULE 3\n",
    "# =============================================================================\n",
    "print(f\"Processing {len(df)} SKUs...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = []\n",
    "for idx, row in df.iterrows():\n",
    "    result = generate_periodic_action(row, df_previous_actions)\n",
    "    results.append(result)\n",
    "    \n",
    "    if (idx + 1) % 10000 == 0:\n",
    "        print(f\"Processed {idx + 1}/{len(df)} SKUs...\")\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(f\"\\n✅ Processed {len(df_results)} SKUs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODULE 3 SUMMARY\n",
      "============================================================\n",
      "\n",
      "Total SKUs: 28444\n",
      "\n",
      "By UTH Status:\n",
      "uth_status\n",
      "Zero Demand    13154\n",
      "None            7793\n",
      "Dropping        4933\n",
      "Growing         2513\n",
      "On Track          51\n",
      "\n",
      "Actions:\n",
      "  Price changes: 16668\n",
      "  Cart rule changes: 12002\n",
      "  SKU discounts to activate: 16538\n",
      "  QD to activate: 1748\n",
      "  Discounts removed (Growing SKUs): 1868\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(\"=\"*60)\n",
    "print(\"MODULE 3 SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nTotal SKUs: {len(df_results)}\")\n",
    "\n",
    "print(f\"\\nBy UTH Status:\")\n",
    "print(df_results['uth_status'].value_counts(dropna=False).to_string())\n",
    "\n",
    "# Actions breakdown\n",
    "price_changes = df_results[df_results['new_price'].notna()]\n",
    "cart_changes = df_results[df_results['new_cart_rule'].notna()]\n",
    "sku_disc_activate = df_results[df_results['activate_sku_discount'] == True]\n",
    "qd_activate = df_results[df_results['activate_qd'] == True]\n",
    "discounts_removed = df_results[df_results['removed_discount'].notna()]\n",
    "\n",
    "print(f\"\\nActions:\")\n",
    "print(f\"  Price changes: {len(price_changes)}\")\n",
    "print(f\"  Cart rule changes: {len(cart_changes)}\")\n",
    "print(f\"  SKU discounts to activate: {len(sku_disc_activate)}\")\n",
    "print(f\"  QD to activate: {len(qd_activate)}\")\n",
    "print(f\"  Discounts removed (Growing SKUs): {len(discounts_removed)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Results exported to: module_3_output_20260124_2047.xlsx\n",
      "Total records: 28377 (after removing 67 duplicates)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXPORT RESULTS\n",
    "# =============================================================================\n",
    "output_cols = [\n",
    "    'product_id', 'warehouse_id', 'cohort_id', 'sku', 'brand', 'cat', 'stocks',\n",
    "    'current_price', 'wac_p', 'uth_qty', 'uth_retailers',\n",
    "    'p80_daily_240d', 'p70_daily_retailers_240d', 'avg_uth_pct',\n",
    "    'sku_disc_cntrb_uth', 't1_cntrb_uth', 't2_cntrb_uth', 't3_cntrb_uth',\n",
    "    'uth_qty_target', 'uth_retailer_target', 'qty_ratio', 'retailer_ratio', 'uth_status',\n",
    "    'new_price', 'price_action', 'current_cart_rule','new_cart_rule',\n",
    "    # SKU Discount fields\n",
    "    'activate_sku_discount',\n",
    "    # QD fields (for qd_handler)\n",
    "    'activate_qd', 'keep_qd_tiers',\n",
    "    'qd_tier_1_qty', 'qd_tier_1_disc_pct',\n",
    "    'qd_tier_2_qty', 'qd_tier_2_disc_pct',\n",
    "    'qd_tier_3_qty', 'qd_tier_3_disc_pct',\n",
    "    # Action tracking\n",
    "    'removed_discount', 'removed_discount_cntrb',\n",
    "    'price_reductions_today', 'action_reason'\n",
    "]\n",
    "\n",
    "# Filter to existing columns\n",
    "output_cols = [c for c in output_cols if c in df_results.columns]\n",
    "\n",
    "# Drop duplicates before saving\n",
    "df_output = df_results[output_cols].drop_duplicates(subset=['product_id', 'warehouse_id'], keep='first')\n",
    "df_output.to_excel(OUTPUT_FILE, index=False)\n",
    "print(f\"\\n✅ Results exported to: {OUTPUT_FILE}\")\n",
    "print(f\"Total records: {len(df_output)} (after removing {len(df_results) - len(df_output)} duplicates)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Push Cart Rules Handler loaded at 2026-01-24 20:49:12 Cairo time\n",
      "✓ API credentials loaded successfully\n",
      "Push Prices Handler loaded at 2026-01-24 20:49:12 Cairo time\n",
      "✓ API credentials loaded successfully\n",
      "✓ Google Sheets client initialized\n",
      "Fetching packing_units ...\n",
      "  Loaded 34786 records\n",
      "\n",
      "======================================================================\n",
      "STEP 1: PUSHING CART RULES\n",
      "======================================================================\n",
      "\n",
      "🧪 MODE: TESTING\n",
      "   Files will be prepared but NOT uploaded to API\n",
      "\n",
      "============================================================\n",
      "PUSH CART RULES - Source: module_3\n",
      "============================================================\n",
      "Total received: 28377\n",
      "Cart rule changes to push: 11579\n",
      "Skipped (no change): 16798\n",
      "\n",
      "Cart rule changes summary:\n",
      "  Increases: 10253\n",
      "  Decreases: 1326\n",
      "\n",
      "📋 Prepared 15468 packing unit cart rules\n",
      "\n",
      "Sample cart rule adjustments (showing products with multiple PUs):\n",
      " product_id  basic_unit_count  final_cart_rule  final_pu_cart_rule\n",
      "          3                 1               12                  12\n",
      "          3                 1               12                  12\n",
      "          3                 1               12                  12\n",
      "          3                 1              150                 150\n",
      "          3                 1              150                 150\n",
      "          3                 1              150                 150\n",
      "          9                 1               45                  45\n",
      "          9                 1                6                   6\n",
      "         10                 1              142                 142\n",
      "         10                 1              141                 141\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 701\n",
      "==================================================\n",
      "  Saved: uploads/module_3_cart_rules_701.xlsx (2841 rows)\n",
      "  🧪 [TESTING] Would upload 2841 cart rules (skipped)\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 702\n",
      "==================================================\n",
      "  Saved: uploads/module_3_cart_rules_702.xlsx (1531 rows)\n",
      "  🧪 [TESTING] Would upload 1531 cart rules (skipped)\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 703\n",
      "==================================================\n",
      "  Saved: uploads/module_3_cart_rules_703.xlsx (2187 rows)\n",
      "  🧪 [TESTING] Would upload 2187 cart rules (skipped)\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 704\n",
      "==================================================\n",
      "  Saved: uploads/module_3_cart_rules_704.xlsx (2246 rows)\n",
      "  🧪 [TESTING] Would upload 2246 cart rules (skipped)\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1124\n",
      "==================================================\n",
      "  Saved: uploads/module_3_cart_rules_1124.xlsx (1116 rows)\n",
      "  🧪 [TESTING] Would upload 1116 cart rules (skipped)\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1125\n",
      "==================================================\n",
      "  Saved: uploads/module_3_cart_rules_1125.xlsx (991 rows)\n",
      "  🧪 [TESTING] Would upload 991 cart rules (skipped)\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 700\n",
      "==================================================\n",
      "  Saved: uploads/module_3_cart_rules_700.xlsx (2249 rows)\n",
      "  🧪 [TESTING] Would upload 2249 cart rules (skipped)\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1123\n",
      "==================================================\n",
      "  Saved: uploads/module_3_cart_rules_1123.xlsx (1193 rows)\n",
      "  🧪 [TESTING] Would upload 1193 cart rules (skipped)\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1126\n",
      "==================================================\n",
      "  Saved: uploads/module_3_cart_rules_1126.xlsx (1114 rows)\n",
      "  🧪 [TESTING] Would upload 1114 cart rules (skipped)\n",
      "\n",
      "============================================================\n",
      "🧪 TESTING MODE COMPLETE - NO CART RULES WERE UPLOADED\n",
      "============================================================\n",
      "Mode: testing\n",
      "Total prepared: 15468\n",
      "\n",
      "============================================================\n",
      "CART RULES RESULT\n",
      "============================================================\n",
      "Mode: testing\n",
      "Cart rule changes: 11579\n",
      "Pushed: 15468\n",
      "Failed: 0\n",
      "\n",
      "======================================================================\n",
      "STEP 2: PUSHING PRICES\n",
      "======================================================================\n",
      "\n",
      "🧪 MODE: TESTING\n",
      "   Files will be prepared but NOT uploaded to API\n",
      "Loading disable_pu_visibility from Google Sheets...\n",
      "  ✓ Loaded 89 products to disable min PU visibility\n",
      "\n",
      "============================================================\n",
      "PUSH PRICES - Source: module_3\n",
      "============================================================\n",
      "Total received: 28377\n",
      "Price changes to push: 15530\n",
      "Skipped (no change): 12847\n",
      "\n",
      "Price changes summary:\n",
      "  Increases: 555\n",
      "  Decreases: 14975\n",
      "\n",
      "📋 Prepared 18696 packing unit prices\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 703\n",
      "==================================================\n",
      "  Saved: uploads/module_3_703.xlsx (2659 rows)\n",
      "  🧪 [TESTING] Would upload 2659 prices (skipped)\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 704\n",
      "==================================================\n",
      "  Saved: uploads/module_3_704.xlsx (2718 rows)\n",
      "  🧪 [TESTING] Would upload 2718 prices (skipped)\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 701\n",
      "==================================================\n",
      "  Saved: uploads/module_3_701.xlsx (3081 rows)\n",
      "  🧪 [TESTING] Would upload 3081 prices (skipped)\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1125\n",
      "==================================================\n",
      "  Saved: uploads/module_3_1125.xlsx (1424 rows)\n",
      "  🧪 [TESTING] Would upload 1424 prices (skipped)\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1123\n",
      "==================================================\n",
      "  Saved: uploads/module_3_1123.xlsx (1525 rows)\n",
      "  🧪 [TESTING] Would upload 1525 prices (skipped)\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1124\n",
      "==================================================\n",
      "  Saved: uploads/module_3_1124.xlsx (1455 rows)\n",
      "  🧪 [TESTING] Would upload 1455 prices (skipped)\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1126\n",
      "==================================================\n",
      "  Saved: uploads/module_3_1126.xlsx (1597 rows)\n",
      "  🧪 [TESTING] Would upload 1597 prices (skipped)\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 702\n",
      "==================================================\n",
      "  Saved: uploads/module_3_702.xlsx (1942 rows)\n",
      "  🧪 [TESTING] Would upload 1942 prices (skipped)\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 700\n",
      "==================================================\n",
      "  Saved: uploads/module_3_700.xlsx (2295 rows)\n",
      "  🧪 [TESTING] Would upload 2295 prices (skipped)\n",
      "\n",
      "============================================================\n",
      "🧪 TESTING MODE COMPLETE - NO PRICES WERE UPLOADED\n",
      "============================================================\n",
      "Mode: testing\n",
      "Total prepared: 18696\n",
      "\n",
      "============================================================\n",
      "PRICES RESULT\n",
      "============================================================\n",
      "Mode: testing\n",
      "Source: module_3\n",
      "Timestamp: 2026-01-24 20:49:34\n",
      "Total received: 28377\n",
      "Price changes: 15530\n",
      "Pushed: 18696\n",
      "Skipped: 12847\n",
      "Failed: 0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PUSH CART RULES & PRICES\n",
    "# =============================================================================\n",
    "# Push cart rules FIRST, then prices\n",
    "# If cart rules fail for certain cohorts, skip those cohorts for prices\n",
    "\n",
    "%run push_cart_rules_handler.ipynb\n",
    "%run push_prices_handler.ipynb\n",
    "pus = get_packing_units()\n",
    "\n",
    "# ⚠️ MODE CONFIGURATION:\n",
    "# - 'testing' (default): Prepare files but DON'T upload to API\n",
    "# - 'live': Prepare files AND upload to MaxAB API\n",
    "PUSH_MODE = 'testing'  # Change to 'live' when ready to push\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: Push Cart Rules First\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 1: PUSHING CART RULES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "cart_result = push_cart_rules(df_output, pus, source_module='module_3', mode=PUSH_MODE)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CART RULES RESULT\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Mode: {cart_result['mode']}\")\n",
    "print(f\"Cart rule changes: {cart_result['cart_rule_changes']}\")\n",
    "print(f\"Pushed: {cart_result['pushed']}\")\n",
    "print(f\"Failed: {cart_result['failed']}\")\n",
    "if cart_result['failed_cohorts']:\n",
    "    print(f\"⚠️ Failed cohorts: {cart_result['failed_cohorts']}\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: Push Prices (skip failed cohorts)\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: PUSHING PRICES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get failed cohorts from cart rules to skip in price push\n",
    "failed_cohorts = cart_result.get('failed_cohorts', [])\n",
    "\n",
    "# Call push_prices with the results, skipping failed cohorts\n",
    "push_result = push_prices(df_output, pus, source_module='module_3', mode=PUSH_MODE, skip_cohorts=failed_cohorts)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PRICES RESULT\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Mode: {push_result['mode']}\")\n",
    "print(f\"Source: {push_result['source_module']}\")\n",
    "print(f\"Timestamp: {push_result['timestamp']}\")\n",
    "print(f\"Total received: {push_result['total_received']}\")\n",
    "print(f\"Price changes: {push_result['price_changes']}\")\n",
    "print(f\"Pushed: {push_result['pushed']}\")\n",
    "print(f\"Skipped: {push_result['skipped']}\")\n",
    "print(f\"Failed: {push_result['failed']}\")\n",
    "if push_result.get('skipped_cohorts'):\n",
    "    print(f\"⚠️ Skipped cohorts (cart rules failed): {push_result['skipped_cohorts']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 3: PROCESSING SKU DISCOUNTS\n",
      "======================================================================\n",
      "/home/ec2-user/.Renviron\n",
      "/home/ec2-user/service_account_key.json\n",
      "SKU Discount Handler loaded at 2026-01-24 20:49:55 Cairo time\n",
      "AWS & API functions defined ✓\n",
      "✓ API credentials loaded successfully\n",
      "Snowflake timezone: America/Los_Angeles\n",
      "Function 1: deactivate_active_sku_discounts() defined ✓\n",
      "Function 2: select_target_retailers() defined ✓\n",
      "Function 3: calculate_discount() & calculate_discounts_batch() defined ✓\n",
      "Function 4: structure_sku_discount_dataframe() defined ✓\n",
      "Function 5: push_sku_discount() defined ✓\n",
      "Main function: process_sku_discounts() defined ✓\n",
      "\n",
      "============================================================\n",
      "SKU DISCOUNT HANDLER MODULE READY\n",
      "============================================================\n",
      "\n",
      "Required input columns from Module 3:\n",
      "  - product_id, warehouse_id, sku, cohort_id\n",
      "  - activate_sku_discount (bool)\n",
      "  - current_price, wac_p\n",
      "\n",
      "Required market price columns:\n",
      "  - below_market\n",
      "  - market_min\n",
      "  - market_25\n",
      "  - market_50\n",
      "  - market_75\n",
      "  - market_max\n",
      "  - above_market\n",
      "\n",
      "Required margin tier columns:\n",
      "  - margin_tier_below\n",
      "  - margin_tier_1\n",
      "  - margin_tier_2\n",
      "  - margin_tier_3\n",
      "  - margin_tier_4\n",
      "  - margin_tier_5\n",
      "  - margin_tier_above_1\n",
      "  - margin_tier_above_2\n",
      "\n",
      "Usage: result = process_sku_discounts(df_output, mode='testing')\n",
      "SKUs needing SKU discount: 16492\n",
      "\n",
      "======================================================================\n",
      "SKU DISCOUNT HANDLER\n",
      "======================================================================\n",
      "Mode: testing\n",
      "Input records: 16492\n",
      "\n",
      "--------------------------------------------------\n",
      "STEP 1: Deactivating existing SKU discounts\n",
      "--------------------------------------------------\n",
      "🧪 Deactivating SKU discounts (mode: testing)\n",
      "  Querying active SKU discounts from Snowflake...\n",
      "  Found 51143 active SKU discounts to deactivate\n",
      "  🧪 [TESTING] Would deactivate 51143 SKU discounts (skipped)\n",
      "\n",
      "--------------------------------------------------\n",
      "STEP 2: Filtering SKUs for discount\n",
      "--------------------------------------------------\n",
      "SKUs to activate: 16492\n",
      "\n",
      "--------------------------------------------------\n",
      "STEP 3: Selecting target retailers\n",
      "--------------------------------------------------\n",
      "Selecting target retailers with criteria: all\n",
      "\n",
      "--------------------------------------------------\n",
      "STEP 4: Calculating discount percentages\n",
      "--------------------------------------------------\n",
      "Calculating discounts for 16492 SKUs...\n",
      "\n",
      "--------------------------------------------------\n",
      "STEP 5: Structuring data for API\n",
      "--------------------------------------------------\n",
      "Structuring 16492 SKU discounts for API...\n",
      "\n",
      "--------------------------------------------------\n",
      "STEP 6: Pushing to API\n",
      "--------------------------------------------------\n",
      "\n",
      "🧪 MODE: TESTING\n",
      "Pushing 16492 SKU discounts...\n",
      "  🧪 [TESTING] Would push SKU discounts (skipped)\n",
      "\n",
      "======================================================================\n",
      "SUMMARY\n",
      "======================================================================\n",
      "Mode: testing\n",
      "Total input: 16492\n",
      "Discounts deactivated: 51143\n",
      "SKUs to activate: 16492\n",
      "Discounts created: 16492\n",
      "Discounts failed: 0\n",
      "\n",
      "============================================================\n",
      "SKU DISCOUNT RESULT\n",
      "============================================================\n",
      "Mode: testing\n",
      "Total input: 16492\n",
      "SKUs to activate: 16492\n",
      "Deactivated: 51143\n",
      "Created: 16492\n",
      "Failed: 0\n",
      "\n",
      "======================================================================\n",
      "MODULE 3 EXECUTION COMPLETE\n",
      "======================================================================\n",
      "Total SKUs processed: 28377\n",
      "Price changes: 27299\n",
      "Cart rule changes: 28017\n",
      "SKUs with SKU discount: 16492\n",
      "SKUs with QD: 1728\n",
      "Output saved to: module_3_output_20260124_2047.xlsx\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 3: PROCESS SKU DISCOUNTS\n",
    "# =============================================================================\n",
    "# This step handles SKU discounts for SKUs that need them based on UTH performance.\n",
    "# Market data has already been refreshed, so we pass the df_output directly.\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 3: PROCESSING SKU DISCOUNTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "%run sku_discount_handler.ipynb\n",
    "\n",
    "# Filter to SKUs that need SKU discount (use df_results to include all columns for discount calculation)\n",
    "df_sku_discount = df_results[df_results['activate_sku_discount'] == True].copy()\n",
    "print(f\"SKUs needing SKU discount: {len(df_sku_discount)}\")\n",
    "\n",
    "if len(df_sku_discount) > 0:\n",
    "    sku_discount_result = process_sku_discounts(df_sku_discount, mode=PUSH_MODE)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SKU DISCOUNT RESULT\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Mode: {sku_discount_result['mode']}\")\n",
    "    print(f\"Total input: {sku_discount_result['total_input']}\")\n",
    "    print(f\"SKUs to activate: {sku_discount_result['to_activate']}\")\n",
    "    print(f\"Deactivated: {sku_discount_result['deactivated']}\")\n",
    "    print(f\"Created: {sku_discount_result['created']}\")\n",
    "    print(f\"Failed: {sku_discount_result['failed']}\")\n",
    "else:\n",
    "    print(\"No SKUs need SKU discounts\")\n",
    "\n",
    "# =============================================================================\n",
    "# FINAL SUMMARY\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODULE 3 EXECUTION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total SKUs processed: {len(df_output)}\")\n",
    "print(f\"Price changes: {(df_output['new_price'] != df_output['current_price']).sum()}\")\n",
    "print(f\"Cart rule changes: {(df_output['new_cart_rule'] != df_output['current_cart_rule']).sum()}\")\n",
    "print(f\"SKUs with SKU discount: {df_output['activate_sku_discount'].sum()}\")\n",
    "print(f\"SKUs with QD: {df_output['activate_qd'].sum()}\")\n",
    "print(f\"Output saved to: {OUTPUT_FILE}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
