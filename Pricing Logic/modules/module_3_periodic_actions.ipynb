{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3: Periodic Action Module (UTH-Based Adjustments)\n",
    "\n",
    "## Purpose\n",
    "This module runs at 12 PM, 3 PM, 6 PM, 9 PM, and 12 AM Cairo time to:\n",
    "1. Adjust prices based on Up-Till-Hour (UTH) performance vs benchmarks\n",
    "2. Manage SKU discounts and Quantity Discounts based on performance\n",
    "3. Adjust cart rules dynamically\n",
    "\n",
    "## UTH Benchmarks\n",
    "- Calculate historical qty from start of day till current hour over the last 4 months\n",
    "- Multiply by P80 all-time-high quantity and P70 retailers\n",
    "\n",
    "## Action Logic\n",
    "- **On Track (±10%)**: No action\n",
    "- **Growing (>110%)**: Deactivate discounts or increase price, reduce cart if too open\n",
    "- **Dropping (<90%)**: Reduce price, increase cart by 20%\n",
    "- **Zero Demand (qty=0 today)**: Market min + SKU discount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.Renviron\n",
      "/home/ec2-user/service_account_key.json\n",
      "Queries Module | Timezone: America/Los_Angeles\n",
      "✅ UTH and Last Hour functions defined\n",
      "\n",
      "==================================================\n",
      "QUERIES MODULE READY\n",
      "==================================================\n",
      "\n",
      "Live Data Functions:\n",
      "  • get_current_stocks()\n",
      "  • get_packing_units()\n",
      "  • get_current_prices()\n",
      "  • get_current_wac()\n",
      "  • get_current_cart_rules()\n",
      "\n",
      "UTH Performance Functions:\n",
      "  • get_uth_performance()         - UTH qty/retailers (Snowflake)\n",
      "  • get_hourly_distribution()     - Historical hour contributions (Snowflake)\n",
      "  • get_last_hour_performance()   - Last hour qty/retailers (DWH)\n",
      "\n",
      "Note: Market prices use MODULE_1_INPUT data\n",
      "Retailer Selection Queries defined ✓\n",
      "  - get_churned_dropped_retailers()\n",
      "  - get_category_not_product_retailers()\n",
      "  - get_out_of_cycle_retailers()\n",
      "  - get_view_no_orders_retailers()\n",
      "  - get_excluded_retailers()\n",
      "  - get_retailers_with_quantity_discount()\n",
      "  - get_retailer_main_warehouse()\n",
      "/home/ec2-user/.Renviron\n",
      "/home/ec2-user/service_account_key.json\n",
      "Market Data Module loaded at 2026-01-26 00:49:34 Cairo time\n",
      "Snowflake timezone: America/Los_Angeles\n",
      "All queries defined ✓\n",
      "Helper functions defined ✓\n",
      "get_market_data() function defined ✓\n",
      "get_margin_tiers() function defined ✓\n",
      "\n",
      "======================================================================\n",
      "MARKET DATA MODULE READY\n",
      "======================================================================\n",
      "\n",
      "Available functions (NO INPUT REQUIRED):\n",
      "  - get_market_data()   : Fetch and process all market prices\n",
      "  - get_margin_tiers()  : Fetch and calculate margin tiers\n",
      "\n",
      "Usage:\n",
      "  %run market_data_module.ipynb\n",
      "  df_market = get_market_data()\n",
      "  df_tiers = get_margin_tiers()\n",
      "======================================================================\n",
      "Module 3: Periodic Actions\n",
      "Run Time (Cairo): 2026-01-26 00:49:34\n",
      "Current Hour (Cairo): 0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS AND SETUP\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "# Run queries_module - this:\n",
    "# 1. Initializes Snowflake credentials (setup_environment_2.initialize_env())\n",
    "# 2. Provides query_snowflake() function\n",
    "# 3. Provides TIMEZONE from Snowflake\n",
    "# 4. Provides get_current_stocks(), get_current_prices(), get_current_wac(), get_current_cart_rules()\n",
    "%run queries_module.ipynb\n",
    "\n",
    "# Run market_data_module - this:\n",
    "# 1. Provides get_market_data() for fetching fresh market prices (NO INPUT REQUIRED)\n",
    "# 2. Provides get_margin_tiers() for fetching margin tiers (NO INPUT REQUIRED)\n",
    "# 3. Fetches Ben Soliman, Marketplace, and Scrapped prices\n",
    "# 4. Fills missing prices from group-level data\n",
    "# 5. Calculates market price percentiles and margin tiers\n",
    "%run market_data_module.ipynb\n",
    "\n",
    "# Cairo timezone\n",
    "CAIRO_TZ = pytz.timezone('Africa/Cairo')\n",
    "CAIRO_NOW = datetime.now(CAIRO_TZ)\n",
    "TODAY = CAIRO_NOW.date()\n",
    "CURRENT_HOUR = CAIRO_NOW.hour\n",
    "\n",
    "# Configuration\n",
    "UTH_GROWING_THRESHOLD = 1.10    # >110% = Growing\n",
    "UTH_DROPPING_THRESHOLD = 0.90   # <90% = Dropping\n",
    "CART_INCREASE_PCT = 0.20        # 20% cart increase\n",
    "CART_DECREASE_PCT = 0.20        # 20% cart decrease\n",
    "MIN_CART_RULE = 2\n",
    "MAX_CART_RULE = 150\n",
    "MIN_PRICE_CHANGE_EGP = 0.25     # Minimum 0.25 EGP for any price change\n",
    "CONTRIBUTION_THRESHOLD = 50     # 50% contribution threshold\n",
    "MAX_PRICE_REDUCTIONS_PER_DAY = 2  # Max price reductions per day\n",
    "# SKU discount percentage will be decided in sku_discount_handler\n",
    "\n",
    "# Input/Output configuration\n",
    "# Data is now loaded from Snowflake instead of Excel\n",
    "INPUT_TABLE = 'MATERIALIZED_VIEWS.Pricing_data_extraction'\n",
    "PREVIOUS_OUTPUT_TABLE = 'MATERIALIZED_VIEWS.pricing_periodic_push'\n",
    "OUTPUT_FILE = f'module_3_output_{CAIRO_NOW.strftime(\"%Y%m%d_%H%M\")}.xlsx'\n",
    "\n",
    "print(f\"Module 3: Periodic Actions\")\n",
    "print(f\"Run Time (Cairo): {CAIRO_NOW.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Current Hour (Cairo): {CURRENT_HOUR}\")\n",
    "print(f\"Input: {INPUT_TABLE} (today's data)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading previous actions from today...\n",
      "No previous Module 3 outputs found for today. This is the first run.\n",
      "Previous actions loaded: 0 records\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD PREVIOUS ACTIONS (Track price reductions per day)\n",
    "# Now loads from Snowflake instead of local Excel files\n",
    "# =============================================================================\n",
    "\n",
    "def load_previous_actions():\n",
    "    \"\"\"Load previous Module 3 outputs from today (from Snowflake) to track price reductions.\"\"\"\n",
    "    try:\n",
    "        # Query today's previous actions from Snowflake\n",
    "        query = f\"\"\"\n",
    "        SELECT * FROM {PREVIOUS_OUTPUT_TABLE}\n",
    "        WHERE DATE(created_at) = '{TODAY}'\n",
    "        ORDER BY created_at\n",
    "        \"\"\"\n",
    "        df = query_snowflake(query)\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            print(\"No previous Module 3 outputs found for today. This is the first run.\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        print(f\"Loaded {len(df)} previous action records from Snowflake\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading previous actions from Snowflake: {e}\")\n",
    "        print(\"This may be the first run or table doesn't exist yet.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def count_price_reductions_today(product_id, warehouse_id, previous_df):\n",
    "    \"\"\"Count how many price reductions this SKU has had today.\"\"\"\n",
    "    if previous_df.empty:\n",
    "        return 0\n",
    "    \n",
    "    mask = (\n",
    "        (previous_df['product_id'] == product_id) & \n",
    "        (previous_df['warehouse_id'] == warehouse_id) &\n",
    "        (previous_df['price_action'] == 'decrease')\n",
    "    )\n",
    "    return mask.sum()\n",
    "\n",
    "print(\"Loading previous actions from today...\")\n",
    "df_previous_actions = load_previous_actions()\n",
    "print(f\"Previous actions loaded: {len(df_previous_actions)} records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snowflake connection ready\n",
      "Timezone: America/Los_Angeles\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SNOWFLAKE CONNECTION\n",
    "# =============================================================================\n",
    "# query_snowflake() and TIMEZONE are provided by queries_module.ipynb\n",
    "# (which also initializes Snowflake credentials from setup_environment_2)\n",
    "print(f\"Snowflake connection ready\")\n",
    "print(f\"Timezone: {TIMEZONE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading today's UTH performance with discount contributions...\n",
      "Loaded 0 UTH records\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# QUERY 1: TODAY'S UTH PERFORMANCE\n",
    "# =============================================================================\n",
    "UTH_LIVE_QUERY = f'''\n",
    "WITH params AS (\n",
    "    SELECT\n",
    "        CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE AS today,\n",
    "        HOUR(CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())) AS current_hour\n",
    "),\n",
    "\n",
    "-- Map dynamic tags to warehouse IDs using name matching\n",
    "qd_det AS (\n",
    "    SELECT DISTINCT \n",
    "        dt.id AS tag_id, \n",
    "        dt.name AS tag_name,\n",
    "        REPLACE(w.name, ' ', '') AS warehouse_name,\n",
    "        w.id AS warehouse_id,\n",
    "        warehouse_name ILIKE '%' || CASE \n",
    "            WHEN SPLIT_PART(tag_name, '_', 1) = 'El' THEN SPLIT_PART(tag_name, '_', 2) \n",
    "            ELSE SPLIT_PART(tag_name, '_', 1) \n",
    "        END || '%' AS contains_flag\n",
    "    FROM dynamic_tags dt\n",
    "    JOIN dynamic_taggables dta ON dt.id = dta.dynamic_tag_id \n",
    "    CROSS JOIN warehouses w \n",
    "    WHERE dt.id > 3000\n",
    "        AND dt.name LIKE '%QD_rets%'\n",
    "        AND w.id IN (1, 236, 337, 8, 339, 170, 501, 401, 703, 632, 797, 962)\n",
    "        AND contains_flag = 'true'\n",
    "),\n",
    "\n",
    "-- Get current active QD configurations\n",
    "qd_config AS (\n",
    "    SELECT * \n",
    "    FROM (\n",
    "        SELECT \n",
    "            product_id,\n",
    "            start_at,\n",
    "            end_at,\n",
    "            packing_unit_id,\n",
    "            id AS qd_id,\n",
    "            qd.warehouse_id,\n",
    "            MAX(CASE WHEN tier = 1 THEN quantity END) AS tier_1_qty,\n",
    "            MAX(CASE WHEN tier = 1 THEN discount_percentage END) AS tier_1_discount_pct,\n",
    "            MAX(CASE WHEN tier = 2 THEN quantity END) AS tier_2_qty,\n",
    "            MAX(CASE WHEN tier = 2 THEN discount_percentage END) AS tier_2_discount_pct,\n",
    "            MAX(CASE WHEN tier = 3 THEN quantity END) AS tier_3_qty,\n",
    "            MAX(CASE WHEN tier = 3 THEN discount_percentage END) AS tier_3_discount_pct\n",
    "        FROM (\n",
    "            SELECT \n",
    "                qd.id,\n",
    "                qdv.product_id,\n",
    "                qdv.packing_unit_id,\n",
    "                qdv.quantity,\n",
    "                qdv.discount_percentage,\n",
    "                qd.dynamic_tag_id,\n",
    "                qd.start_at,\n",
    "                qd.end_at,\n",
    "                ROW_NUMBER() OVER (\n",
    "                    PARTITION BY qdv.product_id, qdv.packing_unit_id, qd.id \n",
    "                    ORDER BY qdv.quantity\n",
    "                ) AS tier\n",
    "            FROM quantity_discounts qd \n",
    "            JOIN quantity_discount_values qdv ON qdv.quantity_discount_id = qd.id\n",
    "            WHERE active = 'true'\n",
    "        ) qd_tiers\n",
    "        JOIN qd_det qd ON qd.tag_id = qd_tiers.dynamic_tag_id\n",
    "        GROUP BY ALL\n",
    "    )\n",
    "    QUALIFY ROW_NUMBER() OVER (PARTITION BY product_id, packing_unit_id, warehouse_id ORDER BY start_at DESC) = 1\n",
    "),\n",
    "\n",
    "-- Today's sales up-till-hour with discount breakdown\n",
    "today_uth_sales AS (\n",
    "    SELECT \n",
    "        pso.warehouse_id,\n",
    "        pso.product_id,\n",
    "        so.retailer_id,\n",
    "        pso.packing_unit_id,\n",
    "        pso.purchased_item_count AS qty,\n",
    "        pso.total_price AS nmv,\n",
    "        pso.ITEM_DISCOUNT_VALUE AS sku_discount_per_unit,\n",
    "        pso.ITEM_QUANTITY_DISCOUNT_VALUE AS qty_discount_per_unit,\n",
    "        qd.tier_1_qty,\n",
    "        qd.tier_2_qty,\n",
    "        qd.tier_3_qty,\n",
    "        -- Determine tier used\n",
    "        CASE \n",
    "            WHEN pso.ITEM_QUANTITY_DISCOUNT_VALUE = 0 OR qd.tier_1_qty IS NULL THEN 'Base'\n",
    "            WHEN qd.tier_3_qty IS NOT NULL AND pso.purchased_item_count >= qd.tier_3_qty THEN 'Tier 3'\n",
    "            WHEN qd.tier_2_qty IS NOT NULL AND pso.purchased_item_count >= qd.tier_2_qty THEN 'Tier 2'\n",
    "            WHEN qd.tier_1_qty IS NOT NULL AND pso.purchased_item_count >= qd.tier_1_qty THEN 'Tier 1'\n",
    "            ELSE 'Base'\n",
    "        END AS tier_used\n",
    "    FROM product_sales_order pso\n",
    "    JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "    LEFT JOIN qd_config qd \n",
    "        ON qd.product_id = pso.product_id \n",
    "        AND qd.packing_unit_id = pso.packing_unit_id\n",
    "        AND qd.warehouse_id = so.warehouse_id\n",
    "    CROSS JOIN params p\n",
    "    WHERE so.created_at::DATE = p.today\n",
    "        AND HOUR(so.created_at) < p.current_hour\n",
    "        AND so.sales_order_status_id NOT IN (7, 12)\n",
    "        AND so.channel IN ('telesales', 'retailer')\n",
    "        AND pso.purchased_item_count <> 0\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    warehouse_id,\n",
    "    product_id,\n",
    "    SUM(qty) AS uth_qty,\n",
    "    SUM(nmv) AS uth_nmv,\n",
    "    COUNT(DISTINCT retailer_id) AS uth_retailers,\n",
    "    -- SKU discount NMV and contribution\n",
    "    SUM(CASE WHEN sku_discount_per_unit > 0 THEN nmv ELSE 0 END) AS sku_discount_nmv_uth,\n",
    "    ROUND(SUM(CASE WHEN sku_discount_per_unit > 0 THEN nmv ELSE 0 END) * 100.0 / NULLIF(SUM(nmv), 0), 2) AS sku_disc_cntrb_uth,\n",
    "    -- Quantity discount NMV and contribution\n",
    "    SUM(CASE WHEN qty_discount_per_unit > 0 THEN nmv ELSE 0 END) AS qty_discount_nmv_uth,\n",
    "    ROUND(SUM(CASE WHEN qty_discount_per_unit > 0 THEN nmv ELSE 0 END) * 100.0 / NULLIF(SUM(nmv), 0), 2) AS qty_disc_cntrb_uth,\n",
    "    -- Tier-level NMV\n",
    "    SUM(CASE WHEN tier_used = 'Tier 1' THEN nmv ELSE 0 END) AS t1_nmv_uth,\n",
    "    SUM(CASE WHEN tier_used = 'Tier 2' THEN nmv ELSE 0 END) AS t2_nmv_uth,\n",
    "    SUM(CASE WHEN tier_used = 'Tier 3' THEN nmv ELSE 0 END) AS t3_nmv_uth,\n",
    "    -- Tier-level contributions\n",
    "    ROUND(SUM(CASE WHEN tier_used = 'Tier 1' THEN nmv ELSE 0 END) * 100.0 / NULLIF(SUM(nmv), 0), 2) AS t1_cntrb_uth,\n",
    "    ROUND(SUM(CASE WHEN tier_used = 'Tier 2' THEN nmv ELSE 0 END) * 100.0 / NULLIF(SUM(nmv), 0), 2) AS t2_cntrb_uth,\n",
    "    ROUND(SUM(CASE WHEN tier_used = 'Tier 3' THEN nmv ELSE 0 END) * 100.0 / NULLIF(SUM(nmv), 0), 2) AS t3_cntrb_uth\n",
    "FROM today_uth_sales\n",
    "GROUP BY warehouse_id, product_id\n",
    "HAVING SUM(nmv) > 0\n",
    "'''\n",
    "\n",
    "print(\"Loading today's UTH performance with discount contributions...\")\n",
    "df_uth_today = query_snowflake(UTH_LIVE_QUERY)\n",
    "print(f\"Loaded {len(df_uth_today)} UTH records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching hourly distribution from Snowflake...\n",
      "  Loaded 771 hourly distribution records\n",
      "Using avg_uth_pct_qty as avg_uth_pct for Module 3 compatibility\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# QUERY 2: HISTORICAL HOURLY DISTRIBUTION (Last 4 Months) - By Category & Warehouse\n",
    "# =============================================================================\n",
    "# Uses get_hourly_distribution() from queries_module\n",
    "\n",
    "df_hourly_dist = get_hourly_distribution()\n",
    "\n",
    "# Rename column for backwards compatibility with rest of Module 3\n",
    "df_hourly_dist['avg_uth_pct'] = df_hourly_dist['avg_uth_pct_qty']\n",
    "print(f\"Using avg_uth_pct_qty as avg_uth_pct for Module 3 compatibility\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading active SKU discounts...\n",
      "Loaded 14453 active SKU discount records\n",
      "Loading active Quantity discounts...\n",
      "Loaded 2040 active QD records\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# QUERY 3 & 4: ACTIVE DISCOUNTS\n",
    "# =============================================================================\n",
    "\n",
    "# SKU Discounts query (from data_extraction.ipynb)\n",
    "ACTIVE_SKU_DISCOUNTS_QUERY = f'''\n",
    "WITH active_sku_discount AS ( \n",
    "    SELECT \n",
    "        x.id AS sku_discount_id,\n",
    "        retailer_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        DISCOUNT_PERCENTAGE,\n",
    "        start_at,\n",
    "        end_at \n",
    "    FROM (\n",
    "        SELECT \n",
    "            sd.*,\n",
    "            f.value::INT AS retailer_id \n",
    "        FROM SKU_DISCOUNTS sd,\n",
    "        LATERAL FLATTEN(\n",
    "            input => SPLIT(\n",
    "                REPLACE(REPLACE(REPLACE(sd.retailer_ids, '{{', ''), '}}', ''), '\"', ''), \n",
    "                ','\n",
    "            )\n",
    "        ) f\n",
    "        WHERE start_at::DATE <= CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE\n",
    "        and end_at::DATE >= CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE\n",
    "            AND active = 'true'\n",
    "    ) x \n",
    "    JOIN SKU_DISCOUNT_VALUES sdv ON x.id = sdv.sku_discount_id\n",
    "    WHERE name_en = 'Special Discounts'\n",
    "    QUALIFY MAX(start_at) OVER (PARTITION BY retailer_id, product_id, packing_unit_id) = start_at \n",
    ")\n",
    "\n",
    "SELECT \n",
    "    product_id, \n",
    "    warehouse_id,\n",
    "    AVG(DISCOUNT_PERCENTAGE) AS active_sku_disc_pct,\n",
    "    1 AS has_active_sku_discount\n",
    "FROM (\n",
    "    SELECT \n",
    "        asd.*,\n",
    "        warehouse_id \n",
    "    FROM active_sku_discount asd \n",
    "    JOIN materialized_views.retailer_polygon rp ON rp.retailer_id = asd.retailer_id\n",
    "    JOIN WAREHOUSE_DISPATCHING_RULES wdr ON wdr.product_id = asd.product_id\n",
    "    JOIN DISPATCHING_POLYGONS dp ON dp.id = wdr.DISPATCHING_POLYGON_ID AND dp.district_id = rp.district_id\n",
    ")\n",
    "GROUP BY ALL\n",
    "'''\n",
    "\n",
    "# Active QD Query - Reuses the same CTE structure from UTH_LIVE_QUERY\n",
    "ACTIVE_QD_QUERY = f'''\n",
    "WITH qd_det AS (\n",
    "    SELECT DISTINCT \n",
    "        dt.id AS tag_id, \n",
    "        dt.name AS tag_name,\n",
    "        REPLACE(w.name, ' ', '') AS warehouse_name,\n",
    "        w.id AS warehouse_id,\n",
    "        warehouse_name ILIKE '%' || CASE \n",
    "            WHEN SPLIT_PART(tag_name, '_', 1) = 'El' THEN SPLIT_PART(tag_name, '_', 2) \n",
    "            ELSE SPLIT_PART(tag_name, '_', 1) \n",
    "        END || '%' AS contains_flag\n",
    "    FROM dynamic_tags dt\n",
    "    JOIN dynamic_taggables dta ON dt.id = dta.dynamic_tag_id \n",
    "    CROSS JOIN warehouses w \n",
    "    WHERE dt.id > 3000\n",
    "        AND dt.name LIKE '%QD_rets%'\n",
    "        AND w.id IN (1, 236, 337, 8, 339, 170, 501, 401, 703, 632, 797, 962)\n",
    "        AND contains_flag = 'true'\n",
    "),\n",
    "\n",
    "qd_config AS (\n",
    "    SELECT * \n",
    "    FROM (\n",
    "        SELECT \n",
    "            product_id,\n",
    "            packing_unit_id,\n",
    "            qd.warehouse_id,\n",
    "            MAX(CASE WHEN tier = 1 THEN quantity END) AS qd_tier_1_qty,\n",
    "            MAX(CASE WHEN tier = 1 THEN discount_percentage END) AS qd_tier_1_disc_pct,\n",
    "            MAX(CASE WHEN tier = 2 THEN quantity END) AS qd_tier_2_qty,\n",
    "            MAX(CASE WHEN tier = 2 THEN discount_percentage END) AS qd_tier_2_disc_pct,\n",
    "            MAX(CASE WHEN tier = 3 THEN quantity END) AS qd_tier_3_qty,\n",
    "            MAX(CASE WHEN tier = 3 THEN discount_percentage END) AS qd_tier_3_disc_pct\n",
    "        FROM (\n",
    "            SELECT \n",
    "                qd.id,\n",
    "                qdv.product_id,\n",
    "                qdv.packing_unit_id,\n",
    "                qdv.quantity,\n",
    "                qdv.discount_percentage,\n",
    "                qd.dynamic_tag_id,\n",
    "                qd.start_at,\n",
    "                ROW_NUMBER() OVER (\n",
    "                    PARTITION BY qdv.product_id, qdv.packing_unit_id, qd.id \n",
    "                    ORDER BY qdv.quantity\n",
    "                ) AS tier\n",
    "            FROM quantity_discounts qd \n",
    "            JOIN quantity_discount_values qdv ON qdv.quantity_discount_id = qd.id\n",
    "            WHERE  active = TRUE\n",
    "        ) qd_tiers\n",
    "        JOIN qd_det qd ON qd.tag_id = qd_tiers.dynamic_tag_id\n",
    "        GROUP BY ALL\n",
    "    )\n",
    "    QUALIFY ROW_NUMBER() OVER (PARTITION BY product_id, packing_unit_id, warehouse_id ORDER BY qd_tier_1_qty DESC) = 1\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    product_id,\n",
    "    warehouse_id,\n",
    "    qd_tier_1_qty,\n",
    "    qd_tier_1_disc_pct,\n",
    "    qd_tier_2_qty,\n",
    "    qd_tier_2_disc_pct,\n",
    "    qd_tier_3_qty,\n",
    "    qd_tier_3_disc_pct,\n",
    "    1 AS has_active_qd\n",
    "FROM qd_config\n",
    "'''\n",
    "\n",
    "print(\"Loading active SKU discounts...\")\n",
    "df_active_sku_disc = query_snowflake(ACTIVE_SKU_DISCOUNTS_QUERY)\n",
    "print(f\"Loaded {len(df_active_sku_disc)} active SKU discount records\")\n",
    "\n",
    "print(\"Loading active Quantity discounts...\")\n",
    "df_active_qd = query_snowflake(ACTIVE_QD_QUERY)\n",
    "print(f\"Loaded {len(df_active_qd)} active QD records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Module 1 data...\n",
      "Loaded 28437 records from Module 1\n",
      "\n",
      "Refreshing live data...\n",
      "Fetching current stocks...\n",
      "  Loaded 1848028 records\n",
      "Fetching current prices...\n",
      "  Loaded 116390 records\n",
      "Fetching current WAC...\n",
      "  Loaded 8147 records\n",
      "Fetching current cart rules...\n",
      "  Loaded 72968 records\n",
      "Live data refreshed: stocks, prices, WAC, cart rules\n",
      "\n",
      "Refreshing market prices and margin tiers...\n",
      "\n",
      "======================================================================\n",
      "FETCHING MARKET DATA\n",
      "======================================================================\n",
      "Timestamp: 2026-01-26 00:24:25 Cairo time\n",
      "\n",
      "Step 1: Fetching raw price data...\n",
      "  1.1 Ben Soliman prices...\n",
      "      Loaded 1563 records\n",
      "  1.2 Marketplace prices...\n",
      "      Loaded 11495 records\n",
      "  1.3 Scrapped prices...\n",
      "      Loaded 5150 records\n",
      "  1.4 Product groups...\n",
      "      Loaded 1603 records\n",
      "  1.5 Sales data (for NMV weighting)...\n",
      "      Loaded 20978 records\n",
      "  1.6 Margin stats...\n",
      "      Loaded 29534 records\n",
      "  1.7 Target margins...\n",
      "      Loaded 478 records\n",
      "  1.8 Product base (WAC)...\n",
      "      Loaded 65214 records\n",
      "\n",
      "Step 2: Joining all market price sources (outer join)...\n",
      "    Market prices base: 16189 records\n",
      "\n",
      "Step 3: Adding cohort IDs and supporting data...\n",
      "    Records after adding cohorts: 24214\n",
      "\n",
      "Step 4: Processing group-level prices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23263/1916930114.py:138: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: pd.Series({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Records after group processing: 25619\n",
      "\n",
      "Step 5: Adding WAC and margin data...\n",
      "    Records with WAC: 25403\n",
      "\n",
      "Step 6: Filtering by price coverage...\n",
      "    Records after price coverage filter: 13619\n",
      "\n",
      "Step 7: Calculating price percentiles...\n",
      "    Records after price analysis: 12884\n",
      "\n",
      "Step 8: Converting prices to margins...\n",
      "\n",
      "======================================================================\n",
      "MARKET DATA COMPLETE\n",
      "======================================================================\n",
      "Total records: 12884\n",
      "  - With marketplace prices: 12321\n",
      "  - With scrapped prices: 6429\n",
      "  - With Ben Soliman prices: 8733\n",
      "  Fetched 12884 market data records\n",
      "\n",
      "======================================================================\n",
      "FETCHING MARGIN TIERS\n",
      "======================================================================\n",
      "Timestamp: 2026-01-26 00:25:15 Cairo time\n",
      "\n",
      "Step 1: Fetching margin boundaries from PRODUCT_STATISTICS...\n",
      "    Loaded 18236 records\n",
      "\n",
      "Step 2: Adding cohort IDs...\n",
      "    Records with cohorts: 25115\n",
      "\n",
      "Step 3: Calculating margin tiers...\n",
      "\n",
      "======================================================================\n",
      "MARGIN TIERS COMPLETE\n",
      "======================================================================\n",
      "Total records: 25115\n",
      "\n",
      "Margin Tier Structure:\n",
      "  margin_tier_below:   effective_min - step (1 below)\n",
      "  margin_tier_1:       effective_min_margin\n",
      "  margin_tier_2:       effective_min + 1*step\n",
      "  margin_tier_3:       effective_min + 2*step\n",
      "  margin_tier_4:       effective_min + 3*step\n",
      "  margin_tier_5:       max_boundary\n",
      "  margin_tier_above_1: max_boundary + 1*step\n",
      "  margin_tier_above_2: max_boundary + 2*step\n",
      "  Fetched 25115 margin tier records\n",
      "Market data refreshed\n",
      "Data merged. Total records: 28512\n",
      "  SKUs with active SKU discount: 14512\n",
      "  SKUs with active QD: 2040\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD DATA FROM SNOWFLAKE (Instead of Excel file)\n",
    "# =============================================================================\n",
    "print(\"Loading data from Snowflake...\")\n",
    "\n",
    "# Query to get today's data from Pricing_data_extraction\n",
    "LOAD_QUERY = f\"\"\"\n",
    "SELECT * FROM {INPUT_TABLE}\n",
    "WHERE created_at = '{datetime.now(CAIRO_TZ).date()}'\n",
    "\"\"\"\n",
    "\n",
    "df = query_snowflake(LOAD_QUERY)\n",
    "print(f\"Loaded {len(df)} records from Snowflake\")\n",
    "\n",
    "# Refresh live data using queries_module\n",
    "print(\"\\nRefreshing live data...\")\n",
    "\n",
    "# Refresh stocks\n",
    "df_fresh_stocks = get_current_stocks()\n",
    "df = df.drop(columns=['stocks'], errors='ignore')\n",
    "df = df.merge(df_fresh_stocks, on=['warehouse_id', 'product_id'], how='left')\n",
    "df['stocks'] = df['stocks'].fillna(0)\n",
    "\n",
    "# Refresh current prices\n",
    "df_fresh_prices = get_current_prices()\n",
    "df = df.drop(columns=['current_price'], errors='ignore')\n",
    "df = df.merge(df_fresh_prices[['cohort_id', 'product_id', 'current_price']], \n",
    "              on=['cohort_id', 'product_id'], how='left')\n",
    "\n",
    "# Refresh WAC\n",
    "df_fresh_wac = get_current_wac()\n",
    "df = df.drop(columns=['wac_p'], errors='ignore')\n",
    "df = df.merge(df_fresh_wac, on='product_id', how='left')\n",
    "\n",
    "# Refresh cart rules\n",
    "df_fresh_cart = get_current_cart_rules()\n",
    "df = df.drop(columns=['current_cart_rule'], errors='ignore')\n",
    "df = df.merge(df_fresh_cart, on=['cohort_id', 'product_id'], how='left')\n",
    "\n",
    "print(f\"Live data refreshed: stocks, prices, WAC, cart rules\")\n",
    "\n",
    "# Refresh market prices and margin tiers using new standalone functions\n",
    "print(\"\\nRefreshing market prices and margin tiers...\")\n",
    "\n",
    "# Get fresh market data (no input required)\n",
    "df_fresh_market = get_market_data()\n",
    "print(f\"  Fetched {len(df_fresh_market)} market data records\")\n",
    "\n",
    "# Get fresh margin tiers (no input required)\n",
    "df_fresh_tiers = get_margin_tiers()\n",
    "print(f\"  Fetched {len(df_fresh_tiers)} margin tier records\")\n",
    "\n",
    "# Drop old market columns and merge fresh data\n",
    "market_cols_to_drop = [\n",
    "    'below_market', 'market_min', 'market_25', 'market_50', \n",
    "    'market_75', 'market_max', 'above_market',\n",
    "    'minimum', 'percentile_25', 'percentile_50', 'percentile_75', 'maximum',\n",
    "    'ben_soliman_price', 'final_min_price', 'final_max_price', 'final_mod_price',\n",
    "    'final_true_min', 'final_true_max', 'min_scrapped', 'scrapped25', \n",
    "    'scrapped50', 'scrapped75', 'max_scrapped'\n",
    "]\n",
    "df = df.drop(columns=[c for c in market_cols_to_drop if c in df.columns], errors='ignore')\n",
    "\n",
    "# Merge fresh market data\n",
    "df = df.merge(\n",
    "    df_fresh_market, \n",
    "    on=['cohort_id', 'product_id','region'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Drop old margin tier columns and merge fresh data\n",
    "margin_tier_cols_to_drop = [\n",
    "    'margin_tier_below', 'margin_tier_1', 'margin_tier_2', 'margin_tier_3',\n",
    "    'margin_tier_4', 'margin_tier_5', 'margin_tier_above_1', 'margin_tier_above_2',\n",
    "    'optimal_bm', 'min_boundary', 'max_boundary', 'median_bm',\n",
    "    'effective_min_margin', 'margin_step'\n",
    "]\n",
    "df = df.drop(columns=[c for c in margin_tier_cols_to_drop if c in df.columns], errors='ignore')\n",
    "\n",
    "# Merge fresh margin tiers\n",
    "df = df.merge(\n",
    "    df_fresh_tiers, \n",
    "    on=['cohort_id', 'product_id','region'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"Market data refreshed\")\n",
    "\n",
    "# Merge UTH today data - drop old columns first\n",
    "uth_cols = ['uth_qty', 'uth_nmv', 'uth_retailers', 'sku_discount_nmv_uth', 'sku_disc_cntrb_uth',\n",
    "            'qty_discount_nmv_uth', 'qty_disc_cntrb_uth', 't1_nmv_uth', 't2_nmv_uth', 't3_nmv_uth',\n",
    "            't1_cntrb_uth', 't2_cntrb_uth', 't3_cntrb_uth']\n",
    "df = df.drop(columns=[c for c in uth_cols if c in df.columns], errors='ignore')\n",
    "\n",
    "if len(df_uth_today) > 0:\n",
    "    df = df.merge(df_uth_today, on=['warehouse_id', 'product_id'], how='left')\n",
    "else:\n",
    "    for col in uth_cols:\n",
    "        df[col] = 0\n",
    "\n",
    "# Merge hourly distribution - drop old column first (now by warehouse_id + cat)\n",
    "df = df.drop(columns=['avg_uth_pct'], errors='ignore')\n",
    "if len(df_hourly_dist) > 0:\n",
    "    df = df.merge(df_hourly_dist, on=['warehouse_id', 'cat'], how='left')\n",
    "else:\n",
    "    df['avg_uth_pct'] = 0.5  # Default 50%\n",
    "\n",
    "# Merge active SKU discounts - drop old columns first\n",
    "sku_disc_cols = ['has_active_sku_discount', 'active_sku_disc_pct', 'active_sku_discount_value']\n",
    "df = df.drop(columns=[c for c in sku_disc_cols if c in df.columns], errors='ignore')\n",
    "\n",
    "if len(df_active_sku_disc) > 0:\n",
    "    df = df.merge(df_active_sku_disc, on=['warehouse_id', 'product_id'], how='left')\n",
    "else:\n",
    "    df['has_active_sku_discount'] = 0\n",
    "    df['active_sku_disc_pct'] = 0\n",
    "\n",
    "# Merge active QD - drop old columns first\n",
    "qd_cols = ['has_active_qd', 'qd_tier_1_qty', 'qd_tier_1_disc_pct', \n",
    "           'qd_tier_2_qty', 'qd_tier_2_disc_pct', 'qd_tier_3_qty', 'qd_tier_3_disc_pct']\n",
    "df = df.drop(columns=[c for c in qd_cols if c in df.columns], errors='ignore')\n",
    "\n",
    "if len(df_active_qd) > 0:\n",
    "    df = df.merge(df_active_qd, on=['warehouse_id', 'product_id'], how='left')\n",
    "else:\n",
    "    df['has_active_qd'] = 0\n",
    "    df['qd_tier_1_qty'] = 0\n",
    "    df['qd_tier_1_disc_pct'] = 0\n",
    "    df['qd_tier_2_qty'] = 0\n",
    "    df['qd_tier_2_disc_pct'] = 0\n",
    "    df['qd_tier_3_qty'] = 0\n",
    "    df['qd_tier_3_disc_pct'] = 0\n",
    "\n",
    "# Fill NaN\n",
    "df['uth_qty'] = df['uth_qty'].fillna(0)\n",
    "df['uth_retailers'] = df['uth_retailers'].fillna(0)\n",
    "df['avg_uth_pct'] = df['avg_uth_pct'].fillna(0.5)\n",
    "df['has_active_sku_discount'] = df['has_active_sku_discount'].fillna(0)\n",
    "df['active_sku_discount_value'] = df.get('active_sku_discount_value', pd.Series([0]*len(df))).fillna(0)\n",
    "df['has_active_qd'] = df['has_active_qd'].fillna(0)\n",
    "df['qd_tier_1_qty'] = df['qd_tier_1_qty'].fillna(0)\n",
    "df['qd_tier_1_disc_pct'] = df['qd_tier_1_disc_pct'].fillna(0)\n",
    "df['qd_tier_2_qty'] = df['qd_tier_2_qty'].fillna(0)\n",
    "df['qd_tier_2_disc_pct'] = df['qd_tier_2_disc_pct'].fillna(0)\n",
    "df['qd_tier_3_qty'] = df['qd_tier_3_qty'].fillna(0)\n",
    "df['qd_tier_3_disc_pct'] = df['qd_tier_3_disc_pct'].fillna(0)\n",
    "\n",
    "print(f\"Data merged. Total records: {len(df)}\")\n",
    "print(f\"  SKUs with active SKU discount: {(df['has_active_sku_discount'] == 1).sum()}\")\n",
    "print(f\"  SKUs with active QD: {(df['has_active_qd'] == 1).sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions loaded.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_margin(price, wac):\n",
    "    if pd.isna(price) or pd.isna(wac) or price == 0:\n",
    "        return None\n",
    "    return (price - wac) / price\n",
    "\n",
    "def get_market_tiers(row):\n",
    "    \"\"\"Get sorted list of market price tiers.\"\"\"\n",
    "    tiers = []\n",
    "    for col in ['minimum', 'percentile_25', 'percentile_50', 'percentile_75', 'maximum']:\n",
    "        val = row.get(col)\n",
    "        if pd.notna(val) and val > 0:\n",
    "            tiers.append(val)\n",
    "    return sorted(set(tiers))\n",
    "\n",
    "def get_margin_tiers(row):\n",
    "    \"\"\"Get sorted list of margin-based price tiers (converted to prices).\"\"\"\n",
    "    tiers = []\n",
    "    wac = row.get('wac_p', 0)\n",
    "    if wac <= 0:\n",
    "        return tiers\n",
    "    \n",
    "    for tier_col in ['margin_tier_below','margin_tier_1', 'margin_tier_2', 'margin_tier_3', \n",
    "                     'margin_tier_4', 'margin_tier_5', 'margin_tier_above_1', 'margin_tier_above_2']:\n",
    "        margin = row.get(tier_col)\n",
    "        if pd.notna(margin) and 0 < margin < 1:\n",
    "            price = wac / (1 - margin)\n",
    "            tiers.append(round(price, 2))\n",
    "    return sorted(set(tiers))\n",
    "\n",
    "def find_next_price_above(current_price, row):\n",
    "    \"\"\"\n",
    "    Find the first price tier ABOVE current_price by at least MIN_PRICE_CHANGE_EGP.\n",
    "    Market first, then margin. Skips tiers less than 0.25 EGP above.\n",
    "    \"\"\"\n",
    "    current_price = float(current_price) if current_price else 0\n",
    "    if pd.isna(current_price) or current_price <= 0:\n",
    "        return current_price\n",
    "    \n",
    "    for tier in get_market_tiers(row):\n",
    "        if tier > current_price + MIN_PRICE_CHANGE_EGP:\n",
    "            return round(tier, 2)\n",
    "    \n",
    "    for tier in get_margin_tiers(row):\n",
    "        if tier > current_price + MIN_PRICE_CHANGE_EGP:\n",
    "            return round(tier, 2)\n",
    "    \n",
    "    return current_price\n",
    "\n",
    "def find_next_price_below(current_price, row):\n",
    "    \"\"\"\n",
    "    Find the first price tier BELOW current_price by at least MIN_PRICE_CHANGE_EGP.\n",
    "    Market first, then margin. Skips tiers less than 0.25 EGP below.\n",
    "    \"\"\"\n",
    "    current_price = float(current_price) if current_price else 0\n",
    "    if pd.isna(current_price) or current_price <= 0:\n",
    "        return current_price\n",
    "    \n",
    "    for tier in reversed(get_market_tiers(row)):\n",
    "        if tier < current_price - MIN_PRICE_CHANGE_EGP:\n",
    "            return round(tier, 2)\n",
    "    \n",
    "    for tier in reversed(get_margin_tiers(row)):\n",
    "        if tier < current_price - MIN_PRICE_CHANGE_EGP:\n",
    "            return round(tier, 2)\n",
    "    \n",
    "    return current_price\n",
    "\n",
    "def find_price_n_steps_below(current_price, n_steps, row):\n",
    "    \"\"\"Find price N steps below current (iteratively find next tier below).\"\"\"\n",
    "    price = current_price\n",
    "    for _ in range(n_steps):\n",
    "        next_price = find_next_price_below(price, row)\n",
    "        if next_price >= price:  # No tier below found\n",
    "            break\n",
    "        price = next_price\n",
    "    return price\n",
    "\n",
    "def is_cart_too_open(row):\n",
    "    \"\"\"Check if cart rule is too open: > normal_refill + 10*std\"\"\"\n",
    "    normal_refill = float(row.get('normal_refill', 5) or 5)\n",
    "    stddev = float(row.get('refill_stddev', 2) or 2)\n",
    "    current_cart = float(row.get('cart_rule', normal_refill) or normal_refill)\n",
    "    threshold = normal_refill + (10 * stddev)\n",
    "    return current_cart > threshold\n",
    "\n",
    "def adjust_cart_rule(current_cart, direction, row):\n",
    "    \"\"\"Adjust cart rule by 20% up or down.\"\"\"\n",
    "    normal_refill = float(row.get('normal_refill', 5) or 5)\n",
    "    stddev = float(row.get('refill_stddev', 2) or 2)\n",
    "    current_cart = float(current_cart or 5)\n",
    "    \n",
    "    if direction == 'increase':\n",
    "        new_cart = current_cart * (1 + CART_INCREASE_PCT)\n",
    "        new_cart = min(new_cart, MAX_CART_RULE)\n",
    "    else:  # decrease\n",
    "        # Formula: max(0.8 * cart, normal_refill + 3*std)\n",
    "        new_cart = current_cart * (1 - CART_DECREASE_PCT)\n",
    "        min_floor = normal_refill + (3 * stddev)\n",
    "        new_cart = max(new_cart, min_floor, MIN_CART_RULE)\n",
    "    \n",
    "    return int(new_cart)\n",
    "\n",
    "def get_highest_qd_tier_contribution(row):\n",
    "    \"\"\"Find which QD tier has highest contribution.\"\"\"\n",
    "    t1 = row.get('yesterday_t1_cntrb', 0) or 0\n",
    "    t2 = row.get('yesterday_t2_cntrb', 0) or 0\n",
    "    t3 = row.get('yesterday_t3_cntrb', 0) or 0\n",
    "    \n",
    "    if t1 >= t2 and t1 >= t3 and t1 > 0:\n",
    "        return 'T1', t1\n",
    "    elif t2 >= t1 and t2 >= t3 and t2 > 0:\n",
    "        return 'T2', t2\n",
    "    elif t3 > 0:\n",
    "        return 'T3', t3\n",
    "    return None, 0\n",
    "\n",
    "print(\"Helper functions loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main engine function loaded.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MAIN ENGINE: GENERATE PERIODIC ACTION\n",
    "# =============================================================================\n",
    "\n",
    "def generate_periodic_action(row, previous_df):\n",
    "    \"\"\"\n",
    "    Generate periodic action based on UTH performance.\n",
    "    \n",
    "    Logic:\n",
    "    - Zero Demand: 1 step below current + SKU discount\n",
    "    - On Track: No action\n",
    "    - Growing: Deactivate discounts or increase price, reduce cart if too open\n",
    "    - Dropping: Based on qty_ratio vs retailer_ratio:\n",
    "        - qty OK, retailers dropping: SKU discount (then price if already has)\n",
    "        - qty dropping, retailers OK: QD (then price if already has)\n",
    "        - both dropping: SKU discount (then price if already has)\n",
    "    - Price reduction max 2x per day\n",
    "    \"\"\"\n",
    "    product_id = row.get('product_id')\n",
    "    warehouse_id = row.get('warehouse_id')\n",
    "    \n",
    "    result = {\n",
    "        'product_id': product_id,\n",
    "        'warehouse_id': warehouse_id,\n",
    "        'cohort_id': row.get('cohort_id'),\n",
    "        'sku': row.get('sku'),\n",
    "        'brand': row.get('brand'),\n",
    "        'cat': row.get('cat'),\n",
    "        'stocks': row.get('stocks', 0),\n",
    "        'current_price': row.get('current_price'),\n",
    "        'wac_p': row.get('wac_p'),\n",
    "        'uth_qty': row.get('uth_qty', 0),\n",
    "        'uth_retailers': row.get('uth_retailers', 0),\n",
    "        'p80_daily_240d': row.get('p80_daily_240d', 1),\n",
    "        'p70_daily_retailers_240d': row.get('p70_daily_retailers_240d', 1),\n",
    "        'avg_uth_pct': row.get('avg_uth_pct', 0.5),\n",
    "        # Today's UTH discount contributions\n",
    "        'sku_disc_cntrb_uth': row.get('sku_disc_cntrb_uth', 0) or 0,\n",
    "        't1_cntrb_uth': row.get('t1_cntrb_uth', 0) or 0,\n",
    "        't2_cntrb_uth': row.get('t2_cntrb_uth', 0) or 0,\n",
    "        't3_cntrb_uth': row.get('t3_cntrb_uth', 0) or 0,\n",
    "        'uth_status': None,\n",
    "        'qty_ratio': None,\n",
    "        'retailer_ratio': None,\n",
    "        'new_price': None,\n",
    "        'price_action': None,\n",
    "        'current_cart_rule':row.get('current_cart_rule'),\n",
    "        'new_cart_rule': None,\n",
    "        'activate_sku_discount': False,  # True = SKU should have discount after this run\n",
    "        'activate_qd': False,             # True = SKU should have QD after this run\n",
    "        'keep_qd_tiers': None,            # List of QD tiers to keep (e.g., ['T1', 'T2'])\n",
    "        # QD tier configuration (passed to qd_handler)\n",
    "        'qd_tier_1_qty': row.get('qd_tier_1_qty', 0) or 0,\n",
    "        'qd_tier_1_disc_pct': row.get('qd_tier_1_disc_pct', 0) or 0,\n",
    "        'qd_tier_2_qty': row.get('qd_tier_2_qty', 0) or 0,\n",
    "        'qd_tier_2_disc_pct': row.get('qd_tier_2_disc_pct', 0) or 0,\n",
    "        'qd_tier_3_qty': row.get('qd_tier_3_qty', 0) or 0,\n",
    "        'qd_tier_3_disc_pct': row.get('qd_tier_3_disc_pct', 0) or 0,\n",
    "        'removed_discount': None,         # Which discount was removed (for Growing)\n",
    "        'removed_discount_cntrb': 0,      # Contribution of removed discount\n",
    "        'price_reductions_today': 0,\n",
    "        'action_reason': None,\n",
    "    }\n",
    "    \n",
    "    # Skip if OOS (price only in Module 2)\n",
    "    if row.get('stocks', 0) <= 0:\n",
    "        result['action_reason'] = 'OOS - skip (price only in Module 2)'\n",
    "        return result\n",
    "    \n",
    "    # Skip if below minimum stock (stock < min selling unit qty)\n",
    "    if row.get('below_min_stock_flag', 0) == 1:\n",
    "        result['action_reason'] = 'Below min stock - skip (cannot sell)'\n",
    "        return result\n",
    "    \n",
    "    # Count previous price reductions today\n",
    "    price_reductions_today = count_price_reductions_today(product_id, warehouse_id, previous_df)\n",
    "    result['price_reductions_today'] = price_reductions_today\n",
    "    can_reduce_price = price_reductions_today < MAX_PRICE_REDUCTIONS_PER_DAY\n",
    "    \n",
    "    # Calculate UTH benchmark: historical_pct * P80_qty\n",
    "    # Convert to float to handle decimal.Decimal from Snowflake\n",
    "    p80_qty = float(row.get('p80_daily_240d', 1) or 1)\n",
    "    p70_retailers = float(row.get('p70_daily_retailers_240d', 1) or 1)\n",
    "    avg_uth_pct = float(row.get('avg_uth_pct', 0.5) or 0.5)\n",
    "    \n",
    "    uth_qty_target = p80_qty * avg_uth_pct\n",
    "    uth_retailer_target = p70_retailers * avg_uth_pct\n",
    "    \n",
    "    uth_qty = float(row.get('uth_qty', 0) or 0)\n",
    "    uth_retailers = float(row.get('uth_retailers', 0) or 0)\n",
    "    \n",
    "    # Calculate UTH ratios\n",
    "    qty_ratio = uth_qty / uth_qty_target if uth_qty_target > 0 else 0\n",
    "    retailer_ratio = uth_retailers / uth_retailer_target if uth_retailer_target > 0 else 0\n",
    "    \n",
    "    result['uth_qty_target'] = round(uth_qty_target, 2)\n",
    "    result['uth_retailer_target'] = round(uth_retailer_target, 2)\n",
    "    result['qty_ratio'] = round(qty_ratio, 2)\n",
    "    result['retailer_ratio'] = round(retailer_ratio, 2)\n",
    "    \n",
    "    current_price = float(row.get('current_price') or 0)\n",
    "    current_cart = float(row.get('current_cart_rule', row.get('normal_refill', 10)) or 10)\n",
    "    has_sku_disc = row.get('has_active_sku_discount', 0) == 1\n",
    "    has_qd = row.get('has_active_qd', 0) == 1\n",
    "    \n",
    "    # Determine if qty/retailers are dropping (below threshold)\n",
    "    qty_dropping = qty_ratio < UTH_DROPPING_THRESHOLD\n",
    "    qty_ok = qty_ratio >= UTH_DROPPING_THRESHOLD\n",
    "    retailer_dropping = retailer_ratio < UTH_DROPPING_THRESHOLD\n",
    "    retailer_ok = retailer_ratio >= UTH_DROPPING_THRESHOLD\n",
    "    \n",
    "    # =========================================================================\n",
    "    # CASE 1: Zero demand today - 1 step below + SKU discount + open cart if tight\n",
    "    # =========================================================================\n",
    "    if uth_qty == 0:\n",
    "        new_price = find_next_price_below(current_price, row)\n",
    "        \n",
    "        # Apply commercial minimum floor\n",
    "        commercial_min = float(row.get('commercial_min_price', row.get('minimum', 0)) or 0)\n",
    "        if pd.notna(commercial_min) and commercial_min > 0:\n",
    "            new_price = max(new_price, commercial_min)\n",
    "        \n",
    "        result['new_price'] = new_price\n",
    "        result['activate_sku_discount'] = True\n",
    "        result['uth_status'] = 'Zero Demand'\n",
    "        result['price_action'] = 'zero_demand_decrease'\n",
    "        \n",
    "        # Check if cart rule is tight (< normal_refill + 10*std) and increase if so\n",
    "        normal_refill = float(row.get('normal_refill', 5) or 5)\n",
    "        stddev = float(row.get('refill_stddev', 2) or 2)\n",
    "        cart_threshold = normal_refill + (10 * stddev)\n",
    "        \n",
    "        if current_cart < cart_threshold:\n",
    "            new_cart = min(cart_threshold, MAX_CART_RULE)\n",
    "            new_cart = max(new_cart, MIN_CART_RULE)\n",
    "            result['new_cart_rule'] = int(new_cart)\n",
    "            result['action_reason'] = f'Zero demand - 1 step below ({current_price:.2f} -> {new_price:.2f}) + SKU discount + open cart to {int(new_cart)}'\n",
    "        else:\n",
    "            result['action_reason'] = f'Zero demand - 1 step below ({current_price:.2f} -> {new_price:.2f}) + SKU discount'\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    # =========================================================================\n",
    "    # CASE 2: On Track (both qty and retailers ±10%)\n",
    "    # If has existing discounts, keep them (they'll be deactivated otherwise)\n",
    "    # =========================================================================\n",
    "    if (UTH_DROPPING_THRESHOLD <= qty_ratio <= UTH_GROWING_THRESHOLD and\n",
    "        UTH_DROPPING_THRESHOLD <= retailer_ratio <= UTH_GROWING_THRESHOLD):\n",
    "        result['uth_status'] = 'On Track'\n",
    "        result['price_action'] = 'hold'\n",
    "        \n",
    "        # Preserve existing discounts (all discounts are deactivated at start of each run)\n",
    "        if has_sku_disc:\n",
    "            result['activate_sku_discount'] = True\n",
    "            result['action_reason'] = f'On Track (qty={qty_ratio:.2f}, ret={retailer_ratio:.2f}) - keep existing SKU discount'\n",
    "        elif has_qd:\n",
    "            result['activate_qd'] = True\n",
    "            result['action_reason'] = f'On Track (qty={qty_ratio:.2f}, ret={retailer_ratio:.2f}) - keep existing QD'\n",
    "        else:\n",
    "            result['action_reason'] = f'On Track (qty={qty_ratio:.2f}, ret={retailer_ratio:.2f}) - no action'\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    # =========================================================================\n",
    "    # CASE 2.5: Retailers Growing but Qty On Track\n",
    "    # Action: Increase price 1 step (high retailer demand, normal qty = opportunity)\n",
    "    # =========================================================================\n",
    "    if (UTH_DROPPING_THRESHOLD <= qty_ratio <= UTH_GROWING_THRESHOLD and\n",
    "        retailer_ratio > UTH_GROWING_THRESHOLD):\n",
    "        result['uth_status'] = 'Retailers Growing'\n",
    "        \n",
    "        new_price = find_next_price_above(current_price, row)\n",
    "        if new_price > current_price:\n",
    "            result['new_price'] = new_price\n",
    "            result['price_action'] = 'retailers_growing_increase'\n",
    "            result['action_reason'] = f'Retailers growing (qty={qty_ratio:.2f}, ret={retailer_ratio:.2f}) - increase price ({current_price:.2f} -> {new_price:.2f})'\n",
    "        else:\n",
    "            result['price_action'] = 'hold'\n",
    "            result['action_reason'] = f'Retailers growing (qty={qty_ratio:.2f}, ret={retailer_ratio:.2f}) - no tier above, hold'\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    # =========================================================================\n",
    "    # CASE 3: Growing (qty > 110%)\n",
    "    # Find discount with HIGHEST contribution (from TODAY's UTH) and remove it\n",
    "    # Keep (re-activate) the others\n",
    "    # If no discounts -> increase price\n",
    "    # =========================================================================\n",
    "    if qty_ratio > UTH_GROWING_THRESHOLD:\n",
    "        result['uth_status'] = 'Growing'\n",
    "        \n",
    "        # Get TODAY's UTH discount contributions (not yesterday's)\n",
    "        sku_disc_cntrb = row.get('sku_disc_cntrb_uth', 0) or 0\n",
    "        t1_cntrb = row.get('t1_cntrb_uth', 0) or 0\n",
    "        t2_cntrb = row.get('t2_cntrb_uth', 0) or 0\n",
    "        t3_cntrb = row.get('t3_cntrb_uth', 0) or 0\n",
    "        \n",
    "        # Build list of EXISTING discounts with their contributions\n",
    "        # Note: We check if tiers EXIST (qty > 0), not just if they had sales today\n",
    "        # A tier can exist but have 0 contribution if no orders used it yet today\n",
    "        active_discounts = []\n",
    "        \n",
    "        # SKU discount: check if it exists (has_sku_disc from active discount query)\n",
    "        if has_sku_disc:\n",
    "            active_discounts.append(('sku_disc', sku_disc_cntrb))  # Include even if cntrb=0\n",
    "        \n",
    "        # QD tiers: check if each tier EXISTS (qty > 0 means the tier is configured)\n",
    "        if has_qd:\n",
    "            qd_t1_qty = row.get('qd_tier_1_qty', 0) or 0\n",
    "            qd_t2_qty = row.get('qd_tier_2_qty', 0) or 0\n",
    "            qd_t3_qty = row.get('qd_tier_3_qty', 0) or 0\n",
    "            \n",
    "            if qd_t1_qty > 0:  # Tier 1 exists\n",
    "                active_discounts.append(('qd_t1', t1_cntrb))  # Include even if cntrb=0\n",
    "            if qd_t2_qty > 0:  # Tier 2 exists\n",
    "                active_discounts.append(('qd_t2', t2_cntrb))  # Include even if cntrb=0\n",
    "            if qd_t3_qty > 0:  # Tier 3 exists\n",
    "                active_discounts.append(('qd_t3', t3_cntrb))  # Include even if cntrb=0\n",
    "        \n",
    "        if active_discounts:\n",
    "            # Sort by contribution descending - remove the highest\n",
    "            active_discounts.sort(key=lambda x: x[1], reverse=True)\n",
    "            highest_disc, highest_cntrb = active_discounts[0]\n",
    "            remaining_discounts = [d[0] for d in active_discounts[1:]]\n",
    "            \n",
    "            # Determine what to keep (re-activate)\n",
    "            keep_sku_disc = 'sku_disc' in remaining_discounts\n",
    "            keep_qd_t1 = 'qd_t1' in remaining_discounts\n",
    "            keep_qd_t2 = 'qd_t2' in remaining_discounts\n",
    "            keep_qd_t3 = 'qd_t3' in remaining_discounts\n",
    "            keep_any_qd = keep_qd_t1 or keep_qd_t2 or keep_qd_t3\n",
    "            \n",
    "            # Set activation flags\n",
    "            if keep_sku_disc:\n",
    "                result['activate_sku_discount'] = True\n",
    "            \n",
    "            if keep_any_qd:\n",
    "                result['activate_qd'] = True\n",
    "                result['keep_qd_tiers'] = [t for t in ['T1', 'T2', 'T3'] \n",
    "                                           if (t == 'T1' and keep_qd_t1) or \n",
    "                                              (t == 'T2' and keep_qd_t2) or \n",
    "                                              (t == 'T3' and keep_qd_t3)]\n",
    "            \n",
    "            result['removed_discount'] = highest_disc\n",
    "            result['removed_discount_cntrb'] = highest_cntrb\n",
    "            result['price_action'] = f'remove_{highest_disc}'\n",
    "            result['action_reason'] = f'Growing (qty={qty_ratio:.2f}) - remove {highest_disc} (cntrb={highest_cntrb}%)'\n",
    "            \n",
    "            if remaining_discounts:\n",
    "                result['action_reason'] += f', keep {remaining_discounts}'\n",
    "        \n",
    "        elif has_sku_disc or has_qd:\n",
    "            # Has discounts but no contribution data - remove all\n",
    "            result['price_action'] = 'remove_all_disc'\n",
    "            result['action_reason'] = f'Growing (qty={qty_ratio:.2f}) - remove all discounts (no contribution data)'\n",
    "        \n",
    "        else:\n",
    "            # No discounts - increase price\n",
    "            new_price = find_next_price_above(current_price, row)\n",
    "            if new_price > current_price:\n",
    "                result['new_price'] = new_price\n",
    "                result['price_action'] = 'increase'\n",
    "                result['action_reason'] = f'Growing (qty={qty_ratio:.2f}) - increase ({current_price:.2f} -> {new_price:.2f})'\n",
    "            else:\n",
    "                result['price_action'] = 'hold'\n",
    "                result['action_reason'] = f'Growing (qty={qty_ratio:.2f}) - no tier above, hold'\n",
    "        \n",
    "        # Check if cart too open\n",
    "        if is_cart_too_open(row):\n",
    "            result['new_cart_rule'] = adjust_cart_rule(current_cart, 'decrease', row)\n",
    "            result['action_reason'] += ' + reduce cart 20%'\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    # =========================================================================\n",
    "    # CASE 4: Dropping - Different actions based on qty vs retailer ratios\n",
    "    # =========================================================================\n",
    "    result['uth_status'] = 'Dropping'\n",
    "    \n",
    "    def apply_price_reduction():\n",
    "        \"\"\"Helper to apply price reduction if allowed.\"\"\"\n",
    "        if not can_reduce_price:\n",
    "            return None, f'Price reduction limit reached ({price_reductions_today}/{MAX_PRICE_REDUCTIONS_PER_DAY} today)'\n",
    "        \n",
    "        new_price = find_next_price_below(current_price, row)\n",
    "        if new_price < current_price:\n",
    "            commercial_min = float(row.get('commercial_min_price', row.get('minimum', 0)) or 0)\n",
    "            if pd.notna(commercial_min) and commercial_min > 0:\n",
    "                new_price = max(new_price, commercial_min)\n",
    "            return new_price, f'decrease ({current_price:.2f} -> {new_price:.2f})'\n",
    "        return None, 'no tier below'\n",
    "    \n",
    "    # CASE 4A: qty OK (≥90%) but retailers dropping (<90%)\n",
    "    # Action: SKU discount (add new OR keep existing), then price if already has\n",
    "    if qty_ok and retailer_dropping:\n",
    "        # Always set activate_sku_discount = True (either adding new or keeping existing)\n",
    "        result['activate_sku_discount'] = True\n",
    "        \n",
    "        if not has_sku_disc:\n",
    "            # Adding new SKU discount\n",
    "            result['price_action'] = 'add_sku_disc'\n",
    "            result['action_reason'] = f'Retailers dropping (ret={retailer_ratio:.2f}, qty OK) - ADD new SKU discount'\n",
    "        else:\n",
    "            # Keeping existing SKU discount + reduce price\n",
    "            new_price, reason = apply_price_reduction()\n",
    "            if new_price:\n",
    "                result['new_price'] = new_price\n",
    "                result['price_action'] = 'keep_sku_disc_and_decrease'\n",
    "                result['action_reason'] = f'Retailers dropping - KEEP SKU disc + {reason}'\n",
    "            else:\n",
    "                result['price_action'] = 'keep_sku_disc'\n",
    "                result['action_reason'] = f'Retailers dropping - KEEP SKU disc ({reason})'\n",
    "    \n",
    "    # CASE 4B: qty dropping (<90%) but retailers OK (≥90%)\n",
    "    # Action: QD (add new OR keep existing), then price if already has\n",
    "    elif qty_dropping and retailer_ok:\n",
    "        # Always set activate_qd = True (either adding new or keeping existing)\n",
    "        result['activate_qd'] = True\n",
    "        \n",
    "        if not has_qd:\n",
    "            # Adding new QD\n",
    "            result['price_action'] = 'add_qd'\n",
    "            result['action_reason'] = f'Qty dropping (qty={qty_ratio:.2f}, ret OK) - ADD new QD'\n",
    "        else:\n",
    "            # Keeping existing QD + reduce price\n",
    "            new_price, reason = apply_price_reduction()\n",
    "            if new_price:\n",
    "                result['new_price'] = new_price\n",
    "                result['price_action'] = 'keep_qd_and_decrease'\n",
    "                result['action_reason'] = f'Qty dropping - KEEP QD + {reason}'\n",
    "            else:\n",
    "                result['price_action'] = 'keep_qd'\n",
    "                result['action_reason'] = f'Qty dropping - KEEP QD ({reason})'\n",
    "    \n",
    "    # CASE 4C: Both dropping (<90%)\n",
    "    # Action: SKU discount (add new OR keep existing), then price if already has\n",
    "    elif qty_dropping and retailer_dropping:\n",
    "        # Always set activate_sku_discount = True (either adding new or keeping existing)\n",
    "        result['activate_sku_discount'] = True\n",
    "        \n",
    "        if not has_sku_disc:\n",
    "            # Adding new SKU discount\n",
    "            result['price_action'] = 'add_sku_disc'\n",
    "            result['action_reason'] = f'Both dropping (qty={qty_ratio:.2f}, ret={retailer_ratio:.2f}) - ADD new SKU discount'\n",
    "        else:\n",
    "            # Keeping existing SKU discount + reduce price\n",
    "            new_price, reason = apply_price_reduction()\n",
    "            if new_price:\n",
    "                result['new_price'] = new_price\n",
    "                result['price_action'] = 'keep_sku_disc_and_decrease'\n",
    "                result['action_reason'] = f'Both dropping - KEEP SKU disc + {reason}'\n",
    "            else:\n",
    "                result['price_action'] = 'keep_sku_disc'\n",
    "                result['action_reason'] = f'Both dropping - KEEP SKU disc ({reason})'\n",
    "    \n",
    "    else:\n",
    "        result['price_action'] = 'hold'\n",
    "        result['action_reason'] = f'Unexpected state (qty={qty_ratio:.2f}, ret={retailer_ratio:.2f})'\n",
    "    \n",
    "    # Increase cart for dropping SKUs\n",
    "    result['new_cart_rule'] = adjust_cart_rule(current_cart, 'increase', row)\n",
    "    result['action_reason'] += ' + increase cart 20%'\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"Main engine function loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 28512 SKUs...\n",
      "============================================================\n",
      "Processed 10000/28512 SKUs...\n",
      "Processed 20000/28512 SKUs...\n",
      "\n",
      "✅ Processed 28512 SKUs\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXECUTE MODULE 3\n",
    "# =============================================================================\n",
    "print(f\"Processing {len(df)} SKUs...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = []\n",
    "for idx, row in df.iterrows():\n",
    "    result = generate_periodic_action(row, df_previous_actions)\n",
    "    results.append(result)\n",
    "    \n",
    "    if (idx + 1) % 10000 == 0:\n",
    "        print(f\"Processed {idx + 1}/{len(df)} SKUs...\")\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(f\"\\n✅ Processed {len(df_results)} SKUs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODULE 3 SUMMARY\n",
      "============================================================\n",
      "\n",
      "Total SKUs: 28512\n",
      "\n",
      "By UTH Status:\n",
      "uth_status\n",
      "Zero Demand    20870\n",
      "None            7642\n",
      "\n",
      "Actions:\n",
      "  Price changes: 20870\n",
      "  Cart rule changes: 10367\n",
      "  SKU discounts to activate: 20870\n",
      "  QD to activate: 0\n",
      "  Discounts removed (Growing SKUs): 0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(\"=\"*60)\n",
    "print(\"MODULE 3 SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nTotal SKUs: {len(df_results)}\")\n",
    "\n",
    "print(f\"\\nBy UTH Status:\")\n",
    "print(df_results['uth_status'].value_counts(dropna=False).to_string())\n",
    "\n",
    "# Actions breakdown\n",
    "price_changes = df_results[df_results['new_price'].notna()]\n",
    "cart_changes = df_results[df_results['new_cart_rule'].notna()]\n",
    "sku_disc_activate = df_results[df_results['activate_sku_discount'] == True]\n",
    "qd_activate = df_results[df_results['activate_qd'] == True]\n",
    "discounts_removed = df_results[df_results['removed_discount'].notna()]\n",
    "\n",
    "print(f\"\\nActions:\")\n",
    "print(f\"  Price changes: {len(price_changes)}\")\n",
    "print(f\"  Cart rule changes: {len(cart_changes)}\")\n",
    "print(f\"  SKU discounts to activate: {len(sku_disc_activate)}\")\n",
    "print(f\"  QD to activate: {len(qd_activate)}\")\n",
    "print(f\"  Discounts removed (Growing SKUs): {len(discounts_removed)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Results exported to: module_3_output_20260126_0023.xlsx\n",
      "Total records: 28437 (after removing 75 duplicates)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXPORT RESULTS\n",
    "# =============================================================================\n",
    "output_cols = [\n",
    "    'product_id', 'warehouse_id', 'cohort_id', 'sku', 'brand', 'cat', 'stocks',\n",
    "    'current_price', 'wac_p', 'uth_qty', 'uth_retailers',\n",
    "    'p80_daily_240d', 'p70_daily_retailers_240d', 'avg_uth_pct',\n",
    "    'sku_disc_cntrb_uth', 't1_cntrb_uth', 't2_cntrb_uth', 't3_cntrb_uth',\n",
    "    'uth_qty_target', 'uth_retailer_target', 'qty_ratio', 'retailer_ratio', 'uth_status',\n",
    "    'new_price', 'price_action', 'current_cart_rule','new_cart_rule',\n",
    "    # SKU Discount fields\n",
    "    'activate_sku_discount',\n",
    "    # QD fields (for qd_handler)\n",
    "    'activate_qd', 'keep_qd_tiers',\n",
    "    'qd_tier_1_qty', 'qd_tier_1_disc_pct',\n",
    "    'qd_tier_2_qty', 'qd_tier_2_disc_pct',\n",
    "    'qd_tier_3_qty', 'qd_tier_3_disc_pct',\n",
    "    # Action tracking\n",
    "    'removed_discount', 'removed_discount_cntrb',\n",
    "    'price_reductions_today', 'action_reason'\n",
    "]\n",
    "\n",
    "# Filter to existing columns\n",
    "output_cols = [c for c in output_cols if c in df_results.columns]\n",
    "\n",
    "# Drop duplicates before saving\n",
    "df_output = df_results[output_cols].drop_duplicates(subset=['product_id', 'warehouse_id'], keep='first')\n",
    "df_output.to_excel(OUTPUT_FILE, index=False)\n",
    "print(f\"\\n✅ Results exported to: {OUTPUT_FILE}\")\n",
    "print(f\"Total records: {len(df_output)} (after removing {len(df_results) - len(df_output)} duplicates)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Push Cart Rules Handler loaded at 2026-01-26 00:25:36 Cairo time\n",
      "✓ API credentials loaded successfully\n",
      "Push Prices Handler loaded at 2026-01-26 00:25:37 Cairo time\n",
      "✓ API credentials loaded successfully\n",
      "✓ Google Sheets client initialized\n",
      "Fetching packing_units ...\n",
      "  Loaded 34838 records\n",
      "\n",
      "======================================================================\n",
      "STEP 1: PUSHING CART RULES\n",
      "======================================================================\n",
      "\n",
      "🧪 MODE: TESTING\n",
      "   Files will be prepared but NOT uploaded to API\n",
      "\n",
      "============================================================\n",
      "PUSH CART RULES - Source: module_3\n",
      "============================================================\n",
      "Total received: 28437\n",
      "Cart rule changes to push: 10051\n",
      "Skipped (no change): 18386\n",
      "\n",
      "Cart rule changes summary:\n",
      "  Increases: 10019\n",
      "  Decreases: 32\n",
      "\n",
      "📋 Prepared 13847 packing unit cart rules\n",
      "\n",
      "Sample cart rule adjustments (showing products with multiple PUs):\n",
      " product_id  basic_unit_count  final_cart_rule  final_pu_cart_rule\n",
      "          3                 1               40                  40\n",
      "          3                 1               26                  26\n",
      "          9                 1               10                  10\n",
      "          9                 1                8                   8\n",
      "          9                 1                8                   8\n",
      "          9                 1                6                   6\n",
      "         13                 1               38                  38\n",
      "         13                 1               21                  21\n",
      "         13                 1               21                  21\n",
      "         13                 1               25                  25\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 701\n",
      "==================================================\n",
      "  Saved: uploads/module_3_cart_rules_701.xlsx (2431 rows)\n",
      "  🧪 [TESTING] Would upload 2431 cart rules (skipped)\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 703\n",
      "==================================================\n",
      "  Saved: uploads/module_3_cart_rules_703.xlsx (1888 rows)\n",
      "  🧪 [TESTING] Would upload 1888 cart rules (skipped)\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 704\n",
      "==================================================\n",
      "  Saved: uploads/module_3_cart_rules_704.xlsx (1917 rows)\n",
      "  🧪 [TESTING] Would upload 1917 cart rules (skipped)\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1123\n",
      "==================================================\n",
      "  Saved: uploads/module_3_cart_rules_1123.xlsx (1047 rows)\n",
      "  🧪 [TESTING] Would upload 1047 cart rules (skipped)\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1124\n",
      "==================================================\n",
      "  Saved: uploads/module_3_cart_rules_1124.xlsx (1063 rows)\n",
      "  🧪 [TESTING] Would upload 1063 cart rules (skipped)\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1125\n",
      "==================================================\n",
      "  Saved: uploads/module_3_cart_rules_1125.xlsx (940 rows)\n",
      "  🧪 [TESTING] Would upload 940 cart rules (skipped)\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1126\n",
      "==================================================\n",
      "  Saved: uploads/module_3_cart_rules_1126.xlsx (1023 rows)\n",
      "  🧪 [TESTING] Would upload 1023 cart rules (skipped)\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 702\n",
      "==================================================\n",
      "  Saved: uploads/module_3_cart_rules_702.xlsx (1425 rows)\n",
      "  🧪 [TESTING] Would upload 1425 cart rules (skipped)\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 700\n",
      "==================================================\n",
      "  Saved: uploads/module_3_cart_rules_700.xlsx (2113 rows)\n",
      "  🧪 [TESTING] Would upload 2113 cart rules (skipped)\n",
      "\n",
      "============================================================\n",
      "🧪 TESTING MODE COMPLETE - NO CART RULES WERE UPLOADED\n",
      "============================================================\n",
      "Mode: testing\n",
      "Total prepared: 13847\n",
      "\n",
      "============================================================\n",
      "CART RULES RESULT\n",
      "============================================================\n",
      "Mode: testing\n",
      "Cart rule changes: 10051\n",
      "Pushed: 13847\n",
      "Failed: 0\n",
      "\n",
      "======================================================================\n",
      "STEP 2: PUSHING PRICES\n",
      "======================================================================\n",
      "\n",
      "🧪 MODE: TESTING\n",
      "   Files will be prepared but NOT uploaded to API\n",
      "Loading disable_pu_visibility from Google Sheets...\n",
      "  ✓ Loaded 89 products to disable min PU visibility\n",
      "\n",
      "============================================================\n",
      "PUSH PRICES - Source: module_3\n",
      "============================================================\n",
      "Total received: 28437\n",
      "Price changes to push: 19086\n",
      "Skipped (no change): 9351\n",
      "\n",
      "Price changes summary:\n",
      "  Increases: 7\n",
      "  Decreases: 19079\n",
      "\n",
      "📋 Prepared 21289 packing unit prices\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 700\n",
      "==================================================\n",
      "  Saved: uploads/module_3_700.xlsx (3253 rows)\n",
      "  🧪 [TESTING] Would upload 3253 prices (skipped)\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 701\n",
      "==================================================\n",
      "  Saved: uploads/module_3_701.xlsx (3364 rows)\n",
      "  🧪 [TESTING] Would upload 3364 prices (skipped)\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 702\n",
      "==================================================\n",
      "  Saved: uploads/module_3_702.xlsx (2215 rows)\n",
      "  🧪 [TESTING] Would upload 2215 prices (skipped)\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 703\n",
      "==================================================\n",
      "  Saved: uploads/module_3_703.xlsx (2807 rows)\n",
      "  🧪 [TESTING] Would upload 2807 prices (skipped)\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 704\n",
      "==================================================\n",
      "  Saved: uploads/module_3_704.xlsx (2850 rows)\n",
      "  🧪 [TESTING] Would upload 2850 prices (skipped)\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1123\n",
      "==================================================\n",
      "  Saved: uploads/module_3_1123.xlsx (1757 rows)\n",
      "  🧪 [TESTING] Would upload 1757 prices (skipped)\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1124\n",
      "==================================================\n",
      "  Saved: uploads/module_3_1124.xlsx (1686 rows)\n",
      "  🧪 [TESTING] Would upload 1686 prices (skipped)\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1125\n",
      "==================================================\n",
      "  Saved: uploads/module_3_1125.xlsx (1568 rows)\n",
      "  🧪 [TESTING] Would upload 1568 prices (skipped)\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1126\n",
      "==================================================\n",
      "  Saved: uploads/module_3_1126.xlsx (1789 rows)\n",
      "  🧪 [TESTING] Would upload 1789 prices (skipped)\n",
      "\n",
      "============================================================\n",
      "🧪 TESTING MODE COMPLETE - NO PRICES WERE UPLOADED\n",
      "============================================================\n",
      "Mode: testing\n",
      "Total prepared: 21289\n",
      "\n",
      "============================================================\n",
      "PRICES RESULT\n",
      "============================================================\n",
      "Mode: testing\n",
      "Source: module_3\n",
      "Timestamp: 2026-01-26 00:25:58\n",
      "Total received: 28437\n",
      "Price changes: 19086\n",
      "Pushed: 21289\n",
      "Skipped: 9351\n",
      "Failed: 0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PUSH CART RULES & PRICES\n",
    "# =============================================================================\n",
    "# Push cart rules FIRST, then prices\n",
    "# If cart rules fail for certain cohorts, skip those cohorts for prices\n",
    "\n",
    "%run push_cart_rules_handler.ipynb\n",
    "%run push_prices_handler.ipynb\n",
    "pus = get_packing_units()\n",
    "\n",
    "# ⚠️ MODE CONFIGURATION:\n",
    "# - 'testing' (default): Prepare files but DON'T upload to API\n",
    "# - 'live': Prepare files AND upload to MaxAB API\n",
    "PUSH_MODE = 'testing'  # Change to 'live' when ready to push\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: Push Cart Rules First\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 1: PUSHING CART RULES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "cart_result = push_cart_rules(df_output, pus, source_module='module_3', mode=PUSH_MODE)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CART RULES RESULT\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Mode: {cart_result['mode']}\")\n",
    "print(f\"Cart rule changes: {cart_result['cart_rule_changes']}\")\n",
    "print(f\"Pushed: {cart_result['pushed']}\")\n",
    "print(f\"Failed: {cart_result['failed']}\")\n",
    "if cart_result['failed_cohorts']:\n",
    "    print(f\"⚠️ Failed cohorts: {cart_result['failed_cohorts']}\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: Push Prices (skip failed cohorts)\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: PUSHING PRICES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get failed cohorts from cart rules to skip in price push\n",
    "failed_cohorts = cart_result.get('failed_cohorts', [])\n",
    "\n",
    "# Call push_prices with the results, skipping failed cohorts\n",
    "push_result = push_prices(df_output, pus, source_module='module_3', mode=PUSH_MODE, skip_cohorts=failed_cohorts)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PRICES RESULT\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Mode: {push_result['mode']}\")\n",
    "print(f\"Source: {push_result['source_module']}\")\n",
    "print(f\"Timestamp: {push_result['timestamp']}\")\n",
    "print(f\"Total received: {push_result['total_received']}\")\n",
    "print(f\"Price changes: {push_result['price_changes']}\")\n",
    "print(f\"Pushed: {push_result['pushed']}\")\n",
    "print(f\"Skipped: {push_result['skipped']}\")\n",
    "print(f\"Failed: {push_result['failed']}\")\n",
    "if push_result.get('skipped_cohorts'):\n",
    "    print(f\"⚠️ Skipped cohorts (cart rules failed): {push_result['skipped_cohorts']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 3: PROCESSING SKU DISCOUNTS\n",
      "======================================================================\n",
      "/home/ec2-user/.Renviron\n",
      "/home/ec2-user/service_account_key.json\n",
      "SKU Discount Handler loaded at 2026-01-26 00:26:20 Cairo time\n",
      "Excluded categories: ['كروت شحن']\n",
      "Excluded brands: ['فيوري', 'العروسة']\n",
      "AWS & API functions defined ✓\n",
      "✓ API credentials loaded successfully\n",
      "Snowflake timezone: America/Los_Angeles\n",
      "Function 1: deactivate_active_sku_discounts() defined ✓\n",
      "/home/ec2-user/.Renviron\n",
      "/home/ec2-user/service_account_key.json\n",
      "Queries Module | Timezone: America/Los_Angeles\n",
      "✅ UTH and Last Hour functions defined\n",
      "\n",
      "==================================================\n",
      "QUERIES MODULE READY\n",
      "==================================================\n",
      "\n",
      "Live Data Functions:\n",
      "  • get_current_stocks()\n",
      "  • get_packing_units()\n",
      "  • get_current_prices()\n",
      "  • get_current_wac()\n",
      "  • get_current_cart_rules()\n",
      "\n",
      "UTH Performance Functions:\n",
      "  • get_uth_performance()         - UTH qty/retailers (Snowflake)\n",
      "  • get_hourly_distribution()     - Historical hour contributions (Snowflake)\n",
      "  • get_last_hour_performance()   - Last hour qty/retailers (DWH)\n",
      "\n",
      "Note: Market prices use MODULE_1_INPUT data\n",
      "Retailer Selection Queries defined ✓\n",
      "  - get_churned_dropped_retailers()\n",
      "  - get_category_not_product_retailers()\n",
      "  - get_out_of_cycle_retailers()\n",
      "  - get_view_no_orders_retailers()\n",
      "  - get_excluded_retailers()\n",
      "  - get_retailers_with_quantity_discount()\n",
      "  - get_retailer_main_warehouse()\n",
      "Function 2: select_target_retailers() defined ✓\n",
      "  - Queries 4 retailer sources (churned, category, cycle, view)\n",
      "  - Applies exclusions (failed orders, inactive, wholesale)\n",
      "  - Removes retailers with existing quantity discounts\n",
      "Function 3: Price selection & discount calculation defined ✓\n",
      "  - margin_to_price()\n",
      "  - build_candidate_prices()\n",
      "  - select_target_price()\n",
      "  - calculate_discount_for_row()\n",
      "  - calculate_discounts_batch()\n",
      "Function 4: structure_sku_discount_dataframe() defined ✓\n",
      "  - clear_output_folder()\n",
      "  - structure_sku_discount_dataframe()\n",
      "  - save_sku_discount_files()\n",
      "Function 5: push_sku_discount() defined ✓\n",
      "  - _get_presigned_url()\n",
      "  - _upload_file_to_s3()\n",
      "  - _validate_sku_discount()\n",
      "  - _proceed_sku_discount()\n",
      "  - _upload_single_file()\n",
      "  - push_sku_discount()\n",
      "Main function: process_sku_discounts() defined ✓\n",
      "\n",
      "============================================================\n",
      "SKU DISCOUNT HANDLER MODULE READY\n",
      "============================================================\n",
      "\n",
      "Required input columns from Module 3:\n",
      "  - product_id, warehouse_id, sku, cohort_id, brand, cat\n",
      "  - activate_sku_discount (bool)\n",
      "  - current_price, new_price, wac_p\n",
      "  - doh, uth_qty, uth_status, active_sku_disc_pct\n",
      "  - target_margin, min_boundary\n",
      "\n",
      "Required market margin columns (prices derived via wac/(1-margin)):\n",
      "  - below_market\n",
      "  - market_min\n",
      "  - market_25\n",
      "  - market_50\n",
      "  - market_75\n",
      "  - market_max\n",
      "  - above_market\n",
      "\n",
      "Required margin tier columns:\n",
      "  - margin_tier_below\n",
      "  - margin_tier_1\n",
      "  - margin_tier_2\n",
      "  - margin_tier_3\n",
      "  - margin_tier_4\n",
      "  - margin_tier_5\n",
      "  - margin_tier_above_1\n",
      "  - margin_tier_above_2\n",
      "\n",
      "Retailer Selection: Queries 4 sources, applies exclusions, removes QD overlap\n",
      "\n",
      "Usage: result = process_sku_discounts(df_output, mode='testing')\n",
      "SKUs needing SKU discount: 20870\n",
      "  Merged market margins and margin tiers from df\n",
      "\n",
      "======================================================================\n",
      "SKU DISCOUNT HANDLER\n",
      "======================================================================\n",
      "Mode: testing\n",
      "Input records: 20870\n",
      "\n",
      "--------------------------------------------------\n",
      "STEP 1: Deactivating existing SKU discounts\n",
      "--------------------------------------------------\n",
      "🧪 Deactivating SKU discounts (mode: testing)\n",
      "  Querying active SKU discounts from Snowflake...\n",
      "  Found 49869 active SKU discounts to deactivate\n",
      "  🧪 [TESTING] Would deactivate 49869 SKU discounts (skipped)\n",
      "\n",
      "--------------------------------------------------\n",
      "STEP 2: Filtering SKUs for discount\n",
      "--------------------------------------------------\n",
      "SKUs flagged for discount: 20870\n",
      "\n",
      "  Applying exclusions...\n",
      "    - Excluded by category: 2\n",
      "    - Excluded by brand: 28\n",
      "\n",
      "  Final SKUs to activate: 20840\n",
      "\n",
      "--------------------------------------------------\n",
      "STEP 3: Calculating discount percentages\n",
      "--------------------------------------------------\n",
      "Calculating discounts for 20840 SKUs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating discounts: 100%|██████████| 20840/20840 [00:08<00:00, 2575.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ✓ Discounts calculated:\n",
      "    - Valid discounts: 15637\n",
      "    - Avg discount: 1.86%\n",
      "    - Discount sources: {'dropping_below_old': 7467, 'zero_demand': 3367, 'overstock': 3193, 'dropping_aggressive': 1966, 'below_min_threshold': 1172, 'no_lower_prices': 1130, 'no_reduction_needed': 1116, 'dropping_lowest': 1019, 'no_candidates': 406, 'overstock_target_margin': 4}\n",
      "\n",
      "  SKUs with valid discounts (>0%): 15637\n",
      "\n",
      "--------------------------------------------------\n",
      "STEP 4: Selecting target retailers\n",
      "--------------------------------------------------\n",
      "\n",
      "  Selecting target retailers...\n",
      "    SKUs with valid discounts: 15637\n",
      "    Created tuple string for 15570 unique product-warehouse combinations\n",
      "\n",
      "    Querying retailer sources...\n",
      "  Fetching churned/dropped retailers...\n",
      "    Found 34664 churned/dropped retailer-product combinations\n",
      "  Fetching category-not-product retailers...\n",
      "    Found 18302112 category-not-product retailer-product combinations\n",
      "  Fetching out-of-cycle retailers...\n",
      "    Found 14626 out-of-cycle retailer-product combinations\n",
      "  Fetching view-no-orders retailers...\n",
      "    Found 1991493 view-no-orders retailer-product combinations\n",
      "\n",
      "    Combining retailer sources...\n",
      "    Total retailer-product combinations before filtering: 19106983\n",
      "\n",
      "    Getting retailer main warehouses...\n",
      "  Fetching retailer main warehouses...\n",
      "    Found 114606 retailer-warehouse mappings\n",
      "    Retailers after warehouse filter: 18800268\n",
      "\n",
      "    Applying exclusions...\n",
      "  Fetching excluded retailers...\n",
      "    Found 126909 retailers to exclude\n",
      "    Excluded 3261753 retailers (failed orders, inactive, wholesale, existing discounts)\n",
      "\n",
      "    Removing retailers with existing quantity discounts...\n",
      "  Fetching retailers with quantity discounts...\n",
      "    Found 11307448 retailer-product combinations with quantity discounts\n",
      "    Removed 1675050 retailer-product combinations with existing QD\n",
      "\n",
      "    ✓ Final retailer-product combinations: 13863465\n",
      "    ✓ Unique retailers: 53062\n",
      "    ✓ Unique products: 2633\n",
      "\n",
      "    ✓ Final output rows: 13870962\n",
      "\n",
      "--------------------------------------------------\n",
      "STEP 5: Structuring data for API\n",
      "--------------------------------------------------\n",
      "Structuring 13870962 SKU discount records for API...\n",
      "  Step 1: Deduplicating...\n",
      "    Records after deduplication: 13863465\n",
      "  Step 2: Merging with packing units...\n",
      "Fetching packing_units ...\n",
      "  Loaded 34838 records\n",
      "    Records after PU merge: 19384254\n",
      "  Step 3: Creating HH_data format...\n",
      "  Step 4: Setting start/end times...\n",
      "    Start: 2026-01-26 00:39\n",
      "    End: 2026-01-26 12:39\n",
      "  Step 5: Grouping by retailer...\n",
      "    Unique retailers: 53062\n",
      "  Step 6: Grouping by discount combinations...\n",
      "    Unique discount combinations: 49028\n",
      "  Step 7: Chunking retailer lists (max 100 per chunk)...\n",
      "    Total chunks: 49028\n",
      "  Step 8: Finalizing columns...\n",
      "  ✓ Structured 49028 records for upload\n",
      "\n",
      "--------------------------------------------------\n",
      "STEP 6: Pushing to API\n",
      "--------------------------------------------------\n",
      "\n",
      "🧪 MODE: TESTING\n",
      "Processing 49028 SKU discount records...\n",
      "\n",
      "  Step 1: Saving files to output folder...\n",
      "\n",
      "Saving SKU discount files...\n",
      "  Clearing output folder...\n",
      "  Cleared 49 files from output folder\n",
      "  Saving 50 files (max 1000 rows each)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving files: 100%|██████████| 50/50 [00:23<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved 50 files to ../output/sku_discount_sheets\n",
      "\n",
      "  🧪 [TESTING] Files saved but S3 upload skipped\n",
      "  Files location: ../output/sku_discount_sheets\n",
      "  Files saved: 50\n",
      "\n",
      "======================================================================\n",
      "SUMMARY\n",
      "======================================================================\n",
      "Mode: testing\n",
      "Total input: 20870\n",
      "Discounts deactivated: 49869\n",
      "SKUs to activate: 20840\n",
      "SKUs with valid discounts: 15637\n",
      "Retailer-product combinations: 13870962\n",
      "Records created/uploaded: 49028\n",
      "Records failed: 0\n",
      "Files saved: 50\n",
      "Output folder: ../output/sku_discount_sheets\n",
      "\n",
      "============================================================\n",
      "SKU DISCOUNT RESULT\n",
      "============================================================\n",
      "Mode: testing\n",
      "Total input: 20870\n",
      "SKUs to activate: 20840\n",
      "Deactivated: 49869\n",
      "Created: 49028\n",
      "Failed: 0\n",
      "\n",
      "======================================================================\n",
      "STEP 4: PROCESSING QUANTITY DISCOUNTS\n",
      "======================================================================\n",
      "/home/ec2-user/.Renviron\n",
      "/home/ec2-user/service_account_key.json\n",
      "✓ QD Handler initialized\n",
      "  Timezone: America/Los_Angeles\n",
      "✓ QD calculation parameters:\n",
      "  MAX_DISCOUNT_PCT: 5.0%\n",
      "  MIN_DISCOUNT_PCT: 0.35%\n",
      "  RATIO RANGE: [1.1, 3.0]\n",
      "\n",
      "✓ Wholesale (T3) parameters:\n",
      "  WS_CAR_COST: 1400 EGP\n",
      "  WS_MAX_TICKET_SIZE: 35000 EGP\n",
      "  WS_MIN_MARGIN: 1.5%\n",
      "  TOP_SKUS_PER_WAREHOUSE: 400\n",
      "\n",
      "✓ Upload parameters:\n",
      "  MAX_GROUP_SIZE: 200\n",
      "  QD_DURATION_HOURS: 12\n",
      "✓ Data fetching functions defined\n",
      "✓ Tier price calculation function defined\n",
      "✓ Wholesale tier calculation function defined\n",
      "✓ process_qd() function defined\n",
      "Helper functions defined ✓\n",
      "✓ API functions defined\n",
      "✓ QD Handler ready to use\n",
      "\n",
      "Available functions:\n",
      "  - process_qd(df_qd, dry_run=True)      : Main function to process QDs from Module 3\n",
      "  - deactivate_active_qd(dry_run=True)   : Deactivate all active QDs\n",
      "  - create_upload_format(df_configs)     : Create upload format DataFrame\n",
      "  - prepare_upload_file(df_upload, ...)  : Prepare final upload file with tag IDs\n",
      "  - post_QD(filename)                    : Upload QD file to API\n",
      "  - prepare_cart_rules_update(df_work, df_qd) : Prepare cart rules update\n",
      "  - upload_cart_rules(cart_rules, ...)   : Upload cart rules by cohort\n",
      "SKUs needing QD processing: 0\n",
      "No SKUs need QD processing\n",
      "\n",
      "======================================================================\n",
      "MODULE 3 EXECUTION COMPLETE\n",
      "======================================================================\n",
      "Total SKUs processed: 28437\n",
      "Price changes: 26728\n",
      "Cart rule changes: 28146\n",
      "SKUs with SKU discount: 20795\n",
      "SKUs with QD: 0\n",
      "Output saved to: module_3_output_20260126_0023.xlsx\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 3: PROCESS SKU DISCOUNTS\n",
    "# =============================================================================\n",
    "# This step handles SKU discounts for SKUs that need them based on UTH performance.\n",
    "# Market data has already been refreshed, so we pass the df_output directly.\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 3: PROCESSING SKU DISCOUNTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "%run sku_discount_handler.ipynb\n",
    "\n",
    "# Filter to SKUs that need SKU discount\n",
    "df_sku_discount = df_results[df_results['activate_sku_discount'] == True].copy()\n",
    "print(f\"SKUs needing SKU discount: {len(df_sku_discount)}\")\n",
    "\n",
    "# Merge market margins and margin tiers from df (not in df_results)\n",
    "sku_discount_extra_cols = [\n",
    "    'product_id', 'warehouse_id',\n",
    "    # Market margins\n",
    "    'below_market', 'market_min', 'market_25', 'market_50', \n",
    "    'market_75', 'market_max', 'above_market',\n",
    "    # Margin tiers\n",
    "    'margin_tier_below', 'margin_tier_1', 'margin_tier_2', 'margin_tier_3', \n",
    "    'margin_tier_4', 'margin_tier_5', 'margin_tier_above_1', 'margin_tier_above_2',\n",
    "    # Other needed columns\n",
    "    'doh', 'zero_demand', 'target_margin', 'min_boundary', 'active_sku_disc_pct'\n",
    "]\n",
    "# Filter to columns that exist in df\n",
    "sku_discount_extra_cols = [c for c in sku_discount_extra_cols if c in df.columns]\n",
    "\n",
    "# Merge the extra columns from df\n",
    "df_sku_discount = df_sku_discount.merge(\n",
    "    df[sku_discount_extra_cols].drop_duplicates(subset=['product_id', 'warehouse_id']),\n",
    "    on=['product_id', 'warehouse_id'],\n",
    "    how='left',\n",
    "    suffixes=('', '_from_df')\n",
    ")\n",
    "print(f\"  Merged market margins and margin tiers from df\")\n",
    "\n",
    "if len(df_sku_discount) > 0:\n",
    "    sku_discount_result = process_sku_discounts(df_sku_discount, mode=PUSH_MODE)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SKU DISCOUNT RESULT\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Mode: {sku_discount_result['mode']}\")\n",
    "    print(f\"Total input: {sku_discount_result['total_input']}\")\n",
    "    print(f\"SKUs to activate: {sku_discount_result['to_activate']}\")\n",
    "    print(f\"Deactivated: {sku_discount_result['deactivated']}\")\n",
    "    print(f\"Created: {sku_discount_result['created']}\")\n",
    "    print(f\"Failed: {sku_discount_result['failed']}\")\n",
    "else:\n",
    "    print(\"No SKUs need SKU discounts\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 4: PROCESSING QUANTITY DISCOUNTS (QD)\n",
    "# =============================================================================\n",
    "# This step handles QD adjustments for SKUs flagged by the action engine.\n",
    "# Only processes SKUs where activate_qd=True and uses keep_qd_tiers to determine\n",
    "# which tiers to maintain.\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 4: PROCESSING QUANTITY DISCOUNTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "%run qd_handler.ipynb\n",
    "\n",
    "# Filter to SKUs that need QD processing\n",
    "df_qd = df_results[df_results['activate_qd'] == True].copy()\n",
    "print(f\"SKUs needing QD processing: {len(df_qd)}\")\n",
    "\n",
    "# Required columns for QD handler\n",
    "# Include all data needed for tier quantity and price calculations\n",
    "qd_columns = [\n",
    "    # Identifiers\n",
    "    'product_id', 'warehouse_id', 'cohort_id', 'sku', 'brand', 'cat',\n",
    "    # Pricing data\n",
    "    'wac_p', 'current_price', 'new_price', 'target_margin', 'min_boundary',\n",
    "    # Cart rules\n",
    "    'current_cart_rule', 'new_cart_rule',\n",
    "    # Market margins (to be converted to prices)\n",
    "    'below_market', 'market_min', 'market_25', 'market_50',\n",
    "    'market_75', 'market_max', 'above_market',\n",
    "    # Margin tiers (to be converted to prices)\n",
    "    'margin_tier_1', 'margin_tier_2', 'margin_tier_3', 'margin_tier_4',\n",
    "    'margin_tier_5', 'margin_tier_above_1', 'margin_tier_above_2',\n",
    "    # Performance data (for top SKU selection)\n",
    "    'mtd_qty',\n",
    "    # QD configuration\n",
    "    'keep_qd_tiers'\n",
    "]\n",
    "# Filter to columns that exist in df_results\n",
    "qd_columns = [c for c in qd_columns if c in df_results.columns]\n",
    "df_qd = df_qd[qd_columns].copy()\n",
    "\n",
    "if len(df_qd) > 0:\n",
    "    qd_result = process_qd(df_qd, mode=PUSH_MODE)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"QD PROCESSING RESULT\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Mode: {qd_result['mode']}\")\n",
    "    print(f\"Total input: {qd_result['total_input']}\")\n",
    "    print(f\"Processed: {qd_result['processed']}\")\n",
    "    print(f\"Failed: {qd_result['failed']}\")\n",
    "else:\n",
    "    print(\"No SKUs need QD processing\")\n",
    "\n",
    "# =============================================================================\n",
    "# FINAL SUMMARY\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODULE 3 EXECUTION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total SKUs processed: {len(df_output)}\")\n",
    "print(f\"Price changes: {(df_output['new_price'] != df_output['current_price']).sum()}\")\n",
    "print(f\"Cart rule changes: {(df_output['new_cart_rule'] != df_output['current_cart_rule']).sum()}\")\n",
    "print(f\"SKUs with SKU discount: {df_output['activate_sku_discount'].sum()}\")\n",
    "print(f\"SKUs with QD: {df_output['activate_qd'].sum()}\")\n",
    "print(f\"Output saved to: {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# UPLOAD RESULTS TO SNOWFLAKE AND SEND SLACK NOTIFICATION\n",
    "# =============================================================================\n",
    "from common_functions import upload_dataframe_to_snowflake, send_text_slack\n",
    "\n",
    "# Add created_at as TIMESTAMP (module runs multiple times per day)\n",
    "df_output['created_at'] = datetime.now(CAIRO_TZ)\n",
    "\n",
    "# Upload to Snowflake\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"UPLOADING RESULTS TO SNOWFLAKE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "upload_status = upload_dataframe_to_snowflake(\n",
    "    \"Egypt\", \n",
    "    df_output, \n",
    "    \"MATERIALIZED_VIEWS\", \n",
    "    \"pricing_periodic_push\", \n",
    "    \"append\", \n",
    "    auto_create_table=True, \n",
    "    conn=None\n",
    ")\n",
    "\n",
    "# Prepare status variables\n",
    "prices_pushed = push_result.get('pushed', 0) if 'push_result' in dir() else 0\n",
    "prices_failed = push_result.get('failed', 0) if 'push_result' in dir() else 0\n",
    "cart_rules_pushed = cart_result.get('pushed', 0) if 'cart_result' in dir() else 0\n",
    "cart_rules_failed = cart_result.get('failed', 0) if 'cart_result' in dir() else 0\n",
    "\n",
    "# SKU discount status\n",
    "sku_disc_processed = len(df_sku_discount) if 'df_sku_discount' in dir() else 0\n",
    "\n",
    "# QD status\n",
    "qd_processed = qd_result.get('processed', 0) if 'qd_result' in dir() and qd_result else 0\n",
    "qd_failed = qd_result.get('failed', 0) if 'qd_result' in dir() and qd_result else 0\n",
    "\n",
    "if upload_status:\n",
    "    slack_message = f\"\"\"✅ *Module 3 - Periodic Actions Completed*\n",
    "\n",
    "📅 Date: {datetime.now(CAIRO_TZ).strftime('%Y-%m-%d')}\n",
    "⏰ Completed at: {datetime.now(CAIRO_TZ).strftime('%H:%M:%S')} Cairo time\n",
    "🔧 Mode: {PUSH_MODE.upper()}\n",
    "\n",
    "📊 *Results:*\n",
    "• Total SKUs processed: {len(df_output):,}\n",
    "• Price changes: {(df_output['new_price'] != df_output['current_price']).sum():,}\n",
    "• Cart rule changes: {(df_output['new_cart_rule'] != df_output['current_cart_rule']).sum():,}\n",
    "\n",
    "📤 *Push Status:*\n",
    "• 💰 Prices: ✅ {prices_pushed} pushed | ❌ {prices_failed} failed\n",
    "• 🛒 Cart Rules: ✅ {cart_rules_pushed} pushed | ❌ {cart_rules_failed} failed\n",
    "• 🏷️ SKU Discounts: {sku_disc_processed} processed\n",
    "• 📦 Quantity Discounts: ✅ {qd_processed} processed | ❌ {qd_failed} failed\n",
    "\n",
    "🗄️ Results uploaded to: MATERIALIZED_VIEWS.pricing_periodic_push\"\"\"\n",
    "    \n",
    "    send_text_slack('new-pricing-logic', slack_message)\n",
    "    print(\"✅ Slack notification sent!\")\n",
    "    print(f\"✅ {len(df_output)} records uploaded to Snowflake\")\n",
    "else:\n",
    "    error_message = f\"\"\"❌ *Module 3 - Periodic Actions Failed*\n",
    "\n",
    "📅 Date: {datetime.now(CAIRO_TZ).strftime('%Y-%m-%d')}\n",
    "⏰ Failed at: {datetime.now(CAIRO_TZ).strftime('%H:%M:%S')} Cairo time\n",
    "⚠️ Upload to Snowflake failed - please check logs\n",
    "\n",
    "📤 *Push Status (before upload failure):*\n",
    "• 💰 Prices: ✅ {prices_pushed} pushed | ❌ {prices_failed} failed\n",
    "• 🛒 Cart Rules: ✅ {cart_rules_pushed} pushed | ❌ {cart_rules_failed} failed\n",
    "• 🏷️ SKU Discounts: {sku_disc_processed} processed\n",
    "• 📦 Quantity Discounts: ✅ {qd_processed} processed | ❌ {qd_failed} failed\"\"\"\n",
    "    \n",
    "    send_text_slack('new-pricing-logic', error_message)\n",
    "    print(\"❌ Error notification sent to Slack!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
