{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3: Periodic Action Module (UTH-Based Adjustments)\n",
    "\n",
    "## Purpose\n",
    "This module runs at 12 PM, 3 PM, 6 PM, 9 PM, and 12 AM Cairo time to:\n",
    "1. Adjust prices based on Up-Till-Hour (UTH) performance vs benchmarks\n",
    "2. Manage SKU discounts and Quantity Discounts based on performance\n",
    "3. Adjust cart rules dynamically\n",
    "\n",
    "## UTH Benchmarks\n",
    "- Calculate historical qty from start of day till current hour over the last 4 months\n",
    "- Multiply by P80 all-time-high quantity and P70 retailers\n",
    "\n",
    "## Action Logic\n",
    "- **On Track (±10%)**: No action\n",
    "- **Growing (>110%)**: Deactivate discounts or increase price, reduce cart if too open\n",
    "- **Dropping (<90%)**: Reduce price, increase cart by 20%\n",
    "- **Zero Demand (qty=0 today)**: Market min + SKU discount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T10:03:02.010358Z",
     "iopub.status.busy": "2026-02-01T10:03:02.010163Z",
     "iopub.status.idle": "2026-02-01T10:03:22.796764Z",
     "shell.execute_reply": "2026-02-01T10:03:22.795608Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Upgrade pip\n",
    "!pip install --upgrade pip\n",
    "# Connectivity\n",
    "!pip install psycopg2-binary\n",
    "!pip install snowflake-connector-python==3.15.0\n",
    "!pip install snowflake-sqlalchemy\n",
    "!pip install warnings\n",
    "!pip install keyring==23.11.0\n",
    "!pip install sqlalchemy==1.4.46\n",
    "!pip install requests\n",
    "!pip install boto3\n",
    "!pip install oauth2client\n",
    "!pip install gspread==5.9.0\n",
    "!pip install gspread_dataframe\n",
    "!pip install google.cloud\n",
    "# Data manipulation and analysis\n",
    "!pip install polars\n",
    "!pip install pandas==2.2.1\n",
    "!pip install numpy\n",
    "!pip install openpyxl\n",
    "!pip install xlsxwriter\n",
    "# Date and time handling\n",
    "!pip install --upgrade datetime\n",
    "!pip install python-time\n",
    "!pip install --upgrade pytz\n",
    "# Progress bar\n",
    "!pip install tqdm\n",
    "# Database data types\n",
    "!pip install db-dtypes\n",
    "# Modeling\n",
    "!pip install statsmodels\n",
    "!pip install scikit-learn\n",
    "!pip install import-ipynb\n",
    "# Plotting\n",
    "!pip install matplotlib\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T10:03:22.798909Z",
     "iopub.status.busy": "2026-02-01T10:03:22.798676Z",
     "iopub.status.idle": "2026-02-01T10:03:27.713266Z",
     "shell.execute_reply": "2026-02-01T10:03:27.712449Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.12/site-packages/snowflake/connector/options.py:104: UserWarning: You have an incompatible version of 'pyarrow' installed (20.0.0), please install a version that adheres to: 'pyarrow<19.0.0; extra == \"pandas\"'\n",
      "  warn_incompatible_dep(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.Renviron\n",
      "/home/ec2-user/service_account_key.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries Module | Timezone: America/Los_Angeles\n",
      "✅ UTH and Last Hour functions defined\n",
      "\n",
      "==================================================\n",
      "QUERIES MODULE READY\n",
      "==================================================\n",
      "\n",
      "Live Data Functions:\n",
      "  • get_current_stocks()\n",
      "  • get_packing_units()\n",
      "  • get_current_prices()\n",
      "  • get_current_wac()\n",
      "  • get_current_cart_rules()\n",
      "\n",
      "UTH Performance Functions:\n",
      "  • get_uth_performance()         - UTH qty/retailers (Snowflake)\n",
      "  • get_hourly_distribution()     - Historical hour contributions (Snowflake)\n",
      "  • get_last_hour_performance()   - Last hour qty/retailers (DWH)\n",
      "\n",
      "Note: Market prices use MODULE_1_INPUT data\n",
      "Retailer Selection Queries defined ✓\n",
      "  - get_churned_dropped_retailers()\n",
      "  - get_category_not_product_retailers()\n",
      "  - get_out_of_cycle_retailers()\n",
      "  - get_view_no_orders_retailers()\n",
      "  - get_excluded_retailers()\n",
      "  - get_retailers_with_quantity_discount()\n",
      "  - get_retailer_main_warehouse()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.Renviron\n",
      "/home/ec2-user/service_account_key.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Data Module loaded at 2026-02-01 12:03:27 Cairo time\n",
      "Snowflake timezone: America/Los_Angeles\n",
      "All queries defined ✓\n",
      "Helper functions defined ✓\n",
      "get_market_data() function defined ✓\n",
      "get_margin_tiers() function defined ✓\n",
      "\n",
      "======================================================================\n",
      "MARKET DATA MODULE READY\n",
      "======================================================================\n",
      "\n",
      "Available functions (NO INPUT REQUIRED):\n",
      "  - get_market_data()   : Fetch and process all market prices\n",
      "  - get_margin_tiers()  : Fetch and calculate margin tiers\n",
      "\n",
      "Usage:\n",
      "  %run market_data_module.ipynb\n",
      "  df_market = get_market_data()\n",
      "  df_tiers = get_margin_tiers()\n",
      "======================================================================\n",
      "Module 3: Periodic Actions\n",
      "Run Time (Cairo): 2026-02-01 12:03:27\n",
      "Current Hour (Cairo): 12\n",
      "Input: MATERIALIZED_VIEWS.Pricing_data_extraction (today's data)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS AND SETUP\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "# Run queries_module - this:\n",
    "# 1. Initializes Snowflake credentials (setup_environment_2.initialize_env())\n",
    "# 2. Provides query_snowflake() function\n",
    "# 3. Provides TIMEZONE from Snowflake\n",
    "# 4. Provides get_current_stocks(), get_current_prices(), get_current_wac(), get_current_cart_rules()\n",
    "%run queries_module.ipynb\n",
    "\n",
    "# Run market_data_module - this:\n",
    "# 1. Provides get_market_data() for fetching fresh market prices (NO INPUT REQUIRED)\n",
    "# 2. Provides get_margin_tiers() for fetching margin tiers (NO INPUT REQUIRED)\n",
    "# 3. Fetches Ben Soliman, Marketplace, and Scrapped prices\n",
    "# 4. Fills missing prices from group-level data\n",
    "# 5. Calculates market price percentiles and margin tiers\n",
    "%run market_data_module.ipynb\n",
    "\n",
    "# Cairo timezone\n",
    "CAIRO_TZ = pytz.timezone('Africa/Cairo')\n",
    "CAIRO_NOW = datetime.now(CAIRO_TZ)\n",
    "TODAY = CAIRO_NOW.date()\n",
    "CURRENT_HOUR = CAIRO_NOW.hour\n",
    "\n",
    "# Configuration\n",
    "UTH_GROWING_THRESHOLD = 1.10    # >110% = Growing\n",
    "UTH_DROPPING_THRESHOLD = 0.90   # <90% = Dropping\n",
    "CART_INCREASE_PCT = 0.20        # 20% cart increase\n",
    "CART_DECREASE_PCT = 0.20        # 20% cart decrease\n",
    "MIN_CART_RULE = 2\n",
    "MAX_CART_RULE = 150\n",
    "MIN_PRICE_CHANGE_EGP = 0.25     # Minimum 0.25 EGP for any price change\n",
    "CONTRIBUTION_THRESHOLD = 50     # 50% contribution threshold\n",
    "MAX_PRICE_REDUCTIONS_PER_DAY = 2  # Max price reductions per day\n",
    "# SKU discount percentage will be decided in sku_discount_handler\n",
    "\n",
    "# Input/Output configuration\n",
    "# Data is now loaded from Snowflake instead of Excel\n",
    "INPUT_TABLE = 'MATERIALIZED_VIEWS.Pricing_data_extraction'\n",
    "PREVIOUS_OUTPUT_TABLE = 'MATERIALIZED_VIEWS.pricing_periodic_push'\n",
    "OUTPUT_FILE = f'module_3_output_{CAIRO_NOW.strftime(\"%Y%m%d_%H%M\")}.xlsx'\n",
    "\n",
    "print(f\"Module 3: Periodic Actions\")\n",
    "print(f\"Run Time (Cairo): {CAIRO_NOW.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Current Hour (Cairo): {CURRENT_HOUR}\")\n",
    "print(f\"Input: {INPUT_TABLE} (today's data)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T10:03:27.715060Z",
     "iopub.status.busy": "2026-02-01T10:03:27.714842Z",
     "iopub.status.idle": "2026-02-01T10:03:28.485945Z",
     "shell.execute_reply": "2026-02-01T10:03:28.485216Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading previous actions from today...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No previous Module 3 outputs found for today. This is the first run.\n",
      "Previous actions loaded: 0 records\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD PREVIOUS ACTIONS (Track price reductions per day)\n",
    "# Now loads from Snowflake instead of local Excel files\n",
    "# =============================================================================\n",
    "\n",
    "def load_previous_actions():\n",
    "    \"\"\"Load previous Module 3 outputs from today (from Snowflake) to track price reductions.\"\"\"\n",
    "    try:\n",
    "        # Query today's previous actions from Snowflake\n",
    "        query = f\"\"\"\n",
    "        SELECT * FROM {PREVIOUS_OUTPUT_TABLE}\n",
    "        WHERE DATE(created_at) = '{TODAY}'\n",
    "        ORDER BY created_at\n",
    "        \"\"\"\n",
    "        df = query_snowflake(query)\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            print(\"No previous Module 3 outputs found for today. This is the first run.\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        print(f\"Loaded {len(df)} previous action records from Snowflake\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading previous actions from Snowflake: {e}\")\n",
    "        print(\"This may be the first run or table doesn't exist yet.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def count_price_reductions_today(product_id, warehouse_id, previous_df):\n",
    "    \"\"\"Count how many price reductions this SKU has had today.\"\"\"\n",
    "    if previous_df.empty:\n",
    "        return 0\n",
    "    \n",
    "    mask = (\n",
    "        (previous_df['product_id'] == product_id) & \n",
    "        (previous_df['warehouse_id'] == warehouse_id) &\n",
    "        (previous_df['price_action'].str.contains('decrease', na=False))\n",
    "    )\n",
    "    return mask.sum()\n",
    "def count_price_increase_today(product_id, warehouse_id, previous_df):\n",
    "    \"\"\"Count how many price increase this SKU has had today.\"\"\"\n",
    "    if previous_df.empty:\n",
    "        return 0\n",
    "    \n",
    "    mask = (\n",
    "        (previous_df['product_id'] == product_id) & \n",
    "        (previous_df['warehouse_id'] == warehouse_id) &\n",
    "        (previous_df['price_action'].str.contains('increase', na=False))\n",
    "    )\n",
    "    return mask.sum()\n",
    "    \n",
    "\n",
    "print(\"Loading previous actions from today...\")\n",
    "df_previous_actions = load_previous_actions()\n",
    "print(f\"Previous actions loaded: {len(df_previous_actions)} records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T10:03:28.487854Z",
     "iopub.status.busy": "2026-02-01T10:03:28.487644Z",
     "iopub.status.idle": "2026-02-01T10:03:28.493087Z",
     "shell.execute_reply": "2026-02-01T10:03:28.492353Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    prev_inc = (\n",
    "        df_previous_actions.assign(\n",
    "            inc_flag=df_previous_actions['price_action'].str.contains('increase', case=False, na=False)\n",
    "        )\n",
    "        .groupby(['product_id', 'warehouse_id'])['inc_flag']\n",
    "        .sum()\n",
    "        .reset_index(name='increase_count')\n",
    "    )\n",
    "except:\n",
    "    prev_inc = pd.DataFrame(columns=['product_id', 'warehouse_id','increase_count'])\n",
    "try:    \n",
    "    prev_red = (\n",
    "    df_previous_actions.assign(\n",
    "        red_flag=df_previous_actions['price_action'].str.contains('decrease', case=False, na=False)\n",
    "    )\n",
    "    .groupby(['product_id', 'warehouse_id'])['red_flag']\n",
    "    .sum()\n",
    "    .reset_index(name='reduced_count') \n",
    "    )\n",
    "except:\n",
    "    prev_red = pd.DataFrame(columns=['product_id', 'warehouse_id','reduced_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T10:03:28.494944Z",
     "iopub.status.busy": "2026-02-01T10:03:28.494636Z",
     "iopub.status.idle": "2026-02-01T10:03:28.499050Z",
     "shell.execute_reply": "2026-02-01T10:03:28.498386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snowflake connection ready\n",
      "Timezone: America/Los_Angeles\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SNOWFLAKE CONNECTION\n",
    "# =============================================================================\n",
    "# query_snowflake() and TIMEZONE are provided by queries_module.ipynb\n",
    "# (which also initializes Snowflake credentials from setup_environment_2)\n",
    "print(f\"Snowflake connection ready\")\n",
    "print(f\"Timezone: {TIMEZONE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T10:03:28.500880Z",
     "iopub.status.busy": "2026-02-01T10:03:28.500694Z",
     "iopub.status.idle": "2026-02-01T10:03:48.382877Z",
     "shell.execute_reply": "2026-02-01T10:03:48.382109Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading today's UTH performance with discount contributions...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6131 UTH records\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# QUERY 1: TODAY'S UTH PERFORMANCE\n",
    "# =============================================================================\n",
    "UTH_LIVE_QUERY = f'''\n",
    "WITH params AS (\n",
    "    SELECT\n",
    "        CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE AS today,\n",
    "        HOUR(CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())) AS current_hour\n",
    "),\n",
    "\n",
    "-- Map dynamic tags to warehouse IDs using name matching\n",
    "qd_det AS (\n",
    "    SELECT DISTINCT \n",
    "        dt.id AS tag_id, \n",
    "        dt.name AS tag_name,\n",
    "        REPLACE(w.name, ' ', '') AS warehouse_name,\n",
    "        w.id AS warehouse_id,\n",
    "        warehouse_name ILIKE '%' || CASE \n",
    "            WHEN SPLIT_PART(tag_name, '_', 1) = 'El' THEN SPLIT_PART(tag_name, '_', 2) \n",
    "            ELSE SPLIT_PART(tag_name, '_', 1) \n",
    "        END || '%' AS contains_flag\n",
    "    FROM dynamic_tags dt\n",
    "    JOIN dynamic_taggables dta ON dt.id = dta.dynamic_tag_id \n",
    "    CROSS JOIN warehouses w \n",
    "    WHERE dt.id > 3000\n",
    "        AND dt.name LIKE '%QD_rets%'\n",
    "        AND w.id IN (1, 236, 337, 8, 339, 170, 501, 401, 703, 632, 797, 962)\n",
    "        AND contains_flag = 'true'\n",
    "),\n",
    "\n",
    "-- Get current active QD configurations\n",
    "qd_config AS (\n",
    "    SELECT * \n",
    "    FROM (\n",
    "        SELECT \n",
    "            product_id,\n",
    "            start_at,\n",
    "            end_at,\n",
    "            packing_unit_id,\n",
    "            id AS qd_id,\n",
    "            qd.warehouse_id,\n",
    "            MAX(CASE WHEN tier = 1 THEN quantity END) AS tier_1_qty,\n",
    "            MAX(CASE WHEN tier = 1 THEN discount_percentage END) AS tier_1_discount_pct,\n",
    "            MAX(CASE WHEN tier = 2 THEN quantity END) AS tier_2_qty,\n",
    "            MAX(CASE WHEN tier = 2 THEN discount_percentage END) AS tier_2_discount_pct,\n",
    "            MAX(CASE WHEN tier = 3 THEN quantity END) AS tier_3_qty,\n",
    "            MAX(CASE WHEN tier = 3 THEN discount_percentage END) AS tier_3_discount_pct\n",
    "        FROM (\n",
    "            SELECT \n",
    "                qd.id,\n",
    "                qdv.product_id,\n",
    "                qdv.packing_unit_id,\n",
    "                qdv.quantity,\n",
    "                qdv.discount_percentage,\n",
    "                qd.dynamic_tag_id,\n",
    "                qd.start_at,\n",
    "                qd.end_at,\n",
    "                ROW_NUMBER() OVER (\n",
    "                    PARTITION BY qdv.product_id, qdv.packing_unit_id, qd.id \n",
    "                    ORDER BY qdv.quantity\n",
    "                ) AS tier\n",
    "            FROM quantity_discounts qd \n",
    "            JOIN quantity_discount_values qdv ON qdv.quantity_discount_id = qd.id\n",
    "            WHERE active = 'true'\n",
    "        ) qd_tiers\n",
    "        JOIN qd_det qd ON qd.tag_id = qd_tiers.dynamic_tag_id\n",
    "        GROUP BY ALL\n",
    "    )\n",
    "    QUALIFY ROW_NUMBER() OVER (PARTITION BY product_id, packing_unit_id, warehouse_id ORDER BY start_at DESC) = 1\n",
    "),\n",
    "\n",
    "-- Today's sales up-till-hour with discount breakdown\n",
    "today_uth_sales AS (\n",
    "    SELECT \n",
    "        pso.warehouse_id,\n",
    "        pso.product_id,\n",
    "        so.retailer_id,\n",
    "        pso.packing_unit_id,\n",
    "        pso.purchased_item_count AS qty,\n",
    "        pso.total_price AS nmv,\n",
    "        pso.ITEM_DISCOUNT_VALUE AS sku_discount_per_unit,\n",
    "        pso.ITEM_QUANTITY_DISCOUNT_VALUE AS qty_discount_per_unit,\n",
    "        qd.tier_1_qty,\n",
    "        qd.tier_2_qty,\n",
    "        qd.tier_3_qty,\n",
    "        -- Determine tier used\n",
    "        CASE \n",
    "            WHEN pso.ITEM_QUANTITY_DISCOUNT_VALUE = 0 OR qd.tier_1_qty IS NULL THEN 'Base'\n",
    "            WHEN qd.tier_3_qty IS NOT NULL AND pso.purchased_item_count >= qd.tier_3_qty THEN 'Tier 3'\n",
    "            WHEN qd.tier_2_qty IS NOT NULL AND pso.purchased_item_count >= qd.tier_2_qty THEN 'Tier 2'\n",
    "            WHEN qd.tier_1_qty IS NOT NULL AND pso.purchased_item_count >= qd.tier_1_qty THEN 'Tier 1'\n",
    "            ELSE 'Base'\n",
    "        END AS tier_used\n",
    "    FROM product_sales_order pso\n",
    "    JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "    LEFT JOIN qd_config qd \n",
    "        ON qd.product_id = pso.product_id \n",
    "        AND qd.packing_unit_id = pso.packing_unit_id\n",
    "        AND qd.warehouse_id = so.warehouse_id\n",
    "    CROSS JOIN params p\n",
    "    WHERE so.created_at::DATE = p.today\n",
    "        AND HOUR(so.created_at) < p.current_hour\n",
    "        AND so.sales_order_status_id NOT IN (7, 12)\n",
    "        AND so.channel IN ('telesales', 'retailer')\n",
    "        AND pso.purchased_item_count <> 0\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    warehouse_id,\n",
    "    product_id,\n",
    "    SUM(qty) AS uth_qty,\n",
    "    SUM(nmv) AS uth_nmv,\n",
    "    COUNT(DISTINCT retailer_id) AS uth_retailers,\n",
    "    -- SKU discount NMV and contribution\n",
    "    SUM(CASE WHEN sku_discount_per_unit > 0 THEN nmv ELSE 0 END) AS sku_discount_nmv_uth,\n",
    "    ROUND(SUM(CASE WHEN sku_discount_per_unit > 0 THEN nmv ELSE 0 END) * 100.0 / NULLIF(SUM(nmv), 0), 2) AS sku_disc_cntrb_uth,\n",
    "    -- Quantity discount NMV and contribution\n",
    "    SUM(CASE WHEN qty_discount_per_unit > 0 THEN nmv ELSE 0 END) AS qty_discount_nmv_uth,\n",
    "    ROUND(SUM(CASE WHEN qty_discount_per_unit > 0 THEN nmv ELSE 0 END) * 100.0 / NULLIF(SUM(nmv), 0), 2) AS qty_disc_cntrb_uth,\n",
    "    -- Tier-level NMV\n",
    "    SUM(CASE WHEN tier_used = 'Tier 1' THEN nmv ELSE 0 END) AS t1_nmv_uth,\n",
    "    SUM(CASE WHEN tier_used = 'Tier 2' THEN nmv ELSE 0 END) AS t2_nmv_uth,\n",
    "    SUM(CASE WHEN tier_used = 'Tier 3' THEN nmv ELSE 0 END) AS t3_nmv_uth,\n",
    "    -- Tier-level contributions\n",
    "    ROUND(SUM(CASE WHEN tier_used = 'Tier 1' THEN nmv ELSE 0 END) * 100.0 / NULLIF(SUM(nmv), 0), 2) AS t1_cntrb_uth,\n",
    "    ROUND(SUM(CASE WHEN tier_used = 'Tier 2' THEN nmv ELSE 0 END) * 100.0 / NULLIF(SUM(nmv), 0), 2) AS t2_cntrb_uth,\n",
    "    ROUND(SUM(CASE WHEN tier_used = 'Tier 3' THEN nmv ELSE 0 END) * 100.0 / NULLIF(SUM(nmv), 0), 2) AS t3_cntrb_uth\n",
    "FROM today_uth_sales\n",
    "GROUP BY warehouse_id, product_id\n",
    "HAVING SUM(nmv) > 0\n",
    "'''\n",
    "\n",
    "print(\"Loading today's UTH performance with discount contributions...\")\n",
    "df_uth_today = query_snowflake(UTH_LIVE_QUERY)\n",
    "print(f\"Loaded {len(df_uth_today)} UTH records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T10:03:48.385018Z",
     "iopub.status.busy": "2026-02-01T10:03:48.384813Z",
     "iopub.status.idle": "2026-02-01T10:03:55.152515Z",
     "shell.execute_reply": "2026-02-01T10:03:55.151807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching hourly distribution from Snowflake...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded 771 hourly distribution records\n",
      "Using avg_uth_pct_qty as avg_uth_pct for Module 3 compatibility\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# QUERY 2: HISTORICAL HOURLY DISTRIBUTION (Last 4 Months) - By Category & Warehouse\n",
    "# =============================================================================\n",
    "# Uses get_hourly_distribution() from queries_module\n",
    "\n",
    "df_hourly_dist = get_hourly_distribution()\n",
    "\n",
    "# Rename column for backwards compatibility with rest of Module 3\n",
    "df_hourly_dist['avg_uth_pct'] = df_hourly_dist['avg_uth_pct_qty']\n",
    "print(f\"Using avg_uth_pct_qty as avg_uth_pct for Module 3 compatibility\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T10:03:55.154673Z",
     "iopub.status.busy": "2026-02-01T10:03:55.154472Z",
     "iopub.status.idle": "2026-02-01T10:04:00.287173Z",
     "shell.execute_reply": "2026-02-01T10:04:00.286367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading active SKU discounts...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5230 active SKU discount records\n",
      "Loading active Quantity discounts...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0 active QD records\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# QUERY 3 & 4: ACTIVE DISCOUNTS\n",
    "# =============================================================================\n",
    "\n",
    "# SKU Discounts query (from data_extraction.ipynb)\n",
    "ACTIVE_SKU_DISCOUNTS_QUERY = f'''\n",
    "WITH active_sku_discount AS ( \n",
    "    SELECT \n",
    "        x.id AS sku_discount_id,\n",
    "        retailer_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        DISCOUNT_PERCENTAGE,\n",
    "        start_at,\n",
    "        end_at \n",
    "    FROM (\n",
    "        SELECT \n",
    "            sd.*,\n",
    "            f.value::INT AS retailer_id \n",
    "        FROM SKU_DISCOUNTS sd,\n",
    "        LATERAL FLATTEN(\n",
    "            input => SPLIT(\n",
    "                REPLACE(REPLACE(REPLACE(sd.retailer_ids, '{{', ''), '}}', ''), '\"', ''), \n",
    "                ','\n",
    "            )\n",
    "        ) f\n",
    "        WHERE start_at::DATE <= CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE\n",
    "        and end_at::DATE >= CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE\n",
    "            AND active = 'true'\n",
    "    ) x \n",
    "    JOIN SKU_DISCOUNT_VALUES sdv ON x.id = sdv.sku_discount_id\n",
    "    WHERE name_en = 'Special Discounts'\n",
    "    QUALIFY MAX(start_at) OVER (PARTITION BY retailer_id, product_id, packing_unit_id) = start_at \n",
    ")\n",
    "\n",
    "SELECT \n",
    "    product_id, \n",
    "    warehouse_id,\n",
    "    AVG(DISCOUNT_PERCENTAGE) AS active_sku_disc_pct,\n",
    "    1 AS has_active_sku_discount\n",
    "FROM (\n",
    "    SELECT \n",
    "        asd.*,\n",
    "        warehouse_id \n",
    "    FROM active_sku_discount asd \n",
    "    JOIN materialized_views.retailer_polygon rp ON rp.retailer_id = asd.retailer_id\n",
    "    JOIN WAREHOUSE_DISPATCHING_RULES wdr ON wdr.product_id = asd.product_id\n",
    "    JOIN DISPATCHING_POLYGONS dp ON dp.id = wdr.DISPATCHING_POLYGON_ID AND dp.district_id = rp.district_id\n",
    ")\n",
    "GROUP BY ALL\n",
    "'''\n",
    "\n",
    "# Active QD Query - Reuses the same CTE structure from UTH_LIVE_QUERY\n",
    "ACTIVE_QD_QUERY = f'''\n",
    "WITH qd_det AS (\n",
    "    SELECT DISTINCT \n",
    "        dt.id AS tag_id, \n",
    "        dt.name AS tag_name,\n",
    "        REPLACE(w.name, ' ', '') AS warehouse_name,\n",
    "        w.id AS warehouse_id,\n",
    "        warehouse_name ILIKE '%' || CASE \n",
    "            WHEN SPLIT_PART(tag_name, '_', 1) = 'El' THEN SPLIT_PART(tag_name, '_', 2) \n",
    "            ELSE SPLIT_PART(tag_name, '_', 1) \n",
    "        END || '%' AS contains_flag\n",
    "    FROM dynamic_tags dt\n",
    "    JOIN dynamic_taggables dta ON dt.id = dta.dynamic_tag_id \n",
    "    CROSS JOIN warehouses w \n",
    "    WHERE dt.id > 3000\n",
    "        AND dt.name LIKE '%QD_rets%'\n",
    "        AND w.id IN (1, 236, 337, 8, 339, 170, 501, 401, 703, 632, 797, 962)\n",
    "        AND contains_flag = 'true'\n",
    "),\n",
    "\n",
    "qd_config AS (\n",
    "    SELECT * \n",
    "    FROM (\n",
    "        SELECT \n",
    "            product_id,\n",
    "            packing_unit_id,\n",
    "            qd.warehouse_id,\n",
    "            MAX(CASE WHEN tier = 1 THEN quantity END) AS qd_tier_1_qty,\n",
    "            MAX(CASE WHEN tier = 1 THEN discount_percentage END) AS qd_tier_1_disc_pct,\n",
    "            MAX(CASE WHEN tier = 2 THEN quantity END) AS qd_tier_2_qty,\n",
    "            MAX(CASE WHEN tier = 2 THEN discount_percentage END) AS qd_tier_2_disc_pct,\n",
    "            MAX(CASE WHEN tier = 3 THEN quantity END) AS qd_tier_3_qty,\n",
    "            MAX(CASE WHEN tier = 3 THEN discount_percentage END) AS qd_tier_3_disc_pct\n",
    "        FROM (\n",
    "            SELECT \n",
    "                qd.id,\n",
    "                qdv.product_id,\n",
    "                qdv.packing_unit_id,\n",
    "                qdv.quantity,\n",
    "                qdv.discount_percentage,\n",
    "                qd.dynamic_tag_id,\n",
    "                qd.start_at,\n",
    "                ROW_NUMBER() OVER (\n",
    "                    PARTITION BY qdv.product_id, qdv.packing_unit_id, qd.id \n",
    "                    ORDER BY qdv.quantity\n",
    "                ) AS tier\n",
    "            FROM quantity_discounts qd \n",
    "            JOIN quantity_discount_values qdv ON qdv.quantity_discount_id = qd.id\n",
    "            WHERE  active = TRUE\n",
    "        ) qd_tiers\n",
    "        JOIN qd_det qd ON qd.tag_id = qd_tiers.dynamic_tag_id\n",
    "        GROUP BY ALL\n",
    "    )\n",
    "    QUALIFY ROW_NUMBER() OVER (PARTITION BY product_id, packing_unit_id, warehouse_id ORDER BY qd_tier_1_qty DESC) = 1\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    product_id,\n",
    "    warehouse_id,\n",
    "    qd_tier_1_qty,\n",
    "    qd_tier_1_disc_pct,\n",
    "    qd_tier_2_qty,\n",
    "    qd_tier_2_disc_pct,\n",
    "    qd_tier_3_qty,\n",
    "    qd_tier_3_disc_pct,\n",
    "    1 AS has_active_qd\n",
    "FROM qd_config\n",
    "'''\n",
    "\n",
    "print(\"Loading active SKU discounts...\")\n",
    "df_active_sku_disc = query_snowflake(ACTIVE_SKU_DISCOUNTS_QUERY)\n",
    "print(f\"Loaded {len(df_active_sku_disc)} active SKU discount records\")\n",
    "\n",
    "print(\"Loading active Quantity discounts...\")\n",
    "df_active_qd = query_snowflake(ACTIVE_QD_QUERY)\n",
    "print(f\"Loaded {len(df_active_qd)} active QD records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T10:04:00.289063Z",
     "iopub.status.busy": "2026-02-01T10:04:00.288859Z",
     "iopub.status.idle": "2026-02-01T10:04:56.140291Z",
     "shell.execute_reply": "2026-02-01T10:04:56.139460Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from Snowflake...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 28382 records from Snowflake\n",
      "\n",
      "Refreshing live data...\n",
      "Fetching current stocks...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded 1854703 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching current prices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded 116397 records\n",
      "Fetching current WAC...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded 8153 records\n",
      "Fetching current cart rules...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded 72968 records\n",
      "Live data refreshed: stocks, prices, WAC, cart rules\n",
      "\n",
      "Refreshing market prices and margin tiers...\n",
      "\n",
      "======================================================================\n",
      "FETCHING MARKET DATA\n",
      "======================================================================\n",
      "Timestamp: 2026-02-01 12:04:10 Cairo time\n",
      "\n",
      "Step 1: Fetching raw price data...\n",
      "  1.1 Ben Soliman prices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Loaded 1537 records\n",
      "  1.2 Marketplace prices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Loaded 11168 records\n",
      "  1.3 Scrapped prices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Loaded 5053 records\n",
      "  1.4 Product groups...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Loaded 1581 records\n",
      "  1.5 Sales data (for NMV weighting)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Loaded 21070 records\n",
      "  1.6 Margin stats...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Loaded 28631 records\n",
      "  1.7 Target margins...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Loaded 436 records\n",
      "  1.8 Product base (WAC)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Loaded 65286 records\n",
      "\n",
      "Step 2: Joining all market price sources (outer join)...\n",
      "    Market prices base: 15853 records\n",
      "\n",
      "Step 3: Adding cohort IDs and supporting data...\n",
      "    Records after adding cohorts: 23740\n",
      "\n",
      "Step 4: Processing group-level prices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26006/3245917641.py:139: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: pd.Series({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Records after group processing: 25131\n",
      "\n",
      "Step 5: Adding WAC and margin data...\n",
      "    Records with WAC: 24888\n",
      "\n",
      "Step 6: Filtering by price coverage...\n",
      "    Records after price coverage filter: 13723\n",
      "\n",
      "Step 7: Calculating price percentiles...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Records after price analysis: 12976\n",
      "\n",
      "Step 8: Converting prices to margins...\n",
      "\n",
      "======================================================================\n",
      "MARKET DATA COMPLETE\n",
      "======================================================================\n",
      "Total records: 12976\n",
      "  - With marketplace prices: 12314\n",
      "  - With scrapped prices: 6320\n",
      "  - With Ben Soliman prices: 8708\n",
      "  Fetched 12976 market data records\n",
      "\n",
      "======================================================================\n",
      "FETCHING MARGIN TIERS\n",
      "======================================================================\n",
      "Timestamp: 2026-02-01 12:04:54 Cairo time\n",
      "\n",
      "Step 1: Fetching margin boundaries from PRODUCT_STATISTICS...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loaded 17939 records\n",
      "\n",
      "Step 2: Adding cohort IDs...\n",
      "    Records with cohorts: 24731\n",
      "\n",
      "Step 3: Calculating margin tiers...\n",
      "\n",
      "======================================================================\n",
      "MARGIN TIERS COMPLETE\n",
      "======================================================================\n",
      "Total records: 24731\n",
      "\n",
      "Margin Tier Structure:\n",
      "  margin_tier_below:   effective_min - step (1 below)\n",
      "  margin_tier_1:       effective_min_margin\n",
      "  margin_tier_2:       effective_min + 1*step\n",
      "  margin_tier_3:       effective_min + 2*step\n",
      "  margin_tier_4:       effective_min + 3*step\n",
      "  margin_tier_5:       max_boundary\n",
      "  margin_tier_above_1: max_boundary + 1*step\n",
      "  margin_tier_above_2: max_boundary + 2*step\n",
      "  Fetched 24731 margin tier records\n",
      "Market data refreshed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data merged. Total records: 28390\n",
      "  SKUs with active SKU discount: 5214\n",
      "  SKUs with active QD: 0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD DATA FROM SNOWFLAKE (Instead of Excel file)\n",
    "# =============================================================================\n",
    "print(\"Loading data from Snowflake...\")\n",
    "\n",
    "# Query to get today's data from Pricing_data_extraction\n",
    "LOAD_QUERY = f\"\"\"\n",
    "SELECT * FROM {INPUT_TABLE}\n",
    "WHERE created_at = '{datetime.now(CAIRO_TZ).date()}'\n",
    "\"\"\"\n",
    "\n",
    "df = query_snowflake(LOAD_QUERY)\n",
    "print(f\"Loaded {len(df)} records from Snowflake\")\n",
    "\n",
    "# Refresh live data using queries_module\n",
    "print(\"\\nRefreshing live data...\")\n",
    "\n",
    "# Refresh stocks\n",
    "df_fresh_stocks = get_current_stocks()\n",
    "df = df.drop(columns=['stocks'], errors='ignore')\n",
    "df = df.merge(df_fresh_stocks, on=['warehouse_id', 'product_id'], how='left')\n",
    "df['stocks'] = df['stocks'].fillna(0)\n",
    "\n",
    "# Refresh current prices\n",
    "df_fresh_prices = get_current_prices()\n",
    "df = df.drop(columns=['current_price'], errors='ignore')\n",
    "df = df.merge(df_fresh_prices[['cohort_id', 'product_id', 'current_price']], \n",
    "              on=['cohort_id', 'product_id'], how='left')\n",
    "\n",
    "# Refresh WAC\n",
    "df_fresh_wac = get_current_wac()\n",
    "df = df.drop(columns=['wac_p'], errors='ignore')\n",
    "df = df.merge(df_fresh_wac, on='product_id', how='left')\n",
    "\n",
    "# Refresh cart rules\n",
    "df_fresh_cart = get_current_cart_rules()\n",
    "df = df.drop(columns=['current_cart_rule'], errors='ignore')\n",
    "df = df.merge(df_fresh_cart, on=['cohort_id', 'product_id'], how='left')\n",
    "\n",
    "print(f\"Live data refreshed: stocks, prices, WAC, cart rules\")\n",
    "\n",
    "# Refresh market prices and margin tiers using new standalone functions\n",
    "print(\"\\nRefreshing market prices and margin tiers...\")\n",
    "\n",
    "# Get fresh market data (no input required)\n",
    "df_fresh_market = get_market_data()\n",
    "print(f\"  Fetched {len(df_fresh_market)} market data records\")\n",
    "\n",
    "# Get fresh margin tiers (no input required)\n",
    "df_fresh_tiers = get_margin_tiers()\n",
    "print(f\"  Fetched {len(df_fresh_tiers)} margin tier records\")\n",
    "\n",
    "# Drop old market columns and merge fresh data\n",
    "market_cols_to_drop = [\n",
    "    'below_market', 'market_min', 'market_25', 'market_50', \n",
    "    'market_75', 'market_max', 'above_market',\n",
    "    'minimum', 'percentile_25', 'percentile_50', 'percentile_75', 'maximum',\n",
    "    'ben_soliman_price', 'final_min_price', 'final_max_price', 'final_mod_price',\n",
    "    'final_true_min', 'final_true_max', 'min_scrapped', 'scrapped25', \n",
    "    'scrapped50', 'scrapped75', 'max_scrapped'\n",
    "]\n",
    "df = df.drop(columns=[c for c in market_cols_to_drop if c in df.columns], errors='ignore')\n",
    "\n",
    "# Merge fresh market data\n",
    "df = df.merge(\n",
    "    df_fresh_market, \n",
    "    on=['cohort_id', 'product_id','region'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Drop old margin tier columns and merge fresh data\n",
    "margin_tier_cols_to_drop = [\n",
    "    'margin_tier_below', 'margin_tier_1', 'margin_tier_2', 'margin_tier_3',\n",
    "    'margin_tier_4', 'margin_tier_5', 'margin_tier_above_1', 'margin_tier_above_2',\n",
    "    'optimal_bm', 'min_boundary', 'max_boundary', 'median_bm',\n",
    "    'effective_min_margin', 'margin_step'\n",
    "]\n",
    "df = df.drop(columns=[c for c in margin_tier_cols_to_drop if c in df.columns], errors='ignore')\n",
    "\n",
    "# Merge fresh margin tiers\n",
    "df = df.merge(\n",
    "    df_fresh_tiers, \n",
    "    on=['cohort_id', 'product_id','region'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"Market data refreshed\")\n",
    "\n",
    "# Merge UTH today data - drop old columns first\n",
    "uth_cols = ['uth_qty', 'uth_nmv', 'uth_retailers', 'sku_discount_nmv_uth', 'sku_disc_cntrb_uth',\n",
    "            'qty_discount_nmv_uth', 'qty_disc_cntrb_uth', 't1_nmv_uth', 't2_nmv_uth', 't3_nmv_uth',\n",
    "            't1_cntrb_uth', 't2_cntrb_uth', 't3_cntrb_uth']\n",
    "df = df.drop(columns=[c for c in uth_cols if c in df.columns], errors='ignore')\n",
    "\n",
    "if len(df_uth_today) > 0:\n",
    "    df = df.merge(df_uth_today, on=['warehouse_id', 'product_id'], how='left')\n",
    "else:\n",
    "    for col in uth_cols:\n",
    "        df[col] = 0\n",
    "\n",
    "# Merge hourly distribution - drop old column first (now by warehouse_id + cat)\n",
    "df = df.drop(columns=['avg_uth_pct'], errors='ignore')\n",
    "if len(df_hourly_dist) > 0:\n",
    "    df = df.merge(df_hourly_dist, on=['warehouse_id', 'cat'], how='left')\n",
    "else:\n",
    "    df['avg_uth_pct'] = 0.5  # Default 50%\n",
    "\n",
    "# Merge active SKU discounts - drop old columns first\n",
    "sku_disc_cols = ['has_active_sku_discount', 'active_sku_disc_pct', 'active_sku_discount_value']\n",
    "df = df.drop(columns=[c for c in sku_disc_cols if c in df.columns], errors='ignore')\n",
    "\n",
    "if len(df_active_sku_disc) > 0:\n",
    "    df = df.merge(df_active_sku_disc, on=['warehouse_id', 'product_id'], how='left')\n",
    "else:\n",
    "    df['has_active_sku_discount'] = 0\n",
    "    df['active_sku_disc_pct'] = 0\n",
    "\n",
    "# Merge active QD - drop old columns first\n",
    "qd_cols = ['has_active_qd', 'qd_tier_1_qty', 'qd_tier_1_disc_pct', \n",
    "           'qd_tier_2_qty', 'qd_tier_2_disc_pct', 'qd_tier_3_qty', 'qd_tier_3_disc_pct']\n",
    "df = df.drop(columns=[c for c in qd_cols if c in df.columns], errors='ignore')\n",
    "\n",
    "if len(df_active_qd) > 0:\n",
    "    df = df.merge(df_active_qd, on=['warehouse_id', 'product_id'], how='left')\n",
    "else:\n",
    "    df['has_active_qd'] = 0\n",
    "    df['qd_tier_1_qty'] = 0\n",
    "    df['qd_tier_1_disc_pct'] = 0\n",
    "    df['qd_tier_2_qty'] = 0\n",
    "    df['qd_tier_2_disc_pct'] = 0\n",
    "    df['qd_tier_3_qty'] = 0\n",
    "    df['qd_tier_3_disc_pct'] = 0\n",
    "\n",
    "# Fill NaN\n",
    "df['uth_qty'] = df['uth_qty'].fillna(0)\n",
    "df['uth_retailers'] = df['uth_retailers'].fillna(0)\n",
    "df['avg_uth_pct'] = df['avg_uth_pct'].fillna(0.5)\n",
    "df['has_active_sku_discount'] = df['has_active_sku_discount'].fillna(0)\n",
    "df['active_sku_discount_value'] = df.get('active_sku_discount_value', pd.Series([0]*len(df))).fillna(0)\n",
    "df['has_active_qd'] = df['has_active_qd'].fillna(0)\n",
    "df['qd_tier_1_qty'] = df['qd_tier_1_qty'].fillna(0)\n",
    "df['qd_tier_1_disc_pct'] = df['qd_tier_1_disc_pct'].fillna(0)\n",
    "df['qd_tier_2_qty'] = df['qd_tier_2_qty'].fillna(0)\n",
    "df['qd_tier_2_disc_pct'] = df['qd_tier_2_disc_pct'].fillna(0)\n",
    "df['qd_tier_3_qty'] = df['qd_tier_3_qty'].fillna(0)\n",
    "df['qd_tier_3_disc_pct'] = df['qd_tier_3_disc_pct'].fillna(0)\n",
    "\n",
    "print(f\"Data merged. Total records: {len(df)}\")\n",
    "print(f\"  SKUs with active SKU discount: {(df['has_active_sku_discount'] == 1).sum()}\")\n",
    "print(f\"  SKUs with active QD: {(df['has_active_qd'] == 1).sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T10:04:56.142189Z",
     "iopub.status.busy": "2026-02-01T10:04:56.141987Z",
     "iopub.status.idle": "2026-02-01T10:04:56.152881Z",
     "shell.execute_reply": "2026-02-01T10:04:56.152158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions loaded.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_margin(price, wac):\n",
    "    if pd.isna(price) or pd.isna(wac) or price == 0:\n",
    "        return None\n",
    "    return (price - wac) / price\n",
    "\n",
    "def get_market_tiers(row):\n",
    "    \"\"\"Get sorted list of market price tiers.\"\"\"\n",
    "    tiers = []\n",
    "    for col in ['minimum', 'percentile_25', 'percentile_50', 'percentile_75', 'maximum']:\n",
    "        val = row.get(col)\n",
    "        if pd.notna(val) and val > 0:\n",
    "            tiers.append(val)\n",
    "    return sorted(set(tiers))\n",
    "\n",
    "def get_margin_tiers(row):\n",
    "    \"\"\"Get sorted list of margin-based price tiers (converted to prices).\"\"\"\n",
    "    tiers = []\n",
    "    wac = row.get('wac_p', 0)\n",
    "    if wac <= 0:\n",
    "        return tiers\n",
    "    \n",
    "    for tier_col in ['margin_tier_below','margin_tier_1', 'margin_tier_2', 'margin_tier_3', \n",
    "                     'margin_tier_4', 'margin_tier_5', 'margin_tier_above_1', 'margin_tier_above_2']:\n",
    "        margin = row.get(tier_col)\n",
    "        if pd.notna(margin) and 0 < margin < 1:\n",
    "            price = wac / (1 - margin)\n",
    "            tiers.append(round(price, 2))\n",
    "    return sorted(set(tiers))\n",
    "\n",
    "def find_next_price_above(current_price, row):\n",
    "    \"\"\"\n",
    "    Find the first price tier ABOVE current_price by at least MIN_PRICE_CHANGE_EGP.\n",
    "    Market first, then margin. Skips tiers less than 0.25 EGP above.\n",
    "    \"\"\"\n",
    "    current_price = float(current_price) if current_price else 0\n",
    "    if pd.isna(current_price) or current_price <= 0:\n",
    "        return current_price\n",
    "    \n",
    "    for tier in get_market_tiers(row):\n",
    "        if tier > current_price + MIN_PRICE_CHANGE_EGP:\n",
    "            return round(tier, 2)\n",
    "    \n",
    "    for tier in get_margin_tiers(row):\n",
    "        if tier > current_price + MIN_PRICE_CHANGE_EGP:\n",
    "            return round(tier, 2)\n",
    "    \n",
    "    return current_price\n",
    "\n",
    "def find_next_price_below(current_price, row):\n",
    "    \"\"\"\n",
    "    Find the first price tier BELOW current_price by at least MIN_PRICE_CHANGE_EGP.\n",
    "    Market first, then margin. Skips tiers less than 0.25 EGP below.\n",
    "    \"\"\"\n",
    "    current_price = float(current_price) if current_price else 0\n",
    "    if pd.isna(current_price) or current_price <= 0:\n",
    "        return current_price\n",
    "    \n",
    "    for tier in reversed(get_market_tiers(row)):\n",
    "        if tier < current_price - MIN_PRICE_CHANGE_EGP:\n",
    "            return round(tier, 2)\n",
    "    \n",
    "    for tier in reversed(get_margin_tiers(row)):\n",
    "        if tier < current_price - MIN_PRICE_CHANGE_EGP:\n",
    "            return round(tier, 2)\n",
    "    \n",
    "    return current_price\n",
    "\n",
    "def find_price_n_steps_below(current_price, n_steps, row):\n",
    "    \"\"\"Find price N steps below current (iteratively find next tier below).\"\"\"\n",
    "    price = current_price\n",
    "    for _ in range(n_steps):\n",
    "        next_price = find_next_price_below(price, row)\n",
    "        if next_price >= price:  # No tier below found\n",
    "            break\n",
    "        price = next_price\n",
    "    return price\n",
    "\n",
    "def is_cart_too_open(row):\n",
    "    \"\"\"Check if cart rule is too open: > normal_refill + 10*std\"\"\"\n",
    "    normal_refill = float(row.get('normal_refill', 5) or 5)\n",
    "    stddev = float(row.get('refill_stddev', 2) or 2)\n",
    "    current_cart = float(row.get('cart_rule', normal_refill) or normal_refill)\n",
    "    threshold = normal_refill + (10 * stddev)\n",
    "    return current_cart > threshold\n",
    "\n",
    "def adjust_cart_rule(current_cart, direction, row):\n",
    "    \"\"\"Adjust cart rule by 20% up or down.\"\"\"\n",
    "    normal_refill = float(row.get('normal_refill', 5) or 5)\n",
    "    stddev = float(row.get('refill_stddev', 2) or 2)\n",
    "    current_cart = float(current_cart or 5)\n",
    "    \n",
    "    if direction == 'increase':\n",
    "        new_cart = current_cart * (1 + CART_INCREASE_PCT)\n",
    "        new_cart = min(new_cart, MAX_CART_RULE)\n",
    "    else:  # decrease\n",
    "        # Formula: max(0.8 * cart, normal_refill + 3*std)\n",
    "        new_cart = current_cart * (1 - CART_DECREASE_PCT)\n",
    "        min_floor = normal_refill + (3 * stddev)\n",
    "        new_cart = max(new_cart, min_floor, MIN_CART_RULE)\n",
    "    \n",
    "    return int(new_cart)\n",
    "\n",
    "def get_highest_qd_tier_contribution(row):\n",
    "    \"\"\"Find which QD tier has highest contribution.\"\"\"\n",
    "    t1 = row.get('yesterday_t1_cntrb', 0) or 0\n",
    "    t2 = row.get('yesterday_t2_cntrb', 0) or 0\n",
    "    t3 = row.get('yesterday_t3_cntrb', 0) or 0\n",
    "    \n",
    "    if t1 >= t2 and t1 >= t3 and t1 > 0:\n",
    "        return 'T1', t1\n",
    "    elif t2 >= t1 and t2 >= t3 and t2 > 0:\n",
    "        return 'T2', t2\n",
    "    elif t3 > 0:\n",
    "        return 'T3', t3\n",
    "    return None, 0\n",
    "\n",
    "print(\"Helper functions loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T10:04:56.155084Z",
     "iopub.status.busy": "2026-02-01T10:04:56.154882Z",
     "iopub.status.idle": "2026-02-01T10:04:56.182684Z",
     "shell.execute_reply": "2026-02-01T10:04:56.181935Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main engine function loaded.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MAIN ENGINE: GENERATE PERIODIC ACTION\n",
    "# =============================================================================\n",
    "\n",
    "def generate_periodic_action(row, previous_df):\n",
    "    \"\"\"\n",
    "    Generate periodic action based on UTH performance.\n",
    "    \n",
    "    Logic:\n",
    "    - Zero Demand: 1 step below current + SKU discount\n",
    "    - On Track: No action\n",
    "    - Growing: Deactivate discounts or increase price, reduce cart if too open\n",
    "    - Dropping: Based on qty_ratio vs retailer_ratio:\n",
    "        - qty OK, retailers dropping: SKU discount (then price if already has)\n",
    "        - qty dropping, retailers OK: QD (then price if already has)\n",
    "        - both dropping: SKU discount (then price if already has)\n",
    "    - Price reduction max 2x per day\n",
    "    \"\"\"\n",
    "    product_id = row.get('product_id')\n",
    "    warehouse_id = row.get('warehouse_id')\n",
    "    \n",
    "    result = {\n",
    "        'product_id': product_id,\n",
    "        'warehouse_id': warehouse_id,\n",
    "        'cohort_id': row.get('cohort_id'),\n",
    "        'sku': row.get('sku'),\n",
    "        'brand': row.get('brand'),\n",
    "        'cat': row.get('cat'),\n",
    "        'stocks': row.get('stocks', 0),\n",
    "        'current_price': row.get('current_price'),\n",
    "        'wac_p': row.get('wac_p'),\n",
    "        'uth_qty': row.get('uth_qty', 0),\n",
    "        'uth_retailers': row.get('uth_retailers', 0),\n",
    "        'p80_daily_240d': row.get('p80_daily_240d', 1),\n",
    "        'p70_daily_retailers_240d': row.get('p70_daily_retailers_240d', 1),\n",
    "        'avg_uth_pct': row.get('avg_uth_pct', 0.5),\n",
    "        # Today's UTH discount contributions\n",
    "        'sku_disc_cntrb_uth': row.get('sku_disc_cntrb_uth', 0) or 0,\n",
    "        't1_cntrb_uth': row.get('t1_cntrb_uth', 0) or 0,\n",
    "        't2_cntrb_uth': row.get('t2_cntrb_uth', 0) or 0,\n",
    "        't3_cntrb_uth': row.get('t3_cntrb_uth', 0) or 0,\n",
    "        'uth_status': None,\n",
    "        'qty_ratio': None,\n",
    "        'retailer_ratio': None,\n",
    "        'new_price': None,\n",
    "        'price_action': None,\n",
    "        'current_cart_rule':row.get('current_cart_rule'),\n",
    "        'new_cart_rule': None,\n",
    "        'activate_sku_discount': False,  # True = SKU should have discount after this run\n",
    "        'activate_qd': False,             # True = SKU should have QD after this run\n",
    "        'keep_qd_tiers': None,            # List of QD tiers to keep (e.g., ['T1', 'T2'])\n",
    "        # QD tier configuration (passed to qd_handler)\n",
    "        'qd_tier_1_qty': row.get('qd_tier_1_qty', 0) or 0,\n",
    "        'qd_tier_1_disc_pct': row.get('qd_tier_1_disc_pct', 0) or 0,\n",
    "        'qd_tier_2_qty': row.get('qd_tier_2_qty', 0) or 0,\n",
    "        'qd_tier_2_disc_pct': row.get('qd_tier_2_disc_pct', 0) or 0,\n",
    "        'qd_tier_3_qty': row.get('qd_tier_3_qty', 0) or 0,\n",
    "        'qd_tier_3_disc_pct': row.get('qd_tier_3_disc_pct', 0) or 0,\n",
    "        'removed_discount': None,         # Which discount was removed (for Growing)\n",
    "        'removed_discount_cntrb': 0,      # Contribution of removed discount\n",
    "        'price_reductions_today': row.get('reduced_count', 0) or 0,\n",
    "        'action_reason': None,\n",
    "        # =====================================================================\n",
    "        # ADDITIONAL COLUMNS FOR QD AND SKU DISCOUNT HANDLERS\n",
    "        # =====================================================================\n",
    "        # Pricing and margin data\n",
    "        'target_margin': row.get('target_margin'),\n",
    "        'min_boundary': row.get('min_boundary'),\n",
    "        'doh': row.get('doh', 0),  # Days on hand - for SKU discount handler\n",
    "        'mtd_qty': row.get('mtd_qty', 0),  # MTD quantity - for QD ranking\n",
    "        # Active SKU discount info - for SKU discount handler\n",
    "        'active_sku_disc_pct': row.get('active_sku_disc_pct', 0),\n",
    "        'has_active_sku_discount': row.get('has_active_sku_discount', 0),\n",
    "        'has_active_qd': row.get('has_active_qd', 0),\n",
    "        # Market margins (converted to prices in handlers)\n",
    "        'below_market': row.get('below_market'),\n",
    "        'market_min': row.get('market_min'),\n",
    "        'market_25': row.get('market_25'),\n",
    "        'market_50': row.get('market_50'),\n",
    "        'market_75': row.get('market_75'),\n",
    "        'market_max': row.get('market_max'),\n",
    "        'above_market': row.get('above_market'),\n",
    "        # Margin tiers (converted to prices in handlers)\n",
    "        'margin_tier_below': row.get('margin_tier_below'),\n",
    "        'margin_tier_1': row.get('margin_tier_1'),\n",
    "        'margin_tier_2': row.get('margin_tier_2'),\n",
    "        'margin_tier_3': row.get('margin_tier_3'),\n",
    "        'margin_tier_4': row.get('margin_tier_4'),\n",
    "        'margin_tier_5': row.get('margin_tier_5'),\n",
    "        'margin_tier_above_1': row.get('margin_tier_above_1'),\n",
    "        'margin_tier_above_2': row.get('margin_tier_above_2'),\n",
    "    }\n",
    "    \n",
    "    # Skip if OOS (price only in Module 2)\n",
    "    if row.get('stocks', 0) <= 0:\n",
    "        result['action_reason'] = 'OOS - skip (price only in Module 2)'\n",
    "        return result\n",
    "    \n",
    "    # Skip if below minimum stock (stock < min selling unit qty)\n",
    "    if row.get('below_min_stock_flag', 0) == 1:\n",
    "        result['action_reason'] = 'Below min stock - skip (cannot sell)'\n",
    "        return result\n",
    "    \n",
    "    # Count previous price reductions today\n",
    "    price_reductions_today = row.get('reduced_count', 0)\n",
    "    can_reduce_price = price_reductions_today < MAX_PRICE_REDUCTIONS_PER_DAY\n",
    "\n",
    "    # Count previous price increase today\n",
    "    price_increase_today = row.get('increase_count', 0)\n",
    "    can_increase_price = price_increase_today < MAX_PRICE_REDUCTIONS_PER_DAY    \n",
    "    \n",
    "    # Calculate UTH benchmark: historical_pct * P80_qty\n",
    "    # Convert to float to handle decimal.Decimal from Snowflake\n",
    "    p80_qty = float(row.get('p80_daily_240d', 1) or 1)\n",
    "    p70_retailers = float(row.get('p70_daily_retailers_240d', 1) or 1)\n",
    "    avg_uth_pct = float(row.get('avg_uth_pct', 0.5) or 0.5)\n",
    "    \n",
    "    uth_qty_target = p80_qty * avg_uth_pct\n",
    "    uth_retailer_target = p70_retailers * avg_uth_pct\n",
    "    \n",
    "    uth_qty = float(row.get('uth_qty', 0) or 0)\n",
    "    uth_retailers = float(row.get('uth_retailers', 0) or 0)\n",
    "    \n",
    "    # Calculate UTH ratios\n",
    "    qty_ratio = uth_qty / uth_qty_target if uth_qty_target > 0 else 0\n",
    "    retailer_ratio = uth_retailers / uth_retailer_target if uth_retailer_target > 0 else 0\n",
    "    \n",
    "    result['uth_qty_target'] = round(uth_qty_target, 2)\n",
    "    result['uth_retailer_target'] = round(uth_retailer_target, 2)\n",
    "    result['qty_ratio'] = round(qty_ratio, 2)\n",
    "    result['retailer_ratio'] = round(retailer_ratio, 2)\n",
    "    \n",
    "    current_price = float(row.get('current_price') or 0)\n",
    "    current_cart = float(row.get('current_cart_rule', row.get('normal_refill', 10)) or 10)\n",
    "    has_sku_disc = row.get('has_active_sku_discount', 0) == 1\n",
    "    has_qd = row.get('has_active_qd', 0) == 1\n",
    "    \n",
    "    # Determine if qty/retailers are dropping (below threshold)\n",
    "    qty_dropping = qty_ratio < UTH_DROPPING_THRESHOLD\n",
    "    qty_ok = qty_ratio >= UTH_DROPPING_THRESHOLD\n",
    "    retailer_dropping = retailer_ratio < UTH_DROPPING_THRESHOLD\n",
    "    retailer_ok = retailer_ratio >= UTH_DROPPING_THRESHOLD\n",
    "    \n",
    "    # =========================================================================\n",
    "    # CASE 1: Zero demand today - 1 step below + SKU discount + open cart if tight\n",
    "    # =========================================================================\n",
    "    if uth_qty == 0:\n",
    "        new_price = find_next_price_below(current_price, row)\n",
    "        \n",
    "        # Apply commercial minimum floor\n",
    "        commercial_min = float(row.get('commercial_min_price', row.get('minimum', 0)) or 0)\n",
    "        if pd.notna(commercial_min) and commercial_min > 0:\n",
    "            new_price = max(new_price, commercial_min)\n",
    "        \n",
    "        result['new_price'] = new_price\n",
    "        result['activate_sku_discount'] = True\n",
    "        result['uth_status'] = 'Zero Demand'\n",
    "        result['price_action'] = 'zero_demand_decrease'\n",
    "        \n",
    "        # Check if cart rule is tight (< normal_refill + 10*std) and increase if so\n",
    "        normal_refill = float(row.get('normal_refill', 5) or 5)\n",
    "        stddev = float(row.get('refill_stddev', 2) or 2)\n",
    "        cart_threshold = normal_refill + (10 * stddev)\n",
    "        \n",
    "        if current_cart < cart_threshold:\n",
    "            new_cart = min(cart_threshold, MAX_CART_RULE)\n",
    "            new_cart = max(new_cart, MIN_CART_RULE)\n",
    "            result['new_cart_rule'] = int(new_cart)\n",
    "            result['action_reason'] = f'Zero demand - 1 step below ({current_price:.2f} -> {new_price:.2f}) + SKU discount + open cart to {int(new_cart)}'\n",
    "        else:\n",
    "            result['action_reason'] = f'Zero demand - 1 step below ({current_price:.2f} -> {new_price:.2f}) + SKU discount'\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    # =========================================================================\n",
    "    # CASE 2: On Track (both qty and retailers ±10%)\n",
    "    # If has existing discounts, keep them (they'll be deactivated otherwise)\n",
    "    # =========================================================================\n",
    "    if (UTH_DROPPING_THRESHOLD <= qty_ratio <= UTH_GROWING_THRESHOLD and\n",
    "        UTH_DROPPING_THRESHOLD <= retailer_ratio <= UTH_GROWING_THRESHOLD):\n",
    "        result['uth_status'] = 'On Track'\n",
    "        result['price_action'] = 'hold'\n",
    "        \n",
    "        # Preserve existing discounts (all discounts are deactivated at start of each run)\n",
    "        if has_sku_disc:\n",
    "            result['activate_sku_discount'] = True\n",
    "            result['action_reason'] = f'On Track (qty={qty_ratio:.2f}, ret={retailer_ratio:.2f}) - keep existing SKU discount'\n",
    "        elif has_qd:\n",
    "            result['activate_qd'] = True\n",
    "            result['action_reason'] = f'On Track (qty={qty_ratio:.2f}, ret={retailer_ratio:.2f}) - keep existing QD'\n",
    "        else:\n",
    "            result['action_reason'] = f'On Track (qty={qty_ratio:.2f}, ret={retailer_ratio:.2f}) - no action'\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    # =========================================================================\n",
    "    # CASE 2.5: Retailers Growing but Qty On Track\n",
    "    # Action: Increase price 1 step (high retailer demand, normal qty = opportunity)\n",
    "    # =========================================================================\n",
    "    if (UTH_DROPPING_THRESHOLD <= qty_ratio <= UTH_GROWING_THRESHOLD and\n",
    "        retailer_ratio > UTH_GROWING_THRESHOLD):\n",
    "        result['uth_status'] = 'Retailers Growing'\n",
    "        if can_increase_price:\n",
    "            new_price = find_next_price_above(current_price, row)\n",
    "        else:\n",
    "            new_price = np.nan\n",
    "        if new_price > current_price:\n",
    "            result['new_price'] = new_price\n",
    "            result['price_action'] = 'retailers_growing_increase'\n",
    "            result['action_reason'] = f'Retailers growing (qty={qty_ratio:.2f}, ret={retailer_ratio:.2f}) - increase price ({current_price:.2f} -> {new_price:.2f})'\n",
    "        else:\n",
    "            result['price_action'] = 'hold'\n",
    "            result['action_reason'] = f'Retailers growing (qty={qty_ratio:.2f}, ret={retailer_ratio:.2f}) - no tier above, hold'\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    # =========================================================================\n",
    "    # CASE 3: Growing (qty > 110%)\n",
    "    # Find discount with HIGHEST contribution (from TODAY's UTH) and remove it\n",
    "    # Keep (re-activate) the others\n",
    "    # If no discounts -> increase price\n",
    "    # =========================================================================\n",
    "    if qty_ratio > UTH_GROWING_THRESHOLD:\n",
    "        result['uth_status'] = 'Growing'\n",
    "        \n",
    "        # Get TODAY's UTH discount contributions (not yesterday's)\n",
    "        sku_disc_cntrb = row.get('sku_disc_cntrb_uth', 0) or 0\n",
    "        t1_cntrb = row.get('t1_cntrb_uth', 0) or 0\n",
    "        t2_cntrb = row.get('t2_cntrb_uth', 0) or 0\n",
    "        t3_cntrb = row.get('t3_cntrb_uth', 0) or 0\n",
    "        \n",
    "        # Build list of EXISTING discounts with their contributions\n",
    "        # Note: We check if tiers EXIST (qty > 0), not just if they had sales today\n",
    "        # A tier can exist but have 0 contribution if no orders used it yet today\n",
    "        active_discounts = []\n",
    "        \n",
    "        # SKU discount: check if it exists (has_sku_disc from active discount query)\n",
    "        if has_sku_disc:\n",
    "            active_discounts.append(('sku_disc', sku_disc_cntrb))  # Include even if cntrb=0\n",
    "        \n",
    "        # QD tiers: check if each tier EXISTS (qty > 0 means the tier is configured)\n",
    "        if has_qd:\n",
    "            qd_t1_qty = row.get('qd_tier_1_qty', 0) or 0\n",
    "            qd_t2_qty = row.get('qd_tier_2_qty', 0) or 0\n",
    "            qd_t3_qty = row.get('qd_tier_3_qty', 0) or 0\n",
    "            \n",
    "            if qd_t1_qty > 0:  # Tier 1 exists\n",
    "                active_discounts.append(('qd_t1', t1_cntrb))  # Include even if cntrb=0\n",
    "            if qd_t2_qty > 0:  # Tier 2 exists\n",
    "                active_discounts.append(('qd_t2', t2_cntrb))  # Include even if cntrb=0\n",
    "            if qd_t3_qty > 0:  # Tier 3 exists\n",
    "                active_discounts.append(('qd_t3', t3_cntrb))  # Include even if cntrb=0\n",
    "        \n",
    "        if active_discounts:\n",
    "            # Sort by contribution descending - remove the highest\n",
    "            active_discounts.sort(key=lambda x: x[1], reverse=True)\n",
    "            highest_disc, highest_cntrb = active_discounts[0]\n",
    "            remaining_discounts = [d[0] for d in active_discounts[1:]]\n",
    "            \n",
    "            # Determine what to keep (re-activate)\n",
    "            keep_sku_disc = 'sku_disc' in remaining_discounts\n",
    "            keep_qd_t1 = 'qd_t1' in remaining_discounts\n",
    "            keep_qd_t2 = 'qd_t2' in remaining_discounts\n",
    "            keep_qd_t3 = 'qd_t3' in remaining_discounts\n",
    "            keep_any_qd = keep_qd_t1 or keep_qd_t2 or keep_qd_t3\n",
    "            \n",
    "            # Set activation flags\n",
    "            if keep_sku_disc:\n",
    "                result['activate_sku_discount'] = True\n",
    "            \n",
    "            if keep_any_qd:\n",
    "                result['activate_qd'] = True\n",
    "                result['keep_qd_tiers'] = [t for t in ['T1', 'T2', 'T3'] \n",
    "                                           if (t == 'T1' and keep_qd_t1) or \n",
    "                                              (t == 'T2' and keep_qd_t2) or \n",
    "                                              (t == 'T3' and keep_qd_t3)]\n",
    "            \n",
    "            result['removed_discount'] = highest_disc\n",
    "            result['removed_discount_cntrb'] = highest_cntrb\n",
    "            result['price_action'] = f'remove_{highest_disc}'\n",
    "            result['action_reason'] = f'Growing (qty={qty_ratio:.2f}) - remove {highest_disc} (cntrb={highest_cntrb}%)'\n",
    "            \n",
    "            if remaining_discounts:\n",
    "                result['action_reason'] += f', keep {remaining_discounts}'\n",
    "        \n",
    "        elif has_sku_disc or has_qd:\n",
    "            # Has discounts but no contribution data - remove all\n",
    "            result['price_action'] = 'remove_all_disc'\n",
    "            result['action_reason'] = f'Growing (qty={qty_ratio:.2f}) - remove all discounts (no contribution data)'\n",
    "        \n",
    "        else:\n",
    "            # No discounts - increase price\n",
    "            if can_increase_price:\n",
    "                new_price = find_next_price_above(current_price, row)\n",
    "            else:\n",
    "                new_price = np.nan\n",
    "\n",
    "            if new_price > current_price:\n",
    "                result['new_price'] = new_price\n",
    "                result['price_action'] = 'increase'\n",
    "                result['action_reason'] = f'Growing (qty={qty_ratio:.2f}) - increase ({current_price:.2f} -> {new_price:.2f})'\n",
    "            else:\n",
    "                result['price_action'] = 'hold'\n",
    "                result['action_reason'] = f'Growing (qty={qty_ratio:.2f}) - no tier above, hold'\n",
    "        \n",
    "        # Check if cart too open\n",
    "        if is_cart_too_open(row):\n",
    "            result['new_cart_rule'] = adjust_cart_rule(current_cart, 'decrease', row)\n",
    "            result['action_reason'] += ' + reduce cart 20%'\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    # =========================================================================\n",
    "    # CASE 4: Dropping - Different actions based on qty vs retailer ratios\n",
    "    # =========================================================================\n",
    "    result['uth_status'] = 'Dropping'\n",
    "    \n",
    "    def apply_price_reduction():\n",
    "        \"\"\"Helper to apply price reduction if allowed.\"\"\"\n",
    "        if not can_reduce_price:\n",
    "            return None, f'Price reduction limit reached ({price_reductions_today}/{MAX_PRICE_REDUCTIONS_PER_DAY} today)'\n",
    "        \n",
    "        new_price = find_next_price_below(current_price, row)\n",
    "        if new_price < current_price:\n",
    "            commercial_min = float(row.get('commercial_min_price', row.get('minimum', 0)) or 0)\n",
    "            if pd.notna(commercial_min) and commercial_min > 0:\n",
    "                new_price = max(new_price, commercial_min)\n",
    "            return new_price, f'decrease ({current_price:.2f} -> {new_price:.2f})'\n",
    "        return None, 'no tier below'\n",
    "    \n",
    "    # CASE 4A: qty OK (≥90%) but retailers dropping (<90%)\n",
    "    # Action: SKU discount (add new OR keep existing), then price if already has\n",
    "    if qty_ok and retailer_dropping:\n",
    "        # Always set activate_sku_discount = True (either adding new or keeping existing)\n",
    "        result['activate_sku_discount'] = True\n",
    "        \n",
    "        if not has_sku_disc:\n",
    "            # Adding new SKU discount\n",
    "            result['price_action'] = 'add_sku_disc'\n",
    "            result['action_reason'] = f'Retailers dropping (ret={retailer_ratio:.2f}, qty OK) - ADD new SKU discount'\n",
    "        else:\n",
    "            # Keeping existing SKU discount + reduce price\n",
    "            new_price, reason = apply_price_reduction()\n",
    "            if new_price:\n",
    "                #result['new_price'] = new_price\n",
    "                result['price_action'] = 'keep_sku_disc_and_decrease'\n",
    "                result['action_reason'] = f'Retailers dropping - KEEP SKU disc + {reason}'\n",
    "            else:\n",
    "                result['price_action'] = 'keep_sku_disc'\n",
    "                result['action_reason'] = f'Retailers dropping - KEEP SKU disc ({reason})'\n",
    "    \n",
    "    # CASE 4B: qty dropping (<90%) but retailers OK (≥90%)\n",
    "    # Action: QD (add new OR keep existing), then price if already has\n",
    "    elif qty_dropping and retailer_ok:\n",
    "        # Always set activate_qd = True (either adding new or keeping existing)\n",
    "        result['activate_qd'] = True\n",
    "        \n",
    "        if not has_qd:\n",
    "            # Adding new QD\n",
    "            result['price_action'] = 'add_qd'\n",
    "            result['action_reason'] = f'Qty dropping (qty={qty_ratio:.2f}, ret OK) - ADD new QD'\n",
    "        else:\n",
    "            # Keeping existing QD + reduce price\n",
    "            new_price, reason = apply_price_reduction()\n",
    "            if new_price:\n",
    "                result['new_price'] = new_price\n",
    "                result['price_action'] = 'keep_qd_and_decrease'\n",
    "                result['action_reason'] = f'Qty dropping - KEEP QD + {reason}'\n",
    "            else:\n",
    "                result['price_action'] = 'keep_qd'\n",
    "                result['action_reason'] = f'Qty dropping - KEEP QD ({reason})'\n",
    "    \n",
    "    # CASE 4C: Both dropping (<90%)\n",
    "    # Action: SKU discount (add new OR keep existing), then price if already has\n",
    "    elif qty_dropping and retailer_dropping:\n",
    "        # Always set activate_sku_discount = True (either adding new or keeping existing)\n",
    "        result['activate_sku_discount'] = True\n",
    "        \n",
    "        if not has_sku_disc:\n",
    "            # Adding new SKU discount\n",
    "            result['price_action'] = 'add_sku_disc'\n",
    "            result['action_reason'] = f'Both dropping (qty={qty_ratio:.2f}, ret={retailer_ratio:.2f}) - ADD new SKU discount'\n",
    "        else:\n",
    "            # Keeping existing SKU discount + reduce price\n",
    "            new_price, reason = apply_price_reduction()\n",
    "            if new_price:\n",
    "                #result['new_price'] = new_price\n",
    "                result['price_action'] = 'keep_sku_disc_and_decrease'\n",
    "                result['action_reason'] = f'Both dropping - KEEP SKU disc + {reason}'\n",
    "            else:\n",
    "                result['price_action'] = 'keep_sku_disc'\n",
    "                result['action_reason'] = f'Both dropping - KEEP SKU disc ({reason})'\n",
    "    \n",
    "    else:\n",
    "        result['price_action'] = 'hold'\n",
    "        result['action_reason'] = f'Unexpected state (qty={qty_ratio:.2f}, ret={retailer_ratio:.2f})'\n",
    "    \n",
    "    # Increase cart for dropping SKUs\n",
    "    result['new_cart_rule'] = adjust_cart_rule(current_cart, 'increase', row)\n",
    "    result['action_reason'] += ' + increase cart 20%'\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"Main engine function loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T10:04:56.184377Z",
     "iopub.status.busy": "2026-02-01T10:04:56.184183Z",
     "iopub.status.idle": "2026-02-01T10:04:56.275960Z",
     "shell.execute_reply": "2026-02-01T10:04:56.275195Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26006/2181086998.py:3: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['increase_count'] = df['increase_count'].fillna(0)\n",
      "/tmp/ipykernel_26006/2181086998.py:4: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['reduced_count'] = df['reduced_count'].fillna(0)\n"
     ]
    }
   ],
   "source": [
    "df = df.merge(prev_inc,on=['product_id','warehouse_id'],how='left')\n",
    "df = df.merge(prev_red,on=['product_id','warehouse_id'],how='left')\n",
    "df['increase_count'] = df['increase_count'].fillna(0)\n",
    "df['reduced_count'] = df['reduced_count'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T10:04:56.277805Z",
     "iopub.status.busy": "2026-02-01T10:04:56.277607Z",
     "iopub.status.idle": "2026-02-01T10:05:01.881114Z",
     "shell.execute_reply": "2026-02-01T10:05:01.880354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 28390 SKUs...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000/28390 SKUs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20000/28390 SKUs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Processed 28390 SKUs\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXECUTE MODULE 3\n",
    "# =============================================================================\n",
    "print(f\"Processing {len(df)} SKUs...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = []\n",
    "for idx, row in df.iterrows():\n",
    "    result = generate_periodic_action(row, df_previous_actions)\n",
    "    results.append(result)\n",
    "    \n",
    "    if (idx + 1) % 10000 == 0:\n",
    "        print(f\"Processed {idx + 1}/{len(df)} SKUs...\")\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(f\"\\n✅ Processed {len(df_results)} SKUs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T10:05:01.883039Z",
     "iopub.status.busy": "2026-02-01T10:05:01.882834Z",
     "iopub.status.idle": "2026-02-01T10:05:01.896938Z",
     "shell.execute_reply": "2026-02-01T10:05:01.896103Z"
    }
   },
   "outputs": [],
   "source": [
    "df_results = df_results.drop_duplicates(subset=['product_id', 'warehouse_id'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T10:05:01.898876Z",
     "iopub.status.busy": "2026-02-01T10:05:01.898662Z",
     "iopub.status.idle": "2026-02-01T10:05:01.922897Z",
     "shell.execute_reply": "2026-02-01T10:05:01.922141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODULE 3 SUMMARY\n",
      "============================================================\n",
      "\n",
      "Total SKUs: 28382\n",
      "\n",
      "By UTH Status:\n",
      "uth_status\n",
      "Zero Demand          13364\n",
      "None                  9622\n",
      "Dropping              2780\n",
      "Growing               2217\n",
      "Retailers Growing      376\n",
      "On Track                23\n",
      "\n",
      "Actions:\n",
      "  Price changes: 14902\n",
      "  Cart rule changes: 16030\n",
      "  SKU discounts to activate: 14087\n",
      "  QD to activate: 2068\n",
      "  Discounts removed (Growing SKUs): 1002\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(\"=\"*60)\n",
    "print(\"MODULE 3 SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nTotal SKUs: {len(df_results)}\")\n",
    "\n",
    "print(f\"\\nBy UTH Status:\")\n",
    "print(df_results['uth_status'].value_counts(dropna=False).to_string())\n",
    "\n",
    "# Actions breakdown\n",
    "price_changes = df_results[df_results['new_price'].notna()]\n",
    "cart_changes = df_results[df_results['new_cart_rule'].notna()]\n",
    "sku_disc_activate = df_results[df_results['activate_sku_discount'] == True]\n",
    "qd_activate = df_results[df_results['activate_qd'] == True]\n",
    "discounts_removed = df_results[df_results['removed_discount'].notna()]\n",
    "\n",
    "print(f\"\\nActions:\")\n",
    "print(f\"  Price changes: {len(price_changes)}\")\n",
    "print(f\"  Cart rule changes: {len(cart_changes)}\")\n",
    "print(f\"  SKU discounts to activate: {len(sku_disc_activate)}\")\n",
    "print(f\"  QD to activate: {len(qd_activate)}\")\n",
    "print(f\"  Discounts removed (Growing SKUs): {len(discounts_removed)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T10:05:01.925308Z",
     "iopub.status.busy": "2026-02-01T10:05:01.925073Z",
     "iopub.status.idle": "2026-02-01T10:05:18.929928Z",
     "shell.execute_reply": "2026-02-01T10:05:18.929110Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Results exported to: module_3_output_20260201_1203.xlsx\n",
      "Total records: 28382 (after removing 0 duplicates)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXPORT RESULTS\n",
    "# =============================================================================\n",
    "output_cols = [\n",
    "    # Identifiers\n",
    "    'product_id', 'warehouse_id', 'cohort_id', 'sku', 'brand', 'cat', 'stocks',\n",
    "    # Pricing data\n",
    "    'current_price', 'wac_p', 'new_price',\n",
    "    'target_margin', 'min_boundary',\n",
    "    # Performance data\n",
    "    'uth_qty', 'uth_retailers',\n",
    "    'p80_daily_240d', 'p70_daily_retailers_240d', 'avg_uth_pct',\n",
    "    'sku_disc_cntrb_uth', 't1_cntrb_uth', 't2_cntrb_uth', 't3_cntrb_uth',\n",
    "    'uth_qty_target', 'uth_retailer_target', 'qty_ratio', 'retailer_ratio', 'uth_status',\n",
    "    'doh', 'mtd_qty',\n",
    "    # Cart rules\n",
    "    'price_action', 'current_cart_rule', 'new_cart_rule',\n",
    "    # SKU Discount fields\n",
    "    'activate_sku_discount', 'active_sku_disc_pct', 'has_active_sku_discount',\n",
    "    # QD fields (for qd_handler)\n",
    "    'activate_qd', 'keep_qd_tiers', 'has_active_qd',\n",
    "    'qd_tier_1_qty', 'qd_tier_1_disc_pct',\n",
    "    'qd_tier_2_qty', 'qd_tier_2_disc_pct',\n",
    "    'qd_tier_3_qty', 'qd_tier_3_disc_pct',\n",
    "    # Market margins (for handlers to convert to prices)\n",
    "    'below_market', 'market_min', 'market_25', 'market_50',\n",
    "    'market_75', 'market_max', 'above_market',\n",
    "    # Margin tiers (for handlers to convert to prices)\n",
    "    'margin_tier_below', 'margin_tier_1', 'margin_tier_2', 'margin_tier_3', 'margin_tier_4',\n",
    "    'margin_tier_5', 'margin_tier_above_1', 'margin_tier_above_2',\n",
    "    # Action tracking\n",
    "    'removed_discount', 'removed_discount_cntrb',\n",
    "    'price_reductions_today', 'action_reason'\n",
    "]\n",
    "\n",
    "# Filter to existing columns\n",
    "output_cols = [c for c in output_cols if c in df_results.columns]\n",
    "\n",
    "# Drop duplicates before saving\n",
    "df_output = df_results[output_cols].drop_duplicates(subset=['product_id', 'warehouse_id'], keep='first')\n",
    "df_output.to_excel(OUTPUT_FILE, index=False)\n",
    "print(f\"\\n✅ Results exported to: {OUTPUT_FILE}\")\n",
    "print(f\"Total records: {len(df_output)} (after removing {len(df_results) - len(df_output)} duplicates)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T10:05:18.931846Z",
     "iopub.status.busy": "2026-02-01T10:05:18.931638Z",
     "iopub.status.idle": "2026-02-01T10:07:46.167118Z",
     "shell.execute_reply": "2026-02-01T10:07:46.166163Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Push Cart Rules Handler loaded at 2026-02-01 12:05:18 Cairo time\n",
      "✓ API credentials loaded successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Push Prices Handler loaded at 2026-02-01 12:05:19 Cairo time\n",
      "✓ API credentials loaded successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Google Sheets client initialized\n",
      "Fetching packing_units ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded 34954 records\n",
      "\n",
      "======================================================================\n",
      "STEP 1: PUSHING CART RULES\n",
      "======================================================================\n",
      "\n",
      "🚀 MODE: LIVE\n",
      "   Files will be prepared AND uploaded to API\n",
      "\n",
      "============================================================\n",
      "PUSH CART RULES - Source: module_3\n",
      "============================================================\n",
      "Total received: 28382\n",
      "Cart rule changes to push: 11873\n",
      "Skipped (no change): 16509\n",
      "\n",
      "Cart rule changes summary:\n",
      "  Increases: 11873\n",
      "  Decreases: 0\n",
      "\n",
      "📋 Prepared 14840 packing unit cart rules\n",
      "\n",
      "Sample cart rule adjustments (showing products with multiple PUs):\n",
      " product_id  basic_unit_count  final_cart_rule  final_pu_cart_rule\n",
      "          3                 1               26                  26\n",
      "          3                 1               18                  18\n",
      "          3                 1                8                   8\n",
      "          3                 1               35                  35\n",
      "          3                 1               56                  56\n",
      "          9                 1               30                  30\n",
      "          9                 1                8                   8\n",
      "          9                 1                5                   5\n",
      "         10                 1               29                  29\n",
      "         10                 1                9                   9\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 700\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved: uploads/module_3_cart_rules_700.xlsx (1544 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|██████████| 1/1 [00:00<00:00, 20.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 702\n",
      "==================================================\n",
      "  Saved: uploads/module_3_cart_rules_702.xlsx (1337 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|██████████| 1/1 [00:00<00:00, 22.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 703\n",
      "==================================================\n",
      "  Saved: uploads/module_3_cart_rules_703.xlsx (2588 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|██████████| 1/1 [00:00<00:00, 12.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1124\n",
      "==================================================\n",
      "  Saved: uploads/module_3_cart_rules_1124.xlsx (1008 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|██████████| 1/1 [00:00<00:00, 31.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1125\n",
      "==================================================\n",
      "  Saved: uploads/module_3_cart_rules_1125.xlsx (890 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|██████████| 1/1 [00:00<00:00, 33.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 701\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved: uploads/module_3_cart_rules_701.xlsx (2784 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|██████████| 1/1 [00:00<00:00, 11.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 704\n",
      "==================================================\n",
      "  Saved: uploads/module_3_cart_rules_704.xlsx (2599 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|██████████| 1/1 [00:00<00:00, 12.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1126\n",
      "==================================================\n",
      "  Saved: uploads/module_3_cart_rules_1126.xlsx (1152 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|██████████| 1/1 [00:00<00:00, 26.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1123\n",
      "==================================================\n",
      "  Saved: uploads/module_3_cart_rules_1123.xlsx (938 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|██████████| 1/1 [00:00<00:00, 31.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Chunk 1 uploaded successfully\n",
      "\n",
      "============================================================\n",
      "🚀 UPLOAD COMPLETE\n",
      "============================================================\n",
      "Mode: live\n",
      "Total prepared: 14840\n",
      "Total failed: 0\n",
      "\n",
      "============================================================\n",
      "CART RULES RESULT\n",
      "============================================================\n",
      "Mode: live\n",
      "Cart rule changes: 11873\n",
      "Pushed: 14840\n",
      "Failed: 0\n",
      "\n",
      "======================================================================\n",
      "STEP 2: PUSHING PRICES\n",
      "======================================================================\n",
      "\n",
      "🚀 MODE: LIVE\n",
      "   Files will be prepared AND uploaded to API\n",
      "Loading disable_pu_visibility from Google Sheets...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Loaded 89 products to disable min PU visibility\n",
      "\n",
      "============================================================\n",
      "PUSH PRICES - Source: module_3\n",
      "============================================================\n",
      "Total received: 28382\n",
      "Price changes to push: 4286\n",
      "Skipped (no change): 24096\n",
      "\n",
      "Price changes summary:\n",
      "  Increases: 1569\n",
      "  Decreases: 2717\n",
      "\n",
      "📋 Prepared 5492 packing unit prices\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 703\n",
      "==================================================\n",
      "  Saved: uploads/module_3_703.xlsx (1148 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|██████████| 1/1 [00:00<00:00, 13.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Chunk 1 uploaded successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Processing cohort: 700\n",
      "==================================================\n",
      "  Saved: uploads/module_3_700.xlsx (694 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|██████████| 1/1 [00:00<00:00, 20.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 701\n",
      "==================================================\n",
      "  Saved: uploads/module_3_701.xlsx (1047 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|██████████| 1/1 [00:00<00:00, 13.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1123\n",
      "==================================================\n",
      "  Saved: uploads/module_3_1123.xlsx (333 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|██████████| 1/1 [00:00<00:00, 39.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1124\n",
      "==================================================\n",
      "  Saved: uploads/module_3_1124.xlsx (287 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|██████████| 1/1 [00:00<00:00, 43.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1125\n",
      "==================================================\n",
      "  Saved: uploads/module_3_1125.xlsx (338 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|██████████| 1/1 [00:00<00:00, 38.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 704\n",
      "==================================================\n",
      "  Saved: uploads/module_3_704.xlsx (969 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|██████████| 1/1 [00:00<00:00, 15.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 702\n",
      "==================================================\n",
      "  Saved: uploads/module_3_702.xlsx (382 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|██████████| 1/1 [00:00<00:00, 34.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Chunk 1 uploaded successfully\n",
      "\n",
      "==================================================\n",
      "Processing cohort: 1126\n",
      "==================================================\n",
      "  Saved: uploads/module_3_1126.xlsx (294 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Split into 1 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  Saving chunks: 100%|██████████| 1/1 [00:00<00:00, 42.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Uploading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Chunk 1 uploaded successfully\n",
      "\n",
      "============================================================\n",
      "🚀 UPLOAD COMPLETE\n",
      "============================================================\n",
      "Mode: live\n",
      "Total prepared: 5492\n",
      "Total failed: 0\n",
      "\n",
      "============================================================\n",
      "PRICES RESULT\n",
      "============================================================\n",
      "Mode: live\n",
      "Source: module_3\n",
      "Timestamp: 2026-02-01 12:06:03\n",
      "Total received: 28382\n",
      "Price changes: 4286\n",
      "Pushed: 5492\n",
      "Skipped: 24096\n",
      "Failed: 0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PUSH CART RULES & PRICES\n",
    "# =============================================================================\n",
    "# Push cart rules FIRST, then prices\n",
    "# If cart rules fail for certain cohorts, skip those cohorts for prices\n",
    "\n",
    "%run push_cart_rules_handler.ipynb\n",
    "%run push_prices_handler.ipynb\n",
    "pus = get_packing_units()\n",
    "\n",
    "# ⚠️ MODE CONFIGURATION:\n",
    "# - 'testing' (default): Prepare files but DON'T upload to API\n",
    "# - 'live': Prepare files AND upload to MaxAB API\n",
    "PUSH_MODE = 'live'  # Change to 'live' when ready to push\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: Push Cart Rules First\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 1: PUSHING CART RULES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "cart_result = push_cart_rules(df_output, pus, source_module='module_3', mode=PUSH_MODE)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CART RULES RESULT\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Mode: {cart_result['mode']}\")\n",
    "print(f\"Cart rule changes: {cart_result['cart_rule_changes']}\")\n",
    "print(f\"Pushed: {cart_result['pushed']}\")\n",
    "print(f\"Failed: {cart_result['failed']}\")\n",
    "if cart_result['failed_cohorts']:\n",
    "    print(f\"⚠️ Failed cohorts: {cart_result['failed_cohorts']}\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: Push Prices (skip failed cohorts)\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: PUSHING PRICES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get failed cohorts from cart rules to skip in price push\n",
    "failed_cohorts = cart_result.get('failed_cohorts', [])\n",
    "\n",
    "# Call push_prices with the results, skipping failed cohorts\n",
    "push_result = push_prices(df_output, pus, source_module='module_3', mode=PUSH_MODE, skip_cohorts=failed_cohorts)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PRICES RESULT\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Mode: {push_result['mode']}\")\n",
    "print(f\"Source: {push_result['source_module']}\")\n",
    "print(f\"Timestamp: {push_result['timestamp']}\")\n",
    "print(f\"Total received: {push_result['total_received']}\")\n",
    "print(f\"Price changes: {push_result['price_changes']}\")\n",
    "print(f\"Pushed: {push_result['pushed']}\")\n",
    "print(f\"Skipped: {push_result['skipped']}\")\n",
    "print(f\"Failed: {push_result['failed']}\")\n",
    "if push_result.get('skipped_cohorts'):\n",
    "    print(f\"⚠️ Skipped cohorts (cart rules failed): {push_result['skipped_cohorts']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T10:07:46.169640Z",
     "iopub.status.busy": "2026-02-01T10:07:46.169136Z",
     "iopub.status.idle": "2026-02-01T10:11:17.237559Z",
     "shell.execute_reply": "2026-02-01T10:11:17.236677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 3: PROCESSING SKU DISCOUNTS\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.Renviron\n",
      "/home/ec2-user/service_account_key.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKU Discount Handler loaded at 2026-02-01 12:07:47 Cairo time\n",
      "Excluded categories: ['كروت شحن']\n",
      "Excluded brands: ['فيوري', 'العروسة']\n",
      "AWS & API functions defined ✓\n",
      "✓ API credentials loaded successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snowflake timezone: America/Los_Angeles\n",
      "Function 1: deactivate_active_sku_discounts() defined ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.Renviron\n",
      "/home/ec2-user/service_account_key.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries Module | Timezone: America/Los_Angeles\n",
      "✅ UTH and Last Hour functions defined\n",
      "\n",
      "==================================================\n",
      "QUERIES MODULE READY\n",
      "==================================================\n",
      "\n",
      "Live Data Functions:\n",
      "  • get_current_stocks()\n",
      "  • get_packing_units()\n",
      "  • get_current_prices()\n",
      "  • get_current_wac()\n",
      "  • get_current_cart_rules()\n",
      "\n",
      "UTH Performance Functions:\n",
      "  • get_uth_performance()         - UTH qty/retailers (Snowflake)\n",
      "  • get_hourly_distribution()     - Historical hour contributions (Snowflake)\n",
      "  • get_last_hour_performance()   - Last hour qty/retailers (DWH)\n",
      "\n",
      "Note: Market prices use MODULE_1_INPUT data\n",
      "Retailer Selection Queries defined ✓\n",
      "  - get_churned_dropped_retailers()\n",
      "  - get_category_not_product_retailers()\n",
      "  - get_out_of_cycle_retailers()\n",
      "  - get_view_no_orders_retailers()\n",
      "  - get_excluded_retailers()\n",
      "  - get_retailers_with_quantity_discount()\n",
      "  - get_retailer_main_warehouse()\n",
      "Function 2: select_target_retailers() defined ✓\n",
      "  - Queries 4 retailer sources (churned, category, cycle, view)\n",
      "  - Applies exclusions (failed orders, inactive, wholesale)\n",
      "  - Removes retailers with existing quantity discounts\n",
      "Function 3: Price selection & discount calculation defined ✓\n",
      "  - margin_to_price()\n",
      "  - build_candidate_prices()\n",
      "  - select_target_price()\n",
      "  - calculate_discount_for_row()\n",
      "  - calculate_discounts_batch()\n",
      "Function 4: structure_sku_discount_dataframe() defined ✓\n",
      "  - clear_output_folder()\n",
      "  - structure_sku_discount_dataframe()\n",
      "  - save_sku_discount_files()\n",
      "Function 5: push_sku_discount() defined ✓\n",
      "  - _get_presigned_url()\n",
      "  - _upload_file_to_s3()\n",
      "  - _validate_sku_discount()\n",
      "  - _proceed_sku_discount()\n",
      "  - _upload_single_file()\n",
      "  - push_sku_discount()\n",
      "Main function: process_sku_discounts() defined ✓\n",
      "\n",
      "============================================================\n",
      "SKU DISCOUNT HANDLER MODULE READY\n",
      "============================================================\n",
      "\n",
      "Required input columns from Module 3:\n",
      "  - product_id, warehouse_id, sku, cohort_id, brand, cat\n",
      "  - activate_sku_discount (bool)\n",
      "  - current_price, new_price, wac_p\n",
      "  - doh, uth_qty, uth_status, active_sku_disc_pct\n",
      "  - target_margin, min_boundary\n",
      "\n",
      "Required market margin columns (prices derived via wac/(1-margin)):\n",
      "  - below_market\n",
      "  - market_min\n",
      "  - market_25\n",
      "  - market_50\n",
      "  - market_75\n",
      "  - market_max\n",
      "  - above_market\n",
      "\n",
      "Required margin tier columns:\n",
      "  - margin_tier_below\n",
      "  - margin_tier_1\n",
      "  - margin_tier_2\n",
      "  - margin_tier_3\n",
      "  - margin_tier_4\n",
      "  - margin_tier_5\n",
      "  - margin_tier_above_1\n",
      "  - margin_tier_above_2\n",
      "\n",
      "Retailer Selection: Queries 4 sources, applies exclusions, removes QD overlap\n",
      "\n",
      "Usage: result = process_sku_discounts(df_output, mode='testing')\n",
      "SKUs needing SKU discount: 14087\n",
      "  Merged market margins and margin tiers from df\n",
      "\n",
      "======================================================================\n",
      "SKU DISCOUNT HANDLER\n",
      "======================================================================\n",
      "Mode: live\n",
      "Input records: 14087\n",
      "\n",
      "--------------------------------------------------\n",
      "STEP 1: Deactivating existing SKU discounts\n",
      "--------------------------------------------------\n",
      "🚀 Deactivating SKU discounts (mode: live)\n",
      "  Querying active SKU discounts from Snowflake...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 0 active SKU discounts to deactivate\n",
      "  No active SKU discounts to deactivate\n",
      "\n",
      "--------------------------------------------------\n",
      "STEP 2: Filtering SKUs for discount\n",
      "--------------------------------------------------\n",
      "SKUs flagged for discount: 14087\n",
      "\n",
      "  Applying exclusions...\n",
      "    - Excluded by category: 4\n",
      "    - Excluded by brand: 18\n",
      "\n",
      "  Final SKUs to activate: 14065\n",
      "\n",
      "--------------------------------------------------\n",
      "STEP 3: Calculating discount percentages\n",
      "--------------------------------------------------\n",
      "Calculating discounts for 14065 SKUs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating discounts:   0%|          | 0/14065 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating discounts:   2%|▏         | 240/14065 [00:00<00:05, 2395.95it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating discounts:   4%|▍         | 620/14065 [00:00<00:04, 3221.14it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating discounts:   7%|▋         | 1004/14065 [00:00<00:03, 3503.34it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating discounts:  10%|▉         | 1386/14065 [00:00<00:03, 3626.72it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating discounts:  12%|█▏        | 1749/14065 [00:00<00:06, 1953.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating discounts:  15%|█▌        | 2135/14065 [00:00<00:05, 2367.21it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating discounts:  18%|█▊        | 2518/14065 [00:00<00:04, 2711.92it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating discounts:  21%|██        | 2904/14065 [00:01<00:03, 3000.29it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating discounts:  23%|██▎       | 3295/14065 [00:01<00:03, 3240.95it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating discounts:  26%|██▌       | 3680/14065 [00:01<00:03, 3408.09it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating discounts:  29%|██▉       | 4069/14065 [00:01<00:02, 3543.73it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating discounts:  32%|███▏      | 4457/14065 [00:01<00:02, 3639.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating discounts:  34%|███▍      | 4843/14065 [00:01<00:02, 3701.70it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating discounts:  37%|███▋      | 5231/14065 [00:01<00:02, 3751.89it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating discounts:  40%|███▉      | 5614/14065 [00:01<00:02, 3768.22it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating discounts:  43%|████▎     | 5999/14065 [00:01<00:02, 3791.85it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating discounts:  45%|████▌     | 6388/14065 [00:01<00:02, 3820.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating discounts:  48%|████▊     | 6777/14065 [00:02<00:01, 3840.80it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating discounts:  51%|█████     | 7168/14065 [00:02<00:01, 3859.48it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating discounts:  54%|█████▎    | 7556/14065 [00:02<00:01, 3861.66it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating discounts:  56%|█████▋    | 7945/14065 [00:02<00:01, 3867.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating discounts:  59%|█████▉    | 8333/14065 [00:02<00:02, 2589.28it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating discounts:  62%|██████▏   | 8719/14065 [00:02<00:01, 2870.45it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating discounts:  65%|██████▍   | 9105/14065 [00:02<00:01, 3107.72it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating discounts:  67%|██████▋   | 9490/14065 [00:02<00:01, 3296.72it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating discounts:  70%|███████   | 9877/14065 [00:03<00:01, 3448.43it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating discounts:  73%|███████▎  | 10266/14065 [00:03<00:01, 3569.15it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating discounts:  76%|███████▌  | 10656/14065 [00:03<00:00, 3661.11it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating discounts:  79%|███████▊  | 11043/14065 [00:03<00:00, 3719.92it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating discounts:  81%|████████▏ | 11428/14065 [00:03<00:00, 3757.28it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating discounts:  84%|████████▍ | 11820/14065 [00:03<00:00, 3804.29it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating discounts:  87%|████████▋ | 12210/14065 [00:03<00:00, 3829.89it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating discounts:  90%|████████▉ | 12599/14065 [00:03<00:00, 3847.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating discounts:  92%|█████████▏| 12987/14065 [00:03<00:00, 3839.35it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating discounts:  95%|█████████▌| 13376/14065 [00:03<00:00, 3851.91it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating discounts:  98%|█████████▊| 13767/14065 [00:04<00:00, 3868.25it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating discounts: 100%|██████████| 14065/14065 [00:04<00:00, 2980.88it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ✓ Discounts calculated:\n",
      "    - Valid discounts: 3602\n",
      "    - Avg discount: 1.94%\n",
      "    - Discount sources: {'zero_demand': 2854, 'no_lower_prices': 2661, 'overstock': 2476, 'below_min_threshold': 1696, 'no_reduction_needed': 1408, 'dropping_aggressive': 1374, 'dropping_lowest': 746, 'dropping_below_old': 523, 'no_candidates': 309, 'on_track_keep_old': 10, 'overstock_target_margin': 8}\n",
      "\n",
      "  SKUs with valid discounts (>0%): 3602\n",
      "\n",
      "--------------------------------------------------\n",
      "STEP 4: Selecting target retailers\n",
      "--------------------------------------------------\n",
      "\n",
      "  Selecting target retailers...\n",
      "    SKUs with valid discounts: 3602\n",
      "    Created tuple string for 3602 unique product-warehouse combinations\n",
      "\n",
      "    Querying retailer sources...\n",
      "  Fetching churned/dropped retailers...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Found 10196 churned/dropped retailer-product combinations\n",
      "  Fetching category-not-product retailers...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Found 4092714 category-not-product retailer-product combinations\n",
      "  Fetching out-of-cycle retailers...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Found 3428 out-of-cycle retailer-product combinations\n",
      "  Fetching view-no-orders retailers...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Found 453842 view-no-orders retailer-product combinations\n",
      "\n",
      "    Combining retailer sources...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total retailer-product combinations before filtering: 4284951\n",
      "\n",
      "    Getting retailer main warehouses...\n",
      "  Fetching retailer main warehouses...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Found 114134 retailer-warehouse mappings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Retailers after warehouse filter: 4209861\n",
      "\n",
      "    Applying exclusions...\n",
      "  Fetching excluded retailers...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Found 127802 retailers to exclude\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Excluded 847714 retailers (failed orders, inactive, wholesale, existing discounts)\n",
      "\n",
      "    Removing retailers with existing quantity discounts...\n",
      "  Fetching retailers with quantity discounts...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    No active quantity discounts found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ✓ Final retailer-product combinations: 3362147\n",
      "    ✓ Unique retailers: 51382\n",
      "    ✓ Unique products: 1457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ✓ Final output rows: 3362147\n",
      "\n",
      "--------------------------------------------------\n",
      "STEP 5: Structuring data for API\n",
      "--------------------------------------------------\n",
      "Structuring 3362147 SKU discount records for API...\n",
      "  Step 1: Deduplicating...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Records after deduplication: 3362147\n",
      "  Step 2: Merging with packing units...\n",
      "Fetching packing_units ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded 34954 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Records after PU merge: 4712667\n",
      "  Step 3: Creating HH_data format...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 4: Setting start/end times...\n",
      "    Start: 01/02/2026 12:19\n",
      "    End: 02/02/2026 00:19\n",
      "  Step 5: Grouping by retailer...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Unique retailers: 51382\n",
      "  Step 6: Grouping by discount combinations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Unique discount combinations: 43346\n",
      "  Step 7: Chunking retailer lists (max 100 per chunk)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total chunks: 43347\n",
      "  Step 8: Finalizing columns...\n",
      "  ✓ Structured 43347 records for upload\n",
      "\n",
      "--------------------------------------------------\n",
      "STEP 6: Pushing to API\n",
      "--------------------------------------------------\n",
      "\n",
      "🚀 MODE: LIVE\n",
      "Processing 43347 SKU discount records...\n",
      "\n",
      "  Step 1: Saving files to output folder...\n",
      "\n",
      "Saving SKU discount files...\n",
      "  Clearing output folder...\n",
      "  Cleared 46 files from output folder\n",
      "  Saving 44 files (max 1000 rows each)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:   0%|          | 0/44 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:   2%|▏         | 1/44 [00:00<00:05,  7.54it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:   5%|▍         | 2/44 [00:00<00:05,  7.60it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:   7%|▋         | 3/44 [00:00<00:06,  6.33it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:   9%|▉         | 4/44 [00:00<00:05,  7.01it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:  11%|█▏        | 5/44 [00:00<00:05,  7.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:  14%|█▎        | 6/44 [00:00<00:06,  6.29it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:  16%|█▌        | 7/44 [00:01<00:05,  6.52it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:  18%|█▊        | 8/44 [00:01<00:05,  6.07it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:  20%|██        | 9/44 [00:01<00:05,  6.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:  23%|██▎       | 10/44 [00:01<00:05,  5.82it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:  25%|██▌       | 11/44 [00:01<00:05,  6.21it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:  27%|██▋       | 12/44 [00:02<00:06,  4.99it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:  30%|██▉       | 13/44 [00:02<00:05,  5.65it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:  32%|███▏      | 14/44 [00:02<00:04,  6.01it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:  34%|███▍      | 15/44 [00:02<00:04,  6.01it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:  36%|███▋      | 16/44 [00:02<00:04,  6.45it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:  39%|███▊      | 17/44 [00:02<00:03,  6.86it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:  41%|████      | 18/44 [00:02<00:03,  7.15it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:  43%|████▎     | 19/44 [00:02<00:03,  7.34it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:  45%|████▌     | 20/44 [00:03<00:03,  7.46it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:  48%|████▊     | 21/44 [00:03<00:03,  7.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:  50%|█████     | 22/44 [00:03<00:03,  6.95it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:  52%|█████▏    | 23/44 [00:03<00:03,  6.31it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:  55%|█████▍    | 24/44 [00:03<00:03,  5.85it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:  57%|█████▋    | 25/44 [00:04<00:03,  5.34it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:  59%|█████▉    | 26/44 [00:04<00:03,  5.72it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:  61%|██████▏   | 27/44 [00:04<00:02,  6.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:  64%|██████▎   | 28/44 [00:04<00:02,  6.10it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:  66%|██████▌   | 29/44 [00:04<00:02,  5.57it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:  68%|██████▊   | 30/44 [00:04<00:02,  5.74it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:  70%|███████   | 31/44 [00:05<00:02,  4.77it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:  73%|███████▎  | 32/44 [00:05<00:02,  5.51it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:  75%|███████▌  | 33/44 [00:05<00:01,  6.11it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:  77%|███████▋  | 34/44 [00:05<00:01,  6.47it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:  80%|███████▉  | 35/44 [00:05<00:01,  6.93it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:  82%|████████▏ | 36/44 [00:05<00:01,  7.07it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:  84%|████████▍ | 37/44 [00:05<00:00,  7.25it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:  86%|████████▋ | 38/44 [00:05<00:00,  7.65it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:  89%|████████▊ | 39/44 [00:06<00:00,  7.61it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:  91%|█████████ | 40/44 [00:06<00:00,  7.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:  93%|█████████▎| 41/44 [00:06<00:00,  6.84it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:  95%|█████████▌| 42/44 [00:06<00:00,  7.31it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files:  98%|█████████▊| 43/44 [00:06<00:00,  7.60it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving files: 100%|██████████| 44/44 [00:06<00:00,  6.54it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved 44 files to ../output/sku_discount_sheets\n",
      "\n",
      "  Step 2: Uploading 44 files via S3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:   0%|          | 0/44 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._0.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:   2%|▏         | 1/44 [00:01<00:57,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._1.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:   5%|▍         | 2/44 [00:02<00:58,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._2.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:   7%|▋         | 3/44 [00:04<01:10,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._3.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:   9%|▉         | 4/44 [00:05<00:58,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._4.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:  11%|█▏        | 5/44 [00:07<00:55,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._5.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:  14%|█▎        | 6/44 [00:09<01:05,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._6.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:  16%|█▌        | 7/44 [00:11<01:00,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._7.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:  18%|█▊        | 8/44 [00:13<01:06,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._8.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:  20%|██        | 9/44 [00:15<01:04,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._9.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:  23%|██▎       | 10/44 [00:17<01:06,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._10.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:  25%|██▌       | 11/44 [00:18<00:57,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._11.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:  27%|██▋       | 12/44 [00:20<00:52,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._12.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:  30%|██▉       | 13/44 [00:21<00:48,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._13.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:  32%|███▏      | 14/44 [00:22<00:46,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._14.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:  34%|███▍      | 15/44 [00:24<00:45,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._15.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:  36%|███▋      | 16/44 [00:25<00:41,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._16.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:  39%|███▊      | 17/44 [00:27<00:39,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._17.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:  41%|████      | 18/44 [00:28<00:37,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._18.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:  43%|████▎     | 19/44 [00:30<00:36,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._19.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:  45%|████▌     | 20/44 [00:32<00:38,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._20.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:  48%|████▊     | 21/44 [00:33<00:36,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._21.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:  50%|█████     | 22/44 [00:35<00:34,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._22.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:  52%|█████▏    | 23/44 [00:37<00:39,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._23.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:  55%|█████▍    | 24/44 [00:40<00:40,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._24.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:  57%|█████▋    | 25/44 [00:42<00:41,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._25.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:  59%|█████▉    | 26/44 [00:44<00:36,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._26.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:  61%|██████▏   | 27/44 [00:45<00:31,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._27.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:  64%|██████▎   | 28/44 [00:47<00:28,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._28.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:  66%|██████▌   | 29/44 [00:49<00:28,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._29.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:  68%|██████▊   | 30/44 [00:51<00:25,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._30.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:  70%|███████   | 31/44 [00:52<00:21,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._31.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:  73%|███████▎  | 32/44 [00:53<00:18,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._32.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:  75%|███████▌  | 33/44 [00:54<00:15,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._33.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:  77%|███████▋  | 34/44 [00:56<00:14,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._34.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:  80%|███████▉  | 35/44 [00:57<00:12,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._35.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:  82%|████████▏ | 36/44 [00:59<00:11,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._36.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:  84%|████████▍ | 37/44 [01:00<00:09,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._37.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:  86%|████████▋ | 38/44 [01:01<00:08,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._38.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:  89%|████████▊ | 39/44 [01:03<00:06,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._39.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:  91%|█████████ | 40/44 [01:05<00:06,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._40.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:  93%|█████████▎| 41/44 [01:06<00:04,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._41.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:  95%|█████████▌| 42/44 [01:07<00:02,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._42.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files:  98%|█████████▊| 43/44 [01:09<00:01,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "    Processing: sku_discount_2026-02-01_NO._43.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files: 100%|██████████| 44/44 [01:10<00:00,  1.29s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Uploading files: 100%|██████████| 44/44 [01:10<00:00,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ✓ Success\n",
      "\n",
      "  ==================================================\n",
      "  UPLOAD SUMMARY\n",
      "  ==================================================\n",
      "  Total files: 44\n",
      "  ✓ Successful: 44\n",
      "  ✗ Failed: 0\n",
      "\n",
      "======================================================================\n",
      "SUMMARY\n",
      "======================================================================\n",
      "Mode: live\n",
      "Total input: 14087\n",
      "Discounts deactivated: 0\n",
      "SKUs to activate: 14065\n",
      "SKUs with valid discounts: 3602\n",
      "Retailer-product combinations: 3362147\n",
      "Records created/uploaded: 44\n",
      "Records failed: 0\n",
      "Files saved: 44\n",
      "Output folder: ../output/sku_discount_sheets\n",
      "\n",
      "============================================================\n",
      "SKU DISCOUNT RESULT\n",
      "============================================================\n",
      "Mode: live\n",
      "Total input: 14087\n",
      "SKUs to activate: 14065\n",
      "Deactivated: 0\n",
      "Created: 44\n",
      "Failed: 0\n",
      "\n",
      "======================================================================\n",
      "STEP 4: PROCESSING QUANTITY DISCOUNTS\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.Renviron\n",
      "/home/ec2-user/service_account_key.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries Module | Timezone: America/Los_Angeles\n",
      "✅ UTH and Last Hour functions defined\n",
      "\n",
      "==================================================\n",
      "QUERIES MODULE READY\n",
      "==================================================\n",
      "\n",
      "Live Data Functions:\n",
      "  • get_current_stocks()\n",
      "  • get_packing_units()\n",
      "  • get_current_prices()\n",
      "  • get_current_wac()\n",
      "  • get_current_cart_rules()\n",
      "\n",
      "UTH Performance Functions:\n",
      "  • get_uth_performance()         - UTH qty/retailers (Snowflake)\n",
      "  • get_hourly_distribution()     - Historical hour contributions (Snowflake)\n",
      "  • get_last_hour_performance()   - Last hour qty/retailers (DWH)\n",
      "\n",
      "Note: Market prices use MODULE_1_INPUT data\n",
      "Retailer Selection Queries defined ✓\n",
      "  - get_churned_dropped_retailers()\n",
      "  - get_category_not_product_retailers()\n",
      "  - get_out_of_cycle_retailers()\n",
      "  - get_view_no_orders_retailers()\n",
      "  - get_excluded_retailers()\n",
      "  - get_retailers_with_quantity_discount()\n",
      "  - get_retailer_main_warehouse()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.Renviron\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/service_account_key.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ QD Handler initialized\n",
      "  Timezone: America/Los_Angeles\n",
      "✓ QD calculation parameters:\n",
      "  MAX_DISCOUNT_PCT: 5.0%\n",
      "  MIN_DISCOUNT_PCT: 0.35%\n",
      "  RATIO RANGE: [1.1, 3.0]\n",
      "\n",
      "✓ Wholesale (T3) parameters:\n",
      "  WS_CAR_COST: 1400 EGP\n",
      "  WS_MAX_TICKET_SIZE: 35000 EGP\n",
      "  WS_MIN_MARGIN: 1.5%\n",
      "  TOP_SKUS_PER_WAREHOUSE: 400\n",
      "\n",
      "✓ Upload parameters:\n",
      "  MAX_GROUP_SIZE: 200\n",
      "  QD_DURATION_HOURS: 12\n",
      "\n",
      "✓ Output directory: qd_uploads\n",
      "✓ Data fetching functions defined\n",
      "✓ Tier price calculation function defined\n",
      "✓ Wholesale tier calculation function defined\n",
      "✓ process_qd() function defined\n",
      "Helper functions defined ✓\n",
      "✓ API functions defined\n",
      "✓ QD Handler ready to use\n",
      "\n",
      "Available functions:\n",
      "  - process_qd(df_qd, dry_run=True)      : Main function to process QDs from Module 3\n",
      "  - deactivate_active_qd(dry_run=True)   : Deactivate all active QDs\n",
      "  - create_upload_format(df_configs)     : Create upload format DataFrame\n",
      "  - prepare_upload_file(df_upload, ...)  : Prepare final upload file with tag IDs\n",
      "  - post_QD(filename)                    : Upload QD file to API\n",
      "  - prepare_cart_rules_update(df_work, df_qd) : Prepare cart rules update\n",
      "  - upload_cart_rules(cart_rules, ...)   : Upload cart rules by cohort\n",
      "SKUs needing QD processing: 2068\n",
      "\n",
      "======================================================================\n",
      "QD HANDLER: PROCESSING QUANTITY DISCOUNTS\n",
      "======================================================================\n",
      "Mode: LIVE\n",
      "Timestamp: 2026-02-01 12:10 Cairo Time\n",
      "Input SKUs: 2068\n",
      "\n",
      "Unique warehouses: 12\n",
      "\n",
      "------------------------------------------------------------\n",
      "STEP 1: Deactivating existing Quantity Discounts...\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "DEACTIVATING ACTIVE QUANTITY DISCOUNTS\n",
      "============================================================\n",
      "Mode: LIVE\n",
      "\n",
      "Step 1: Querying active Quantity Discounts from Snowflake...\n",
      "Fetching  qd ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded 0 records\n",
      "  No active Quantity Discounts found.\n",
      "\n",
      "------------------------------------------------------------\n",
      "STEP 2: Getting top-selling packing units...\n",
      "------------------------------------------------------------\n",
      "  Fetching top-selling packing units (last 90 days)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26006/1508261643.py:73: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Found packing units for 2068 product-warehouse combinations\n",
      "  Matched 2068 SKUs with packing units\n",
      "  Using new_price: 0 SKUs\n",
      "  Using current_price (fallback): 2068 SKUs\n",
      "\n",
      "------------------------------------------------------------\n",
      "STEP 3: Getting warehouse ticket statistics...\n",
      "------------------------------------------------------------\n",
      "  Fetching warehouse ticket statistics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26006/1508261643.py:425: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Got stats for 13 warehouses\n",
      "  Merged ticket stats for 2068 SKUs\n",
      "\n",
      "------------------------------------------------------------\n",
      "STEP 4: Calculating tier quantities...\n",
      "------------------------------------------------------------\n",
      "  Calculating tier quantities from order history...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26006/1508261643.py:314: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Calculated tiers for 1938 product-warehouse combinations\n",
      "  1938 SKUs have tier quantities\n",
      "\n",
      "------------------------------------------------------------\n",
      "STEP 5: Calculating T1 & T2 prices...\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Valid T1 & T2 prices: 597 / 2068\n",
      "\n",
      "  Price source distribution:\n",
      "    insufficient_valid_prices: 1404\n",
      "    margin_tier_margin_tier: 221\n",
      "    market_market: 70\n",
      "    market_market_ratio_up: 62\n",
      "    margin_tier_market_ratio_up: 61\n",
      "\n",
      "------------------------------------------------------------\n",
      "STEP 6: Calculating T3 (wholesale) prices...\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Valid T3 prices: 216 / 2068\n",
      "\n",
      "  T3 Statistics:\n",
      "    Average multiplier: 7.4x\n",
      "    Average discount: 1.61%\n",
      "    Average margin: 4.38%\n",
      "\n",
      "------------------------------------------------------------\n",
      "STEP 7: Validating T3 constraints...\n",
      "------------------------------------------------------------\n",
      "  Invalidated 2 SKUs where T3 price >= T2 price\n",
      "  Final valid T3 count: 214\n",
      "\n",
      "  Checking tier quantity ratios...\n",
      "\n",
      "------------------------------------------------------------\n",
      "STEP 8: Applying keep_qd_tiers filter and calculating tier flags...\n",
      "------------------------------------------------------------\n",
      "\n",
      "  Validating unique discount ordering (T1 < T2 < T3)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  SKUs with valid tiers after filtering: 572\n",
      "  Total tier entries: 1350\n",
      "    T1 valid: 572\n",
      "    T2 valid: 572\n",
      "    T3 valid: 206\n",
      "\n",
      "------------------------------------------------------------\n",
      "STEP 9: Selecting top 400 tier entries per warehouse...\n",
      "------------------------------------------------------------\n",
      "  Before filtering: 572 SKUs (1350 tier entries)\n",
      "  After top 400 limit: 572 SKUs (1350 tier entries)\n",
      "\n",
      "  Tier entries per warehouse:\n",
      "    Warehouse 1: 78 SKUs, 185 tiers\n",
      "    Warehouse 8: 42 SKUs, 101 tiers\n",
      "    Warehouse 170: 54 SKUs, 127 tiers\n",
      "    Warehouse 236: 89 SKUs, 197 tiers\n",
      "    Warehouse 337: 49 SKUs, 117 tiers\n",
      "    Warehouse 339: 46 SKUs, 107 tiers\n",
      "    Warehouse 401: 27 SKUs, 65 tiers\n",
      "    Warehouse 501: 23 SKUs, 59 tiers\n",
      "    Warehouse 632: 12 SKUs, 28 tiers\n",
      "    Warehouse 703: 37 SKUs, 98 tiers\n",
      "    Warehouse 797: 36 SKUs, 90 tiers\n",
      "    Warehouse 962: 79 SKUs, 176 tiers\n",
      "\n",
      "------------------------------------------------------------\n",
      "STEP 10: Building QD configurations...\n",
      "------------------------------------------------------------\n",
      "  Valid QD configs: 572\n",
      "\n",
      "  Tier distribution in configs:\n",
      "    T1: 572 configs\n",
      "    T2: 572 configs\n",
      "    T3 (wholesale): 206 configs\n",
      "    Total tier entries: 1350\n",
      "\n",
      "  Sample QD configs:\n",
      "    عصير جهينة بيور تفاح - 1 لتر: [T1:qty=4,disc=0.59%, T2:qty=7,disc=1.17%, T3:qty=51,disc=1.48%]\n",
      "    مناديل بابيا سحب نعومة و نقاء : [T1:qty=4,disc=0.52%, T2:qty=7,disc=1.16%, T3:qty=69,disc=1.48%]\n",
      "    شويبس اناناس بلاستيك - 250 مل: [T1:qty=4,disc=1.31%, T2:qty=9,disc=3.24%]\n",
      "    اوكسى جل بلاك - 30 جم: [T1:qty=4,disc=1.10%, T2:qty=7,disc=2.29%]\n",
      "    سفن اب تربو - 390 مل: [T1:qty=4,disc=0.58%, T2:qty=7,disc=1.12%, T3:qty=252,disc=1.48%]\n",
      "\n",
      "------------------------------------------------------------\n",
      "STEP 10.5: Saving data for review...\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved review file: qd_uploads/QD_detailed_review_20260201_1210.xlsx\n",
      "    Total SKUs: 572\n",
      "    Columns: 27\n",
      "\n",
      "------------------------------------------------------------\n",
      "STEP 11: Creating new Quantity Discounts...\n",
      "------------------------------------------------------------\n",
      "  Creating 572 Quantity Discounts...\n",
      "\n",
      "  Creating upload format...\n",
      "  Upload format created: 12 warehouse rows\n",
      "\n",
      "  Per warehouse breakdown:\n",
      "    WH 1: Group 1 = 107 items, Group 2 = 78 items\n",
      "    WH 8: Group 1 = 59 items, Group 2 = 42 items\n",
      "    WH 170: Group 1 = 73 items, Group 2 = 54 items\n",
      "    WH 236: Group 1 = 108 items, Group 2 = 89 items\n",
      "    WH 337: Group 1 = 68 items, Group 2 = 49 items\n",
      "    WH 339: Group 1 = 61 items, Group 2 = 46 items\n",
      "    WH 401: Group 1 = 38 items, Group 2 = 27 items\n",
      "    WH 501: Group 1 = 36 items, Group 2 = 23 items\n",
      "    WH 632: Group 1 = 16 items, Group 2 = 12 items\n",
      "    WH 703: Group 1 = 61 items, Group 2 = 37 items\n",
      "    WH 797: Group 1 = 54 items, Group 2 = 36 items\n",
      "    WH 962: Group 1 = 97 items, Group 2 = 79 items\n",
      "\n",
      "  Preparing upload file...\n",
      "  ✓ Saved upload file: qd_uploads/QD_upload_20260201_1210.xlsx (12 warehouses)\n",
      "\n",
      "  Uploading QD file to API...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Upload succeeded (status: 200)\n",
      "\n",
      "  Creation Result:\n",
      "    Created: 572\n",
      "    Failed: 0\n",
      "\n",
      "------------------------------------------------------------\n",
      "STEP 12: Updating cart rules...\n",
      "------------------------------------------------------------\n",
      "  Uploading cart rules...\n",
      "\n",
      "  Cart rules to update: 550 products across 9 cohorts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Cohort 700: 78 rules uploaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Cohort 701: 158 rules uploaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Cohort 702: 36 rules uploaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Cohort 703: 86 rules uploaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Cohort 704: 93 rules uploaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Cohort 1123: 37 rules uploaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Cohort 1124: 23 rules uploaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Cohort 1125: 12 rules uploaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Cohort 1126: 27 rules uploaded\n",
      "\n",
      "  Cart Rules Result:\n",
      "    Cohorts updated: 9\n",
      "    Cohorts failed: 0\n",
      "\n",
      "======================================================================\n",
      "QD HANDLER - SUMMARY\n",
      "======================================================================\n",
      "Mode: LIVE\n",
      "Total SKUs in input: 2068\n",
      "SKUs with valid T1 & T2 prices: 597\n",
      "SKUs with valid T3 prices: 216\n",
      "SKUs after keep_qd_tiers & 400 tier limit: 572\n",
      "Total tier entries: 1350\n",
      "Valid QD configs: 572\n",
      "QD found active: 0\n",
      "QD deactivated: 0\n",
      "QD created: 572\n",
      "QD creation failed: 0\n",
      "Cart rules updated: 550 products\n",
      "======================================================================\n",
      "\n",
      "============================================================\n",
      "QD PROCESSING RESULT\n",
      "============================================================\n",
      "Mode: live\n",
      "Total input: 2068\n",
      "Processed: 572\n",
      "Failed: 0\n",
      "\n",
      "======================================================================\n",
      "MODULE 3 EXECUTION COMPLETE\n",
      "======================================================================\n",
      "Total SKUs processed: 28382\n",
      "Price changes: 17766\n",
      "Cart rule changes: 24225\n",
      "SKUs with SKU discount: 14087\n",
      "SKUs with QD: 2068\n",
      "Output saved to: module_3_output_20260201_1203.xlsx\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 3: PROCESS SKU DISCOUNTS\n",
    "# =============================================================================\n",
    "# This step handles SKU discounts for SKUs that need them based on UTH performance.\n",
    "# Market data has already been refreshed, so we pass the df_output directly.\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 3: PROCESSING SKU DISCOUNTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "%run sku_discount_handler.ipynb\n",
    "\n",
    "# Filter to SKUs that need SKU discount\n",
    "df_sku_discount = df_results[df_results['activate_sku_discount'] == True].copy()\n",
    "print(f\"SKUs needing SKU discount: {len(df_sku_discount)}\")\n",
    "\n",
    "# Merge market margins and margin tiers from df (not in df_results)\n",
    "sku_discount_extra_cols = [\n",
    "    'product_id', 'warehouse_id',\n",
    "    # Market margins\n",
    "    'below_market', 'market_min', 'market_25', 'market_50', \n",
    "    'market_75', 'market_max', 'above_market',\n",
    "    # Margin tiers\n",
    "    'margin_tier_below', 'margin_tier_1', 'margin_tier_2', 'margin_tier_3', \n",
    "    'margin_tier_4', 'margin_tier_5', 'margin_tier_above_1', 'margin_tier_above_2',\n",
    "    # Other needed columns\n",
    "    'doh', 'zero_demand', 'target_margin', 'min_boundary', 'active_sku_disc_pct'\n",
    "]\n",
    "# Filter to columns that exist in df\n",
    "sku_discount_extra_cols = [c for c in sku_discount_extra_cols if c in df.columns]\n",
    "\n",
    "# Merge the extra columns from df\n",
    "df_sku_discount = df_sku_discount.merge(\n",
    "    df[sku_discount_extra_cols].drop_duplicates(subset=['product_id', 'warehouse_id']),\n",
    "    on=['product_id', 'warehouse_id'],\n",
    "    how='left',\n",
    "    suffixes=('', '_from_df')\n",
    ")\n",
    "print(f\"  Merged market margins and margin tiers from df\")\n",
    "\n",
    "if len(df_sku_discount) > 0:\n",
    "    sku_discount_result = process_sku_discounts(df_sku_discount, mode=PUSH_MODE)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SKU DISCOUNT RESULT\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Mode: {sku_discount_result['mode']}\")\n",
    "    print(f\"Total input: {sku_discount_result['total_input']}\")\n",
    "    print(f\"SKUs to activate: {sku_discount_result['to_activate']}\")\n",
    "    print(f\"Deactivated: {sku_discount_result['deactivated']}\")\n",
    "    print(f\"Created: {sku_discount_result['created']}\")\n",
    "    print(f\"Failed: {sku_discount_result['failed']}\")\n",
    "else:\n",
    "    print(\"No SKUs need SKU discounts\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 4: PROCESSING QUANTITY DISCOUNTS (QD)\n",
    "# =============================================================================\n",
    "# This step handles QD adjustments for SKUs flagged by the action engine.\n",
    "# Only processes SKUs where activate_qd=True and uses keep_qd_tiers to determine\n",
    "# which tiers to maintain.\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 4: PROCESSING QUANTITY DISCOUNTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "%run qd_handler.ipynb\n",
    "\n",
    "# Filter to SKUs that need QD processing\n",
    "df_qd = df_results[df_results['activate_qd'] == True].copy()\n",
    "print(f\"SKUs needing QD processing: {len(df_qd)}\")\n",
    "\n",
    "# Required columns for QD handler\n",
    "# Include all data needed for tier quantity and price calculations\n",
    "qd_columns = [\n",
    "    # Identifiers\n",
    "    'product_id', 'warehouse_id', 'cohort_id', 'sku', 'brand', 'cat',\n",
    "    # Pricing data\n",
    "    'wac_p', 'current_price', 'new_price', 'target_margin', 'min_boundary',\n",
    "    # Cart rules\n",
    "    'current_cart_rule', 'new_cart_rule',\n",
    "    # Market margins (to be converted to prices)\n",
    "    'below_market', 'market_min', 'market_25', 'market_50',\n",
    "    'market_75', 'market_max', 'above_market',\n",
    "    # Margin tiers (to be converted to prices)\n",
    "    'margin_tier_1', 'margin_tier_2', 'margin_tier_3', 'margin_tier_4',\n",
    "    'margin_tier_5', 'margin_tier_above_1', 'margin_tier_above_2',\n",
    "    # Performance data (for top SKU selection)\n",
    "    'mtd_qty',\n",
    "    # QD configuration\n",
    "    'keep_qd_tiers'\n",
    "]\n",
    "# Filter to columns that exist in df_results\n",
    "qd_columns = [c for c in qd_columns if c in df_results.columns]\n",
    "df_qd = df_qd[qd_columns].copy()\n",
    "\n",
    "if len(df_qd) > 0:\n",
    "    qd_result = process_qd(df_qd, False)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"QD PROCESSING RESULT\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Mode: {qd_result['mode']}\")\n",
    "    print(f\"Total input: {qd_result['total_input']}\")\n",
    "    print(f\"Processed: {qd_result['processed']}\")\n",
    "    print(f\"Failed: {qd_result['failed']}\")\n",
    "else:\n",
    "    print(\"No SKUs need QD processing\")\n",
    "\n",
    "# =============================================================================\n",
    "# FINAL SUMMARY\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODULE 3 EXECUTION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total SKUs processed: {len(df_output)}\")\n",
    "print(f\"Price changes: {(df_output['new_price'] != df_output['current_price']).sum()}\")\n",
    "print(f\"Cart rule changes: {(df_output['new_cart_rule'] != df_output['current_cart_rule']).sum()}\")\n",
    "print(f\"SKUs with SKU discount: {df_output['activate_sku_discount'].sum()}\")\n",
    "print(f\"SKUs with QD: {df_output['activate_qd'].sum()}\")\n",
    "print(f\"Output saved to: {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T10:11:17.239573Z",
     "iopub.status.busy": "2026-02-01T10:11:17.239356Z",
     "iopub.status.idle": "2026-02-01T10:11:29.593352Z",
     "shell.execute_reply": "2026-02-01T10:11:29.592478Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "UPLOADING RESULTS TO SNOWFLAKE\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/service_account_key.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/Pricing Runs/Prediction_Scripts_2/Happy_hour/git/Mustafa/Pricing Logic/modules/../common_functions.py:698: UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)\n",
      "  success, _, _, _ = write_pandas(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.12/site-packages/slack/deprecation.py:14: UserWarning: slack package is deprecated. Please use slack_sdk.web/webhook/rtm package instead. For more info, go to https://docs.slack.dev/tools/python-slack-sdk/v3-migration/\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/service_account_key.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message Sent\n",
      "✅ Slack notification sent!\n",
      "✅ 28382 records uploaded to Snowflake\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# UPLOAD RESULTS TO SNOWFLAKE AND SEND SLACK NOTIFICATION\n",
    "# =============================================================================\n",
    "from common_functions import upload_dataframe_to_snowflake, send_text_slack\n",
    "\n",
    "# Add created_at as TIMESTAMP (module runs multiple times per day)\n",
    "df_output = df_output.drop(columns=['keep_qd_tiers'])\n",
    "df_output['keep_qd_tiers'] = np.nan\n",
    "df_output['created_at'] = datetime.now(CAIRO_TZ).replace(second=0, microsecond=0)\n",
    "\n",
    "# Upload to Snowflake\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"UPLOADING RESULTS TO SNOWFLAKE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "upload_status = upload_dataframe_to_snowflake(\n",
    "    \"Egypt\", \n",
    "    df_output, \n",
    "    \"MATERIALIZED_VIEWS\", \n",
    "    \"pricing_periodic_push\", \n",
    "    \"append\", \n",
    "    auto_create_table=True, \n",
    "    conn=None\n",
    ")\n",
    "\n",
    "# Prepare status variables\n",
    "prices_pushed = push_result.get('pushed', 0) if 'push_result' in dir() else 0\n",
    "prices_failed = push_result.get('failed', 0) if 'push_result' in dir() else 0\n",
    "cart_rules_pushed = cart_result.get('pushed', 0) if 'cart_result' in dir() else 0\n",
    "cart_rules_failed = cart_result.get('failed', 0) if 'cart_result' in dir() else 0\n",
    "\n",
    "# SKU discount status\n",
    "sku_disc_processed = len(df_sku_discount) if 'df_sku_discount' in dir() else 0\n",
    "\n",
    "# QD status\n",
    "qd_processed = qd_result.get('processed', 0) if 'qd_result' in dir() and qd_result else 0\n",
    "qd_failed = qd_result.get('failed', 0) if 'qd_result' in dir() and qd_result else 0\n",
    "df_output.columns = df_output.columns.str.lower()\n",
    "if upload_status:\n",
    "    slack_message = f\"\"\"✅ *Module 3 - Periodic Actions Completed*\n",
    "\n",
    "📅 Date: {datetime.now(CAIRO_TZ).strftime('%Y-%m-%d')}\n",
    "⏰ Completed at: {datetime.now(CAIRO_TZ).strftime('%H:%M:%S')} Cairo time\n",
    "🔧 Mode: {PUSH_MODE.upper()}\n",
    "\n",
    "📊 *Results:*\n",
    "• Total SKUs processed: {len(df_output):,}\n",
    "• Price changes: {(df_output['new_price'] != df_output['current_price']).sum():,}\n",
    "• Cart rule changes: {(df_output['new_cart_rule'] != df_output['current_cart_rule']).sum():,}\n",
    "\n",
    "📤 *Push Status:*\n",
    "• 💰 Prices: ✅ {prices_pushed} pushed | ❌ {prices_failed} failed\n",
    "• 🛒 Cart Rules: ✅ {cart_rules_pushed} pushed | ❌ {cart_rules_failed} failed\n",
    "• 🏷️ SKU Discounts: {sku_disc_processed} processed\n",
    "• 📦 Quantity Discounts: ✅ {qd_processed} processed | ❌ {qd_failed} failed\n",
    "\n",
    "🗄️ Results uploaded to: MATERIALIZED_VIEWS.pricing_periodic_push\"\"\"\n",
    "    \n",
    "    send_text_slack('new-pricing-logic', slack_message)\n",
    "    print(\"✅ Slack notification sent!\")\n",
    "    print(f\"✅ {len(df_output)} records uploaded to Snowflake\")\n",
    "else:\n",
    "    error_message = f\"\"\"❌ *Module 3 - Periodic Actions Failed*\n",
    "\n",
    "📅 Date: {datetime.now(CAIRO_TZ).strftime('%Y-%m-%d')}\n",
    "⏰ Failed at: {datetime.now(CAIRO_TZ).strftime('%H:%M:%S')} Cairo time\n",
    "⚠️ Upload to Snowflake failed - please check logs\n",
    "\n",
    "📤 *Push Status (before upload failure):*\n",
    "• 💰 Prices: ✅ {prices_pushed} pushed | ❌ {prices_failed} failed\n",
    "• 🛒 Cart Rules: ✅ {cart_rules_pushed} pushed | ❌ {cart_rules_failed} failed\n",
    "• 🏷️ SKU Discounts: {sku_disc_processed} processed\n",
    "• 📦 Quantity Discounts: ✅ {qd_processed} processed | ❌ {qd_failed} failed\"\"\"\n",
    "    \n",
    "    send_text_slack('new-pricing-logic', error_message)\n",
    "    print(\"❌ Error notification sent to Slack!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
