{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Scientific Marketplace Pricing Analysis\n",
        "\n",
        "This notebook fetches marketplace prices and applies scientific outlier detection using **MAD (Median Absolute Deviation)** to produce clean, valid price ranges per SKU.\n",
        "\n",
        "## Key Features:\n",
        "- **MAD-based outlier detection**: Robust to up to 50% outliers (Iglewicz & Hoaglin, 1993)\n",
        "- **Configurable percentile ranges**: Default P10-P90, easily adjustable\n",
        "- **Transparent logging**: Track rows removed at each filtering step\n",
        "- **WAC validation**: Filter prices within acceptable cost margins\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CONFIGURATION - All adjustable parameters in one place\n",
        "# =============================================================================\n",
        "\n",
        "# MAD Outlier Detection Settings\n",
        "# Threshold of 3.5 is the standard recommendation by Iglewicz and Hoaglin (1993)\n",
        "# Lower values = more aggressive outlier removal, Higher values = more permissive\n",
        "MAD_THRESHOLD = 3.5\n",
        "\n",
        "# WAC (Weighted Average Cost) Filter Settings\n",
        "# Prices outside this percentage range from WAC4 will be excluded\n",
        "WAC_LOWER_BOUND = -40  # Minimum acceptable % difference from WAC4\n",
        "WAC_UPPER_BOUND = 40   # Maximum acceptable % difference from WAC4\n",
        "\n",
        "# Percentile Settings for Price Bounds\n",
        "# Define multiple percentiles to get a full price distribution (4-5 price points)\n",
        "PERCENTILES = [10, 25, 50, 75, 90]  # P10, P25, P50 (median), P75, P90\n",
        "\n",
        "# Sales Data Settings\n",
        "SALES_LOOKBACK_DAYS = 100  # Days to look back for historical sales\n",
        "RECENT_SALES_DAYS = 5      # Days to consider as \"recent\" sales\n",
        "\n",
        "# Display Settings\n",
        "SAMPLE_PRODUCT_ID = 1309  # Product ID to use for sample output verification\n",
        "\n",
        "print(\"Configuration loaded successfully!\")\n",
        "print(f\"  - MAD Threshold: {MAD_THRESHOLD}\")\n",
        "print(f\"  - WAC Range: {WAC_LOWER_BOUND}% to {WAC_UPPER_BOUND}%\")\n",
        "print(f\"  - Percentiles: {PERCENTILES}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# IMPORTS AND ENVIRONMENT SETUP\n",
        "# =============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "from IPython.display import display\n",
        "import os\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Initialize environment (for Snowflake credentials)\n",
        "import setup_environment_2\n",
        "import importlib\n",
        "importlib.reload(setup_environment_2)\n",
        "setup_environment_2.initialize_env()\n",
        "\n",
        "print(\"Environment initialized successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# HELPER FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "def query_snowflake(query, columns=[]):\n",
        "    \"\"\"\n",
        "    Execute a Snowflake query and return results as a DataFrame.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    query : str\n",
        "        SQL query to execute\n",
        "    columns : list\n",
        "        Column names for the resulting DataFrame\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        Query results\n",
        "    \"\"\"\n",
        "    import snowflake.connector\n",
        "    \n",
        "    con = snowflake.connector.connect(\n",
        "        user=os.environ[\"SNOWFLAKE_USERNAME\"],\n",
        "        account=os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
        "        password=os.environ[\"SNOWFLAKE_PASSWORD\"],\n",
        "        database=os.environ[\"SNOWFLAKE_DATABASE\"]\n",
        "    )\n",
        "    try:\n",
        "        cur = con.cursor()\n",
        "        cur.execute(\"USE WAREHOUSE COMPUTE_WH\")\n",
        "        cur.execute(query)\n",
        "        if len(columns) == 0:\n",
        "            out = pd.DataFrame(np.array(cur.fetchall()))\n",
        "        else:\n",
        "            out = pd.DataFrame(np.array(cur.fetchall()), columns=columns)\n",
        "        return out\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return pd.DataFrame()\n",
        "    finally:\n",
        "        cur.close()\n",
        "        con.close()\n",
        "\n",
        "\n",
        "def mad_filter(data, threshold=3.5):\n",
        "    \"\"\"\n",
        "    Apply MAD (Median Absolute Deviation) outlier detection.\n",
        "    \n",
        "    MAD is more robust than standard deviation for non-normal distributions.\n",
        "    It can handle up to 50% outliers, while std breaks down at ~25%.\n",
        "    \n",
        "    The modified Z-score formula:\n",
        "        M_i = 0.6745 * (x_i - median) / MAD\n",
        "    \n",
        "    Where 0.6745 is the consistency constant that makes MAD comparable \n",
        "    to standard deviation for normally distributed data.\n",
        "    \n",
        "    Reference: Iglewicz, B. and Hoaglin, D. (1993), \n",
        "               \"How to Detect and Handle Outliers\"\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    data : array-like\n",
        "        Numeric data to filter\n",
        "    threshold : float\n",
        "        Modified Z-score threshold (default 3.5 per Iglewicz & Hoaglin)\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    np.ndarray\n",
        "        Boolean mask where True = inlier, False = outlier\n",
        "    \"\"\"\n",
        "    data = np.array(data, dtype=float)\n",
        "    median = np.median(data)\n",
        "    mad = np.median(np.abs(data - median))\n",
        "    \n",
        "    # Handle edge case where MAD is 0 (all values are the same)\n",
        "    if mad == 0:\n",
        "        return np.ones(len(data), dtype=bool)\n",
        "    \n",
        "    # Calculate modified Z-scores\n",
        "    # 0.6745 is the consistency constant for normal distribution\n",
        "    modified_z_scores = 0.6745 * (data - median) / mad\n",
        "    \n",
        "    return np.abs(modified_z_scores) < threshold\n",
        "\n",
        "\n",
        "def apply_mad_filter_to_group(group, column, threshold=3.5):\n",
        "    \"\"\"\n",
        "    Apply MAD filter to a specific column within a group.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    group : pd.DataFrame\n",
        "        Grouped DataFrame\n",
        "    column : str\n",
        "        Column name to filter on\n",
        "    threshold : float\n",
        "        MAD threshold\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        Filtered group with outliers removed\n",
        "    \"\"\"\n",
        "    if len(group) < 3:\n",
        "        # Not enough data points for meaningful outlier detection\n",
        "        return group\n",
        "    \n",
        "    mask = mad_filter(group[column].values, threshold)\n",
        "    return group[mask]\n",
        "\n",
        "\n",
        "def get_percentile_prices(group, column='price', percentiles=[10, 25, 50, 75, 90]):\n",
        "    \"\"\"\n",
        "    Calculate multiple percentile-based prices for a group.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    group : pd.DataFrame\n",
        "        Grouped DataFrame\n",
        "    column : str\n",
        "        Column to calculate percentiles on\n",
        "    percentiles : list\n",
        "        List of percentiles to calculate (e.g., [10, 25, 50, 75, 90])\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    pd.Series\n",
        "        Statistics including all percentile values and count\n",
        "    \"\"\"\n",
        "    values = group[column].astype(float)\n",
        "    \n",
        "    # Build result dictionary with percentile prices\n",
        "    result = {}\n",
        "    for p in percentiles:\n",
        "        result[f'price_p{p}'] = values.quantile(p / 100)\n",
        "    \n",
        "    # Add additional statistics\n",
        "    result['price_count'] = len(values)\n",
        "    result['true_min'] = values.min()\n",
        "    result['true_max'] = values.max()\n",
        "    \n",
        "    return pd.Series(result)\n",
        "\n",
        "\n",
        "def log_filtering_step(df_before, df_after, step_name):\n",
        "    \"\"\"\n",
        "    Log the results of a filtering step.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    df_before : pd.DataFrame\n",
        "        DataFrame before filtering\n",
        "    df_after : pd.DataFrame\n",
        "        DataFrame after filtering\n",
        "    step_name : str\n",
        "        Name of the filtering step\n",
        "    \"\"\"\n",
        "    rows_removed = len(df_before) - len(df_after)\n",
        "    pct_removed = (rows_removed / len(df_before) * 100) if len(df_before) > 0 else 0\n",
        "    \n",
        "    print(f\"  {step_name}:\")\n",
        "    print(f\"    - Before: {len(df_before):,} rows\")\n",
        "    print(f\"    - After: {len(df_after):,} rows\")\n",
        "    print(f\"    - Removed: {rows_removed:,} rows ({pct_removed:.2f}%)\")\n",
        "\n",
        "\n",
        "print(\"Helper functions loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Data Fetching\n",
        "\n",
        "Fetch marketplace prices, packing unit mappings, and WAC (cost) data from Snowflake.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# FETCH MARKETPLACE PRICES\n",
        "# =============================================================================\n",
        "# Get active seller prices with region mapping\n",
        "\n",
        "print(\"Fetching marketplace prices...\")\n",
        "\n",
        "mp_query = '''\n",
        "WITH seller_region AS (\n",
        "    SELECT \n",
        "        seller_retailer.retailer_id,\n",
        "        CASE \n",
        "            WHEN regions.name_en = 'Greater Cairo' THEN cities.name_en \n",
        "            ELSE regions.name_en \n",
        "        END AS region,\n",
        "        seller_id,\n",
        "        seller_retailer.POLYGON_ID\n",
        "    FROM MATERIALIZED_VIEWS.SELLERS_RETAILERS_MAPPING seller_retailer\n",
        "    JOIN retailers ON retailers.id = seller_retailer.retailer_id\n",
        "    JOIN materialized_views.retailer_polygon ON materialized_views.retailer_polygon.retailer_id = retailers.id\n",
        "    JOIN districts ON districts.id = materialized_views.retailer_polygon.district_id\n",
        "    JOIN cities ON cities.id = districts.city_id\n",
        "    JOIN states ON states.id = cities.state_id\n",
        "    JOIN regions ON regions.id = states.region_id\n",
        "    JOIN egypt_marketplace.sellers ON sellers.id = seller_retailer.seller_id AND sellers.status = 'ACTIVE'\n",
        "),\n",
        "\n",
        "recent_price AS (\n",
        "    SELECT\n",
        "        wp.product_id AS product_id,\n",
        "        wp.packing_unit_id AS product_pu,\n",
        "        wp.price AS price,\n",
        "        wp.max_per_order,\n",
        "        warehouses.seller_id AS seller_id,\n",
        "        warehouses.MIN_TICKET_SIZE\n",
        "    FROM egypt_marketplace.warehouse_products wp\n",
        "    LEFT JOIN egypt_marketplace.warehouses ON warehouses.id = wp.warehouse_id \n",
        "    WHERE wp.AVAILABLE > 0 \n",
        "        AND wp.total_stock > 0\n",
        "        AND activation = 'true'\n",
        ")\n",
        "\n",
        "SELECT DISTINCT\n",
        "    seller_region.region,\n",
        "    recent_price.*\n",
        "FROM recent_price\n",
        "JOIN seller_region ON seller_region.seller_id = recent_price.seller_id\n",
        "'''\n",
        "\n",
        "mp_raw = query_snowflake(\n",
        "    mp_query, \n",
        "    columns=['region', 'product_id', 'product_pu', 'price', 'max_per_order', 'seller_id', 'min_ticket_size']\n",
        ")\n",
        "\n",
        "# Convert data types\n",
        "for col in ['product_id', 'product_pu', 'price', 'max_per_order', 'seller_id', 'min_ticket_size']:\n",
        "    mp_raw[col] = pd.to_numeric(mp_raw[col])\n",
        "\n",
        "print(f\"  Fetched {len(mp_raw):,} marketplace price records\")\n",
        "print(f\"  Unique products: {mp_raw['product_id'].nunique():,}\")\n",
        "print(f\"  Unique sellers: {mp_raw['seller_id'].nunique():,}\")\n",
        "print(f\"  Regions: {mp_raw['region'].unique().tolist()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# FETCH PACKING UNIT DATA\n",
        "# =============================================================================\n",
        "# Get packing unit to product mapping with basic unit count (BUC)\n",
        "\n",
        "print(\"Fetching packing unit data...\")\n",
        "\n",
        "pu_query = '''\n",
        "SELECT\n",
        "    product_id,\n",
        "    PACKING_UNIT_ID AS pu_id,\n",
        "    BASIC_UNIT_COUNT AS buc\n",
        "FROM PACKING_UNIT_PRODUCTS\n",
        "'''\n",
        "\n",
        "packing_units = query_snowflake(pu_query, columns=['product_id', 'pu_id', 'buc'])\n",
        "\n",
        "# Convert data types\n",
        "packing_units['product_id'] = pd.to_numeric(packing_units['product_id'])\n",
        "packing_units['pu_id'] = pd.to_numeric(packing_units['pu_id'])\n",
        "packing_units['buc'] = pd.to_numeric(packing_units['buc'])\n",
        "\n",
        "print(f\"  Fetched {len(packing_units):,} packing unit mappings\")\n",
        "print(f\"  Unique products: {packing_units['product_id'].nunique():,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# FETCH WAC (WEIGHTED AVERAGE COST) DATA\n",
        "# =============================================================================\n",
        "# Get current cost data for price validation\n",
        "\n",
        "print(\"Fetching WAC data...\")\n",
        "\n",
        "wac_query = '''\n",
        "SELECT \n",
        "    f.product_id,\n",
        "    f.wac1,\n",
        "    f.wac4,\n",
        "    f.wac_p\n",
        "FROM finance.all_cogs f\n",
        "WHERE current_timestamp BETWEEN f.from_date AND f.to_date\n",
        "'''\n",
        "\n",
        "wac_data = query_snowflake(wac_query, columns=['product_id', 'wac1', 'wac4', 'wac_p'])\n",
        "\n",
        "# Convert data types\n",
        "wac_data['product_id'] = pd.to_numeric(wac_data['product_id'])\n",
        "wac_data['wac1'] = pd.to_numeric(wac_data['wac1'])\n",
        "wac_data['wac4'] = pd.to_numeric(wac_data['wac4'])\n",
        "wac_data['wac_p'] = pd.to_numeric(wac_data['wac_p'])\n",
        "\n",
        "print(f\"  Fetched WAC data for {len(wac_data):,} products\")\n",
        "\n",
        "# Create packing unit WAC by joining with BUC\n",
        "pu_wac = pd.merge(packing_units, wac_data, on='product_id', how='left')\n",
        "pu_wac['pu_wac1'] = pu_wac['buc'] * pu_wac['wac1']\n",
        "pu_wac['pu_wac4'] = pu_wac['buc'] * pu_wac['wac4']\n",
        "\n",
        "print(f\"  Created PU-level WAC for {len(pu_wac):,} product-PU combinations\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: WAC Mapping and Initial Filtering\n",
        "\n",
        "Map prices to packing units and filter based on WAC4 percentage bounds.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# WAC MAPPING AND INITIAL FILTER\n",
        "# =============================================================================\n",
        "# Join marketplace prices with WAC data and filter by acceptable cost margins\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"STEP 2: WAC Mapping and Initial Filtering\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Rename column for clarity\n",
        "mp_data = mp_raw.copy()\n",
        "mp_data.rename(columns={'product_pu': 'mp_pu_id'}, inplace=True)\n",
        "\n",
        "# Join with packing unit WAC data\n",
        "mp_with_wac = pd.merge(\n",
        "    mp_data,\n",
        "    pu_wac,\n",
        "    how='inner',\n",
        "    on='product_id'\n",
        ")\n",
        "\n",
        "print(f\"\\nAfter joining with WAC data: {len(mp_with_wac):,} rows\")\n",
        "\n",
        "# Calculate percentage difference from WAC4\n",
        "# Formula: (price - pu_wac4) / pu_wac4 * 100\n",
        "mp_with_wac['wac4_pct_diff'] = (\n",
        "    (mp_with_wac['price'].astype(float) - mp_with_wac['pu_wac4'].astype(float)) \n",
        "    / mp_with_wac['pu_wac4'].astype(float) * 100\n",
        ").round(2)\n",
        "\n",
        "# Apply WAC filter using configurable bounds\n",
        "print(f\"\\nApplying WAC4 filter: {WAC_LOWER_BOUND}% to {WAC_UPPER_BOUND}%\")\n",
        "\n",
        "before_wac_filter = len(mp_with_wac)\n",
        "mp_wac_filtered = mp_with_wac[\n",
        "    (mp_with_wac['wac4_pct_diff'] >= WAC_LOWER_BOUND) & \n",
        "    (mp_with_wac['wac4_pct_diff'] <= WAC_UPPER_BOUND)\n",
        "].copy()\n",
        "\n",
        "log_filtering_step(mp_with_wac, mp_wac_filtered, \"WAC4 Filter\")\n",
        "\n",
        "# Select relevant columns for further processing\n",
        "mp_clean = mp_wac_filtered[[\n",
        "    'region', 'product_id', 'price', 'max_per_order', \n",
        "    'seller_id', 'min_ticket_size', 'pu_id'\n",
        "]].copy()\n",
        "\n",
        "print(f\"\\nData ready for outlier detection: {len(mp_clean):,} rows\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Scientific Outlier Removal (MAD)\n",
        "\n",
        "Apply MAD-based outlier detection to ticket size, max order, and price columns.\n",
        "MAD (Median Absolute Deviation) is robust to up to 50% outliers, making it ideal for marketplace data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SCIENTIFIC OUTLIER REMOVAL USING MAD\n",
        "# =============================================================================\n",
        "# Apply MAD filter to each metric per group (region, product_id, pu_id)\n",
        "# MAD is more robust than IQR for non-normal distributions\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"STEP 3: Scientific Outlier Removal (MAD)\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Using MAD threshold: {MAD_THRESHOLD}\")\n",
        "\n",
        "# Define grouping columns\n",
        "GROUP_COLS = ['region', 'product_id', 'pu_id']\n",
        "\n",
        "# Start with WAC-filtered data\n",
        "df = mp_clean.copy()\n",
        "initial_count = len(df)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 3.1 Filter outliers in min_ticket_size\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n3.1 Filtering min_ticket_size outliers...\")\n",
        "\n",
        "df_before = df.copy()\n",
        "df = df.groupby(GROUP_COLS, group_keys=False).apply(\n",
        "    lambda g: apply_mad_filter_to_group(g, 'min_ticket_size', MAD_THRESHOLD)\n",
        ")\n",
        "log_filtering_step(df_before, df, \"MAD filter on min_ticket_size\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 3.2 Filter outliers in max_per_order\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n3.2 Filtering max_per_order outliers...\")\n",
        "\n",
        "df_before = df.copy()\n",
        "df = df.groupby(GROUP_COLS, group_keys=False).apply(\n",
        "    lambda g: apply_mad_filter_to_group(g, 'max_per_order', MAD_THRESHOLD)\n",
        ")\n",
        "log_filtering_step(df_before, df, \"MAD filter on max_per_order\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 3.3 Filter outliers in price\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n3.3 Filtering price outliers...\")\n",
        "\n",
        "df_before = df.copy()\n",
        "df = df.groupby(GROUP_COLS, group_keys=False).apply(\n",
        "    lambda g: apply_mad_filter_to_group(g, 'price', MAD_THRESHOLD)\n",
        ")\n",
        "log_filtering_step(df_before, df, \"MAD filter on price\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Summary\n",
        "# -----------------------------------------------------------------------------\n",
        "mp_filtered = df.copy()\n",
        "final_count = len(mp_filtered)\n",
        "total_removed = initial_count - final_count\n",
        "total_pct = (total_removed / initial_count * 100) if initial_count > 0 else 0\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"OUTLIER REMOVAL SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Initial records: {initial_count:,}\")\n",
        "print(f\"Final records: {final_count:,}\")\n",
        "print(f\"Total removed: {total_removed:,} ({total_pct:.2f}%)\")\n",
        "print(f\"Unique SKUs (region-product-pu): {mp_filtered.groupby(GROUP_COLS).ngroups:,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Multi-Percentile Price Calculation\n",
        "\n",
        "Calculate 5 percentile-based prices (P10, P25, P50, P75, P90) for each SKU to get a full price distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CALCULATE PRICE RANGES (MULTIPLE PERCENTILES)\n",
        "# =============================================================================\n",
        "# Compute multiple percentile-based prices per SKU\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"STEP 4: Price Range Calculation\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Calculating percentiles: {PERCENTILES}\")\n",
        "\n",
        "# Calculate price statistics per group with multiple percentiles\n",
        "price_bounds = (\n",
        "    mp_filtered\n",
        "    .groupby(GROUP_COLS)\n",
        "    .apply(lambda g: get_percentile_prices(\n",
        "        g, \n",
        "        column='price', \n",
        "        percentiles=PERCENTILES\n",
        "    ))\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Calculate mode price per group (most common price point)\n",
        "def calculate_mode_price(group):\n",
        "    \"\"\"Get the mode (most frequent) price for a group.\"\"\"\n",
        "    mode_result = group['price'].mode()\n",
        "    return mode_result.iloc[0] if len(mode_result) > 0 else np.nan\n",
        "\n",
        "mode_prices = (\n",
        "    mp_filtered\n",
        "    .groupby(GROUP_COLS)\n",
        "    .apply(calculate_mode_price)\n",
        "    .reset_index(name='price_mode')\n",
        ")\n",
        "\n",
        "# Merge mode prices into price bounds\n",
        "price_bounds = pd.merge(price_bounds, mode_prices, on=GROUP_COLS, how='left')\n",
        "\n",
        "# Round all numeric columns\n",
        "numeric_cols = [f'price_p{p}' for p in PERCENTILES] + ['true_min', 'true_max', 'price_mode']\n",
        "for col in numeric_cols:\n",
        "    if col in price_bounds.columns:\n",
        "        price_bounds[col] = price_bounds[col].round(2)\n",
        "\n",
        "print(f\"\\nGenerated price ranges for {len(price_bounds):,} SKUs\")\n",
        "print(f\"\\nOutput columns: {price_bounds.columns.tolist()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Sales-Weighted Average (Optional)\n",
        "\n",
        "Calculate weighted average prices based on actual sales NMV to understand market-validated pricing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# FETCH SALES DATA FOR WEIGHTED AVERAGES\n",
        "# =============================================================================\n",
        "# Get historical and recent sales to calculate NMV-weighted average prices\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"STEP 5: Sales-Weighted Average Calculation\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 5.1 Fetch historical sales data\n",
        "# -----------------------------------------------------------------------------\n",
        "print(f\"\\nFetching historical sales (last {SALES_LOOKBACK_DAYS} days)...\")\n",
        "\n",
        "historical_sales_query = f'''\n",
        "SELECT\n",
        "    seller_id,\n",
        "    product_id,\n",
        "    packing_unit_id AS pu_id,\n",
        "    item_price,\n",
        "    SUM(sop.total_price) AS nmv\n",
        "FROM egypt_marketplace.sales_orders so\n",
        "JOIN egypt_marketplace.sales_order_products sop ON sop.order_id = so.id\n",
        "WHERE so.status = 6 \n",
        "    AND so.created_at::date >= current_date - {SALES_LOOKBACK_DAYS}\n",
        "GROUP BY ALL\n",
        "'''\n",
        "\n",
        "mp_sales_historical = query_snowflake(\n",
        "    historical_sales_query, \n",
        "    columns=['seller_id', 'product_id', 'pu_id', 'item_price', 'nmv']\n",
        ")\n",
        "\n",
        "for col in ['seller_id', 'product_id', 'pu_id', 'item_price', 'nmv']:\n",
        "    mp_sales_historical[col] = pd.to_numeric(mp_sales_historical[col])\n",
        "\n",
        "print(f\"  Fetched {len(mp_sales_historical):,} historical sales records\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 5.2 Fetch recent sales data\n",
        "# -----------------------------------------------------------------------------\n",
        "print(f\"\\nFetching recent sales (last {RECENT_SALES_DAYS} days)...\")\n",
        "\n",
        "recent_sales_query = f'''\n",
        "SELECT\n",
        "    seller_id,\n",
        "    product_id,\n",
        "    packing_unit_id AS pu_id,\n",
        "    item_price,\n",
        "    SUM(sop.total_price) AS nmv\n",
        "FROM egypt_marketplace.sales_orders so\n",
        "JOIN egypt_marketplace.sales_order_products sop ON sop.order_id = so.id\n",
        "WHERE so.status NOT IN (3, 7, 8)\n",
        "    AND so.created_at::date >= current_date - {RECENT_SALES_DAYS}\n",
        "GROUP BY ALL\n",
        "'''\n",
        "\n",
        "mp_sales_recent = query_snowflake(\n",
        "    recent_sales_query, \n",
        "    columns=['seller_id', 'product_id', 'pu_id', 'item_price', 'nmv']\n",
        ")\n",
        "\n",
        "for col in ['seller_id', 'product_id', 'pu_id', 'item_price', 'nmv']:\n",
        "    mp_sales_recent[col] = pd.to_numeric(mp_sales_recent[col])\n",
        "\n",
        "print(f\"  Fetched {len(mp_sales_recent):,} recent sales records\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CALCULATE WEIGHTED AVERAGE PRICES\n",
        "# =============================================================================\n",
        "# Join sales with filtered MP data and compute NMV-weighted averages\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 5.3 Calculate historical weighted average\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\nCalculating historical weighted average prices...\")\n",
        "\n",
        "# Join filtered data with historical sales\n",
        "filtered_with_sales = pd.merge(\n",
        "    mp_filtered,\n",
        "    mp_sales_historical,\n",
        "    how='inner',\n",
        "    on=['seller_id', 'product_id', 'pu_id']\n",
        ")\n",
        "\n",
        "# Calculate weighted average: sum(price * nmv) / sum(nmv)\n",
        "filtered_with_sales['price'] = pd.to_numeric(filtered_with_sales['price'])\n",
        "filtered_with_sales['nmv'] = pd.to_numeric(filtered_with_sales['nmv'])\n",
        "\n",
        "weighted_avg_historical = (\n",
        "    filtered_with_sales\n",
        "    .groupby(GROUP_COLS)\n",
        "    .apply(lambda g: (g['price'] * g['nmv']).sum() / g['nmv'].sum())\n",
        "    .reset_index(name='weighted_avg_price')\n",
        ")\n",
        "\n",
        "print(f\"  Calculated weighted average for {len(weighted_avg_historical):,} SKUs\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 5.4 Calculate recent weighted average\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\nCalculating recent weighted average prices...\")\n",
        "\n",
        "# Join filtered data with recent sales\n",
        "filtered_with_recent = pd.merge(\n",
        "    mp_filtered,\n",
        "    mp_sales_recent,\n",
        "    how='inner',\n",
        "    on=['seller_id', 'product_id', 'pu_id']\n",
        ")\n",
        "\n",
        "filtered_with_recent['price'] = pd.to_numeric(filtered_with_recent['price'])\n",
        "filtered_with_recent['nmv'] = pd.to_numeric(filtered_with_recent['nmv'])\n",
        "\n",
        "weighted_avg_recent = (\n",
        "    filtered_with_recent\n",
        "    .groupby(GROUP_COLS)\n",
        "    .apply(lambda g: (g['price'] * g['nmv']).sum() / g['nmv'].sum())\n",
        "    .reset_index(name='weighted_avg_price_recent')\n",
        ")\n",
        "\n",
        "print(f\"  Calculated recent weighted average for {len(weighted_avg_recent):,} SKUs\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 5.5 Merge weighted averages\n",
        "# -----------------------------------------------------------------------------\n",
        "weighted_averages = pd.merge(\n",
        "    weighted_avg_historical, \n",
        "    weighted_avg_recent, \n",
        "    on=GROUP_COLS, \n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Round values\n",
        "weighted_averages['weighted_avg_price'] = weighted_averages['weighted_avg_price'].round(2)\n",
        "weighted_averages['weighted_avg_price_recent'] = weighted_averages['weighted_avg_price_recent'].round(2)\n",
        "\n",
        "print(f\"\\nMerged weighted averages: {len(weighted_averages):,} SKUs\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Final Output and Validation\n",
        "\n",
        "Merge all data and produce the final clean marketplace price dataset with validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# FINAL OUTPUT: MERGE ALL DATA\n",
        "# =============================================================================\n",
        "# Combine price bounds with weighted averages for the final dataset\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"STEP 6: Final Output Generation\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Merge price bounds with weighted averages\n",
        "mp_final = pd.merge(\n",
        "    price_bounds, \n",
        "    weighted_averages, \n",
        "    on=GROUP_COLS, \n",
        "    how='left'  # Keep all price bounds, even without sales data\n",
        ")\n",
        "\n",
        "# Remove duplicates if any\n",
        "mp_final = mp_final.drop_duplicates()\n",
        "\n",
        "# Final rounding\n",
        "numeric_cols = mp_final.select_dtypes(include=[np.number]).columns\n",
        "mp_final[numeric_cols] = mp_final[numeric_cols].round(2)\n",
        "\n",
        "print(f\"\\nFinal dataset: {len(mp_final):,} SKUs\")\n",
        "print(f\"\\nColumn summary:\")\n",
        "for col in mp_final.columns:\n",
        "    null_count = mp_final[col].isnull().sum()\n",
        "    null_pct = (null_count / len(mp_final) * 100)\n",
        "    print(f\"  - {col}: {null_count:,} nulls ({null_pct:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# VALIDATION: SAMPLE OUTPUT\n",
        "# =============================================================================\n",
        "# Display sample data for verification\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"VALIDATION: Sample Output\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Show sample for a specific product\n",
        "print(f\"\\n1. Sample output for product_id = {SAMPLE_PRODUCT_ID}:\")\n",
        "sample_product = mp_final[mp_final['product_id'] == SAMPLE_PRODUCT_ID]\n",
        "if len(sample_product) > 0:\n",
        "    display(sample_product)\n",
        "else:\n",
        "    print(f\"   No data found for product_id {SAMPLE_PRODUCT_ID}\")\n",
        "\n",
        "# Show summary statistics using min and max percentiles\n",
        "p_min, p_max = PERCENTILES[0], PERCENTILES[-1]\n",
        "print(\"\\n2. Overall price range statistics:\")\n",
        "print(f\"   - Min P{p_min} price: {mp_final[f'price_p{p_min}'].min():.2f}\")\n",
        "print(f\"   - Max P{p_max} price: {mp_final[f'price_p{p_max}'].max():.2f}\")\n",
        "print(f\"   - Average spread (P{p_max}-P{p_min}): {(mp_final[f'price_p{p_max}'] - mp_final[f'price_p{p_min}']).mean():.2f}\")\n",
        "\n",
        "# Show region distribution\n",
        "print(\"\\n3. SKUs per region:\")\n",
        "region_counts = mp_final.groupby('region').size().sort_values(ascending=False)\n",
        "for region, count in region_counts.items():\n",
        "    print(f\"   - {region}: {count:,}\")\n",
        "\n",
        "# Show data quality metrics\n",
        "print(\"\\n4. Data quality metrics:\")\n",
        "print(f\"   - Total SKUs with price ranges: {len(mp_final):,}\")\n",
        "print(f\"   - SKUs with weighted avg price: {mp_final['weighted_avg_price'].notna().sum():,}\")\n",
        "print(f\"   - SKUs with recent weighted avg: {mp_final['weighted_avg_price_recent'].notna().sum():,}\")\n",
        "print(f\"   - Average seller count per SKU: {mp_final['price_count'].mean():.1f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# FINAL OUTPUT: mp_final DataFrame\n",
        "# =============================================================================\n",
        "# The main output is the mp_final DataFrame containing:\n",
        "#   - region: Geographic region\n",
        "#   - product_id: Product identifier\n",
        "#   - pu_id: Packing unit identifier\n",
        "#   - price_p10, price_p25, price_p50, price_p75, price_p90: Percentile prices\n",
        "#   - price_count: Number of sellers\n",
        "#   - true_min: Actual minimum price\n",
        "#   - true_max: Actual maximum price\n",
        "#   - price_mode: Most common price\n",
        "#   - weighted_avg_price: NMV-weighted average (historical)\n",
        "#   - weighted_avg_price_recent: NMV-weighted average (recent)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"FINAL OUTPUT: mp_final\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nDataFrame shape: {mp_final.shape}\")\n",
        "print(f\"\\nFirst 10 rows:\")\n",
        "display(mp_final.head(10))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"NOTEBOOK COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\"\"\n",
        "The mp_final DataFrame is ready for use in pricing logic.\n",
        "\n",
        "Key outputs (5 price points per SKU):\n",
        "- price_p10: Floor price (10th percentile)\n",
        "- price_p25: Low price (25th percentile)  \n",
        "- price_p50: Median price (50th percentile)\n",
        "- price_p75: High price (75th percentile)\n",
        "- price_p90: Ceiling price (90th percentile)\n",
        "- price_mode: Most common price\n",
        "- weighted_avg_price: Market-validated price based on actual sales\n",
        "\n",
        "Configuration used:\n",
        "- MAD Threshold: {MAD_THRESHOLD}\n",
        "- WAC Range: {WAC_LOWER_BOUND}% to {WAC_UPPER_BOUND}%\n",
        "- Percentiles: {PERCENTILES}\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
