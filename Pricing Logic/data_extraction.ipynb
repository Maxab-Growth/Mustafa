{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction Module for Pricing & Offers System\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import snowflake.connector\n",
    "import setup_environment_2\n",
    "import pytz\n",
    "from common_functions import upload_dataframe_to_snowflake,send_text_slack\n",
    "\n",
    "# Cairo timezone for consistent timestamps\n",
    "CAIRO_TZ = pytz.timezone('Africa/Cairo')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Region\n",
    "REGION = \"Egypt\"\n",
    "\n",
    "# Snowflake Warehouse\n",
    "WAREHOUSE = \"COMPUTE_WH\"\n",
    "\n",
    "# Date Variables\n",
    "from datetime import datetime, timedelta\n",
    "TODAY = datetime.now(CAIRO_TZ).date()\n",
    "YESTERDAY = TODAY - timedelta(days=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_environment_2.initialize_env()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warehouse & Cohort Mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warehouse Mapping: (region, warehouse_name, warehouse_id, cohort_id)\n",
    "WAREHOUSE_MAPPING = [\n",
    "    ('Cairo', 'Mostorod', 1, 700),\n",
    "    ('Giza', 'Barageel', 236, 701),\n",
    "    ('Giza', 'Sakkarah', 962, 701),\n",
    "    ('Delta West', 'El-Mahala', 337, 703),\n",
    "    ('Delta West', 'Tanta', 8, 703),\n",
    "    ('Delta East', 'Mansoura FC', 339, 704),\n",
    "    ('Delta East', 'Sharqya', 170, 704),\n",
    "    ('Upper Egypt', 'Assiut FC', 501, 1124),\n",
    "    ('Upper Egypt', 'Bani sweif', 401, 1126),\n",
    "    ('Upper Egypt', 'Menya Samalot', 703, 1123),\n",
    "    ('Upper Egypt', 'Sohag', 632, 1125),\n",
    "    ('Alexandria', 'Khorshed Alex', 797, 702),\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# All Cohort IDs\n",
    "COHORT_IDS = [700, 701, 702, 703, 704, 1123, 1124, 1125, 1126]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snowflake Query Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_snowflake(query):\n",
    "    \"\"\"Execute a query on Snowflake and return results as DataFrame.\"\"\"\n",
    "    con = snowflake.connector.connect(\n",
    "        user=os.environ[\"SNOWFLAKE_USERNAME\"],\n",
    "        account=os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "        password=os.environ[\"SNOWFLAKE_PASSWORD\"],\n",
    "        database=os.environ[\"SNOWFLAKE_DATABASE\"]\n",
    "    )\n",
    "    try:\n",
    "        cur = con.cursor()\n",
    "        cur.execute(\"USE WAREHOUSE COMPUTE_WH\")\n",
    "        cur.execute(query)\n",
    "        data = cur.fetchall()\n",
    "        columns = [desc[0].lower() for desc in cur.description]  # Get column names from cursor\n",
    "        return pd.DataFrame(data, columns=columns)\n",
    "    except Exception as e:\n",
    "        print(f\"Snowflake Error: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    finally:\n",
    "        cur.close()\n",
    "        con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snowflake_timezone():\n",
    "    \"\"\"Get the current timezone from Snowflake.\"\"\"\n",
    "    query = \"SHOW PARAMETERS LIKE 'TIMEZONE'\"\n",
    "    result = query_snowflake(query)\n",
    "    return result.value[0] if len(result) > 0 else \"UTC\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_warehouse_df():\n",
    "    \"\"\"Get warehouse mapping as DataFrame.\"\"\"\n",
    "    return pd.DataFrame(\n",
    "        WAREHOUSE_MAPPING,\n",
    "        columns=['region', 'warehouse', 'warehouse_id', 'cohort_id'])\n",
    "    \n",
    "\n",
    "\n",
    "def convert_to_numeric(df):\n",
    "    \"\"\"Convert DataFrame columns to numeric where possible.\"\"\"\n",
    "    df.columns = df.columns.str.lower()\n",
    "    for col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='ignore')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Snowflake Timezone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMEZONE = get_snowflake_timezone()\n",
    "print(f\"Snowflake timezone: {TIMEZONE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 8. ALL-TIME HIGH MARGIN QUERY (P80 margin weighted by gross profit)\n",
    "# This calculates the top 80% margin based on days with best gross profit\n",
    "# Gross Profit = NMV Ã— margin, so high-margin + high-sales days rank higher\n",
    "# =============================================================================\n",
    "ALL_TIME_HIGH_MARGIN_QUERY = f'''\n",
    "WITH daily_margin_data AS (\n",
    "    SELECT \n",
    "        pso.warehouse_id,\n",
    "        pso.product_id,\n",
    "        so.created_at::DATE AS sale_date,\n",
    "        SUM(pso.total_price) AS daily_nmv,\n",
    "        SUM(COALESCE(f.wac_p, 0) * pso.purchased_item_count * pso.basic_unit_count) AS daily_cogs,\n",
    "        CASE \n",
    "            WHEN SUM(pso.total_price) > 0 \n",
    "            THEN (SUM(pso.total_price) - SUM(COALESCE(f.wac_p, 0) * pso.purchased_item_count * pso.basic_unit_count)) / SUM(pso.total_price)\n",
    "            ELSE 0 \n",
    "        END AS daily_margin\n",
    "    FROM product_sales_order pso\n",
    "    JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "    JOIN finance.all_cogs f ON f.product_id = pso.product_id\n",
    "        AND f.from_date::DATE <= so.created_at::DATE \n",
    "        AND f.to_date::DATE > so.created_at::DATE\n",
    "    WHERE so.created_at::DATE >= CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE - 240\n",
    "        AND so.created_at::DATE < CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE\n",
    "        AND so.sales_order_status_id NOT IN (7, 12)\n",
    "        AND so.channel IN ('telesales', 'retailer')\n",
    "        AND pso.purchased_item_count <> 0\n",
    "    GROUP BY pso.warehouse_id, pso.product_id, so.created_at::DATE\n",
    "),\n",
    "\n",
    "-- Calculate gross profit and rank days by it\n",
    "ranked_by_gross_profit AS (\n",
    "    SELECT \n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        sale_date,\n",
    "        daily_nmv,\n",
    "        daily_margin,\n",
    "        daily_nmv * daily_margin AS gross_profit,\n",
    "        -- Rank by gross profit (1 = highest gross profit day)\n",
    "        ROW_NUMBER() OVER (\n",
    "            PARTITION BY warehouse_id, product_id \n",
    "            ORDER BY daily_nmv * daily_margin DESC\n",
    "        ) AS gp_rank,\n",
    "        COUNT(*) OVER (PARTITION BY warehouse_id, product_id) AS total_days\n",
    "    FROM daily_margin_data\n",
    "    WHERE daily_nmv > 0 AND daily_margin > 0\n",
    ")\n",
    "\n",
    "-- Take P80 of margins from TOP-ranked days by gross profit\n",
    "-- P80 = take the 80th percentile of margins from the best-performing days\n",
    "SELECT \n",
    "    warehouse_id,\n",
    "    product_id,\n",
    "    -- P80 margin from days ranked by gross profit\n",
    "    PERCENTILE_CONT(0.8) WITHIN GROUP (ORDER BY daily_margin) AS all_time_high_margin,\n",
    "    -- Also provide some context stats\n",
    "    MAX(daily_margin) AS max_daily_margin,\n",
    "    AVG(daily_margin) AS avg_daily_margin,\n",
    "    COUNT(*) AS days_with_profit\n",
    "FROM ranked_by_gross_profit\n",
    "-- Include only top 80% of days by gross profit rank\n",
    "WHERE gp_rank <= GREATEST(1, CEIL(total_days * 0.8))\n",
    "GROUP BY warehouse_id, product_id\n",
    "'''\n",
    "\n",
    "print(\"All-time high margin query defined (P80 margin weighted by gross profit)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Market Prices Extraction Queries\n",
    "Queries for external market price data:\n",
    "1. **Ben Soliman Prices** - Competitor reference prices\n",
    "2. **Marketplace Prices** - Min, Max, Mod prices from marketplace\n",
    "3. **Scrapped Data** - Competitor prices from scraping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. BEN SOLIMAN PRICES QUERY\n",
    "# =============================================================================\n",
    "BEN_SOLIMAN_QUERY = f'''\n",
    "WITH lower as (\n",
    "    select distinct product_id, sku, new_d*bs_price as ben_soliman_price, INJECTION_DATE\n",
    "    from (\n",
    "        select maxab_product_id as product_id, maxab_sku as sku, INJECTION_DATE, wac1, wac_p,\n",
    "            (bs_price/bs_unit_count) as bs_price, diff, cu_price,\n",
    "            case when p1 > 1 then child_quantity else 0 end as scheck,\n",
    "            round(p1/2)*2 as p1, p2,\n",
    "            case when (ROUND(p1 / scheck) * scheck) = 0 then p1 else (ROUND(p1 / scheck) * scheck) end as new_d\n",
    "        from (\n",
    "            select sm.*, wac1, wac_p, \n",
    "                abs((bs_price/bs_unit_count)-(wac_p*maxab_basic_unit_count))/(wac_p*maxab_basic_unit_count) as diff,\n",
    "                cpc.price as cu_price, pup.child_quantity,\n",
    "                round((cu_price/(bs_price/bs_unit_count))) as p1, \n",
    "                round(((bs_price/bs_unit_count)/cu_price)) as p2\n",
    "            from materialized_views.savvy_mapping sm \n",
    "            join finance.all_cogs f on f.product_id = sm.maxab_product_id \n",
    "                and CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP()) between f.from_Date and f.to_date\n",
    "            join PACKING_UNIT_PRODUCTS pu on pu.product_id = sm.maxab_product_id and pu.IS_BASIC_UNIT = 1 \n",
    "            join cohort_product_packing_units cpc on cpc.PRODUCT_PACKING_UNIT_ID = pu.id and cohort_id = 700 \n",
    "            join packing_unit_products pup on pup.product_id = sm.maxab_product_id and pup.is_basic_unit = 1  \n",
    "            where bs_price is not null \n",
    "                and INJECTION_DATE::date >= CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - 5 \n",
    "                and diff > 0.3 and p1 > 1\n",
    "        )\n",
    "    )\n",
    "    qualify max(INJECTION_DATE) over(partition by product_id) = INJECTION_DATE\n",
    "),\n",
    "\n",
    "m_bs as (\n",
    "    select z.* from (\n",
    "        select maxab_product_id as product_id, maxab_sku as sku, avg(bs_final_price) as ben_soliman_price, INJECTION_DATE\n",
    "        from (\n",
    "            select *, row_number() over(partition by maxab_product_id order by diff) as rnk_2 \n",
    "            from (\n",
    "                select *, (bs_final_price-wac_p)/wac_p as diff_2 \n",
    "                from (\n",
    "                    select *, bs_price/maxab_basic_unit_count as bs_final_price \n",
    "                    from (\n",
    "                        select *, row_number() over(partition by maxab_product_id, maxab_pu order by diff) as rnk \n",
    "                        from (\n",
    "                            select *, max(INJECTION_DATE::date) over(partition by maxab_product_id, maxab_pu) as max_date\n",
    "                            from (\n",
    "                                select sm.*, wac1, wac_p, \n",
    "                                    abs(bs_price-(wac_p*maxab_basic_unit_count))/(wac_p*maxab_basic_unit_count) as diff \n",
    "                                from materialized_views.savvy_mapping sm \n",
    "                                join finance.all_cogs f on f.product_id = sm.maxab_product_id \n",
    "                                    and CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP()) between f.from_Date and f.to_date\n",
    "                                where bs_price is not null \n",
    "                                    and INJECTION_DATE::date >= CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - 5 \n",
    "                                    and diff < 0.3\n",
    "                            )\n",
    "                            qualify max_date = INJECTION_DATE\n",
    "                        ) qualify rnk = 1 \n",
    "                    )\n",
    "                ) where diff_2 between -0.5 and 0.5 \n",
    "            ) qualify rnk_2 = 1 \n",
    "        ) group by all\n",
    "    ) z \n",
    "    join finance.all_cogs f on f.product_id = z.product_id \n",
    "        and CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP()) between f.from_Date and f.to_date\n",
    "    where ben_soliman_price between f.wac_p*0.8 and f.wac_p*1.3\n",
    ")\n",
    "\n",
    "select product_id, avg(ben_soliman_price) as ben_soliman_price\n",
    "from (\n",
    "    select * from (\n",
    "        select * from m_bs \n",
    "        union all\n",
    "        select * from lower\n",
    "    )\n",
    "    qualify max(INJECTION_DATE) over(partition by product_id) = INJECTION_DATE\n",
    ")\n",
    "group by all\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2. MARKETPLACE PRICES QUERY (with region fallback)\n",
    "# =============================================================================\n",
    "MARKETPLACE_PRICES_QUERY = f'''\n",
    "WITH MP as (\n",
    "    select region, product_id,\n",
    "        min(min_price) as min_price, min(max_price) as max_price,\n",
    "        min(mod_price) as mod_price, min(true_min) as true_min, min(true_max) as true_max\n",
    "    from (\n",
    "        select mp.region, mp.product_id, mp.pu_id,\n",
    "            min_price/BASIC_UNIT_COUNT as min_price,\n",
    "            max_price/BASIC_UNIT_COUNT as max_price,\n",
    "            mod_price/BASIC_UNIT_COUNT as mod_price,\n",
    "            TRUE_MIN_PRICE/BASIC_UNIT_COUNT as true_min,\n",
    "            TRUE_MAX_PRICE/BASIC_UNIT_COUNT as true_max\n",
    "        from materialized_views.marketplace_prices mp \n",
    "        join packing_unit_products pup on pup.product_id = mp.product_id and pup.packing_unit_id = mp.pu_id\n",
    "        join finance.all_cogs f on f.product_id = mp.product_id \n",
    "            and CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP()) between f.from_date and f.to_date\n",
    "        where least(min_price, mod_price) between wac_p*0.9 and wac_p*1.3 \n",
    "    )\n",
    "    group by all \n",
    "),\n",
    "\n",
    "region_mapping AS (\n",
    "    SELECT * FROM (VALUES\n",
    "        ('Delta East', 'Delta West'), ('Delta West', 'Delta East'),\n",
    "        ('Alexandria', 'Cairo'), ('Alexandria', 'Giza'),\n",
    "        ('Upper Egypt', 'Cairo'), ('Upper Egypt', 'Giza'),\n",
    "        ('Cairo', 'Giza'), ('Giza', 'Cairo'),\n",
    "        ('Delta West', 'Cairo'), ('Delta East', 'Cairo'),\n",
    "        ('Delta West', 'Giza'), ('Delta East', 'Giza')\n",
    "    ) AS region_mapping(region, fallback_region)\n",
    "),\n",
    "\n",
    "all_regions as (\n",
    "    SELECT * FROM (VALUES\n",
    "        ('Cairo'), ('Giza'), ('Delta West'), ('Delta East'), ('Upper Egypt'), ('Alexandria')\n",
    "    ) AS x(region)\n",
    "),\n",
    "\n",
    "full_data as (\n",
    "    select products.id as product_id, ar.region\n",
    "    from products, all_regions ar\n",
    "    where activation = 'true'\n",
    ")\n",
    "\n",
    "select region, product_id,\n",
    "    min(final_min_price) as final_min_price, \n",
    "    min(final_max_price) as final_max_price,\n",
    "    min(final_mod_price) as final_mod_price, \n",
    "    min(final_true_min) as final_true_min,\n",
    "    min(final_true_max) as final_true_max\n",
    "from (\n",
    "    SELECT distinct w.region, w.product_id,\n",
    "        COALESCE(m1.min_price, m2.min_price) AS final_min_price,\n",
    "        COALESCE(m1.max_price, m2.max_price) AS final_max_price,\n",
    "        COALESCE(m1.mod_price, m2.mod_price) AS final_mod_price,\n",
    "        COALESCE(m1.true_min, m2.true_min) AS final_true_min,\n",
    "        COALESCE(m1.true_max, m2.true_max) AS final_true_max\n",
    "    FROM full_data w\n",
    "    LEFT JOIN MP m1 ON w.region = m1.region and w.product_id = m1.product_id\n",
    "    LEFT JOIN region_mapping rm ON w.region = rm.region\n",
    "    LEFT JOIN MP m2 ON rm.fallback_region = m2.region AND w.product_id = m2.product_id\n",
    ")\n",
    "where final_min_price is not null \n",
    "group by all\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3. SCRAPPED DATA QUERY (Competitor prices from scraping)\n",
    "# =============================================================================\n",
    "SCRAPPED_DATA_QUERY = f'''\n",
    "select product_id, region,\n",
    "    MIN(market_price) AS min_scrapped,\n",
    "    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY market_price) AS scrapped25,\n",
    "    PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY market_price) AS scrapped50,\n",
    "    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY market_price) AS scrapped75,\n",
    "    MAX(market_price) AS max_scrapped\n",
    "from (\n",
    "    select distinct cmp.*, max(date) over(partition by region, cmp.product_id, competitor) as max_date\n",
    "    from MATERIALIZED_VIEWS.CLEANED_MARKET_PRICES cmp\n",
    "    join finance.all_cogs f on f.product_id = cmp.product_id \n",
    "        and CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP()) between f.from_date and f.to_date \n",
    "    where date >= CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - 7 \n",
    "        and MARKET_PRICE between f.wac_p * 0.8 and wac_p * 1.3\n",
    "    qualify date = max_date \n",
    ")\n",
    "group by all\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Additional Data Queries (Sales, Groups, WAC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4. PRODUCT BASE DATA QUERY (product_id, sku, brand, cat, wac1, wac_p, current_price)\n",
    "# =============================================================================\n",
    "PRODUCT_BASE_QUERY = f'''\n",
    "WITH skus_prices AS (\n",
    "    WITH local_prices AS (\n",
    "        SELECT  \n",
    "            CASE \n",
    "                WHEN cpu.cohort_id IN (700, 695) THEN 'Cairo'\n",
    "                WHEN cpu.cohort_id IN (701) THEN 'Giza'\n",
    "                WHEN cpu.cohort_id IN (704, 698) THEN 'Delta East'\n",
    "                WHEN cpu.cohort_id IN (703, 697) THEN 'Delta West'\n",
    "                WHEN cpu.cohort_id IN (696, 1123, 1124, 1125, 1126) THEN 'Upper Egypt'\n",
    "                WHEN cpu.cohort_id IN (702, 699) THEN 'Alexandria'\n",
    "            END AS region,\n",
    "            cohort_id,\n",
    "            pu.product_id,\n",
    "            pu.packing_unit_id,\n",
    "            pu.basic_unit_count,\n",
    "            AVG(cpu.price) AS price\n",
    "        FROM cohort_product_packing_units cpu\n",
    "        JOIN PACKING_UNIT_PRODUCTS pu ON pu.id = cpu.product_packing_unit_id\n",
    "        WHERE cpu.cohort_id IN (700,701,702,703,704,695,696,697,698,699,1123,1124,1125,1126)\n",
    "            AND cpu.created_at::date <> '2023-07-31'\n",
    "            AND cpu.is_customized = TRUE\n",
    "        GROUP BY ALL\n",
    "    ),\n",
    "    \n",
    "    live_prices AS (\n",
    "        SELECT \n",
    "            region, cohort_id, product_id, \n",
    "            pu_id AS packing_unit_id, \n",
    "            buc AS basic_unit_count, \n",
    "            NEW_PRICE AS price\n",
    "        FROM materialized_views.DBDP_PRICES\n",
    "        WHERE created_at = CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date\n",
    "            AND DATE_PART('hour', CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::time) \n",
    "                BETWEEN SPLIT_PART(time_slot, '-', 1)::int AND (SPLIT_PART(time_slot, '-', 1)::int) + 1\n",
    "            AND cohort_id IN (700,701,702,703,704,695,696,697,698,699,1123,1124,1125,1126)\n",
    "    ),\n",
    "    \n",
    "    prices AS (\n",
    "        SELECT *\n",
    "        FROM (\n",
    "            SELECT *, 1 AS priority FROM live_prices\n",
    "            UNION ALL\n",
    "            SELECT *, 2 AS priority FROM local_prices\n",
    "        )\n",
    "        QUALIFY ROW_NUMBER() OVER (PARTITION BY region, cohort_id, product_id, packing_unit_id ORDER BY priority) = 1\n",
    "    )\n",
    "    \n",
    "    SELECT region, cohort_id, product_id, price\n",
    "    FROM prices\n",
    "    WHERE basic_unit_count = 1\n",
    "        AND ((product_id = 1309 AND packing_unit_id = 2) OR (product_id <> 1309))\n",
    ")\n",
    "\n",
    "SELECT distinct\n",
    "    region, cohort_id, p.product_id,\n",
    "    CONCAT(products.name_ar, ' ', products.size, ' ', product_units.name_ar) AS sku,\n",
    "    b.name_ar AS brand,\n",
    "    cat.name_ar AS cat,\n",
    "    wac1, wac_p, p.price as current_price\n",
    "FROM skus_prices p\n",
    "JOIN finance.all_cogs c ON c.product_id = p.product_id \n",
    "    AND CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP()) BETWEEN c.from_date AND c.to_date\n",
    "JOIN products ON products.id = p.product_id\n",
    "JOIN categories cat ON cat.id = products.category_id\n",
    "JOIN brands b ON b.id = products.brand_id\n",
    "JOIN product_units ON product_units.id = products.unit_id\n",
    "WHERE wac1 > 0 AND wac_p > 0\n",
    "GROUP BY ALL\n",
    "'''\n",
    "\n",
    "# =============================================================================\n",
    "# 5. SALES DATA QUERY (120-day NMV by cohort/product)\n",
    "# =============================================================================\n",
    "SALES_QUERY = f'''\n",
    "SELECT DISTINCT cpc.cohort_id, pso.product_id,\n",
    "    CONCAT(products.name_ar,' ',products.size,' ',product_units.name_ar) as sku,\n",
    "    brands.name_ar as brand, categories.name_ar as cat,\n",
    "    sum(pso.total_price) as nmv\n",
    "FROM product_sales_order pso\n",
    "JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "JOIN COHORT_PRICING_CHANGES cpc ON cpc.id = pso.COHORT_PRICING_CHANGE_id\n",
    "JOIN products ON products.id = pso.product_id\n",
    "JOIN brands ON products.brand_id = brands.id \n",
    "JOIN categories ON products.category_id = categories.id\n",
    "JOIN product_units ON product_units.id = products.unit_id \n",
    "WHERE so.created_at::date BETWEEN CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - 120 \n",
    "    AND CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - 1 \n",
    "    AND so.sales_order_status_id NOT IN (7, 12)\n",
    "    AND so.channel IN ('telesales', 'retailer')\n",
    "    AND pso.purchased_item_count <> 0\n",
    "    AND cpc.cohort_id IN (700,701,702,703,704,1123,1124,1125,1126)\n",
    "GROUP BY ALL\n",
    "'''\n",
    "\n",
    "# =============================================================================\n",
    "# 6. MARGIN STATS QUERY (STD and average margins)  \n",
    "# =============================================================================\n",
    "MARGIN_STATS_QUERY = f'''\n",
    "select product_id, cohort_id, \n",
    "    (0.6*product_std) + (0.3*brand_std) + (0.1*cat_std) as std, \n",
    "    avg_margin\n",
    "from (\n",
    "    select product_id, cohort_id, \n",
    "        stddev(product_margin) as product_std, \n",
    "        stddev(brand_margin) as brand_std,\n",
    "        stddev(cat_margin) as cat_std, \n",
    "        avg(product_margin) as avg_margin\n",
    "    from (\n",
    "        select distinct product_id, order_date, cohort_id,\n",
    "            (nmv-cogs_p)/nmv as product_margin, \n",
    "            (brand_nmv-brand_cogs)/brand_nmv as brand_margin,\n",
    "            (cat_nmv-cat_cogs)/cat_nmv as cat_margin\n",
    "        from (\n",
    "            SELECT DISTINCT so.created_at::date as order_date, cpc.cohort_id, pso.product_id,\n",
    "                brands.name_ar as brand, categories.name_ar as cat,\n",
    "                sum(COALESCE(f.wac_p,0) * pso.purchased_item_count * pso.basic_unit_count) as cogs_p,\n",
    "                sum(pso.total_price) as nmv,\n",
    "                sum(nmv) over(partition by order_date, cat, brand) as brand_nmv,\n",
    "                sum(cogs_p) over(partition by order_date, cat, brand) as brand_cogs,\n",
    "                sum(nmv) over(partition by order_date, cat) as cat_nmv,\n",
    "                sum(cogs_p) over(partition by order_date, cat) as cat_cogs\n",
    "            FROM product_sales_order pso\n",
    "            JOIN sales_orders so ON so.id = pso.sales_order_id   \n",
    "            JOIN COHORT_PRICING_CHANGES cpc on cpc.id = pso.cohort_pricing_change_id\n",
    "            JOIN products on products.id = pso.product_id\n",
    "            JOIN brands on products.brand_id = brands.id \n",
    "            JOIN categories ON products.category_id = categories.id\n",
    "            JOIN finance.all_cogs f ON f.product_id = pso.product_id\n",
    "                AND f.from_date::date <= so.created_at::date AND f.to_date::date > so.created_at::date\n",
    "            WHERE so.created_at::date between \n",
    "                date_trunc('month', CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - 120) \n",
    "                and CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date\n",
    "                AND so.sales_order_status_id not in (7,12)\n",
    "                AND so.channel IN ('telesales','retailer')\n",
    "                AND pso.purchased_item_count <> 0\n",
    "            GROUP BY ALL\n",
    "        )\n",
    "    ) group by all \n",
    ")\n",
    "'''\n",
    "\n",
    "# =============================================================================\n",
    "# 7. TARGET MARGINS QUERY\n",
    "# =============================================================================\n",
    "TARGET_MARGINS_QUERY = f'''\n",
    "WITH cat_brand_target as (\n",
    "    SELECT DISTINCT cat, brand, margin as target_bm\n",
    "    FROM performance.commercial_targets cplan\n",
    "    QUALIFY CASE \n",
    "        WHEN DATE_TRUNC('month', MAX(DATE) OVER()) = DATE_TRUNC('month', CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date) \n",
    "        THEN DATE_TRUNC('month', CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date)\n",
    "        ELSE DATE_TRUNC('month', CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - INTERVAL '1 month') \n",
    "    END = DATE_TRUNC('month', date)\n",
    "),\n",
    "cat_target as (\n",
    "    select cat, sum(target_bm * (target_nmv/cat_total)) as cat_target_margin\n",
    "    from (\n",
    "        select *, sum(target_nmv) over(partition by cat) as cat_total\n",
    "        from (\n",
    "            select cat, brand, avg(target_bm) as target_bm, sum(target_nmv) as target_nmv\n",
    "            from (\n",
    "                SELECT DISTINCT date, city as region, cat, brand, margin as target_bm, nmv as target_nmv\n",
    "                FROM performance.commercial_targets cplan\n",
    "                QUALIFY CASE \n",
    "                    WHEN DATE_TRUNC('month', MAX(DATE) OVER()) = DATE_TRUNC('month', CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date) \n",
    "                    THEN DATE_TRUNC('month', CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date)\n",
    "                    ELSE DATE_TRUNC('month', CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - INTERVAL '1 month') \n",
    "                END = DATE_TRUNC('month', date)\n",
    "            ) group by all\n",
    "        )\n",
    "    ) group by all \n",
    ")\n",
    "SELECT DISTINCT cbt.cat, cbt.brand, cbt.target_bm, ct.cat_target_margin\n",
    "FROM cat_brand_target cbt\n",
    "LEFT JOIN cat_target ct ON ct.cat = cbt.cat\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Execute All Queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Execute all queries\n",
    "# =============================================================================\n",
    "print(\"Loading data from Snowflake...\")\n",
    "\n",
    "# NOTE: Ben Soliman, Marketplace, and Scrapped prices are now fetched via\n",
    "# market_data_module.ipynb get_market_data() function - no need to load here\n",
    "\n",
    "# 1. Product Base Data (product_id, sku, brand, cat, wac1, wac_p, current_price)\n",
    "print(\"  1. Loading product base data...\")\n",
    "df_product_base = query_snowflake(PRODUCT_BASE_QUERY)\n",
    "df_product_base = convert_to_numeric(df_product_base)\n",
    "print(f\"     Loaded {len(df_product_base)} product base records\")\n",
    "\n",
    "# 2. Sales Data\n",
    "print(\"  2. Loading sales data...\")\n",
    "df_sales = query_snowflake(SALES_QUERY)\n",
    "df_sales = convert_to_numeric(df_sales)\n",
    "print(f\"     Loaded {len(df_sales)} sales records\")\n",
    "\n",
    "# 3. Margin Stats\n",
    "print(\"  3. Loading margin stats...\")\n",
    "df_margin_stats = query_snowflake(MARGIN_STATS_QUERY)\n",
    "df_margin_stats = convert_to_numeric(df_margin_stats)\n",
    "print(f\"     Loaded {len(df_margin_stats)} margin stat records\")\n",
    "\n",
    "# 4. Target Margins\n",
    "print(\"  4. Loading target margins...\")\n",
    "df_targets = query_snowflake(TARGET_MARGINS_QUERY)\n",
    "df_targets = convert_to_numeric(df_targets)\n",
    "print(f\"     Loaded {len(df_targets)} target margin records\")\n",
    "\n",
    "# 5. Product Groups (from PostgreSQL)\n",
    "print(\"  5. Loading product groups...\")\n",
    "df_groups = query_snowflake(\n",
    "    '''SELECT * FROM materialized_views.sku_commercial_groups'''\n",
    ")\n",
    "df_groups.columns = df_groups.columns.str.lower()\n",
    "df_groups = convert_to_numeric(df_groups)\n",
    "print(f\"     Loaded {len(df_groups)} group records\")\n",
    "\n",
    "# 6. All-Time High Margin (P80 margin weighted by gross profit)\n",
    "print(\"  6. Loading all-time high margin data...\")\n",
    "df_all_time_high_margin = query_snowflake(ALL_TIME_HIGH_MARGIN_QUERY)\n",
    "df_all_time_high_margin = convert_to_numeric(df_all_time_high_margin)\n",
    "print(f\"     Loaded {len(df_all_time_high_margin)} all-time high margin records\")\n",
    "\n",
    "print(\"\\nBase queries completed!\")\n",
    "print(\"NOTE: Market data (Ben Soliman, Marketplace, Scrapped) will be fetched via market_data_module\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"df_product_base DataFrame available with columns:\")\n",
    "print(\"  - region, cohort_id, product_id, sku, brand, cat, wac1, wac_p, current_price\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART A: Get market_data from market_data_module\n",
    "# =============================================================================\n",
    "# Instead of duplicating the market data processing logic here, \n",
    "# we use the centralized market_data_module which handles:\n",
    "# - Ben Soliman prices\n",
    "# - Marketplace prices  \n",
    "# - Scrapped prices\n",
    "# - Group-level price aggregation\n",
    "# - Price coverage filtering\n",
    "# - Price percentile calculation\n",
    "# - Margin tier conversion\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Loading market_data from market_data_module...\")\n",
    "print(\"(This fetches Ben Soliman, Marketplace, and Scrapped prices from Snowflake)\")\n",
    "print()\n",
    "\n",
    "# Run market_data_module to get access to get_market_data() function\n",
    "%run modules/market_data_module.ipynb\n",
    "\n",
    "# Get fresh market data using the module (no input required)\n",
    "market_data = get_market_data()\n",
    "\n",
    "# The market_data now contains:\n",
    "# - cohort_id, product_id, region\n",
    "# - Raw prices: ben_soliman_price, final_min_price, final_max_price, etc.\n",
    "# - Percentiles: minimum, percentile_25, percentile_50, percentile_75, maximum\n",
    "# - Margins: below_market, market_min, market_25, market_50, market_75, market_max, above_market\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"MARKET DATA LOADED FROM MODULE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total records: {len(market_data)}\")\n",
    "print(f\"  - With marketplace prices: {(~market_data['final_min_price'].isna()).sum()}\")\n",
    "print(f\"  - With scrapped prices: {(~market_data['min_scrapped'].isna()).sum()}\")\n",
    "print(f\"  - With Ben Soliman prices: {(~market_data['ben_soliman_price'].isna()).sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART A: Build Main pricing_data DataFrame\n",
    "Start with df_product_base (all our SKUs) and LEFT JOIN the processed market_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART B: Build Main pricing_data DataFrame from df_product_base\n",
    "# =============================================================================\n",
    "print(\"Building main pricing_data DataFrame...\")\n",
    "\n",
    "# =============================================================================\n",
    "# Step 1: Start with df_product_base as the MAIN dataframe (all our SKUs)\n",
    "# =============================================================================\n",
    "print(\"  Step 1: Starting with product base (all SKUs)...\")\n",
    "pricing_data = df_product_base.copy()\n",
    "print(f\"     Product base: {len(pricing_data)} records\")\n",
    "\n",
    "# =============================================================================\n",
    "# Step 2: Add warehouse mapping (warehouse_id and warehouse name)\n",
    "# =============================================================================\n",
    "print(\"  Step 2: Adding warehouse mapping...\")\n",
    "warehouse_df = get_warehouse_df()\n",
    "pricing_data = pricing_data.merge(\n",
    "    warehouse_df[['cohort_id', 'warehouse_id', 'warehouse']], \n",
    "    on='cohort_id'\n",
    ")\n",
    "print(f\"     After warehouse mapping: {len(pricing_data)} records\")\n",
    "\n",
    "# =============================================================================\n",
    "# Step 3: LEFT JOIN processed market_data\n",
    "# =============================================================================\n",
    "print(\"  Step 3: Left joining processed market data...\")\n",
    "pricing_data = pricing_data.merge(\n",
    "    market_data, \n",
    "    on=['cohort_id', 'product_id','region'], \n",
    "    how='left'\n",
    ")\n",
    "print(f\"     After market data join: {len(pricing_data)} records\")\n",
    "\n",
    "# =============================================================================\n",
    "# Step 4: LEFT JOIN supporting data (sales, margins, targets, groups)\n",
    "# =============================================================================\n",
    "print(\"  Step 4: Left joining supporting data...\")\n",
    "\n",
    "# Merge sales data (nmv only)\n",
    "pricing_data = pricing_data.merge(\n",
    "    df_sales[['cohort_id', 'product_id', 'nmv']], \n",
    "    on=['cohort_id', 'product_id'], \n",
    "    how='left'\n",
    ")\n",
    "pricing_data['nmv'] = pricing_data['nmv'].fillna(0)\n",
    "\n",
    "# Merge margin statistics (by cohort_id + product_id)\n",
    "pricing_data = pricing_data.merge(df_margin_stats, on=['cohort_id', 'product_id'], how='left')\n",
    "\n",
    "# Merge target margins (by brand + cat)\n",
    "pricing_data = pricing_data.merge(df_targets, on=['brand', 'cat'], how='left')\n",
    "pricing_data['target_margin'] = pricing_data['target_bm'].fillna(pricing_data['cat_target_margin']).fillna(0)\n",
    "pricing_data = pricing_data.drop(columns=['target_bm', 'cat_target_margin'], errors='ignore')\n",
    "\n",
    "# Fill NaN values with defaults\n",
    "pricing_data['std'] = pricing_data['std'].fillna(0.01)\n",
    "pricing_data['avg_margin'] = pricing_data['avg_margin'].fillna(0)\n",
    "\n",
    "# Merge product groups\n",
    "pricing_data = pricing_data.merge(df_groups, on='product_id', how='left')\n",
    "\n",
    "# =============================================================================\n",
    "# Step 5: Calculate current margin\n",
    "# =============================================================================\n",
    "pricing_data['current_margin'] = (pricing_data['current_price'] - pricing_data['wac_p']) / pricing_data['current_price']\n",
    "\n",
    "# Remove duplicates\n",
    "pricing_data = pricing_data.drop_duplicates(subset=['cohort_id', 'product_id','warehouse_id'])\n",
    "\n",
    "# =============================================================================\n",
    "# Reorder columns\n",
    "# =============================================================================\n",
    "final_columns = [\n",
    "    # Product Base Info\n",
    "    'cohort_id', 'product_id', 'region', 'warehouse_id', 'warehouse', 'sku', 'brand', 'cat',\n",
    "    # Cost & Price\n",
    "    'wac1', 'wac_p', 'current_price', 'current_margin',\n",
    "    # Sales\n",
    "    'nmv',\n",
    "    # Market Prices (raw)\n",
    "    'ben_soliman_price', \n",
    "    'final_min_price', 'final_max_price', 'final_mod_price', 'final_true_min', 'final_true_max',\n",
    "    'min_scrapped', 'scrapped25', 'scrapped50', 'scrapped75', 'max_scrapped',\n",
    "    # Price Percentiles\n",
    "    'minimum', 'percentile_25', 'percentile_50', 'percentile_75', 'maximum',\n",
    "    # Margin Tiers\n",
    "    'below_market', 'market_min', 'market_25', 'market_50', 'market_75', 'market_max', 'above_market',\n",
    "    # Supporting Data\n",
    "    'std', 'avg_margin', 'target_margin', 'group'\n",
    "]\n",
    "pricing_data = pricing_data[[c for c in final_columns if c in pricing_data.columns]]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"PRICING DATA COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total records: {len(pricing_data)}\")\n",
    "print(f\"\\nRecords with market data: {len(pricing_data[~pricing_data['minimum'].isna()])}\")\n",
    "print(f\"Records without market data: {len(pricing_data[pricing_data['minimum'].isna()])}\")\n",
    "print(f\"\\nRecords with sales (nmv > 0): {len(pricing_data[pricing_data['nmv'] > 0])}\")\n",
    "print(f\"Records without sales (nmv = 0): {len(pricing_data[pricing_data['nmv'] == 0])}\")\n",
    "print(f\"\\nSample data:\")\n",
    "pricing_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discount Analysis - Price & Margin After Discount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Discount Query - Get discount percentage by warehouse and product\n",
    "# =============================================================================\n",
    "DISCOUNT_QUERY = f'''\n",
    "SELECT warehouse_id, product_id, total_discount/total_nmv AS discount_perc\n",
    "FROM (\n",
    "    SELECT  \n",
    "        pso.warehouse_id,\n",
    "        pso.product_id,\n",
    "        SUM(pso.total_price) AS total_nmv,\n",
    "        SUM((ITEM_QUANTITY_DISCOUNT_VALUE * pso.purchased_item_count) + \n",
    "            (ITEM_DISCOUNT_VALUE * pso.purchased_item_count)) AS total_discount\n",
    "    FROM product_sales_order pso \n",
    "    JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "    WHERE so.created_at::DATE >= CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE - 1 \n",
    "        AND so.sales_order_status_id NOT IN (7, 12)\n",
    "        AND so.channel IN ('telesales', 'retailer')\n",
    "        AND pso.purchased_item_count <> 0\n",
    "    GROUP BY ALL\n",
    ")\n",
    "WHERE total_nmv > 0\n",
    "'''\n",
    "\n",
    "# Execute discount query\n",
    "print(\"Loading discount data...\")\n",
    "df_discount = query_snowflake(DISCOUNT_QUERY)\n",
    "df_discount = convert_to_numeric(df_discount)\n",
    "print(f\"Loaded {len(df_discount)} discount records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Create pricing_with_discount DataFrame\n",
    "# =============================================================================\n",
    "print(\"Creating pricing_with_discount DataFrame...\")\n",
    "\n",
    "# Copy pricing_data\n",
    "pricing_with_discount = pricing_data.copy()\n",
    "\n",
    "# Merge discount data (by warehouse_id + product_id)\n",
    "pricing_with_discount = pricing_with_discount.merge(\n",
    "    df_discount[['warehouse_id', 'product_id', 'discount_perc']], \n",
    "    on=['warehouse_id', 'product_id'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing discount_perc with 0 (no discount)\n",
    "pricing_with_discount['discount_perc'] = pricing_with_discount['discount_perc'].fillna(0)\n",
    "\n",
    "# Merge all-time high margin data (P80 margin weighted by gross profit)\n",
    "pricing_with_discount = pricing_with_discount.merge(\n",
    "    df_all_time_high_margin[['warehouse_id', 'product_id', 'all_time_high_margin']], \n",
    "    on=['warehouse_id', 'product_id'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing all_time_high_margin with target_margin (fallback)\n",
    "pricing_with_discount['all_time_high_margin'] = pricing_with_discount['all_time_high_margin'].fillna(\n",
    "    pricing_with_discount['target_margin']\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# Calculate price and margin after discount\n",
    "# =============================================================================\n",
    "# Price after discount = current_price * (1 - discount_perc)\n",
    "pricing_with_discount['price_after_discount'] = (\n",
    "    pricing_with_discount['current_price'] * (1 - pricing_with_discount['discount_perc'])\n",
    ")\n",
    "\n",
    "# Margin after discount = (price_after_discount - wac_p) / price_after_discount\n",
    "pricing_with_discount['margin_after_discount'] = (\n",
    "    (pricing_with_discount['price_after_discount'] - pricing_with_discount['wac_p']) / \n",
    "    pricing_with_discount['price_after_discount']\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"PRICING WITH DISCOUNT DATA COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total records: {len(pricing_with_discount)}\")\n",
    "print(f\"Records with discount (discount_perc > 0): {len(pricing_with_discount[pricing_with_discount['discount_perc'] > 0])}\")\n",
    "print(f\"Records without discount: {len(pricing_with_discount[pricing_with_discount['discount_perc'] == 0])}\")\n",
    "print(f\"\\nNew columns added:\")\n",
    "print(\"  - discount_perc: discount percentage from sales\")\n",
    "print(\"  - price_after_discount: current_price * (1 - discount_perc)\")\n",
    "print(\"  - margin_after_discount: (price_after_discount - wac_p) / price_after_discount\")\n",
    "print(f\"\\nSample data with discounts:\")\n",
    "pricing_with_discount[pricing_with_discount['discount_perc'] > 0][\n",
    "    ['product_id', 'warehouse_id', 'current_price', 'current_margin', \n",
    "     'discount_perc', 'price_after_discount', 'margin_after_discount']\n",
    "].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Price Position - Determine where price_after_discount falls in market tiers\n",
    "# =============================================================================\n",
    "\n",
    "def get_price_position(row):\n",
    "    \"\"\"Determine the price position relative to market price tiers.\"\"\"\n",
    "    price = row['price_after_discount']\n",
    "    wac = row['wac_p']\n",
    "    \n",
    "    # Check if we have market data (minimum price exists)\n",
    "    if pd.isna(row['minimum']) or pd.isna(price):\n",
    "        return \"No Market Data\"\n",
    "    \n",
    "    # Get price tiers\n",
    "    min_price = row['minimum']\n",
    "    p25 = row['percentile_25']\n",
    "    p50 = row['percentile_50']\n",
    "    p75 = row['percentile_75']\n",
    "    max_price = row['maximum']\n",
    "    \n",
    "    # Calculate below_market and above_market prices from margins\n",
    "    # margin = (price - wac) / price  =>  price = wac / (1 - margin)\n",
    "    below_market_margin = row['below_market']\n",
    "    above_market_margin = row['above_market']\n",
    "    \n",
    "    below_market_price = wac / (1 - below_market_margin) if below_market_margin < 1 else min_price\n",
    "    above_market_price = wac / (1 - above_market_margin) if above_market_margin < 1 else max_price\n",
    "    \n",
    "    # Determine position based on price tiers\n",
    "    if price < below_market_price:\n",
    "        return \"Below Market\"\n",
    "    elif price < min_price:\n",
    "        return \"Below Min\"\n",
    "    elif price < p25:\n",
    "        return \"At Min\"\n",
    "    elif price < p50:\n",
    "        return \"At 25th\"\n",
    "    elif price < p75:\n",
    "        return \"At 50th\"\n",
    "    elif price < max_price:\n",
    "        return \"At 75th\"\n",
    "    elif price < above_market_price:\n",
    "        return \"At Max\"\n",
    "    else:\n",
    "        return \"Above Market\"\n",
    "\n",
    "# Apply price position function\n",
    "pricing_with_discount['price_position'] = pricing_with_discount.apply(get_price_position, axis=1)\n",
    "\n",
    "# Summary of price positions\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"PRICE POSITION ANALYSIS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(\"\\nPrice Position Distribution:\")\n",
    "print(pricing_with_discount['price_position'].value_counts().to_string())\n",
    "print(f\"\\nPrice Position Percentages:\")\n",
    "print((pricing_with_discount['price_position'].value_counts(normalize=True) * 100).round(2).astype(str) + '%')\n",
    "\n",
    "# Sample data showing price position\n",
    "print(f\"\\nSample data with price position:\")\n",
    "pricing_with_discount[\n",
    "    ['product_id', 'warehouse_id', 'sku', 'current_price', 'discount_perc', \n",
    "     'price_after_discount', 'minimum', 'maximum', 'price_position']\n",
    "].head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Stock Query - Get available stock by warehouse and product\n",
    "# =============================================================================\n",
    "STOCK_QUERY = '''\n",
    "SELECT \n",
    "    pw.warehouse_id,\n",
    "    pw.product_id,\n",
    "    pw.available_stock::INTEGER AS stocks\n",
    "FROM product_warehouse pw\n",
    "WHERE pw.warehouse_id NOT IN (6, 9, 10)\n",
    "    AND pw.is_basic_unit = 1\n",
    "'''\n",
    "\n",
    "# Execute stock query\n",
    "print(\"Loading stock data...\")\n",
    "df_stocks = query_snowflake(STOCK_QUERY)\n",
    "df_stocks = convert_to_numeric(df_stocks)\n",
    "print(f\"Loaded {len(df_stocks)} stock records\")\n",
    "\n",
    "# Merge stock data with pricing_with_discount\n",
    "pricing_with_discount = pricing_with_discount.merge(\n",
    "    df_stocks[['warehouse_id', 'product_id', 'stocks']], \n",
    "    on=['warehouse_id', 'product_id'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing stocks with 0\n",
    "pricing_with_discount['stocks'] = pricing_with_discount['stocks'].fillna(0).astype(int)\n",
    "\n",
    "print(f\"\\nStock data merged!\")\n",
    "print(f\"Records with stock (stocks > 0): {len(pricing_with_discount[pricing_with_discount['stocks'] > 0])}\")\n",
    "print(f\"Records without stock (stocks = 0): {len(pricing_with_discount[pricing_with_discount['stocks'] == 0])}\")\n",
    "print(f\"\\nSample data with stocks:\")\n",
    "pricing_with_discount[\n",
    "    ['product_id', 'warehouse_id', 'sku', 'stocks', 'price_after_discount', 'price_position']\n",
    "].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Zero Demand Query - Identify SKUs with zero/low demand\n",
    "# =============================================================================\n",
    "ZERO_DEMAND_QUERY = f'''\n",
    "WITH last_oss AS (\n",
    "    SELECT product_id, warehouse_id, TIMESTAMP AS last_in_stock_day\n",
    "    FROM (\n",
    "        SELECT *, ROW_NUMBER() OVER(PARTITION BY product_id, warehouse_id ORDER BY TIMESTAMP DESC) AS rnk \n",
    "        FROM materialized_views.STOCK_DAY_CLOSE\n",
    "        WHERE AVAILABLE_STOCK = 0 \n",
    "            AND TIMESTAMP >= CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE - 120\n",
    "        QUALIFY rnk = 1 \n",
    "    )\n",
    "),\n",
    "\n",
    "current_stocks AS (\n",
    "    SELECT product_id, warehouse_id, AVAILABLE_STOCK, activation\n",
    "    FROM PRODUCT_WAREHOUSE\n",
    "    WHERE IS_BASIC_UNIT = 1\n",
    "        AND CASE WHEN product_id = 1309 THEN packing_unit_id <> 23 ELSE TRUE END\n",
    "),\n",
    "\n",
    "prs AS (\n",
    "    SELECT DISTINCT \n",
    "        product_purchased_receipts.product_id,\n",
    "        purchased_receipts.warehouse_id,\n",
    "        purchased_receipts.date::DATE AS date,\n",
    "        product_purchased_receipts.purchased_item_count * product_purchased_receipts.basic_unit_count AS purchase_min_count\n",
    "    FROM product_purchased_receipts\n",
    "    JOIN purchased_receipts ON purchased_receipts.id = product_purchased_receipts.purchased_receipt_id\n",
    "    JOIN last_oss lo ON product_purchased_receipts.product_id = lo.product_id \n",
    "        AND lo.warehouse_id = purchased_receipts.warehouse_id \n",
    "        AND purchased_receipts.date > lo.last_in_stock_day \n",
    "    WHERE product_purchased_receipts.purchased_item_count <> 0\n",
    "        AND purchased_receipts.purchased_receipt_status_id IN (4, 5, 7)\n",
    "        AND purchased_receipts.date::DATE >= CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE - 120\n",
    "),\n",
    "\n",
    "main AS (\n",
    "    SELECT \n",
    "        prs.product_id, \n",
    "        prs.warehouse_id, \n",
    "        MIN(date) AS first_order_date, \n",
    "        SUM(purchase_min_count) AS total_recieved, \n",
    "        cs.AVAILABLE_STOCK, \n",
    "        cs.activation\n",
    "    FROM prs \n",
    "    JOIN current_stocks cs ON cs.product_id = prs.product_id AND prs.warehouse_id = cs.warehouse_id\n",
    "    GROUP BY prs.product_id, prs.warehouse_id, cs.AVAILABLE_STOCK, cs.activation\n",
    "),\n",
    "\n",
    "sold_days AS (\n",
    "    SELECT product_id, warehouse_id, COUNT(DISTINCT o_date) AS sales_days\n",
    "    FROM (\n",
    "        SELECT DISTINCT\n",
    "            so.created_at::DATE AS o_date,\n",
    "            pso.warehouse_id,\n",
    "            pso.product_id,\n",
    "            SUM(pso.purchased_item_count * basic_unit_count) AS daily_qty\n",
    "        FROM product_sales_order pso\n",
    "        JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "        JOIN main m ON m.product_id = pso.product_id \n",
    "            AND m.warehouse_id = pso.warehouse_id \n",
    "            AND so.created_at::DATE >= m.first_order_date\n",
    "        WHERE so.created_at::DATE BETWEEN CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE - 120 \n",
    "            AND CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE\n",
    "            AND so.sales_order_status_id NOT IN (7, 12)\n",
    "            AND so.channel IN ('telesales', 'retailer')\n",
    "            AND pso.purchased_item_count <> 0\n",
    "        GROUP BY o_date, pso.warehouse_id, pso.product_id\n",
    "    )\n",
    "    GROUP BY product_id, warehouse_id\n",
    ")\n",
    "\n",
    "SELECT DISTINCT warehouse_id, product_id\n",
    "FROM (\n",
    "    SELECT m.product_id, m.warehouse_id, m.first_order_date, m.activation,\n",
    "        COALESCE(sd.sales_days, 0) AS sales_days,\n",
    "        COALESCE(sd.sales_days, 0)::FLOAT / NULLIF((CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE - 1) - m.first_order_date, 0) AS perc_days\n",
    "    FROM main m \n",
    "    LEFT JOIN sold_days sd ON sd.product_id = m.product_id AND sd.warehouse_id = m.warehouse_id\n",
    "    WHERE m.first_order_date < CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE - 10\n",
    ")\n",
    "WHERE perc_days <= 0.3\n",
    "    AND activation = 'true'\n",
    "'''\n",
    "\n",
    "# Execute zero demand query\n",
    "print(\"Loading zero demand SKUs...\")\n",
    "df_zero_demand = query_snowflake(ZERO_DEMAND_QUERY)\n",
    "df_zero_demand = convert_to_numeric(df_zero_demand)\n",
    "print(f\"Loaded {len(df_zero_demand)} zero demand SKU records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Add Zero Demand Flag to pricing_with_discount\n",
    "# =============================================================================\n",
    "\n",
    "# Add a marker column to identify zero demand SKUs\n",
    "df_zero_demand['zero_demand'] = 1\n",
    "\n",
    "# Merge with pricing_with_discount\n",
    "pricing_with_discount = pricing_with_discount.merge(\n",
    "    df_zero_demand[['warehouse_id', 'product_id', 'zero_demand']], \n",
    "    on=['warehouse_id', 'product_id'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing values with 0 (not zero demand)\n",
    "pricing_with_discount['zero_demand'] = pricing_with_discount['zero_demand'].fillna(0).astype(int)\n",
    "\n",
    "print(f\"Zero demand flag added!\")\n",
    "print(f\"SKUs flagged as zero demand: {len(pricing_with_discount[pricing_with_discount['zero_demand'] == 1])}\")\n",
    "print(f\"SKUs with normal demand: {len(pricing_with_discount[pricing_with_discount['zero_demand'] == 0])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# OOS Yesterday Query - Identify SKUs out of stock yesterday\n",
    "# =============================================================================\n",
    "OOS_YESTERDAY_QUERY = f'''\n",
    "SELECT DISTINCT product_id, warehouse_id,\n",
    "    CASE WHEN opening_stocks = 0 AND closing_stocks = 0 THEN 1\n",
    "         ELSE 0 \n",
    "    END AS oos_yesterday\n",
    "FROM (\n",
    "    SELECT \n",
    "        timestamp,\n",
    "        product_id,\n",
    "        warehouse_id, \n",
    "        AVAILABLE_STOCK AS closing_stocks,\n",
    "        LAG(AVAILABLE_STOCK) OVER (PARTITION BY product_id, warehouse_id ORDER BY TIMESTAMP) AS opening_stocks\n",
    "    FROM materialized_views.stock_day_close\n",
    "    WHERE timestamp::DATE >= CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE - 2\n",
    "    QUALIFY opening_stocks IS NOT NULL\n",
    ")\n",
    "WHERE oos_yesterday = 1\n",
    "'''\n",
    "\n",
    "# Execute OOS yesterday query\n",
    "print(\"Loading OOS yesterday data...\")\n",
    "df_oos_yesterday = query_snowflake(OOS_YESTERDAY_QUERY)\n",
    "df_oos_yesterday = convert_to_numeric(df_oos_yesterday)\n",
    "print(f\"Loaded {len(df_oos_yesterday)} OOS yesterday records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Add OOS Yesterday Flag to pricing_with_discount\n",
    "# =============================================================================\n",
    "\n",
    "# Merge with pricing_with_discount\n",
    "pricing_with_discount = pricing_with_discount.merge(\n",
    "    df_oos_yesterday[['warehouse_id', 'product_id', 'oos_yesterday']], \n",
    "    on=['warehouse_id', 'product_id'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing values with 0 (not OOS yesterday)\n",
    "pricing_with_discount['oos_yesterday'] = pricing_with_discount['oos_yesterday'].fillna(0).astype(int)\n",
    "\n",
    "print(f\"OOS yesterday flag added!\")\n",
    "print(f\"SKUs out of stock yesterday: {len(pricing_with_discount[pricing_with_discount['oos_yesterday'] == 1])}\")\n",
    "print(f\"SKUs in stock yesterday: {len(pricing_with_discount[pricing_with_discount['oos_yesterday'] == 0])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Running Rate Query - Get in-stock running rate by warehouse and product\n",
    "# =============================================================================\n",
    "RUNNING_RATE_QUERY = f'''\n",
    "WITH params AS (\n",
    "    SELECT\n",
    "        CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE AS run_date,\n",
    "        DATEADD(month, -3, CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE) AS history_start\n",
    "),\n",
    "\n",
    "-- Daily sales aggregation\n",
    "sales_base AS (\n",
    "    SELECT\n",
    "        pso.product_id,\n",
    "        pso.warehouse_id,\n",
    "        DATE_TRUNC('day', pso.created_at)::DATE AS date,\n",
    "        SUM(pso.purchased_item_count * pso.basic_unit_count) AS sold_units,\n",
    "        SUM(pso.purchased_item_count * pso.basic_unit_count * pso.item_price)\n",
    "            / NULLIF(SUM(pso.purchased_item_count * pso.basic_unit_count), 0) AS avg_selling_price,\n",
    "        COUNT(DISTINCT so.retailer_id) AS retailer_count\n",
    "    FROM product_sales_order pso\n",
    "    JOIN sales_orders so ON pso.sales_order_id = so.id\n",
    "    WHERE DATE_TRUNC('day', pso.created_at)::DATE >= (SELECT history_start FROM params)\n",
    "    GROUP BY 1, 2, 3\n",
    "),\n",
    "\n",
    "-- Stock daily metrics\n",
    "stock_daily AS (\n",
    "    SELECT\n",
    "        product_id,\n",
    "        warehouse_id,\n",
    "        DATE_TRUNC('day', TIMESTAMP)::DATE AS date,\n",
    "        MAX_BY(available_stock, TIMESTAMP) AS stock_closing,\n",
    "        24 * SUM(CASE WHEN activation = FALSE OR available_stock = 0 THEN 1 ELSE 0 END)::FLOAT \n",
    "            / NULLIF(COUNT(*), 0) AS oos_hours,\n",
    "        MAX(CASE WHEN activation = TRUE AND available_stock > 0 THEN 1 ELSE 0 END) AS in_stock_flag\n",
    "    FROM materialized_views.STOCK_SNAP_SHOTS_RECENT\n",
    "    WHERE product_id IS NOT NULL\n",
    "    GROUP BY product_id, warehouse_id, date\n",
    "),\n",
    "\n",
    "-- Join sales + stock + WAC (only in-stock days)\n",
    "base_data AS (\n",
    "    SELECT\n",
    "        sb.product_id,\n",
    "        sb.warehouse_id,\n",
    "        sb.date,\n",
    "        sb.sold_units,\n",
    "        sb.avg_selling_price,\n",
    "        sb.retailer_count,\n",
    "        sd.oos_hours,\n",
    "        sd.in_stock_flag,\n",
    "        ac.wac_p AS wac,\n",
    "        CASE WHEN DAYOFWEEKISO(sb.date) IN (5, 6) THEN 1 ELSE 0 END AS is_weekend\n",
    "    FROM sales_base sb\n",
    "    LEFT JOIN stock_daily sd ON sb.product_id = sd.product_id \n",
    "        AND sb.warehouse_id = sd.warehouse_id AND sb.date = sd.date\n",
    "    LEFT JOIN finance.ALL_COGS ac ON sb.product_id = ac.product_id \n",
    "        AND sb.date BETWEEN ac.from_date AND ac.to_date\n",
    "    WHERE sd.in_stock_flag = 1\n",
    "),\n",
    "\n",
    "-- Stats per SKU x Warehouse\n",
    "sku_wh_stats AS (\n",
    "    SELECT\n",
    "        product_id, warehouse_id,\n",
    "        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY sold_units) AS med_units,\n",
    "        PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY sold_units) AS pct95_units,\n",
    "        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY retailer_count) AS med_retailers,\n",
    "        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY \n",
    "            CASE WHEN avg_selling_price IS NULL OR avg_selling_price = 0 THEN 0 \n",
    "            ELSE (avg_selling_price - COALESCE(wac, 0)) / NULLIF(avg_selling_price, 0) END\n",
    "        ) AS med_margin\n",
    "    FROM base_data\n",
    "    GROUP BY product_id, warehouse_id\n",
    "),\n",
    "\n",
    "-- Cap outliers and adjust for retailer spikes\n",
    "adjusted AS (\n",
    "    SELECT\n",
    "        b.product_id, b.warehouse_id, b.date, b.in_stock_flag, b.oos_hours, b.is_weekend,\n",
    "        b.avg_selling_price, b.wac, s.med_margin,\n",
    "        CASE \n",
    "            WHEN b.retailer_count > GREATEST(2, s.med_retailers * 2) \n",
    "                AND b.retailer_count > 0 AND s.med_retailers IS NOT NULL\n",
    "            THEN ROUND(LEAST(b.sold_units, s.pct95_units) * (s.med_retailers::FLOAT / NULLIF(b.retailer_count::FLOAT, 0)), 0)\n",
    "            ELSE LEAST(b.sold_units, s.pct95_units)\n",
    "        END AS units_adjusted\n",
    "    FROM base_data b\n",
    "    LEFT JOIN sku_wh_stats s ON b.product_id = s.product_id AND b.warehouse_id = s.warehouse_id\n",
    "),\n",
    "\n",
    "-- Apply weights (recency, stock availability, weekend, margin)\n",
    "weighted AS (\n",
    "    SELECT\n",
    "        product_id, warehouse_id, date, units_adjusted,\n",
    "        (\n",
    "            -- Recency weight\n",
    "            CASE WHEN date >= DATEADD(day, -21, (SELECT run_date FROM params)) THEN 1.5\n",
    "                 WHEN date >= DATEADD(day, -90, (SELECT run_date FROM params)) THEN 1.0\n",
    "                 ELSE 0.5 END\n",
    "            -- In-stock weight\n",
    "            * CASE WHEN in_stock_flag = 1 AND COALESCE(oos_hours, 0) < 12 THEN 1.4\n",
    "                   WHEN in_stock_flag = 1 AND COALESCE(oos_hours, 0) >= 12 THEN 0.9\n",
    "                   ELSE 0.6 END\n",
    "            -- Weekend weight\n",
    "            * CASE WHEN is_weekend = 1 THEN 0.7 ELSE 1.0 END\n",
    "            -- Margin weight\n",
    "            * CASE WHEN avg_selling_price IS NULL OR avg_selling_price = 0 OR med_margin IS NULL THEN 1.0\n",
    "                   WHEN ((avg_selling_price - COALESCE(wac, 0)) / NULLIF(avg_selling_price, 0)) < med_margin\n",
    "                   THEN 1.0 + LEAST((med_margin - ((avg_selling_price - COALESCE(wac, 0)) / NULLIF(avg_selling_price, 0))) * 2.0, 0.6)\n",
    "                   WHEN ((avg_selling_price - COALESCE(wac, 0)) / NULLIF(avg_selling_price, 0)) > med_margin\n",
    "                   THEN 1.0 - LEAST((((avg_selling_price - COALESCE(wac, 0)) / NULLIF(avg_selling_price, 0)) - med_margin) * 2.0, 0.4)\n",
    "                   ELSE 1.0 END\n",
    "        ) AS final_weight\n",
    "    FROM adjusted\n",
    "    WHERE units_adjusted IS NOT NULL\n",
    "),\n",
    "\n",
    "-- Weighted average forecast\n",
    "forecast_base AS (\n",
    "    SELECT\n",
    "        product_id, warehouse_id,\n",
    "        SUM(units_adjusted * final_weight) / NULLIF(SUM(final_weight), 0) AS weighted_avg_units\n",
    "    FROM weighted\n",
    "    GROUP BY product_id, warehouse_id\n",
    "),\n",
    "\n",
    "-- Zero-sales last 4 days (with stock) exclusion flag\n",
    "last4_flag AS (\n",
    "    SELECT product_id, warehouse_id,\n",
    "        CASE WHEN COUNT(*) = 4 \n",
    "             AND SUM(CASE WHEN COALESCE(sold_units, 0) = 0 AND in_stock_flag = 1 THEN 1 ELSE 0 END) = 4\n",
    "        THEN 1 ELSE 0 END AS exclude_flag\n",
    "    FROM base_data\n",
    "    WHERE date >= DATEADD(day, -4, (SELECT run_date FROM params)) \n",
    "        AND date < (SELECT run_date FROM params)\n",
    "    GROUP BY product_id, warehouse_id\n",
    "),\n",
    "\n",
    "-- Zero sales excluded (in stock but no sales)\n",
    "zero_sales_excluded AS (\n",
    "    SELECT DISTINCT s.warehouse_id, s.product_id\n",
    "    FROM (\n",
    "        SELECT pw.warehouse_id, pw.product_id, SUM(pw.available_stock)::INT AS stocks\n",
    "        FROM product_warehouse pw\n",
    "        WHERE pw.warehouse_id NOT IN (6, 9, 10) AND pw.is_basic_unit = 1 AND pw.available_stock > 0\n",
    "        GROUP BY pw.warehouse_id, pw.product_id\n",
    "    ) s\n",
    "    LEFT JOIN (\n",
    "        SELECT pso.product_id, pso.warehouse_id, SUM(pso.total_price) AS nmv\n",
    "        FROM product_sales_order pso\n",
    "        JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "        WHERE so.created_at::date BETWEEN CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE - 5 \n",
    "            AND CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE - 1\n",
    "            AND so.sales_order_status_id NOT IN (7, 12) AND so.channel IN ('telesales', 'retailer')\n",
    "            AND pso.purchased_item_count <> 0\n",
    "        GROUP BY pso.product_id, pso.warehouse_id\n",
    "    ) md ON md.product_id = s.product_id AND md.warehouse_id = s.warehouse_id\n",
    "    LEFT JOIN finance.all_cogs f ON f.product_id = s.product_id\n",
    "        AND f.from_date::date <= CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE\n",
    "        AND f.to_date::date > CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE\n",
    "    LEFT JOIN (\n",
    "        SELECT pr.warehouse_id, ppr.product_id, SUM(ppr.final_price) AS total_prs\n",
    "        FROM product_purchased_receipts ppr\n",
    "        JOIN purchased_receipts pr ON pr.id = ppr.purchased_receipt_id\n",
    "        WHERE pr.date::date >= CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE - 4\n",
    "            AND pr.is_actual = 'true' AND pr.purchased_receipt_status_id IN (4, 5, 7)\n",
    "            AND ppr.purchased_item_count <> 0\n",
    "        GROUP BY pr.warehouse_id, ppr.product_id\n",
    "    ) prs ON prs.product_id = s.product_id AND prs.warehouse_id = s.warehouse_id\n",
    "    WHERE COALESCE(md.nmv, 0) = 0 \n",
    "        AND COALESCE(prs.total_prs, 0) < 0.7 * (COALESCE(f.wac_p, 0) * s.stocks)\n",
    "),\n",
    "\n",
    "-- First sale date for new products\n",
    "first_sale AS (\n",
    "    SELECT product_id, warehouse_id, MIN(date) AS first_sale_date\n",
    "    FROM base_data WHERE sold_units > 0\n",
    "    GROUP BY product_id, warehouse_id\n",
    ")\n",
    "\n",
    "-- Final output: running rate per warehouse/product\n",
    "SELECT\n",
    "    fb.warehouse_id,\n",
    "    fb.product_id,\n",
    "    CASE\n",
    "        WHEN l4.exclude_flag = 1 THEN 0\n",
    "        WHEN fs.first_sale_date >= DATEADD(day, -2, (SELECT run_date FROM params))\n",
    "        THEN GREATEST(CEIL(fb.weighted_avg_units), 1)\n",
    "        ELSE CEIL(fb.weighted_avg_units)\n",
    "    END AS In_stock_rr\n",
    "FROM forecast_base fb\n",
    "LEFT JOIN last4_flag l4 ON fb.product_id = l4.product_id AND fb.warehouse_id = l4.warehouse_id\n",
    "LEFT JOIN first_sale fs ON fb.product_id = fs.product_id AND fb.warehouse_id = fs.warehouse_id\n",
    "LEFT JOIN zero_sales_excluded zse ON fb.product_id = zse.product_id AND fb.warehouse_id = zse.warehouse_id\n",
    "WHERE zse.product_id IS NULL\n",
    "'''\n",
    "\n",
    "# Execute running rate query\n",
    "print(\"Loading running rate data (this may take a moment)...\")\n",
    "df_running_rate = query_snowflake(RUNNING_RATE_QUERY)\n",
    "df_running_rate = convert_to_numeric(df_running_rate)\n",
    "print(f\"Loaded {len(df_running_rate)} running rate records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Merge Running Rate and Calculate DOH (Days on Hand)\n",
    "# =============================================================================\n",
    "\n",
    "# Merge running rate data with pricing_with_discount\n",
    "pricing_with_discount = pricing_with_discount.merge(\n",
    "    df_running_rate[['warehouse_id', 'product_id', 'in_stock_rr']], \n",
    "    on=['warehouse_id', 'product_id'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing running rate with 0\n",
    "pricing_with_discount['in_stock_rr'] = pricing_with_discount['in_stock_rr'].fillna(0)\n",
    "\n",
    "# Calculate DOH (Days on Hand) = stocks / in_stock_rr\n",
    "# Handle division by zero - if running rate is 0, DOH is infinite (use 999)\n",
    "pricing_with_discount['doh'] = np.select(\n",
    "    [\n",
    "        (pricing_with_discount['in_stock_rr'] > 0) & (pricing_with_discount['stocks'] > 0),\n",
    "        pricing_with_discount['stocks'] == 0\n",
    "    ],\n",
    "    [\n",
    "        pricing_with_discount['stocks'] / pricing_with_discount['in_stock_rr'],\n",
    "        0\n",
    "    ],\n",
    "    default=999\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Product Classification Query - ABC Classification based on order contribution\n",
    "# =============================================================================\n",
    "PRODUCT_CLASSIFICATION_QUERY = f'''\n",
    "WITH order_counts AS (\n",
    "    SELECT \n",
    "        pso.warehouse_id,\n",
    "        pso.product_id,\n",
    "        COUNT(DISTINCT pso.sales_order_id) AS order_count\n",
    "    FROM product_sales_order pso\n",
    "    JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "    WHERE so.created_at::DATE >= CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE - 90\n",
    "        AND so.sales_order_status_id NOT IN (7, 12)\n",
    "        AND so.channel IN ('telesales', 'retailer')\n",
    "        AND pso.purchased_item_count <> 0\n",
    "    GROUP BY pso.warehouse_id, pso.product_id\n",
    "),\n",
    "\n",
    "warehouse_totals AS (\n",
    "    SELECT \n",
    "        warehouse_id,\n",
    "        SUM(order_count) AS total_orders\n",
    "    FROM order_counts\n",
    "    GROUP BY warehouse_id\n",
    "),\n",
    "\n",
    "ranked_products AS (\n",
    "    SELECT \n",
    "        oc.warehouse_id,\n",
    "        oc.product_id,\n",
    "        oc.order_count,\n",
    "        wt.total_orders,\n",
    "        oc.order_count::FLOAT / NULLIF(wt.total_orders, 0) AS contribution,\n",
    "        SUM(oc.order_count::FLOAT / NULLIF(wt.total_orders, 0)) \n",
    "            OVER (PARTITION BY oc.warehouse_id ORDER BY oc.order_count DESC \n",
    "                  ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_contribution\n",
    "    FROM order_counts oc\n",
    "    JOIN warehouse_totals wt ON oc.warehouse_id = wt.warehouse_id\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    warehouse_id,\n",
    "    product_id,\n",
    "    order_count,\n",
    "    contribution,\n",
    "    cumulative_contribution,\n",
    "    CASE \n",
    "        WHEN cumulative_contribution <= 0.3 THEN 'A'\n",
    "        WHEN cumulative_contribution <= 0.75 THEN 'B'\n",
    "        ELSE 'C'\n",
    "    END AS abc_class\n",
    "FROM ranked_products\n",
    "'''\n",
    "\n",
    "# Execute product classification query\n",
    "print(\"Loading product classification data...\")\n",
    "df_classification = query_snowflake(PRODUCT_CLASSIFICATION_QUERY)\n",
    "df_classification = convert_to_numeric(df_classification)\n",
    "print(f\"Loaded {len(df_classification)} product classification records\")\n",
    "print(f\"\\nClassification distribution:\")\n",
    "print(df_classification['abc_class'].value_counts().to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Add ABC Classification to pricing_with_discount\n",
    "# =============================================================================\n",
    "\n",
    "# Merge classification data with pricing_with_discount\n",
    "pricing_with_discount = pricing_with_discount.merge(\n",
    "    df_classification[['warehouse_id', 'product_id', 'order_count', 'contribution', 'abc_class']], \n",
    "    on=['warehouse_id', 'product_id'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing values - products without orders in last 3 months get class 'C'\n",
    "pricing_with_discount['order_count'] = pricing_with_discount['order_count'].fillna(0).astype(int)\n",
    "pricing_with_discount['contribution'] = pricing_with_discount['contribution'].fillna(0)\n",
    "pricing_with_discount['abc_class'] = pricing_with_discount['abc_class'].fillna('C')\n",
    "\n",
    "print(f\"ABC Classification added!\")\n",
    "print(f\"\\nClassification in pricing_with_discount:\")\n",
    "print(pricing_with_discount['abc_class'].value_counts().to_string())\n",
    "print(f\"\\nSample data with classification:\")\n",
    "pricing_with_discount[\n",
    "    ['product_id', 'warehouse_id', 'sku', 'order_count', 'contribution', 'abc_class', 'stocks', 'doh']\n",
    "].head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PO (Purchase Order) Data Query - Last PO status and rejection count\n",
    "# =============================================================================\n",
    "PO_DATA_QUERY = '''\n",
    "WITH last_data AS (\n",
    "    SELECT product_id, warehouse_id, confirmation_status, PO_date::DATE AS last_po_date, ordered_qty\n",
    "    FROM (\n",
    "        SELECT \n",
    "            product_id,\n",
    "            Target_WAREHOUSE_ID AS warehouse_id,\n",
    "            confirmation_status,\n",
    "            created_at AS PO_date,\n",
    "            MIN_QUANTITY AS ordered_qty,\n",
    "            reason,\n",
    "            MAX(created_at) OVER (PARTITION BY product_id, Target_WAREHOUSE_ID) AS last_po\n",
    "        FROM retool.PO_INITIAL_PLAN\n",
    "        WHERE created_at::DATE >= CURRENT_DATE - 15 \n",
    "    ) x\n",
    "    WHERE last_po = PO_date\n",
    "),\n",
    "\n",
    "last_15_data AS (\n",
    "    SELECT \n",
    "        product_id,\n",
    "        target_WAREHOUSE_ID AS warehouse_id,\n",
    "        COUNT(DISTINCT CASE WHEN confirmation_status <> 'yes' THEN created_at END) AS no_last_15\n",
    "    FROM retool.PO_INITIAL_PLAN\n",
    "    WHERE created_at::DATE >= CURRENT_DATE - 15 \n",
    "    GROUP BY 1, 2\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    ld.product_id,\n",
    "    ld.warehouse_id,\n",
    "    ld.confirmation_status,\n",
    "    ld.last_po_date,\n",
    "    ld.ordered_qty,\n",
    "    COALESCE(lfd.no_last_15, 0) AS no_last_15\n",
    "FROM last_data ld \n",
    "LEFT JOIN last_15_data lfd \n",
    "    ON lfd.product_id = ld.product_id \n",
    "    AND lfd.warehouse_id = ld.warehouse_id\n",
    "'''\n",
    "\n",
    "# Execute PO data query using dwh_pg_query\n",
    "print(\"Loading PO data...\")\n",
    "df_po_data = setup_environment_2.dwh_pg_query(\n",
    "    PO_DATA_QUERY, \n",
    "    columns=['product_id', 'warehouse_id', 'confirmation_status', 'last_po_date', 'ordered_qty', 'no_last_15']\n",
    ")\n",
    "df_po_data.columns = df_po_data.columns.str.lower()\n",
    "df_po_data = convert_to_numeric(df_po_data)\n",
    "print(f\"Loaded {len(df_po_data)} PO records\")\n",
    "print(f\"\\nConfirmation status distribution:\")\n",
    "print(df_po_data['confirmation_status'].value_counts().to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Add PO Data to pricing_with_discount\n",
    "# =============================================================================\n",
    "\n",
    "# Merge PO data with pricing_with_discount\n",
    "pricing_with_discount = pricing_with_discount.merge(\n",
    "    df_po_data[['warehouse_id', 'product_id', 'confirmation_status', 'last_po_date', 'ordered_qty', 'no_last_15']], \n",
    "    on=['warehouse_id', 'product_id'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing values\n",
    "pricing_with_discount['ordered_qty'] = pricing_with_discount['ordered_qty'].fillna(0)\n",
    "pricing_with_discount['no_last_15'] = pricing_with_discount['no_last_15'].fillna(0).astype(int)\n",
    "\n",
    "print(f\"PO data added!\")\n",
    "print(f\"\\nRecords with PO data: {len(pricing_with_discount[~pricing_with_discount['confirmation_status'].isna()])}\")\n",
    "print(f\"Records without PO data: {len(pricing_with_discount[pricing_with_discount['confirmation_status'].isna()])}\")\n",
    "print(f\"\\nSample data with PO info:\")\n",
    "pricing_with_discount[\n",
    "    ['product_id', 'warehouse_id', 'sku', 'confirmation_status', 'last_po_date', 'ordered_qty', 'no_last_15']\n",
    "].dropna(subset=['confirmation_status']).head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Leadtime Query - Supplier leadtime by brand, category, and warehouse\n",
    "# =============================================================================\n",
    "LEADTIME_QUERY = '''\n",
    "SELECT brand, cat, warehouse_id, leadtime\n",
    "FROM (\n",
    "    SELECT a.*, b.name_ar AS brand, c.name_ar AS cat\n",
    "    FROM (\n",
    "        SELECT DISTINCT \n",
    "            sl.supplier_id, \n",
    "            warehouse_id, \n",
    "            category_id, \n",
    "            brand_id, \n",
    "            sl.updated_at, \n",
    "            leadtime,\n",
    "            MAX(sl.updated_at) OVER (PARTITION BY sl.supplier_id, warehouse_id) AS last_update\n",
    "        FROM retool.SUPPLIER_MOQ sl \n",
    "        JOIN retool.PO_SUPPLIER_MAPPING sm ON sl.supplier_id = sm.supplier_id \n",
    "    ) a\n",
    "    JOIN brands b ON b.id = a.brand_id \n",
    "    JOIN categories c ON c.id = a.category_id\n",
    "    WHERE a.updated_at = last_update\n",
    ") d\n",
    "'''\n",
    "\n",
    "# Execute leadtime query using dwh_pg_query\n",
    "print(\"Loading leadtime data...\")\n",
    "df_leadtime = setup_environment_2.dwh_pg_query(\n",
    "    LEADTIME_QUERY, \n",
    "    columns=['brand', 'cat', 'warehouse_id', 'leadtime']\n",
    ")\n",
    "df_leadtime.columns = df_leadtime.columns.str.lower()\n",
    "df_leadtime = convert_to_numeric(df_leadtime)\n",
    "print(f\"Loaded {len(df_leadtime)} leadtime records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Add Leadtime to pricing_with_discount\n",
    "# =============================================================================\n",
    "\n",
    "# Merge leadtime data with pricing_with_discount (by brand, cat, warehouse_id)\n",
    "pricing_with_discount = pricing_with_discount.merge(\n",
    "    df_leadtime[['brand', 'cat', 'warehouse_id', 'leadtime']], \n",
    "    on=['brand', 'cat', 'warehouse_id'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing leadtime with 0 or a default value\n",
    "pricing_with_discount['leadtime'] = pricing_with_discount['leadtime'].fillna(72)\n",
    "\n",
    "\n",
    "print(f\"Leadtime data added!\")\n",
    "print(f\"\\nRecords with leadtime: {len(pricing_with_discount[pricing_with_discount['leadtime'] > 0])}\")\n",
    "print(f\"Records without leadtime: {len(pricing_with_discount[pricing_with_discount['leadtime'] == 0])}\")\n",
    "print(f\"\\nLeadtime distribution:\")\n",
    "print(pricing_with_discount['leadtime'].describe())\n",
    "\n",
    "# =============================================================================\n",
    "# Calculate Expected Receiving Day\n",
    "# If confirmation_status is 'no': add 2 extra days (48 hours) before adding leadtime\n",
    "# expected_receiving_day = last_po_date + ((2 + leadtime) / 24) if not confirmed\n",
    "# expected_receiving_day = last_po_date + (leadtime / 24) if confirmed\n",
    "# =============================================================================\n",
    "\n",
    "# Convert last_po_date to datetime if not already\n",
    "pricing_with_discount['last_po_date'] = pd.to_datetime(pricing_with_discount['last_po_date'], errors='coerce')\n",
    "\n",
    "# Calculate adjusted leadtime: add 48 hours (2 days) if confirmation_status is 'no'\n",
    "pricing_with_discount['adjusted_leadtime'] = np.where(\n",
    "    pricing_with_discount['confirmation_status'].str.lower() == 'no',\n",
    "    pricing_with_discount['leadtime'] + 48,  # Add 2 days (48 hours) if not confirmed\n",
    "    pricing_with_discount['leadtime']\n",
    ")\n",
    "\n",
    "# Calculate expected receiving day (leadtime is in hours, divide by 24 for days)\n",
    "pricing_with_discount['expected_receiving_day'] = pricing_with_discount['last_po_date'] + pd.to_timedelta(\n",
    "    pricing_with_discount['adjusted_leadtime'] / 24, unit='D'\n",
    ")\n",
    "\n",
    "# Set expected_receiving_day to empty if it's in the past (smaller than today)\n",
    "pricing_with_discount['expected_receiving_day'] = np.where(\n",
    "    pricing_with_discount['expected_receiving_day'] < pd.Timestamp(TODAY),\n",
    "    pd.NaT,\n",
    "    pricing_with_discount['expected_receiving_day']\n",
    ")\n",
    "# Convert back to datetime (np.where returns object type)\n",
    "pricing_with_discount['expected_receiving_day'] = pd.to_datetime(pricing_with_discount['expected_receiving_day'])\n",
    "\n",
    "print(f\"\\nExpected receiving day calculated!\")\n",
    "print(f\"Records with expected receiving day (future dates only): {len(pricing_with_discount[~pricing_with_discount['expected_receiving_day'].isna()])}\")\n",
    "print(f\"Records with past expected dates (set to empty): {len(pricing_with_discount[pricing_with_discount['expected_receiving_day'].isna() & pricing_with_discount['last_po_date'].notna()])}\")\n",
    "print(f\"Records with confirmation_status='no' (added 2 extra days): {len(pricing_with_discount[pricing_with_discount['confirmation_status'].str.lower() == 'no'])}\")\n",
    "print(f\"\\nSample data with expected receiving day:\")\n",
    "pricing_with_discount[~pricing_with_discount['last_po_date'].isna()][\n",
    "    ['product_id', 'warehouse_id', 'sku', 'confirmation_status', 'last_po_date', 'leadtime', 'adjusted_leadtime', 'expected_receiving_day', 'doh']\n",
    "].head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SKIP: Margin Boundaries Query - Now fetched via market_data_module.get_margin_tiers()\n",
    "# =============================================================================\n",
    "# The margin boundaries and tier calculation is now centralized in market_data_module.\n",
    "# We'll use get_margin_tiers() to get pre-calculated margin tiers.\n",
    "\n",
    "print(\"Loading margin tiers from market_data_module...\")\n",
    "df_margin_tiers = get_margin_tiers()\n",
    "print(f\"Loaded {len(df_margin_tiers)} margin tier records from module\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Add Margin Tiers from market_data_module (Pre-calculated)\n",
    "# =============================================================================\n",
    "# The margin tiers are now calculated in market_data_module.get_margin_tiers()\n",
    "# We just need to merge them with pricing_with_discount\n",
    "\n",
    "# Merge pre-calculated margin tiers\n",
    "pricing_with_discount = pricing_with_discount.merge(\n",
    "    df_margin_tiers[[\n",
    "        'product_id', 'region', 'cohort_id',\n",
    "        'optimal_bm', 'min_boundary', 'max_boundary', 'median_bm',\n",
    "        'effective_min_margin', 'margin_step',\n",
    "        'margin_tier_below', 'margin_tier_1', 'margin_tier_2', 'margin_tier_3',\n",
    "        'margin_tier_4', 'margin_tier_5', 'margin_tier_above_1', 'margin_tier_above_2'\n",
    "    ]], \n",
    "    on=['product_id', 'region','cohort_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"Margin tiers merged from module!\")\n",
    "print(f\"\\nRecords with margin tiers: {len(pricing_with_discount[~pricing_with_discount['max_boundary'].isna()])}\")\n",
    "print(f\"Records without margin tiers: {len(pricing_with_discount[pricing_with_discount['max_boundary'].isna()])}\")\n",
    "\n",
    "print(f\"\\nMargin Tier Structure (from market_data_module):\")\n",
    "print(f\"  margin_tier_below:   effective_min - step (1 below)\")\n",
    "print(f\"  margin_tier_1:       effective_min_margin\")\n",
    "print(f\"  margin_tier_2:       effective_min + 1*step\")\n",
    "print(f\"  margin_tier_3:       effective_min + 2*step\")\n",
    "print(f\"  margin_tier_4:       effective_min + 3*step\")\n",
    "print(f\"  margin_tier_5:       max_boundary\")\n",
    "print(f\"  margin_tier_above_1: max_boundary + 1*step\")\n",
    "print(f\"  margin_tier_above_2: max_boundary + 2*step\")\n",
    "\n",
    "print(f\"\\nSample margin tiers:\")\n",
    "pricing_with_discount[~pricing_with_discount['max_boundary'].isna()][\n",
    "    ['product_id', 'sku', 'effective_min_margin', 'max_boundary', 'margin_step',\n",
    "     'margin_tier_below', 'margin_tier_1', 'margin_tier_3', 'margin_tier_5', \n",
    "     'margin_tier_above_1', 'margin_tier_above_2']\n",
    "].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Minimum Selling Quantity Query - Get min selling qty per product\n",
    "# =============================================================================\n",
    "MIN_SELLING_QTY_QUERY = f'''\n",
    "SELECT product_id, min_selling_qty\n",
    "FROM (\n",
    "    SELECT *, MIN(basic_unit_count) OVER (PARTITION BY product_id) AS min_selling_qty\n",
    "    FROM (\n",
    "        SELECT DISTINCT\n",
    "            pso.product_id,\n",
    "            pso.PACKING_UNIT_ID,\n",
    "            pup.basic_unit_count,\n",
    "            SUM(pso.total_price) AS nmv,\n",
    "            SUM(pso.total_price) / SUM(nmv) OVER (PARTITION BY pso.product_id) AS cntrb\n",
    "        FROM product_sales_order pso\n",
    "        JOIN PACKING_UNIT_PRODUCTS pup ON pup.product_id = pso.product_id \n",
    "            AND pup.PACKING_UNIT_ID = pso.PACKING_UNIT_ID\n",
    "        JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "        WHERE so.created_at::DATE >= CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE - 120\n",
    "            AND so.sales_order_status_id NOT IN (7, 12)\n",
    "            AND so.channel IN ('telesales', 'retailer')\n",
    "            AND pso.purchased_item_count <> 0\n",
    "        GROUP BY ALL\n",
    "        QUALIFY cntrb > 0.05\n",
    "    )\n",
    "    QUALIFY basic_unit_count = min_selling_qty\n",
    ")\n",
    "'''\n",
    "\n",
    "# Execute min selling qty query\n",
    "print(\"Loading minimum selling quantity data...\")\n",
    "df_min_selling_qty = query_snowflake(MIN_SELLING_QTY_QUERY)\n",
    "df_min_selling_qty = convert_to_numeric(df_min_selling_qty)\n",
    "print(f\"Loaded {len(df_min_selling_qty)} min selling qty records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Add Min Selling Qty and Below Min Stock Flag to pricing_with_discount\n",
    "# =============================================================================\n",
    "\n",
    "# Merge min selling qty with pricing_with_discount (by product_id)\n",
    "pricing_with_discount = pricing_with_discount.merge(\n",
    "    df_min_selling_qty[['product_id', 'min_selling_qty']], \n",
    "    on='product_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing min_selling_qty with 1 (default)\n",
    "pricing_with_discount['min_selling_qty'] = pricing_with_discount['min_selling_qty'].fillna(1).astype(int)\n",
    "\n",
    "# Create flag: below_min_stock_flag = 1 if (RR = 0 AND stocks > 0 AND stocks < min_selling_qty)\n",
    "pricing_with_discount['below_min_stock_flag'] = np.where(\n",
    "    (pricing_with_discount['in_stock_rr'] == 0) & \n",
    "    (pricing_with_discount['stocks'] > 0) &\n",
    "    (pricing_with_discount['stocks'] < pricing_with_discount['min_selling_qty']),\n",
    "    1, 0\n",
    ")\n",
    "\n",
    "print(f\"Min selling qty and below_min_stock_flag added!\")\n",
    "print(f\"\\nSKUs flagged (zero RR & stocks < min_selling_qty): {len(pricing_with_discount[pricing_with_discount['below_min_stock_flag'] == 1])}\")\n",
    "print(f\"SKUs not flagged: {len(pricing_with_discount[pricing_with_discount['below_min_stock_flag'] == 0])}\")\n",
    "print(f\"\\nSample flagged SKUs:\")\n",
    "pricing_with_discount[pricing_with_discount['below_min_stock_flag'] == 1][\n",
    "    ['product_id', 'warehouse_id', 'sku', 'stocks', 'min_selling_qty', 'in_stock_rr', 'below_min_stock_flag']\n",
    "].head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Yesterday's Discount Analysis Query\n",
    "# Gets: SKU discount, Quantity discount, Tier 1/2/3 NMV breakdown and contributions\n",
    "# =============================================================================\n",
    "YESTERDAY_DISCOUNT_QUERY = f'''\n",
    "WITH qd_det AS (\n",
    "    -- Map dynamic tags to warehouse IDs using name matching\n",
    "    SELECT DISTINCT \n",
    "        dt.id AS tag_id, \n",
    "        dt.name AS tag_name,\n",
    "        REPLACE(w.name, ' ', '') AS warehouse_name,\n",
    "        w.id AS warehouse_id,\n",
    "        warehouse_name ILIKE '%' || CASE \n",
    "            WHEN SPLIT_PART(tag_name, '_', 1) = 'El' THEN SPLIT_PART(tag_name, '_', 2) \n",
    "            ELSE SPLIT_PART(tag_name, '_', 1) \n",
    "        END || '%' AS contains_flag\n",
    "    FROM dynamic_tags dt\n",
    "    JOIN dynamic_taggables dta ON dt.id = dta.dynamic_tag_id \n",
    "    CROSS JOIN warehouses w \n",
    "    WHERE dt.id > 3000\n",
    "        AND dt.name LIKE '%QD_rets%'\n",
    "        AND w.id IN (1, 236, 337, 8, 339, 170, 501, 401, 703, 632, 797, 962)\n",
    "        AND contains_flag = 'true'\n",
    "),\n",
    "\n",
    "qd_config AS (\n",
    "    SELECT * \n",
    "    FROM (\n",
    "        SELECT \n",
    "            product_id,\n",
    "            start_at,\n",
    "            end_at,\n",
    "            packing_unit_id,\n",
    "            id AS qd_id,\n",
    "            qd.warehouse_id,\n",
    "            MAX(CASE WHEN tier = 1 THEN quantity END) AS tier_1_qty,\n",
    "            MAX(CASE WHEN tier = 1 THEN discount_percentage END) AS tier_1_discount_pct,\n",
    "            MAX(CASE WHEN tier = 2 THEN quantity END) AS tier_2_qty,\n",
    "            MAX(CASE WHEN tier = 2 THEN discount_percentage END) AS tier_2_discount_pct,\n",
    "            MAX(CASE WHEN tier = 3 THEN quantity END) AS tier_3_qty,\n",
    "            MAX(CASE WHEN tier = 3 THEN discount_percentage END) AS tier_3_discount_pct\n",
    "        FROM (\n",
    "            SELECT \n",
    "                qd.id,\n",
    "                qdv.product_id,\n",
    "                qdv.packing_unit_id,\n",
    "                qdv.quantity,\n",
    "                qdv.discount_percentage,\n",
    "                qd.dynamic_tag_id,\n",
    "                qd.start_at,\n",
    "                qd.end_at,\n",
    "                ROW_NUMBER() OVER (\n",
    "                    PARTITION BY qdv.product_id, qdv.packing_unit_id, qd.id \n",
    "                    ORDER BY qdv.quantity\n",
    "                ) AS tier\n",
    "            FROM quantity_discounts qd \n",
    "            JOIN quantity_discount_values qdv ON qd.id = qdv.quantity_discount_id \n",
    "            WHERE CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE - 1 \n",
    "                  BETWEEN qd.start_at::DATE AND qd.end_at::DATE\n",
    "                AND qd.start_at::DATE >= CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE - 5\n",
    "        ) qd_tiers\n",
    "        JOIN qd_det qd ON qd.tag_id = qd_tiers.dynamic_tag_id\n",
    "        GROUP BY ALL\n",
    "    )\n",
    "    QUALIFY ROW_NUMBER() OVER (PARTITION BY product_id, packing_unit_id, warehouse_id ORDER BY start_at DESC) = 1\n",
    "),\n",
    "\n",
    "-- Get all sales from yesterday\n",
    "yesterday_sales AS (\n",
    "    SELECT \n",
    "        pso.warehouse_id,\n",
    "        pso.product_id,\n",
    "        so.retailer_id,\n",
    "        pso.packing_unit_id,\n",
    "        pso.purchased_item_count AS qty,\n",
    "        pso.total_price AS nmv,\n",
    "        pso.item_price / pso.basic_unit_count AS unit_price,\n",
    "        pso.ITEM_DISCOUNT_VALUE AS sku_discount_per_unit,\n",
    "        pso.ITEM_QUANTITY_DISCOUNT_VALUE AS qty_discount_per_unit,\n",
    "        pso.ITEM_DISCOUNT_VALUE * pso.purchased_item_count AS sku_discount_total,\n",
    "        pso.ITEM_QUANTITY_DISCOUNT_VALUE * pso.purchased_item_count AS qty_discount_total,\n",
    "        qd.tier_1_qty,\n",
    "        qd.tier_2_qty,\n",
    "        qd.tier_3_qty,\n",
    "        qd.tier_1_discount_pct,\n",
    "        qd.tier_2_discount_pct,\n",
    "        qd.tier_3_discount_pct,\n",
    "        -- Determine tier used\n",
    "        CASE \n",
    "            WHEN pso.ITEM_QUANTITY_DISCOUNT_VALUE = 0 OR qd.tier_1_qty IS NULL THEN 'Base'\n",
    "            WHEN qd.tier_3_qty IS NOT NULL AND pso.purchased_item_count >= qd.tier_3_qty THEN 'Tier 3'\n",
    "            WHEN qd.tier_2_qty IS NOT NULL AND pso.purchased_item_count >= qd.tier_2_qty THEN 'Tier 2'\n",
    "            WHEN qd.tier_1_qty IS NOT NULL AND pso.purchased_item_count >= qd.tier_1_qty THEN 'Tier 1'\n",
    "            ELSE 'Base'\n",
    "        END AS tier_used\n",
    "    FROM product_sales_order pso\n",
    "    JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "    LEFT JOIN qd_config qd \n",
    "        ON qd.product_id = pso.product_id \n",
    "        AND qd.packing_unit_id = pso.packing_unit_id\n",
    "        AND qd.warehouse_id = so.warehouse_id\n",
    "    WHERE so.created_at::DATE = CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE - 1\n",
    "        AND so.sales_order_status_id NOT IN (7, 12)\n",
    "        AND so.channel IN ('telesales', 'retailer')\n",
    "        AND pso.purchased_item_count <> 0\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    warehouse_id,\n",
    "    product_id,\n",
    "    SUM(nmv) AS total_nmv,\n",
    "    SUM(CASE WHEN sku_discount_per_unit > 0 THEN nmv ELSE 0 END) AS sku_discount_nmv,\n",
    "    SUM(CASE WHEN qty_discount_per_unit > 0 THEN nmv ELSE 0 END) AS qty_discount_nmv,\n",
    "    SUM(CASE WHEN tier_used = 'Tier 1' THEN nmv ELSE 0 END) AS tier1_nmv,\n",
    "    SUM(CASE WHEN tier_used = 'Tier 2' THEN nmv ELSE 0 END) AS tier2_nmv,\n",
    "    SUM(CASE WHEN tier_used = 'Tier 3' THEN nmv ELSE 0 END) AS tier3_nmv,\n",
    "    -- Tier quantities and discount percentages (from the active QD config)\n",
    "    MAX(tier_1_qty) AS tier_1_qty,\n",
    "    MAX(tier_1_discount_pct) AS tier_1_discount_pct,\n",
    "    MAX(tier_2_qty) AS tier_2_qty,\n",
    "    MAX(tier_2_discount_pct) AS tier_2_discount_pct,\n",
    "    MAX(tier_3_qty) AS tier_3_qty,\n",
    "    MAX(tier_3_discount_pct) AS tier_3_discount_pct\n",
    "FROM yesterday_sales\n",
    "GROUP BY warehouse_id, product_id\n",
    "HAVING SUM(nmv) > 0\n",
    "ORDER BY total_nmv DESC\n",
    "'''\n",
    "\n",
    "# Execute yesterday discount query\n",
    "print(\"Loading yesterday's discount analysis data...\")\n",
    "df_yesterday_discount = query_snowflake(YESTERDAY_DISCOUNT_QUERY)\n",
    "df_yesterday_discount = convert_to_numeric(df_yesterday_discount)\n",
    "print(f\"Loaded {len(df_yesterday_discount)} SKU discount records from yesterday\")\n",
    "\n",
    "# Calculate contributions in Python\n",
    "df_yesterday_discount['sku_discount_nmv_cntrb'] = (\n",
    "    df_yesterday_discount['sku_discount_nmv'] / df_yesterday_discount['total_nmv'] * 100\n",
    ").round(2)\n",
    "df_yesterday_discount['qty_discount_nmv_cntrb'] = (\n",
    "    df_yesterday_discount['qty_discount_nmv'] / df_yesterday_discount['total_nmv'] * 100\n",
    ").round(2)\n",
    "df_yesterday_discount['tier1_nmv_cntrb'] = (\n",
    "    df_yesterday_discount['tier1_nmv'] / df_yesterday_discount['total_nmv'] * 100\n",
    ").round(2)\n",
    "df_yesterday_discount['tier2_nmv_cntrb'] = (\n",
    "    df_yesterday_discount['tier2_nmv'] / df_yesterday_discount['total_nmv'] * 100\n",
    ").round(2)\n",
    "df_yesterday_discount['tier3_nmv_cntrb'] = (\n",
    "    df_yesterday_discount['tier3_nmv'] / df_yesterday_discount['total_nmv'] * 100\n",
    ").round(2)\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"YESTERDAY'S DISCOUNT ANALYSIS SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nTotal NMV yesterday: {df_yesterday_discount['total_nmv'].sum():,.0f}\")\n",
    "print(f\"SKU Discount NMV: {df_yesterday_discount['sku_discount_nmv'].sum():,.0f}\")\n",
    "print(f\"Quantity Discount NMV: {df_yesterday_discount['qty_discount_nmv'].sum():,.0f}\")\n",
    "print(f\"\\nNMV by Tier:\")\n",
    "print(f\"  Tier 1: {df_yesterday_discount['tier1_nmv'].sum():,.0f}\")\n",
    "print(f\"  Tier 2: {df_yesterday_discount['tier2_nmv'].sum():,.0f}\")\n",
    "print(f\"  Tier 3: {df_yesterday_discount['tier3_nmv'].sum():,.0f}\")\n",
    "\n",
    "df_yesterday_discount.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Add Yesterday's Discount Analysis to pricing_with_discount (Contributions Only)\n",
    "# =============================================================================\n",
    "\n",
    "# Merge yesterday discount data with pricing_with_discount - contributions + tier config\n",
    "pricing_with_discount = pricing_with_discount.merge(\n",
    "    df_yesterday_discount[[\n",
    "        'warehouse_id', 'product_id', \n",
    "        'sku_discount_nmv_cntrb', 'qty_discount_nmv_cntrb',\n",
    "        'tier1_nmv_cntrb', 'tier2_nmv_cntrb', 'tier3_nmv_cntrb',\n",
    "        'tier_1_qty', 'tier_1_discount_pct',\n",
    "        'tier_2_qty', 'tier_2_discount_pct',\n",
    "        'tier_3_qty', 'tier_3_discount_pct'\n",
    "    ]].rename(columns={\n",
    "        'sku_discount_nmv_cntrb': 'yesterday_sku_disc_cntrb',\n",
    "        'qty_discount_nmv_cntrb': 'yesterday_qty_disc_cntrb',\n",
    "        'tier1_nmv_cntrb': 'yesterday_t1_cntrb',\n",
    "        'tier2_nmv_cntrb': 'yesterday_t2_cntrb',\n",
    "        'tier3_nmv_cntrb': 'yesterday_t3_cntrb',\n",
    "        'tier_1_qty': 'qd_tier_1_qty',\n",
    "        'tier_1_discount_pct': 'qd_tier_1_disc_pct',\n",
    "        'tier_2_qty': 'qd_tier_2_qty',\n",
    "        'tier_2_discount_pct': 'qd_tier_2_disc_pct',\n",
    "        'tier_3_qty': 'qd_tier_3_qty',\n",
    "        'tier_3_discount_pct': 'qd_tier_3_disc_pct'\n",
    "    }), \n",
    "    on=['warehouse_id', 'product_id'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill NaN for SKUs that had no sales yesterday\n",
    "contrib_cols = [\n",
    "    'yesterday_sku_disc_cntrb', 'yesterday_qty_disc_cntrb',\n",
    "    'yesterday_t1_cntrb', 'yesterday_t2_cntrb', 'yesterday_t3_cntrb'\n",
    "]\n",
    "for col in contrib_cols:\n",
    "    if col in pricing_with_discount.columns:\n",
    "        pricing_with_discount[col] = pricing_with_discount[col].fillna(0)\n",
    "\n",
    "# Fill NaN for QD tier config (0 means no tier configured)\n",
    "qd_config_cols = [\n",
    "    'qd_tier_1_qty', 'qd_tier_1_disc_pct',\n",
    "    'qd_tier_2_qty', 'qd_tier_2_disc_pct',\n",
    "    'qd_tier_3_qty', 'qd_tier_3_disc_pct'\n",
    "]\n",
    "for col in qd_config_cols:\n",
    "    if col in pricing_with_discount.columns:\n",
    "        pricing_with_discount[col] = pricing_with_discount[col].fillna(0)\n",
    "\n",
    "print(f\"Yesterday's discount contributions and QD tier config added!\")\n",
    "print(f\"\\nSKUs with discount data: {len(pricing_with_discount[pricing_with_discount['yesterday_sku_disc_cntrb'] > 0]) + len(pricing_with_discount[pricing_with_discount['yesterday_qty_disc_cntrb'] > 0])}\")\n",
    "print(f\"SKUs with QD tier config: {len(pricing_with_discount[pricing_with_discount['qd_tier_1_qty'] > 0])}\")\n",
    "print(f\"\\nSample data with yesterday's discount contributions and QD tiers:\")\n",
    "pricing_with_discount[pricing_with_discount['qd_tier_1_qty'] > 0][\n",
    "    ['product_id', 'warehouse_id', 'sku', \n",
    "     'yesterday_sku_disc_cntrb', 'yesterday_qty_disc_cntrb',\n",
    "     'qd_tier_1_qty', 'qd_tier_1_disc_pct', 'qd_tier_2_qty', 'qd_tier_2_disc_pct', 'qd_tier_3_qty', 'qd_tier_3_disc_pct']\n",
    "].head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Performance Benchmark Query\n",
    "# Gets: Yesterday qty, Recent 7d qty, MTD qty, and P80 benchmarks (240 days)\n",
    "# Uses materialized_views.stock_day_close for in-stock determination\n",
    "# =============================================================================\n",
    "PERFORMANCE_BENCHMARK_QUERY = f'''\n",
    "WITH params AS (\n",
    "    SELECT\n",
    "        CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE AS today,\n",
    "        CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE - 1 AS yesterday,\n",
    "        CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE - 240 AS history_start,\n",
    "        DATE_TRUNC('month', CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE) AS current_month_start,\n",
    "        DAY(CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE) AS current_day_of_month\n",
    "),\n",
    "\n",
    "-- Daily sales aggregation (240 days) - includes qty and retailer count\n",
    "daily_sales AS (\n",
    "    SELECT\n",
    "        pso.warehouse_id,\n",
    "        pso.product_id,\n",
    "        so.created_at::DATE AS sale_date,\n",
    "        SUM(pso.purchased_item_count * pso.basic_unit_count) AS daily_qty,\n",
    "        COUNT(DISTINCT so.retailer_id) AS daily_retailers\n",
    "    FROM product_sales_order pso\n",
    "    JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "    CROSS JOIN params p\n",
    "    WHERE so.created_at::DATE >= p.history_start\n",
    "        AND so.created_at::DATE < p.today\n",
    "        AND so.sales_order_status_id NOT IN (7, 12)\n",
    "        AND so.channel IN ('telesales', 'retailer')\n",
    "        AND pso.purchased_item_count <> 0\n",
    "    GROUP BY pso.warehouse_id, pso.product_id, so.created_at::DATE\n",
    "),\n",
    "\n",
    "-- Daily stock status using stock_day_close\n",
    "-- In-stock = opening (prev day close) > 0 AND closing > 0\n",
    "daily_stock AS (\n",
    "    SELECT\n",
    "        sdc.warehouse_id,\n",
    "        sdc.product_id,\n",
    "        sdc.TIMESTAMP::DATE AS stock_date,\n",
    "        sdc.available_stock,\n",
    "        LAG(sdc.available_stock, 1) OVER (\n",
    "            PARTITION BY sdc.warehouse_id, sdc.product_id \n",
    "            ORDER BY sdc.TIMESTAMP::DATE\n",
    "        ) AS opening_stock,\n",
    "        CASE \n",
    "            WHEN LAG(sdc.available_stock, 1) OVER (\n",
    "                    PARTITION BY sdc.warehouse_id, sdc.product_id ORDER BY sdc.TIMESTAMP::DATE\n",
    "                 ) > 0 \n",
    "                 AND sdc.available_stock > 0 \n",
    "            THEN 1 \n",
    "            ELSE 0 \n",
    "        END AS in_stock_flag\n",
    "    FROM materialized_views.stock_day_close sdc\n",
    "    CROSS JOIN params p\n",
    "    WHERE sdc.TIMESTAMP::DATE >= p.history_start - 1  -- Need one extra day for LAG\n",
    "        AND sdc.TIMESTAMP::DATE < p.today\n",
    "),\n",
    "\n",
    "-- Combine sales with stock status\n",
    "daily_with_stock AS (\n",
    "    SELECT\n",
    "        COALESCE(ds.warehouse_id, st.warehouse_id) AS warehouse_id,\n",
    "        COALESCE(ds.product_id, st.product_id) AS product_id,\n",
    "        COALESCE(ds.sale_date, st.stock_date) AS the_date,\n",
    "        COALESCE(ds.daily_qty, 0) AS daily_qty,\n",
    "        COALESCE(ds.daily_retailers, 0) AS daily_retailers,\n",
    "        COALESCE(st.in_stock_flag, 0) AS in_stock_flag\n",
    "    FROM daily_sales ds\n",
    "    FULL OUTER JOIN daily_stock st \n",
    "        ON ds.warehouse_id = st.warehouse_id \n",
    "        AND ds.product_id = st.product_id \n",
    "        AND ds.sale_date = st.stock_date\n",
    "    WHERE COALESCE(ds.sale_date, st.stock_date) >= (SELECT history_start FROM params)\n",
    "),\n",
    "\n",
    "-- Calculate P80 benchmark (in-stock days only, 240 days, EXCLUDING last 7 days)\n",
    "p80_daily_benchmark AS (\n",
    "    SELECT\n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        PERCENTILE_CONT(0.8) WITHIN GROUP (ORDER BY daily_qty) AS p80_daily_240d,\n",
    "        AVG(daily_qty) AS avg_daily_240d,\n",
    "        STDDEV(daily_qty) AS std_daily_240d,\n",
    "        COUNT(*) AS in_stock_days_240d\n",
    "    FROM daily_with_stock\n",
    "    CROSS JOIN params p\n",
    "    WHERE in_stock_flag = 1\n",
    "        AND the_date >= p.history_start\n",
    "        AND the_date < p.today - 7  -- Exclude last 7 days from benchmark\n",
    "    GROUP BY warehouse_id, product_id\n",
    "),\n",
    "\n",
    "-- Calculate P70 retailer benchmark (in-stock days only, 240 days, EXCLUDING last 7 days)\n",
    "p70_retailer_benchmark AS (\n",
    "    SELECT\n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        PERCENTILE_CONT(0.7) WITHIN GROUP (ORDER BY daily_retailers) AS p70_daily_retailers_240d,\n",
    "        AVG(daily_retailers) AS avg_daily_retailers_240d,\n",
    "        STDDEV(daily_retailers) AS std_daily_retailers_240d\n",
    "    FROM daily_with_stock\n",
    "    CROSS JOIN params p\n",
    "    WHERE in_stock_flag = 1\n",
    "        AND the_date >= p.history_start\n",
    "        AND the_date < p.today - 7  -- Exclude last 7 days from benchmark\n",
    "    GROUP BY warehouse_id, product_id\n",
    "),\n",
    "\n",
    "-- Calculate 7-day rolling SUM for P80 recent benchmark\n",
    "rolling_7d AS (\n",
    "    SELECT\n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        the_date,\n",
    "        SUM(daily_qty) OVER (\n",
    "            PARTITION BY warehouse_id, product_id \n",
    "            ORDER BY the_date \n",
    "            ROWS BETWEEN 6 PRECEDING AND CURRENT ROW\n",
    "        ) AS rolling_7d_sum,\n",
    "        SUM(in_stock_flag) OVER (\n",
    "            PARTITION BY warehouse_id, product_id \n",
    "            ORDER BY the_date \n",
    "            ROWS BETWEEN 6 PRECEDING AND CURRENT ROW\n",
    "        ) AS in_stock_days_7d\n",
    "    FROM daily_with_stock\n",
    "),\n",
    "\n",
    "p80_7d_benchmark AS (\n",
    "    SELECT\n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        PERCENTILE_CONT(0.8) WITHIN GROUP (ORDER BY rolling_7d_sum) AS p80_7d_rolling_240d\n",
    "    FROM rolling_7d\n",
    "    CROSS JOIN params p\n",
    "    WHERE the_date >= p.history_start + 7  -- Need 7 days for rolling\n",
    "        AND the_date < p.today - 7  -- Exclude last 7 days from benchmark\n",
    "        AND in_stock_days_7d >= 4  -- At least 4 of 7 days in stock\n",
    "    GROUP BY warehouse_id, product_id\n",
    "),\n",
    "\n",
    "-- MTD benchmark: P80 of same MTD period totals (last 12 months)\n",
    "-- Sum all sales from day 1 to current day of month for each historical month\n",
    "mtd_historical AS (\n",
    "    SELECT\n",
    "        dws.warehouse_id,\n",
    "        dws.product_id,\n",
    "        DATE_TRUNC('month', dws.the_date) AS period_month_start,\n",
    "        SUM(dws.daily_qty) AS mtd_total_qty  -- Sum of all days from 1 to current_day_of_month\n",
    "    FROM daily_with_stock dws\n",
    "    CROSS JOIN params p\n",
    "    WHERE DAY(dws.the_date) <= p.current_day_of_month  -- Only days up to current day of month\n",
    "    GROUP BY dws.warehouse_id, dws.product_id, DATE_TRUNC('month', dws.the_date)\n",
    "),\n",
    "\n",
    "mtd_by_period AS (\n",
    "    SELECT\n",
    "        mh.warehouse_id,\n",
    "        mh.product_id,\n",
    "        mh.period_month_start,\n",
    "        mh.mtd_total_qty AS mtd_qty_at_day  -- Total MTD qty for that month\n",
    "    FROM mtd_historical mh\n",
    "    CROSS JOIN params p\n",
    "    WHERE mh.period_month_start >= DATEADD(month, -12, p.current_month_start)\n",
    "        AND mh.period_month_start < p.current_month_start\n",
    "),\n",
    "\n",
    "p80_mtd_benchmark AS (\n",
    "    SELECT\n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        PERCENTILE_CONT(0.8) WITHIN GROUP (ORDER BY mtd_qty_at_day) AS p80_mtd_12mo,\n",
    "        AVG(mtd_qty_at_day) AS avg_mtd_12mo\n",
    "    FROM mtd_by_period\n",
    "    GROUP BY warehouse_id, product_id\n",
    "    HAVING COUNT(*) >= 3  -- At least 3 months of data\n",
    "),\n",
    "\n",
    "-- Current period quantities\n",
    "current_metrics AS (\n",
    "    SELECT\n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        -- Yesterday\n",
    "        SUM(CASE WHEN the_date = (SELECT yesterday FROM params) THEN daily_qty ELSE 0 END) AS yesterday_qty,\n",
    "        SUM(CASE WHEN the_date = (SELECT yesterday FROM params) THEN daily_retailers ELSE 0 END) AS yesterday_retailers,\n",
    "        -- Recent 7 days\n",
    "        SUM(CASE WHEN the_date >= (SELECT today FROM params) - 7 AND the_date < (SELECT today FROM params) THEN daily_qty ELSE 0 END) AS recent_7d_qty,\n",
    "        SUM(CASE WHEN the_date >= (SELECT today FROM params) - 7 AND the_date < (SELECT today FROM params) AND in_stock_flag = 1 THEN 1 ELSE 0 END) AS recent_7d_in_stock_days,\n",
    "        -- MTD\n",
    "        SUM(CASE WHEN the_date >= (SELECT current_month_start FROM params) AND the_date < (SELECT today FROM params) THEN daily_qty ELSE 0 END) AS mtd_qty,\n",
    "        SUM(CASE WHEN the_date >= (SELECT current_month_start FROM params) AND the_date < (SELECT today FROM params) AND in_stock_flag = 1 THEN 1 ELSE 0 END) AS mtd_in_stock_days\n",
    "    FROM daily_with_stock\n",
    "    GROUP BY warehouse_id, product_id\n",
    ")\n",
    "\n",
    "-- Final output\n",
    "SELECT\n",
    "    cm.warehouse_id,\n",
    "    cm.product_id,\n",
    "    \n",
    "    -- Current period quantities\n",
    "    cm.yesterday_qty,\n",
    "    cm.yesterday_retailers,\n",
    "    cm.recent_7d_qty,\n",
    "    cm.recent_7d_in_stock_days,\n",
    "    cm.mtd_qty,\n",
    "    cm.mtd_in_stock_days,\n",
    "    \n",
    "    -- Quantity Benchmarks (P80)\n",
    "    COALESCE(pb.p80_daily_240d, 1) AS p80_daily_240d,\n",
    "    COALESCE(pb.avg_daily_240d, 0) AS avg_daily_240d,\n",
    "    COALESCE(pb.std_daily_240d, 0) AS std_daily_240d,\n",
    "    COALESCE(pb.in_stock_days_240d, 0) AS in_stock_days_240d,\n",
    "    COALESCE(p7.p80_7d_rolling_240d, pb.p80_daily_240d * 7, 1) AS p80_7d_sum_240d,\n",
    "    COALESCE(pm.p80_mtd_12mo, pb.p80_daily_240d * (SELECT current_day_of_month FROM params), 1) AS p80_mtd_12mo,\n",
    "    \n",
    "    -- Retailer Benchmarks (P70)\n",
    "    COALESCE(pr.p70_daily_retailers_240d, 1) AS p70_daily_retailers_240d,\n",
    "    COALESCE(pr.avg_daily_retailers_240d, 0) AS avg_daily_retailers_240d,\n",
    "    COALESCE(pr.std_daily_retailers_240d, 0) AS std_daily_retailers_240d,\n",
    "    \n",
    "    -- Performance ratios (all comparing sums to sums)\n",
    "    -- Yesterday: daily qty vs P80 daily\n",
    "    ROUND(cm.yesterday_qty / NULLIF(COALESCE(pb.p80_daily_240d, 1), 0), 2) AS yesterday_ratio,\n",
    "    -- Recent 7d: 7-day sum vs P80 of 7-day sums\n",
    "    ROUND(cm.recent_7d_qty / NULLIF(COALESCE(p7.p80_7d_rolling_240d, pb.p80_daily_240d * 7, 1), 0), 2) AS recent_ratio,\n",
    "    -- MTD: MTD sum vs P80 of historical MTD sums\n",
    "    ROUND(cm.mtd_qty / NULLIF(COALESCE(pm.p80_mtd_12mo, pb.p80_daily_240d * (SELECT current_day_of_month FROM params), 1), 0), 2) AS mtd_ratio,\n",
    "    -- Retailer ratio: yesterday retailers vs P70 daily retailers\n",
    "    ROUND(cm.yesterday_retailers / NULLIF(COALESCE(pr.p70_daily_retailers_240d, 1), 0), 2) AS yesterday_retailer_ratio\n",
    "\n",
    "FROM current_metrics cm\n",
    "LEFT JOIN p80_daily_benchmark pb ON cm.warehouse_id = pb.warehouse_id AND cm.product_id = pb.product_id\n",
    "LEFT JOIN p80_7d_benchmark p7 ON cm.warehouse_id = p7.warehouse_id AND cm.product_id = p7.product_id\n",
    "LEFT JOIN p80_mtd_benchmark pm ON cm.warehouse_id = pm.warehouse_id AND cm.product_id = pm.product_id\n",
    "LEFT JOIN p70_retailer_benchmark pr ON cm.warehouse_id = pr.warehouse_id AND cm.product_id = pr.product_id\n",
    "where cm.warehouse_id in (1, 236, 337, 8, 339, 170, 501, 401, 703, 632, 797, 962)\n",
    "'''\n",
    "\n",
    "# Execute benchmark query\n",
    "print(\"Loading performance benchmark data (this may take a moment due to 240-day history)...\")\n",
    "df_benchmarks = query_snowflake(PERFORMANCE_BENCHMARK_QUERY)\n",
    "df_benchmarks = convert_to_numeric(df_benchmarks)\n",
    "print(f\"Loaded {len(df_benchmarks)} benchmark records\")\n",
    "\n",
    "# =============================================================================\n",
    "# Apply Minimum Thresholds to Benchmark Values\n",
    "# - Daily quantity benchmarks should not be below 5\n",
    "# - Daily retailers benchmarks should not be less than 2\n",
    "# This ensures performance calculations don't use unrealistic benchmarks\n",
    "# =============================================================================\n",
    "MIN_DAILY_QTY_BENCHMARK = 5\n",
    "MIN_DAILY_RETAILERS_BENCHMARK = 2\n",
    "\n",
    "# Apply minimums to daily benchmarks\n",
    "df_benchmarks['p80_daily_240d'] = df_benchmarks['p80_daily_240d'].clip(lower=MIN_DAILY_QTY_BENCHMARK)\n",
    "df_benchmarks['p70_daily_retailers_240d'] = df_benchmarks['p70_daily_retailers_240d'].clip(lower=MIN_DAILY_RETAILERS_BENCHMARK)\n",
    "\n",
    "# Apply proportional minimums to interval-based benchmarks\n",
    "df_benchmarks['p80_7d_sum_240d'] = df_benchmarks['p80_7d_sum_240d'].clip(lower=MIN_DAILY_QTY_BENCHMARK * 7)  # 35\n",
    "\n",
    "# For MTD, calculate dynamic minimum based on days in current period\n",
    "# mtd_in_stock_days represents how many days of data we have in current month\n",
    "df_benchmarks['p80_mtd_12mo'] = df_benchmarks.apply(\n",
    "    lambda row: max(row['p80_mtd_12mo'], MIN_DAILY_QTY_BENCHMARK * max(row['mtd_in_stock_days'], 1)),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(f\"Applied minimum thresholds: qty >= {MIN_DAILY_QTY_BENCHMARK}/day, retailers >= {MIN_DAILY_RETAILERS_BENCHMARK}/day\")\n",
    "\n",
    "# Preview\n",
    "df_benchmarks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Add Performance Benchmarks and Tags to pricing_with_discount\n",
    "# =============================================================================\n",
    "\n",
    "# Merge benchmark data with pricing_with_discount\n",
    "pricing_with_discount = pricing_with_discount.merge(\n",
    "    df_benchmarks[[\n",
    "        'warehouse_id', 'product_id',\n",
    "        'yesterday_qty', 'yesterday_retailers', 'recent_7d_qty', 'recent_7d_in_stock_days', 'mtd_qty', 'mtd_in_stock_days',\n",
    "        'p80_daily_240d', 'avg_daily_240d','std_daily_240d', 'in_stock_days_240d',\n",
    "        'p80_7d_sum_240d', 'p80_mtd_12mo',\n",
    "        'p70_daily_retailers_240d', 'avg_daily_retailers_240d', 'std_daily_retailers_240d',\n",
    "        'yesterday_ratio', 'recent_ratio', 'mtd_ratio', 'yesterday_retailer_ratio'\n",
    "    ]], \n",
    "    on=['warehouse_id', 'product_id'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill NaN values\n",
    "qty_cols = ['yesterday_qty', 'yesterday_retailers', 'recent_7d_qty', 'recent_7d_in_stock_days', 'mtd_qty', 'mtd_in_stock_days']\n",
    "for col in qty_cols:\n",
    "    pricing_with_discount[col] = pricing_with_discount[col].fillna(0)\n",
    "\n",
    "benchmark_cols = ['p80_daily_240d', 'p80_7d_sum_240d', 'p80_mtd_12mo', 'p70_daily_retailers_240d']\n",
    "for col in benchmark_cols:\n",
    "    pricing_with_discount[col] = pricing_with_discount[col].fillna(1)  # Default to 1 to avoid division issues\n",
    "\n",
    "ratio_cols = ['yesterday_ratio', 'recent_ratio', 'mtd_ratio', 'yesterday_retailer_ratio']\n",
    "for col in ratio_cols:\n",
    "    pricing_with_discount[col] = pricing_with_discount[col].fillna(0)\n",
    "\n",
    "pricing_with_discount['avg_daily_240d'] = pricing_with_discount['avg_daily_240d'].fillna(0)\n",
    "pricing_with_discount['in_stock_days_240d'] = pricing_with_discount['in_stock_days_240d'].fillna(0)\n",
    "pricing_with_discount['avg_daily_retailers_240d'] = pricing_with_discount['avg_daily_retailers_240d'].fillna(0)\n",
    "pricing_with_discount['std_daily_retailers_240d'] = pricing_with_discount['std_daily_retailers_240d'].fillna(0)\n",
    "\n",
    "# =============================================================================\n",
    "# Performance Tags - Classify each ratio\n",
    "# =============================================================================\n",
    "def get_performance_tag(ratio):\n",
    "    \"\"\"\n",
    "    Classify performance based on ratio to benchmark\n",
    "    On Track: 90% to 115% of benchmark\n",
    "    Upper tiers: start from 115%\n",
    "    Lower tiers: start from 90%\n",
    "    \"\"\"\n",
    "    if pd.isna(ratio) or ratio == 0:\n",
    "        return 'No Data'\n",
    "    elif ratio >= 1.75:\n",
    "        return 'Star Performer'      # ðŸŒŸ 75%+ above benchmark\n",
    "    elif ratio > 1.15:\n",
    "        return 'Over Achiever'       # ðŸ”¥ 15%+ above benchmark  \n",
    "    elif ratio >= 0.90:\n",
    "        return 'On Track'            # âœ… Meeting benchmark (90%-115%)\n",
    "    elif ratio >= 0.70:\n",
    "        return 'Underperforming'     # âš ï¸ 10%-30% below benchmark\n",
    "    elif ratio >= 0.40:\n",
    "        return 'Struggling'          # ðŸ”» 30%-60% below benchmark\n",
    "    else:\n",
    "        return 'Critical'            # ðŸš¨ 60%+ below benchmark\n",
    "\n",
    "# Apply tags to each timeframe\n",
    "pricing_with_discount['yesterday_status'] = pricing_with_discount['yesterday_ratio'].apply(get_performance_tag)\n",
    "pricing_with_discount['recent_status'] = pricing_with_discount['recent_ratio'].apply(get_performance_tag)\n",
    "pricing_with_discount['mtd_status'] = pricing_with_discount['mtd_ratio'].apply(get_performance_tag)\n",
    "\n",
    "# =============================================================================\n",
    "# Combined Performance Score (weighted average of ratios)\n",
    "# Approach 2: Scale Weights by In-Stock Percentage\n",
    "# =============================================================================\n",
    "\n",
    "# Calculate days in month so far (excluding today)\n",
    "days_in_month_so_far = max(TODAY.day - 1, 1)  # At least 1 to avoid division by zero\n",
    "\n",
    "# Calculate in-stock percentages for each period\n",
    "pricing_with_discount['yesterday_in_stock_pct'] = 1 - pricing_with_discount['oos_yesterday']\n",
    "pricing_with_discount['recent_7d_in_stock_pct'] = pricing_with_discount['recent_7d_in_stock_days'] / 7\n",
    "pricing_with_discount['mtd_in_stock_pct'] = pricing_with_discount['mtd_in_stock_days'] / days_in_month_so_far\n",
    "\n",
    "# Base weights: Yesterday 20%, Recent 7d 40%, MTD 40%\n",
    "# Scale by in-stock percentage\n",
    "# NOTE: MTD weight = 0 for first 3 days of month (unreliable data)\n",
    "MTD_RELIABLE_DAY = 3  # Only use MTD when day >= 3\n",
    "pricing_with_discount['yesterday_raw_weight'] = 0.2 * pricing_with_discount['yesterday_in_stock_pct']\n",
    "pricing_with_discount['recent_7d_raw_weight'] = 0.4 * pricing_with_discount['recent_7d_in_stock_pct']\n",
    "pricing_with_discount['mtd_raw_weight'] = np.where(\n",
    "    TODAY.day >= MTD_RELIABLE_DAY,\n",
    "    0.4 * pricing_with_discount['mtd_in_stock_pct'],\n",
    "    0  # Set MTD weight to 0 at start of month\n",
    ")\n",
    "\n",
    "# Total raw weight for normalization\n",
    "pricing_with_discount['total_raw_weight'] = (\n",
    "    pricing_with_discount['yesterday_raw_weight'] + \n",
    "    pricing_with_discount['recent_7d_raw_weight'] + \n",
    "    pricing_with_discount['mtd_raw_weight']\n",
    ")\n",
    "\n",
    "# Normalized weights (sum to 1)\n",
    "pricing_with_discount['yesterday_norm_weight'] = np.where(\n",
    "    pricing_with_discount['total_raw_weight'] > 0,\n",
    "    pricing_with_discount['yesterday_raw_weight'] / pricing_with_discount['total_raw_weight'],\n",
    "    0\n",
    ")\n",
    "pricing_with_discount['recent_7d_norm_weight'] = np.where(\n",
    "    pricing_with_discount['total_raw_weight'] > 0,\n",
    "    pricing_with_discount['recent_7d_raw_weight'] / pricing_with_discount['total_raw_weight'],\n",
    "    0\n",
    ")\n",
    "pricing_with_discount['mtd_norm_weight'] = np.where(\n",
    "    pricing_with_discount['total_raw_weight'] > 0,\n",
    "    pricing_with_discount['mtd_raw_weight'] / pricing_with_discount['total_raw_weight'],\n",
    "    0\n",
    ")\n",
    "\n",
    "# Combined performance ratio with dynamic weights based on in-stock days\n",
    "pricing_with_discount['combined_perf_ratio'] = (\n",
    "    pricing_with_discount['yesterday_norm_weight'] * pricing_with_discount['yesterday_ratio'].clip(upper=3) +\n",
    "    pricing_with_discount['recent_7d_norm_weight'] * pricing_with_discount['recent_ratio'].clip(upper=3) +\n",
    "    pricing_with_discount['mtd_norm_weight'] * pricing_with_discount['mtd_ratio'].clip(upper=3)\n",
    ")\n",
    "\n",
    "# Handle cases where total_raw_weight = 0 (completely OOS in all periods)\n",
    "pricing_with_discount['combined_perf_ratio'] = pricing_with_discount['combined_perf_ratio'].fillna(0)\n",
    "\n",
    "# Clean up intermediate columns (optional - keep for debugging)\n",
    "weight_debug_cols = ['yesterday_in_stock_pct', 'recent_7d_in_stock_pct', 'mtd_in_stock_pct',\n",
    "                     'yesterday_raw_weight', 'recent_7d_raw_weight', 'mtd_raw_weight', 'total_raw_weight',\n",
    "                     'yesterday_norm_weight', 'recent_7d_norm_weight', 'mtd_norm_weight']\n",
    "# Uncomment to drop: pricing_with_discount.drop(columns=weight_debug_cols, inplace=True)\n",
    "\n",
    "print(f\"\\nDynamic weight calculation complete!\")\n",
    "print(f\"Days in month so far: {days_in_month_so_far}\")\n",
    "print(f\"\\nSample of weight distributions:\")\n",
    "print(pricing_with_discount[pricing_with_discount['total_raw_weight'] > 0][\n",
    "    ['product_id', 'warehouse_id', 'oos_yesterday', 'recent_7d_in_stock_days', 'mtd_in_stock_days',\n",
    "     'yesterday_norm_weight', 'recent_7d_norm_weight', 'mtd_norm_weight', 'combined_perf_ratio']\n",
    "].head(10))\n",
    "\n",
    "pricing_with_discount['combined_status'] = pricing_with_discount['combined_perf_ratio'].apply(get_performance_tag)\n",
    "\n",
    "# =============================================================================\n",
    "# High Performer Flag (for immediate action consideration)\n",
    "# =============================================================================\n",
    "# Flag SKUs that are significantly over-achieving and may need action (price increase, etc.)\n",
    "pricing_with_discount['high_performer_flag'] = np.where(\n",
    "    (pricing_with_discount['yesterday_ratio'] >= 1.5) & \n",
    "    (pricing_with_discount['recent_ratio'] >= 1.3) &\n",
    "    (pricing_with_discount['mtd_ratio'] >= 1.2),\n",
    "    1, 0\n",
    ")\n",
    "\n",
    "# Star performer flag (exceptional - all metrics 2x+ benchmark)\n",
    "pricing_with_discount['star_performer_flag'] = np.where(\n",
    "    (pricing_with_discount['yesterday_ratio'] >= 2.0) & \n",
    "    (pricing_with_discount['recent_ratio'] >= 1.5) &\n",
    "    (pricing_with_discount['mtd_ratio'] >= 1.5),\n",
    "    1, 0\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# Summary\n",
    "# =============================================================================\n",
    "print(f\"Performance benchmarks added!\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"PERFORMANCE STATUS DISTRIBUTION\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\nYesterday Status:\")\n",
    "print(pricing_with_discount['yesterday_status'].value_counts().to_string())\n",
    "\n",
    "print(f\"\\nRecent 7d Status:\")\n",
    "print(pricing_with_discount['recent_status'].value_counts().to_string())\n",
    "\n",
    "print(f\"\\nMTD Status:\")\n",
    "print(pricing_with_discount['mtd_status'].value_counts().to_string())\n",
    "\n",
    "print(f\"\\nCombined Status:\")\n",
    "print(pricing_with_discount['combined_status'].value_counts().to_string())\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"HIGH PERFORMERS (Action Candidates)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"High Performers (flag=1): {len(pricing_with_discount[pricing_with_discount['high_performer_flag'] == 1])}\")\n",
    "print(f\"Star Performers (flag=1): {len(pricing_with_discount[pricing_with_discount['star_performer_flag'] == 1])}\")\n",
    "\n",
    "# Show top performers\n",
    "print(f\"\\nTop 15 Star Performers:\")\n",
    "pricing_with_discount[pricing_with_discount['star_performer_flag'] == 1].nlargest(15, 'combined_perf_ratio')[\n",
    "    ['product_id', 'warehouse_id', 'sku', \n",
    "     'yesterday_ratio', 'recent_ratio', 'mtd_ratio', 'combined_perf_ratio',\n",
    "     'yesterday_status', 'combined_status']\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# No NMV in Last 4 Months Flag\n",
    "# Identifies SKUs that have not generated any NMV in the past 4 months (120 days)\n",
    "# =============================================================================\n",
    "NO_NMV_4M_QUERY = f'''\n",
    "WITH nmv_last_4m AS (\n",
    "    SELECT \n",
    "        pso.warehouse_id,\n",
    "        pso.product_id,\n",
    "        SUM(pso.total_price) AS total_nmv_4m\n",
    "    FROM product_sales_order pso\n",
    "    JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "    WHERE so.created_at::DATE >= CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE - 120\n",
    "        AND so.created_at::DATE < CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE\n",
    "        AND so.sales_order_status_id NOT IN (7, 12)\n",
    "        AND so.channel IN ('telesales', 'retailer')\n",
    "        AND pso.purchased_item_count <> 0\n",
    "    GROUP BY pso.warehouse_id, pso.product_id\n",
    "    HAVING SUM(pso.total_price) > 0\n",
    ")\n",
    "SELECT \n",
    "    warehouse_id,\n",
    "    product_id,\n",
    "    total_nmv_4m\n",
    "FROM nmv_last_4m\n",
    "'''\n",
    "\n",
    "# Execute query\n",
    "print(\"Loading SKUs with NMV in last 4 months...\")\n",
    "df_nmv_4m = query_snowflake(NO_NMV_4M_QUERY)\n",
    "df_nmv_4m = convert_to_numeric(df_nmv_4m)\n",
    "print(f\"Found {len(df_nmv_4m)} SKU-warehouse combinations with NMV in last 4 months\")\n",
    "\n",
    "# Merge and create no_nmv_4m flag\n",
    "pricing_with_discount = pricing_with_discount.merge(\n",
    "    df_nmv_4m[['warehouse_id', 'product_id', 'total_nmv_4m']],\n",
    "    on=['warehouse_id', 'product_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Flag SKUs with no NMV in last 4 months\n",
    "# 1 = No NMV (should potentially be filtered), 0 = Has NMV\n",
    "pricing_with_discount['no_nmv_4m'] = np.where(\n",
    "    pricing_with_discount['total_nmv_4m'].isna() | (pricing_with_discount['total_nmv_4m'] == 0),\n",
    "    1, 0\n",
    ")\n",
    "\n",
    "# Fill NaN for total_nmv_4m\n",
    "pricing_with_discount['total_nmv_4m'] = pricing_with_discount['total_nmv_4m'].fillna(0)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"NO NMV IN LAST 4 MONTHS ANALYSIS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total records: {len(pricing_with_discount)}\")\n",
    "print(f\"SKUs with NO NMV in 4 months (no_nmv_4m=1): {len(pricing_with_discount[pricing_with_discount['no_nmv_4m'] == 1])}\")\n",
    "print(f\"SKUs with NMV in 4 months (no_nmv_4m=0): {len(pricing_with_discount[pricing_with_discount['no_nmv_4m'] == 0])}\")\n",
    "\n",
    "# Show sample of SKUs with no NMV\n",
    "print(f\"\\nSample SKUs with no NMV in last 4 months:\")\n",
    "pricing_with_discount[pricing_with_discount['no_nmv_4m'] == 1][\n",
    "    ['product_id', 'warehouse_id', 'sku', 'stocks', 'in_stock_rr', 'zero_demand', 'no_nmv_4m']\n",
    "].head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Normal Refill Query - Avg qty & stddev for frequent retailers (last 120 days)\n",
    "# Frequent retailer definition based on ABC classification (from existing dataframe):\n",
    "#   - Class A: bought 4+ times\n",
    "#   - Class B: bought 3+ times\n",
    "#   - Class C: bought 2+ times\n",
    "# =============================================================================\n",
    "NORMAL_REFILL_QUERY = f'''\n",
    "WITH params AS (\n",
    "    SELECT \n",
    "        CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE AS today,\n",
    "        CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE - 120 AS history_start\n",
    "),\n",
    "\n",
    "-- Get retailer order counts per product-warehouse (last 120 days)\n",
    "retailer_orders AS (\n",
    "    SELECT \n",
    "        pso.warehouse_id,\n",
    "        pso.product_id,\n",
    "        so.retailer_id,\n",
    "        COUNT(DISTINCT so.id) AS order_count\n",
    "    FROM product_sales_order pso\n",
    "    JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "    CROSS JOIN params p\n",
    "    WHERE so.created_at::DATE >= p.history_start\n",
    "        AND so.created_at::DATE < p.today\n",
    "        AND so.sales_order_status_id NOT IN (7, 12)\n",
    "        AND so.channel IN ('telesales', 'retailer')\n",
    "        AND pso.purchased_item_count <> 0\n",
    "    GROUP BY pso.warehouse_id, pso.product_id, so.retailer_id\n",
    "),\n",
    "\n",
    "-- Get individual order quantities per retailer\n",
    "order_quantities AS (\n",
    "    SELECT \n",
    "        pso.warehouse_id,\n",
    "        pso.product_id,\n",
    "        so.retailer_id,\n",
    "        so.id AS order_id,\n",
    "        SUM(pso.purchased_item_count * pso.basic_unit_count) AS order_qty\n",
    "    FROM product_sales_order pso\n",
    "    JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "    CROSS JOIN params p\n",
    "    WHERE so.created_at::DATE >= p.history_start\n",
    "        AND so.created_at::DATE < p.today\n",
    "        AND so.sales_order_status_id NOT IN (7, 12)\n",
    "        AND so.channel IN ('telesales', 'retailer')\n",
    "        AND pso.purchased_item_count <> 0\n",
    "    GROUP BY pso.warehouse_id, pso.product_id, so.retailer_id, so.id\n",
    ")\n",
    "\n",
    "-- Return retailer-level data with order counts for Python filtering\n",
    "SELECT \n",
    "    oq.warehouse_id,\n",
    "    oq.product_id,\n",
    "    oq.retailer_id,\n",
    "    ro.order_count,\n",
    "    oq.order_id,\n",
    "    oq.order_qty\n",
    "FROM order_quantities oq\n",
    "JOIN retailer_orders ro \n",
    "    ON ro.warehouse_id = oq.warehouse_id \n",
    "    AND ro.product_id = oq.product_id \n",
    "    AND ro.retailer_id = oq.retailer_id\n",
    "'''\n",
    "\n",
    "# Execute normal refill query\n",
    "print(\"Loading retailer order data for normal refill calculation (last 120 days)...\")\n",
    "df_retailer_orders = query_snowflake(NORMAL_REFILL_QUERY)\n",
    "df_retailer_orders = convert_to_numeric(df_retailer_orders)\n",
    "print(f\"Loaded {len(df_retailer_orders)} retailer order records\")\n",
    "\n",
    "# Get ABC classification from existing dataframe\n",
    "abc_mapping = pricing_with_discount[['warehouse_id', 'product_id', 'abc_class']].drop_duplicates()\n",
    "print(f\"ABC classification mapping: {len(abc_mapping)} product-warehouse combinations\")\n",
    "\n",
    "# Merge ABC classification into retailer orders\n",
    "df_retailer_orders = df_retailer_orders.merge(\n",
    "    abc_mapping,\n",
    "    on=['warehouse_id', 'product_id'],\n",
    "    how='inner'\n",
    ")\n",
    "print(f\"Records after ABC merge: {len(df_retailer_orders)}\")\n",
    "\n",
    "# Filter frequent retailers based on ABC class thresholds\n",
    "# Class A: 4+ orders, Class B: 3+ orders, Class C: 2+ orders\n",
    "df_frequent = df_retailer_orders[\n",
    "    ((df_retailer_orders['abc_class'] == 'A') & (df_retailer_orders['order_count'] >= 4)) |\n",
    "    ((df_retailer_orders['abc_class'] == 'B') & (df_retailer_orders['order_count'] >= 3)) |\n",
    "    ((df_retailer_orders['abc_class'] == 'C') & (df_retailer_orders['order_count'] >= 2))\n",
    "].copy()\n",
    "print(f\"Records from frequent retailers: {len(df_frequent)}\")\n",
    "\n",
    "# Calculate normal_refill (avg qty) and refill_stddev per product-warehouse\n",
    "df_normal_refill = df_frequent.groupby(['warehouse_id', 'product_id']).agg(\n",
    "    frequent_retailer_count=('retailer_id', 'nunique'),\n",
    "    frequent_order_count=('order_id', 'nunique'),\n",
    "    normal_refill=('order_qty', 'mean'),\n",
    "    refill_stddev=('order_qty', 'std')\n",
    ").reset_index()\n",
    "\n",
    "# Round values and fill NaN stddev (when only 1 order)\n",
    "df_normal_refill['normal_refill'] = df_normal_refill['normal_refill'].round(2)\n",
    "df_normal_refill['refill_stddev'] = df_normal_refill['refill_stddev'].fillna(0).round(2)\n",
    "\n",
    "# Filter to products with at least 2 orders for meaningful stats\n",
    "df_normal_refill = df_normal_refill[df_normal_refill['frequent_order_count'] >= 2]\n",
    "print(f\"Final normal refill records (min 2 orders): {len(df_normal_refill)}\")\n",
    "\n",
    "# Merge with pricing_with_discount\n",
    "pricing_with_discount = pricing_with_discount.merge(\n",
    "    df_normal_refill[['warehouse_id', 'product_id', 'frequent_retailer_count', \n",
    "                      'frequent_order_count', 'normal_refill', 'refill_stddev']],\n",
    "    on=['warehouse_id', 'product_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill NaN values\n",
    "pricing_with_discount['frequent_retailer_count'] = pricing_with_discount['frequent_retailer_count'].fillna(0)\n",
    "pricing_with_discount['frequent_order_count'] = pricing_with_discount['frequent_order_count'].fillna(0)\n",
    "pricing_with_discount['normal_refill'] = pricing_with_discount['normal_refill'].fillna(0)\n",
    "pricing_with_discount['refill_stddev'] = pricing_with_discount['refill_stddev'].fillna(0)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"NORMAL REFILL ANALYSIS (Frequent Retailers - 120 days)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Records with normal_refill data: {len(pricing_with_discount[pricing_with_discount['normal_refill'] > 0])}\")\n",
    "print(f\"Records without normal_refill data: {len(pricing_with_discount[pricing_with_discount['normal_refill'] == 0])}\")\n",
    "print(f\"\\nNormal refill distribution:\")\n",
    "print(pricing_with_discount[pricing_with_discount['normal_refill'] > 0]['normal_refill'].describe())\n",
    "print(f\"\\nSample data:\")\n",
    "pricing_with_discount[pricing_with_discount['normal_refill'] > 0][\n",
    "    ['product_id', 'warehouse_id', 'sku', 'abc_class', 'frequent_retailer_count', \n",
    "     'frequent_order_count', 'normal_refill', 'refill_stddev', 'in_stock_rr']\n",
    "].head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Live Cart Rules Query - Get current cart rules from the system\n",
    "# Merges on product_id and cohort_id\n",
    "# =============================================================================\n",
    "LIVE_CART_RULES_QUERY = f'''\n",
    "SELECT \n",
    "    cppu.cohort_id,\n",
    "    pup.product_id,\n",
    "    pup.packing_unit_id,\n",
    "    pup.basic_unit_count,\n",
    "    COALESCE(cppu.MAX_PER_SALES_ORDER, cppu2.MAX_PER_SALES_ORDER) AS current_cart_rule\n",
    "FROM COHORT_PRODUCT_PACKING_UNITS cppu \n",
    "JOIN PACKING_UNIT_PRODUCTS pup ON cppu.PRODUCT_PACKING_UNIT_ID = pup.id \n",
    "JOIN cohorts c ON c.id = cppu.cohort_id\n",
    "LEFT JOIN COHORT_PRODUCT_PACKING_UNITS cppu2 \n",
    "    ON cppu.PRODUCT_PACKING_UNIT_ID = cppu2.PRODUCT_PACKING_UNIT_ID \n",
    "    AND cppu2.cohort_id = c.FALLBACK_COHORT_ID\n",
    "WHERE cppu.cohort_id IN ({','.join(map(str, COHORT_IDS))})\n",
    "'''\n",
    "\n",
    "# Execute live cart rules query\n",
    "print(\"Loading live cart rules...\")\n",
    "df_cart_rules = query_snowflake(LIVE_CART_RULES_QUERY)\n",
    "df_cart_rules = convert_to_numeric(df_cart_rules)\n",
    "print(f\"Loaded {len(df_cart_rules)} cart rule records\")\n",
    "\n",
    "# Aggregate to product-cohort level (take the cart rule for basic unit, or min if multiple)\n",
    "# Filter to basic unit (packing_unit_id where basic_unit_count = 1) for simpler merging\n",
    "df_cart_rules_basic = df_cart_rules[df_cart_rules['basic_unit_count'] == 1].copy()\n",
    "print(f\"Basic unit cart rules: {len(df_cart_rules_basic)}\")\n",
    "\n",
    "# If no basic unit, take the minimum cart rule per product-cohort\n",
    "df_cart_rules_agg = df_cart_rules.groupby(['cohort_id', 'product_id']).agg(\n",
    "    current_cart_rule=('current_cart_rule', 'min')\n",
    ").reset_index()\n",
    "\n",
    "# Prefer basic unit cart rule, fallback to aggregated\n",
    "df_cart_rules_final = df_cart_rules_basic[['cohort_id', 'product_id', 'current_cart_rule']].drop_duplicates()\n",
    "df_cart_rules_final = df_cart_rules_final.merge(\n",
    "    df_cart_rules_agg[['cohort_id', 'product_id', 'current_cart_rule']].rename(columns={'current_cart_rule': 'cart_rule_agg'}),\n",
    "    on=['cohort_id', 'product_id'],\n",
    "    how='outer'\n",
    ")\n",
    "df_cart_rules_final['current_cart_rule'] = df_cart_rules_final['current_cart_rule'].fillna(df_cart_rules_final['cart_rule_agg'])\n",
    "df_cart_rules_final = df_cart_rules_final[['cohort_id', 'product_id', 'current_cart_rule']].drop_duplicates()\n",
    "print(f\"Final cart rules (product-cohort level): {len(df_cart_rules_final)}\")\n",
    "\n",
    "# Merge with pricing_with_discount\n",
    "pricing_with_discount = pricing_with_discount.merge(\n",
    "    df_cart_rules_final,\n",
    "    on=['cohort_id', 'product_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill NaN cart rules with 0 (no cart rule set)\n",
    "pricing_with_discount['current_cart_rule'] = pricing_with_discount['current_cart_rule'].fillna(0)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"LIVE CART RULES ANALYSIS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Records with cart rule > 0: {len(pricing_with_discount[pricing_with_discount['current_cart_rule'] > 0])}\")\n",
    "print(f\"Records without cart rule: {len(pricing_with_discount[pricing_with_discount['current_cart_rule'] == 0])}\")\n",
    "print(f\"\\nCart rule distribution:\")\n",
    "print(pricing_with_discount[pricing_with_discount['current_cart_rule'] > 0]['current_cart_rule'].describe())\n",
    "print(f\"\\nSample data with cart rules:\")\n",
    "pricing_with_discount[pricing_with_discount['current_cart_rule'] > 0][\n",
    "    ['product_id', 'cohort_id', 'warehouse_id', 'sku', 'current_price', 'current_cart_rule', 'in_stock_rr']\n",
    "].head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Commercial Constraint Minimum Price Query\n",
    "# Gets the minimum price constraints from finance.minimum_prices\n",
    "# =============================================================================\n",
    "COMMERCIAL_MIN_PRICE_QUERY = f'''\n",
    "WITH to_remove AS (\n",
    "    SELECT \n",
    "        check_date AS start_date,\n",
    "        (check_date + INTERVAL '1 month') + 6 AS end_date \n",
    "    FROM (\n",
    "        SELECT \n",
    "            CASE \n",
    "                WHEN DATE_PART('day', CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE) < 7 \n",
    "                THEN DATE_TRUNC('month', CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE - INTERVAL '1 month') \n",
    "                ELSE DATE_FROM_PARTS(\n",
    "                    YEAR(CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE), \n",
    "                    MONTH(CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE), \n",
    "                    1\n",
    "                )  \n",
    "            END AS check_date\n",
    "    )\n",
    ") \n",
    "\n",
    "SELECT  \n",
    "    sku_id AS product_id,\n",
    "    sku,\n",
    "    brand AS comm_brand,\n",
    "    cat AS comm_cat,\n",
    "    region,\n",
    "    created_at AS comm_created_at,\n",
    "    min_price AS commercial_min_price\n",
    "FROM (\n",
    "    SELECT \n",
    "        product_id AS sku_id,\n",
    "        product_name AS sku,\n",
    "        brand,\n",
    "        category AS cat,\n",
    "        region,\n",
    "        min_price,\n",
    "        created_at,\n",
    "        MAX(created_at) OVER (PARTITION BY product_id, region) AS latest_date\n",
    "    FROM finance.minimum_prices\n",
    "    WHERE is_deleted = 'false'\n",
    "        AND created_at BETWEEN (SELECT start_date FROM to_remove) AND (SELECT end_date FROM to_remove)\n",
    ") comm\n",
    "WHERE created_at = latest_date\n",
    "'''\n",
    "\n",
    "# Execute commercial min price query\n",
    "print(\"Loading commercial minimum price constraints...\")\n",
    "df_commercial_min = query_snowflake(COMMERCIAL_MIN_PRICE_QUERY)\n",
    "df_commercial_min = convert_to_numeric(df_commercial_min)\n",
    "print(f\"Loaded {len(df_commercial_min)} commercial min price records\")\n",
    "\n",
    "# Merge with pricing_with_discount on product_id and region\n",
    "pricing_with_discount = pricing_with_discount.merge(\n",
    "    df_commercial_min[['product_id', 'region', 'commercial_min_price']],\n",
    "    on=['product_id', 'region'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill NaN with 0 (no commercial constraint)\n",
    "pricing_with_discount['commercial_min_price'] = pricing_with_discount['commercial_min_price'].fillna(0)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"COMMERCIAL MINIMUM PRICE CONSTRAINTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Records with commercial min price: {len(pricing_with_discount[pricing_with_discount['commercial_min_price'] > 0])}\")\n",
    "print(f\"Records without commercial min price: {len(pricing_with_discount[pricing_with_discount['commercial_min_price'] == 0])}\")\n",
    "print(f\"\\nCommercial min price distribution:\")\n",
    "print(pricing_with_discount[pricing_with_discount['commercial_min_price'] > 0]['commercial_min_price'].describe())\n",
    "print(f\"\\nSample data with commercial constraints:\")\n",
    "pricing_with_discount[pricing_with_discount['commercial_min_price'] > 0][\n",
    "    ['product_id', 'region', 'warehouse_id', 'sku', 'current_price', 'commercial_min_price', 'price_after_discount']\n",
    "].head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Active SKU Discount Query - Get current SKU discount percentage per warehouse\n",
    "# =============================================================================\n",
    "ACTIVE_SKU_DISCOUNT_QUERY = f'''\n",
    "WITH active_sku_discount AS ( \n",
    "    SELECT \n",
    "        x.id AS sku_discount_id,\n",
    "        retailer_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        DISCOUNT_PERCENTAGE,\n",
    "        start_at,\n",
    "        end_at \n",
    "    FROM (\n",
    "        SELECT \n",
    "            sd.*,\n",
    "            f.value::INT AS retailer_id \n",
    "        FROM SKU_DISCOUNTS sd,\n",
    "        LATERAL FLATTEN(\n",
    "            input => SPLIT(\n",
    "                REPLACE(REPLACE(REPLACE(sd.retailer_ids, '{{', ''), '}}', ''), '\"', ''), \n",
    "                ','\n",
    "            )\n",
    "        ) f\n",
    "        WHERE start_at::DATE <= CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE\n",
    "            AND end_at::DATE >= CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE\n",
    "    ) x \n",
    "    JOIN SKU_DISCOUNT_VALUES sdv ON x.id = sdv.sku_discount_id\n",
    "    WHERE name_en = 'Special Discounts'\n",
    "    QUALIFY MAX(start_at) OVER (PARTITION BY retailer_id, product_id, packing_unit_id) = start_at \n",
    ")\n",
    "\n",
    "SELECT \n",
    "    product_id, \n",
    "    warehouse_id,\n",
    "    AVG(DISCOUNT_PERCENTAGE) AS active_sku_disc_pct \n",
    "FROM (\n",
    "    SELECT \n",
    "        asd.*,\n",
    "        warehouse_id \n",
    "    FROM active_sku_discount asd \n",
    "    JOIN materialized_views.retailer_polygon rp ON rp.retailer_id = asd.retailer_id\n",
    "    JOIN WAREHOUSE_DISPATCHING_RULES wdr ON wdr.product_id = asd.product_id\n",
    "    JOIN DISPATCHING_POLYGONS dp ON dp.id = wdr.DISPATCHING_POLYGON_ID AND dp.district_id = rp.district_id\n",
    ")\n",
    "GROUP BY ALL\n",
    "'''\n",
    "\n",
    "# Execute active SKU discount query\n",
    "print(\"Loading active SKU discount data...\")\n",
    "df_active_sku_disc = query_snowflake(ACTIVE_SKU_DISCOUNT_QUERY)\n",
    "df_active_sku_disc = convert_to_numeric(df_active_sku_disc)\n",
    "print(f\"Loaded {len(df_active_sku_disc)} active SKU discount records\")\n",
    "\n",
    "# Merge with pricing_with_discount\n",
    "pricing_with_discount = pricing_with_discount.merge(\n",
    "    df_active_sku_disc[['product_id', 'warehouse_id', 'active_sku_disc_pct']],\n",
    "    on=['product_id', 'warehouse_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill NaN with 0 (no active SKU discount)\n",
    "pricing_with_discount['active_sku_disc_pct'] = pricing_with_discount['active_sku_disc_pct'].fillna(0)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ACTIVE SKU DISCOUNT ANALYSIS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Records with active SKU discount: {len(pricing_with_discount[pricing_with_discount['active_sku_disc_pct'] > 0])}\")\n",
    "print(f\"Records without active SKU discount: {len(pricing_with_discount[pricing_with_discount['active_sku_disc_pct'] == 0])}\")\n",
    "print(f\"\\nActive SKU discount distribution:\")\n",
    "print(pricing_with_discount[pricing_with_discount['active_sku_disc_pct'] > 0]['active_sku_disc_pct'].describe())\n",
    "print(f\"\\nSample data with active SKU discounts:\")\n",
    "pricing_with_discount[pricing_with_discount['active_sku_disc_pct'] > 0][\n",
    "    ['product_id', 'warehouse_id', 'sku', 'current_price', 'active_sku_disc_pct', 'discount_perc']\n",
    "].head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pricing_with_discount[(pricing_with_discount['no_nmv_4m']==0)|(pricing_with_discount['stocks']>0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates before saving\n",
    "final_df = final_df.drop_duplicates(subset=['product_id', 'warehouse_id'], keep='first')\n",
    "final_df.to_excel('pricing_with_discount.xlsx', index=False)\n",
    "print(f\"Exported {len(final_df)} records (duplicates removed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mfinal_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreated_at\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m TODAY\n",
      "\u001b[1;31mNameError\u001b[0m: name 'final_df' is not defined"
     ]
    }
   ],
   "source": [
    "final_df['created_at'] = TODAY\n",
    "final_df['created_at'] =pd.to_datetime(final_df['created_at']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = upload_dataframe_to_snowflake(\"Egypt\", final_df, \"MATERIALIZED_VIEWS\", \"Pricing_data_extraction\", \"append\", auto_create_table=True, conn=None)\n",
    "\n",
    "# Send Slack notification\n",
    "if status:\n",
    "    slack_message = f\"\"\"âœ… *Data Extraction Script Completed Successfully*\n",
    "    \n",
    "ðŸ“… Date: {TODAY}\n",
    "ðŸ“Š Records uploaded: {len(final_df):,}\n",
    "ðŸ—„ï¸ Table: MATERIALIZED_VIEWS.Pricing_data_extraction\n",
    "â° Completed at: {datetime.now(CAIRO_TZ).strftime('%Y-%m-%d %H:%M:%S')} Cairo time\"\"\"\n",
    "    \n",
    "    send_text_slack('new-pricing-logic',slack_message)\n",
    "    print(\"âœ… Slack notification sent!\")\n",
    "else:\n",
    "    error_message = f\"\"\"âŒ *Data Extraction Script Failed*\n",
    "    \n",
    "ðŸ“… Date: {TODAY}\n",
    "â° Failed at: {datetime.now(CAIRO_TZ).strftime('%Y-%m-%d %H:%M:%S')} Cairo time\n",
    "âš ï¸ Upload to Snowflake failed - please check logs\"\"\"\n",
    "    \n",
    "    send_text_slack('new-pricing-logic',error_message)\n",
    "    print(\"âŒ Error notification sent to Slack!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
