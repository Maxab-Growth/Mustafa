{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Extraction Module for Pricing & Offers System\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import snowflake.connector\n",
        "import setup_environment_2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Region\n",
        "REGION = \"Egypt\"\n",
        "\n",
        "# Snowflake Warehouse\n",
        "WAREHOUSE = \"COMPUTE_WH\"\n",
        "\n",
        "# Date Variables\n",
        "from datetime import datetime, timedelta\n",
        "TODAY = datetime.now().date()\n",
        "YESTERDAY = TODAY - timedelta(days=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "NoCredentialsError",
          "evalue": "Unable to locate credentials",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNoCredentialsError\u001b[0m                        Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msetup_environment_2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\scripts\\Mustafa\\Pricing Logic\\setup_environment_2.py:67\u001b[0m, in \u001b[0;36minitialize_env\u001b[1;34m()\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minitialize_env\u001b[39m():\n\u001b[1;32m---> 67\u001b[0m     db_secret \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(\u001b[43mget_secret\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrds/mainsystem/redash\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     68\u001b[0m     dwh_reader_secret \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(get_secret(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprod/db/datawarehouse/metabase\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     69\u001b[0m     dwh_writer_secret \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(get_secret(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprod/db/datawarehouse/sagemaker\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
            "File \u001b[1;32md:\\scripts\\Mustafa\\Pricing Logic\\setup_environment_2.py:34\u001b[0m, in \u001b[0;36mget_secret\u001b[1;34m(secret_name)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# In this sample we only handle the specific exceptions for the 'GetSecretValue' API.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# See https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# We rethrow the exception by default.\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 34\u001b[0m     get_secret_value_response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_secret_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mSecretId\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msecret_name\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mresponse[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDecryptionFailureException\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;66;03m# Secrets Manager can't decrypt the protected secret text using the provided KMS key.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;66;03m# Deal with the exception here, and/or rethrow at your discretion.\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\botocore\\client.py:602\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    598\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    599\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    600\u001b[0m     )\n\u001b[0;32m    601\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[1;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\botocore\\context.py:123\u001b[0m, in \u001b[0;36mwith_current_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook:\n\u001b[0;32m    122\u001b[0m     hook()\n\u001b[1;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\botocore\\client.py:1060\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[1;34m(self, operation_name, api_params)\u001b[0m\n\u001b[0;32m   1056\u001b[0m     maybe_compress_request(\n\u001b[0;32m   1057\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mconfig, request_dict, operation_model\n\u001b[0;32m   1058\u001b[0m     )\n\u001b[0;32m   1059\u001b[0m     apply_request_checksum(request_dict)\n\u001b[1;32m-> 1060\u001b[0m     http, parsed_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1061\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_context\u001b[49m\n\u001b[0;32m   1062\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1064\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mevents\u001b[38;5;241m.\u001b[39memit(\n\u001b[0;32m   1065\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter-call.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mservice_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1066\u001b[0m     http_response\u001b[38;5;241m=\u001b[39mhttp,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1069\u001b[0m     context\u001b[38;5;241m=\u001b[39mrequest_context,\n\u001b[0;32m   1070\u001b[0m )\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\botocore\\client.py:1084\u001b[0m, in \u001b[0;36mBaseClient._make_request\u001b[1;34m(self, operation_model, request_dict, request_context)\u001b[0m\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_make_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, operation_model, request_dict, request_context):\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1084\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_endpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1086\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mevents\u001b[38;5;241m.\u001b[39memit(\n\u001b[0;32m   1087\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter-call-error.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_model\u001b[38;5;241m.\u001b[39mservice_id\u001b[38;5;241m.\u001b[39mhyphenize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperation_model\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1088\u001b[0m             exception\u001b[38;5;241m=\u001b[39me,\n\u001b[0;32m   1089\u001b[0m             context\u001b[38;5;241m=\u001b[39mrequest_context,\n\u001b[0;32m   1090\u001b[0m         )\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\botocore\\endpoint.py:119\u001b[0m, in \u001b[0;36mEndpoint.make_request\u001b[1;34m(self, operation_model, request_dict)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmake_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, operation_model, request_dict):\n\u001b[0;32m    114\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaking request for \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m with params: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    116\u001b[0m         operation_model,\n\u001b[0;32m    117\u001b[0m         request_dict,\n\u001b[0;32m    118\u001b[0m     )\n\u001b[1;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\botocore\\endpoint.py:196\u001b[0m, in \u001b[0;36mEndpoint._send_request\u001b[1;34m(self, request_dict, operation_model)\u001b[0m\n\u001b[0;32m    194\u001b[0m context \u001b[38;5;241m=\u001b[39m request_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_retries_context(context, attempts)\n\u001b[1;32m--> 196\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m success_response, exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_response(\n\u001b[0;32m    198\u001b[0m     request, operation_model, context\n\u001b[0;32m    199\u001b[0m )\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_needs_retry(\n\u001b[0;32m    201\u001b[0m     attempts,\n\u001b[0;32m    202\u001b[0m     operation_model,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m     exception,\n\u001b[0;32m    206\u001b[0m ):\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\botocore\\endpoint.py:132\u001b[0m, in \u001b[0;36mEndpoint.create_request\u001b[1;34m(self, params, operation_model)\u001b[0m\n\u001b[0;32m    130\u001b[0m     service_id \u001b[38;5;241m=\u001b[39m operation_model\u001b[38;5;241m.\u001b[39mservice_model\u001b[38;5;241m.\u001b[39mservice_id\u001b[38;5;241m.\u001b[39mhyphenize()\n\u001b[0;32m    131\u001b[0m     event_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrequest-created.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mservice_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperation_model\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 132\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event_emitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperation_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m prepared_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_request(request)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prepared_request\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\botocore\\hooks.py:412\u001b[0m, in \u001b[0;36mEventAliaser.emit\u001b[1;34m(self, event_name, **kwargs)\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21memit\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    411\u001b[0m     aliased_event_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_alias_event_name(event_name)\n\u001b[1;32m--> 412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_emitter\u001b[38;5;241m.\u001b[39memit(aliased_event_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\botocore\\hooks.py:256\u001b[0m, in \u001b[0;36mHierarchicalEmitter.emit\u001b[1;34m(self, event_name, **kwargs)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21memit\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    246\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;124;03m    Emit an event by name with arguments passed as keyword args.\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;124;03m             handlers.\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_emit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\botocore\\hooks.py:239\u001b[0m, in \u001b[0;36mHierarchicalEmitter._emit\u001b[1;34m(self, event_name, kwargs, stop_on_response)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers_to_call:\n\u001b[0;32m    238\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: calling handler \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m, event_name, handler)\n\u001b[1;32m--> 239\u001b[0m     response \u001b[38;5;241m=\u001b[39m handler(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    240\u001b[0m     responses\u001b[38;5;241m.\u001b[39mappend((handler, response))\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_on_response \u001b[38;5;129;01mand\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\botocore\\signers.py:108\u001b[0m, in \u001b[0;36mRequestSigner.handler\u001b[1;34m(self, operation_name, request, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mhandler\u001b[39m(\u001b[38;5;28mself\u001b[39m, operation_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;66;03m# This is typically hooked up to the \"request-created\" event\u001b[39;00m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;66;03m# from a client's event emitter.  When a new request is created\u001b[39;00m\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;66;03m# this method is invoked to sign the request.\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;66;03m# Don't call this method directly.\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msign\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\botocore\\signers.py:200\u001b[0m, in \u001b[0;36mRequestSigner.sign\u001b[1;34m(self, operation_name, request, region_name, signing_type, expires_in, signing_name)\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m--> 200\u001b[0m \u001b[43mauth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_auth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\botocore\\auth.py:422\u001b[0m, in \u001b[0;36mSigV4Auth.add_auth\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21madd_auth\u001b[39m(\u001b[38;5;28mself\u001b[39m, request):\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcredentials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 422\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m NoCredentialsError()\n\u001b[0;32m    423\u001b[0m     datetime_now \u001b[38;5;241m=\u001b[39m get_current_datetime()\n\u001b[0;32m    424\u001b[0m     request\u001b[38;5;241m.\u001b[39mcontext[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m datetime_now\u001b[38;5;241m.\u001b[39mstrftime(SIGV4_TIMESTAMP)\n",
            "\u001b[1;31mNoCredentialsError\u001b[0m: Unable to locate credentials"
          ]
        }
      ],
      "source": [
        "setup_environment_2.initialize_env()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Warehouse & Cohort Mapping\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Warehouse Mapping: (region, warehouse_name, warehouse_id, cohort_id)\n",
        "WAREHOUSE_MAPPING = [\n",
        "    ('Cairo', 'El-Marg', 38, 700),\n",
        "    ('Cairo', 'Mostorod', 1, 700),\n",
        "    ('Giza', 'Barageel', 236, 701),\n",
        "    ('Giza', 'Sakkarah', 962, 701),\n",
        "    ('Delta West', 'El-Mahala', 337, 703),\n",
        "    ('Delta West', 'Tanta', 8, 703),\n",
        "    ('Delta East', 'Mansoura FC', 339, 704),\n",
        "    ('Delta East', 'Sharqya', 170, 704),\n",
        "    ('Upper Egypt', 'Assiut FC', 501, 1124),\n",
        "    ('Upper Egypt', 'Bani sweif', 401, 1126),\n",
        "    ('Upper Egypt', 'Menya Samalot', 703, 1123),\n",
        "    ('Upper Egypt', 'Sohag', 632, 1125),\n",
        "    ('Alexandria', 'Khorshed Alex', 797, 702),\n",
        "]\n",
        "\n",
        "# Region to Cohort Mapping\n",
        "REGION_COHORT_MAPPING = {\n",
        "    'Cairo': 700,\n",
        "    'Giza': 701,\n",
        "    'Delta West': 703,\n",
        "    'Delta East': 704,\n",
        "    'Upper Egypt': 1124,\n",
        "    'Alexandria': 702,\n",
        "}\n",
        "\n",
        "# All Cohort IDs\n",
        "COHORT_IDS = [700, 701, 702, 703, 704, 1123, 1124, 1125, 1126]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Snowflake Query Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def query_snowflake(query):\n",
        "    \"\"\"Execute a query on Snowflake and return results as DataFrame.\"\"\"\n",
        "    con = snowflake.connector.connect(\n",
        "        user=os.environ[\"SNOWFLAKE_USERNAME\"],\n",
        "        account=os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
        "        password=os.environ[\"SNOWFLAKE_PASSWORD\"],\n",
        "        database=os.environ[\"SNOWFLAKE_DATABASE\"]\n",
        "    )\n",
        "    try:\n",
        "        cur = con.cursor()\n",
        "        cur.execute(\"USE WAREHOUSE COMPUTE_WH\")\n",
        "        cur.execute(query)\n",
        "        data = cur.fetchall()\n",
        "        columns = [desc[0].lower() for desc in cur.description]  # Get column names from cursor\n",
        "        return pd.DataFrame(data, columns=columns)\n",
        "    except Exception as e:\n",
        "        print(f\"Snowflake Error: {e}\")\n",
        "        return pd.DataFrame()\n",
        "    finally:\n",
        "        cur.close()\n",
        "        con.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_snowflake_timezone():\n",
        "    \"\"\"Get the current timezone from Snowflake.\"\"\"\n",
        "    query = \"SHOW PARAMETERS LIKE 'TIMEZONE'\"\n",
        "    result = query_snowflake(query)\n",
        "    return result[1].values[0] if len(result) > 0 else \"UTC\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_warehouse_df():\n",
        "    \"\"\"Get warehouse mapping as DataFrame.\"\"\"\n",
        "    return pd.DataFrame(\n",
        "        WAREHOUSE_MAPPING,\n",
        "        columns=['region', 'warehouse', 'warehouse_id', 'cohort_id']\n",
        "    )\n",
        "\n",
        "\n",
        "def get_cohort_by_region(region):\n",
        "    \"\"\"Get cohort ID for a given region.\"\"\"\n",
        "    return REGION_COHORT_MAPPING.get(region)\n",
        "\n",
        "\n",
        "def convert_to_numeric(df):\n",
        "    \"\"\"Convert DataFrame columns to numeric where possible.\"\"\"\n",
        "    df.columns = df.columns.str.lower()\n",
        "    for col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='ignore')\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get Snowflake Timezone\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TIMEZONE = get_snowflake_timezone()\n",
        "print(f\"Snowflake timezone: {TIMEZONE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Market Prices Extraction Queries\n",
        "Queries for external market price data:\n",
        "1. **Ben Soliman Prices** - Competitor reference prices\n",
        "2. **Marketplace Prices** - Min, Max, Mod prices from marketplace\n",
        "3. **Scrapped Data** - Competitor prices from scraping\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 1. BEN SOLIMAN PRICES QUERY\n",
        "# =============================================================================\n",
        "BEN_SOLIMAN_QUERY = f'''\n",
        "WITH lower as (\n",
        "    select distinct product_id, sku, new_d*bs_price as ben_soliman_price, INJECTION_DATE\n",
        "    from (\n",
        "        select maxab_product_id as product_id, maxab_sku as sku, INJECTION_DATE, wac1, wac_p,\n",
        "            (bs_price/bs_unit_count) as bs_price, diff, cu_price,\n",
        "            case when p1 > 1 then child_quantity else 0 end as scheck,\n",
        "            round(p1/2)*2 as p1, p2,\n",
        "            case when (ROUND(p1 / scheck) * scheck) = 0 then p1 else (ROUND(p1 / scheck) * scheck) end as new_d\n",
        "        from (\n",
        "            select sm.*, wac1, wac_p, \n",
        "                abs((bs_price/bs_unit_count)-(wac_p*maxab_basic_unit_count))/(wac_p*maxab_basic_unit_count) as diff,\n",
        "                cpc.price as cu_price, pup.child_quantity,\n",
        "                round((cu_price/(bs_price/bs_unit_count))) as p1, \n",
        "                round(((bs_price/bs_unit_count)/cu_price)) as p2\n",
        "            from materialized_views.savvy_mapping sm \n",
        "            join finance.all_cogs f on f.product_id = sm.maxab_product_id \n",
        "                and CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP()) between f.from_Date and f.to_date\n",
        "            join PACKING_UNIT_PRODUCTS pu on pu.product_id = sm.maxab_product_id and pu.IS_BASIC_UNIT = 1 \n",
        "            join cohort_product_packing_units cpc on cpc.PRODUCT_PACKING_UNIT_ID = pu.id and cohort_id = 700 \n",
        "            join packing_unit_products pup on pup.product_id = sm.maxab_product_id and pup.is_basic_unit = 1  \n",
        "            where bs_price is not null \n",
        "                and INJECTION_DATE::date >= CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - 5 \n",
        "                and diff > 0.3 and p1 > 1\n",
        "        )\n",
        "    )\n",
        "    qualify max(INJECTION_DATE) over(partition by product_id) = INJECTION_DATE\n",
        "),\n",
        "\n",
        "m_bs as (\n",
        "    select z.* from (\n",
        "        select maxab_product_id as product_id, maxab_sku as sku, avg(bs_final_price) as ben_soliman_price, INJECTION_DATE\n",
        "        from (\n",
        "            select *, row_number() over(partition by maxab_product_id order by diff) as rnk_2 \n",
        "            from (\n",
        "                select *, (bs_final_price-wac_p)/wac_p as diff_2 \n",
        "                from (\n",
        "                    select *, bs_price/maxab_basic_unit_count as bs_final_price \n",
        "                    from (\n",
        "                        select *, row_number() over(partition by maxab_product_id, maxab_pu order by diff) as rnk \n",
        "                        from (\n",
        "                            select *, max(INJECTION_DATE::date) over(partition by maxab_product_id, maxab_pu) as max_date\n",
        "                            from (\n",
        "                                select sm.*, wac1, wac_p, \n",
        "                                    abs(bs_price-(wac_p*maxab_basic_unit_count))/(wac_p*maxab_basic_unit_count) as diff \n",
        "                                from materialized_views.savvy_mapping sm \n",
        "                                join finance.all_cogs f on f.product_id = sm.maxab_product_id \n",
        "                                    and CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP()) between f.from_Date and f.to_date\n",
        "                                where bs_price is not null \n",
        "                                    and INJECTION_DATE::date >= CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - 5 \n",
        "                                    and diff < 0.3\n",
        "                            )\n",
        "                            qualify max_date = INJECTION_DATE\n",
        "                        ) qualify rnk = 1 \n",
        "                    )\n",
        "                ) where diff_2 between -0.5 and 0.5 \n",
        "            ) qualify rnk_2 = 1 \n",
        "        ) group by all\n",
        "    ) z \n",
        "    join finance.all_cogs f on f.product_id = z.product_id \n",
        "        and CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP()) between f.from_Date and f.to_date\n",
        "    where ben_soliman_price between f.wac_p*0.8 and f.wac_p*1.3\n",
        ")\n",
        "\n",
        "select product_id, avg(ben_soliman_price) as ben_soliman_price\n",
        "from (\n",
        "    select * from (\n",
        "        select * from m_bs \n",
        "        union all\n",
        "        select * from lower\n",
        "    )\n",
        "    qualify max(INJECTION_DATE) over(partition by product_id) = INJECTION_DATE\n",
        ")\n",
        "group by all\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 2. MARKETPLACE PRICES QUERY (with region fallback)\n",
        "# =============================================================================\n",
        "MARKETPLACE_PRICES_QUERY = f'''\n",
        "WITH MP as (\n",
        "    select region, product_id,\n",
        "        min(min_price) as min_price, min(max_price) as max_price,\n",
        "        min(mod_price) as mod_price, min(true_min) as true_min, min(true_max) as true_max\n",
        "    from (\n",
        "        select mp.region, mp.product_id, mp.pu_id,\n",
        "            min_price/BASIC_UNIT_COUNT as min_price,\n",
        "            max_price/BASIC_UNIT_COUNT as max_price,\n",
        "            mod_price/BASIC_UNIT_COUNT as mod_price,\n",
        "            TRUE_MIN_PRICE/BASIC_UNIT_COUNT as true_min,\n",
        "            TRUE_MAX_PRICE/BASIC_UNIT_COUNT as true_max\n",
        "        from materialized_views.marketplace_prices mp \n",
        "        join packing_unit_products pup on pup.product_id = mp.product_id and pup.packing_unit_id = mp.pu_id\n",
        "        join finance.all_cogs f on f.product_id = mp.product_id \n",
        "            and CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP()) between f.from_date and f.to_date\n",
        "        where least(min_price, mod_price) between wac_p*0.9 and wac_p*1.3 \n",
        "    )\n",
        "    group by all \n",
        "),\n",
        "\n",
        "region_mapping AS (\n",
        "    SELECT * FROM (VALUES\n",
        "        ('Delta East', 'Delta West'), ('Delta West', 'Delta East'),\n",
        "        ('Alexandria', 'Cairo'), ('Alexandria', 'Giza'),\n",
        "        ('Upper Egypt', 'Cairo'), ('Upper Egypt', 'Giza'),\n",
        "        ('Cairo', 'Giza'), ('Giza', 'Cairo'),\n",
        "        ('Delta West', 'Cairo'), ('Delta East', 'Cairo'),\n",
        "        ('Delta West', 'Giza'), ('Delta East', 'Giza')\n",
        "    ) AS region_mapping(region, fallback_region)\n",
        "),\n",
        "\n",
        "all_regions as (\n",
        "    SELECT * FROM (VALUES\n",
        "        ('Cairo'), ('Giza'), ('Delta West'), ('Delta East'), ('Upper Egypt'), ('Alexandria')\n",
        "    ) AS x(region)\n",
        "),\n",
        "\n",
        "full_data as (\n",
        "    select products.id as product_id, ar.region\n",
        "    from products, all_regions ar\n",
        "    where activation = 'true'\n",
        ")\n",
        "\n",
        "select region, product_id,\n",
        "    min(final_min_price) as final_min_price, \n",
        "    min(final_max_price) as final_max_price,\n",
        "    min(final_mod_price) as final_mod_price, \n",
        "    min(final_true_min) as final_true_min,\n",
        "    min(final_true_max) as final_true_max\n",
        "from (\n",
        "    SELECT distinct w.region, w.product_id,\n",
        "        COALESCE(m1.min_price, m2.min_price) AS final_min_price,\n",
        "        COALESCE(m1.max_price, m2.max_price) AS final_max_price,\n",
        "        COALESCE(m1.mod_price, m2.mod_price) AS final_mod_price,\n",
        "        COALESCE(m1.true_min, m2.true_min) AS final_true_min,\n",
        "        COALESCE(m1.true_max, m2.true_max) AS final_true_max\n",
        "    FROM full_data w\n",
        "    LEFT JOIN MP m1 ON w.region = m1.region and w.product_id = m1.product_id\n",
        "    LEFT JOIN region_mapping rm ON w.region = rm.region\n",
        "    LEFT JOIN MP m2 ON rm.fallback_region = m2.region AND w.product_id = m2.product_id\n",
        ")\n",
        "where final_min_price is not null \n",
        "group by all\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 3. SCRAPPED DATA QUERY (Competitor prices from scraping)\n",
        "# =============================================================================\n",
        "SCRAPPED_DATA_QUERY = f'''\n",
        "select product_id, region,\n",
        "    MIN(market_price) AS min_scrapped,\n",
        "    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY market_price) AS scrapped25,\n",
        "    PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY market_price) AS scrapped50,\n",
        "    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY market_price) AS scrapped75,\n",
        "    MAX(market_price) AS max_scrapped\n",
        "from (\n",
        "    select distinct cmp.*, max(date) over(partition by region, cmp.product_id, competitor) as max_date\n",
        "    from MATERIALIZED_VIEWS.CLEANED_MARKET_PRICES cmp\n",
        "    join finance.all_cogs f on f.product_id = cmp.product_id \n",
        "        and CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP()) between f.from_date and f.to_date \n",
        "    where date >= CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - 7 \n",
        "        and MARKET_PRICE between f.wac_p * 0.8 and wac_p * 1.3\n",
        "    qualify date = max_date \n",
        ")\n",
        "group by all\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Additional Data Queries (Sales, Groups, WAC)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 4. PRODUCT BASE DATA QUERY (product_id, sku, brand, cat, wac1, wac_p, current_price)\n",
        "# =============================================================================\n",
        "PRODUCT_BASE_QUERY = f'''\n",
        "WITH skus_prices AS (\n",
        "    WITH local_prices AS (\n",
        "        SELECT  \n",
        "            CASE \n",
        "                WHEN cpu.cohort_id IN (700, 695) THEN 'Cairo'\n",
        "                WHEN cpu.cohort_id IN (701) THEN 'Giza'\n",
        "                WHEN cpu.cohort_id IN (704, 698) THEN 'Delta East'\n",
        "                WHEN cpu.cohort_id IN (703, 697) THEN 'Delta West'\n",
        "                WHEN cpu.cohort_id IN (696, 1123, 1124, 1125, 1126) THEN 'Upper Egypt'\n",
        "                WHEN cpu.cohort_id IN (702, 699) THEN 'Alexandria'\n",
        "            END AS region,\n",
        "            cohort_id,\n",
        "            pu.product_id,\n",
        "            pu.packing_unit_id,\n",
        "            pu.basic_unit_count,\n",
        "            AVG(cpu.price) AS price\n",
        "        FROM cohort_product_packing_units cpu\n",
        "        JOIN PACKING_UNIT_PRODUCTS pu ON pu.id = cpu.product_packing_unit_id\n",
        "        WHERE cpu.cohort_id IN (700,701,702,703,704,695,696,697,698,699,1123,1124,1125,1126)\n",
        "            AND cpu.created_at::date <> '2023-07-31'\n",
        "            AND cpu.is_customized = TRUE\n",
        "        GROUP BY ALL\n",
        "    ),\n",
        "    \n",
        "    live_prices AS (\n",
        "        SELECT \n",
        "            region, cohort_id, product_id, \n",
        "            pu_id AS packing_unit_id, \n",
        "            buc AS basic_unit_count, \n",
        "            NEW_PRICE AS price\n",
        "        FROM materialized_views.DBDP_PRICES\n",
        "        WHERE created_at = CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date\n",
        "            AND DATE_PART('hour', CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::time) \n",
        "                BETWEEN SPLIT_PART(time_slot, '-', 1)::int AND (SPLIT_PART(time_slot, '-', 1)::int) + 1\n",
        "            AND cohort_id IN (700,701,702,703,704,695,696,697,698,699,1123,1124,1125,1126)\n",
        "    ),\n",
        "    \n",
        "    prices AS (\n",
        "        SELECT *\n",
        "        FROM (\n",
        "            SELECT *, 1 AS priority FROM live_prices\n",
        "            UNION ALL\n",
        "            SELECT *, 2 AS priority FROM local_prices\n",
        "        )\n",
        "        QUALIFY ROW_NUMBER() OVER (PARTITION BY region, cohort_id, product_id, packing_unit_id ORDER BY priority) = 1\n",
        "    )\n",
        "    \n",
        "    SELECT region, cohort_id, product_id, price\n",
        "    FROM prices\n",
        "    WHERE basic_unit_count = 1\n",
        "        AND ((product_id = 1309 AND packing_unit_id = 2) OR (product_id <> 1309))\n",
        ")\n",
        "\n",
        "SELECT distinct\n",
        "    region, cohort_id, p.product_id,\n",
        "    CONCAT(products.name_ar, ' ', products.size, ' ', product_units.name_ar) AS sku,\n",
        "    b.name_ar AS brand,\n",
        "    cat.name_ar AS cat,\n",
        "    wac1, wac_p, p.price as current_price\n",
        "FROM skus_prices p\n",
        "JOIN finance.all_cogs c ON c.product_id = p.product_id \n",
        "    AND CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP()) BETWEEN c.from_date AND c.to_date\n",
        "JOIN products ON products.id = p.product_id\n",
        "JOIN categories cat ON cat.id = products.category_id\n",
        "JOIN brands b ON b.id = products.brand_id\n",
        "JOIN product_units ON product_units.id = products.unit_id\n",
        "WHERE wac1 > 0 AND wac_p > 0\n",
        "GROUP BY ALL\n",
        "'''\n",
        "\n",
        "# =============================================================================\n",
        "# 5. SALES DATA QUERY (120-day NMV by cohort/product)\n",
        "# =============================================================================\n",
        "SALES_QUERY = f'''\n",
        "SELECT DISTINCT cpc.cohort_id, pso.product_id,\n",
        "    CONCAT(products.name_ar,' ',products.size,' ',product_units.name_ar) as sku,\n",
        "    brands.name_ar as brand, categories.name_ar as cat,\n",
        "    sum(pso.total_price) as nmv\n",
        "FROM product_sales_order pso\n",
        "JOIN sales_orders so ON so.id = pso.sales_order_id\n",
        "JOIN COHORT_PRICING_CHANGES cpc ON cpc.id = pso.COHORT_PRICING_CHANGE_id\n",
        "JOIN products ON products.id = pso.product_id\n",
        "JOIN brands ON products.brand_id = brands.id \n",
        "JOIN categories ON products.category_id = categories.id\n",
        "JOIN product_units ON product_units.id = products.unit_id \n",
        "WHERE so.created_at::date BETWEEN CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - 120 \n",
        "    AND CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - 1 \n",
        "    AND so.sales_order_status_id NOT IN (7, 12)\n",
        "    AND so.channel IN ('telesales', 'retailer')\n",
        "    AND pso.purchased_item_count <> 0\n",
        "    AND cpc.cohort_id IN (700,701,702,703,704,1123,1124,1125,1126)\n",
        "GROUP BY ALL\n",
        "'''\n",
        "\n",
        "# =============================================================================\n",
        "# 6. MARGIN STATS QUERY (STD and average margins)  \n",
        "# =============================================================================\n",
        "MARGIN_STATS_QUERY = f'''\n",
        "select product_id, cohort_id, \n",
        "    (0.6*product_std) + (0.3*brand_std) + (0.1*cat_std) as std, \n",
        "    avg_margin\n",
        "from (\n",
        "    select product_id, cohort_id, \n",
        "        stddev(product_margin) as product_std, \n",
        "        stddev(brand_margin) as brand_std,\n",
        "        stddev(cat_margin) as cat_std, \n",
        "        avg(product_margin) as avg_margin\n",
        "    from (\n",
        "        select distinct product_id, order_date, cohort_id,\n",
        "            (nmv-cogs_p)/nmv as product_margin, \n",
        "            (brand_nmv-brand_cogs)/brand_nmv as brand_margin,\n",
        "            (cat_nmv-cat_cogs)/cat_nmv as cat_margin\n",
        "        from (\n",
        "            SELECT DISTINCT so.created_at::date as order_date, cpc.cohort_id, pso.product_id,\n",
        "                brands.name_ar as brand, categories.name_ar as cat,\n",
        "                sum(COALESCE(f.wac_p,0) * pso.purchased_item_count * pso.basic_unit_count) as cogs_p,\n",
        "                sum(pso.total_price) as nmv,\n",
        "                sum(nmv) over(partition by order_date, cat, brand) as brand_nmv,\n",
        "                sum(cogs_p) over(partition by order_date, cat, brand) as brand_cogs,\n",
        "                sum(nmv) over(partition by order_date, cat) as cat_nmv,\n",
        "                sum(cogs_p) over(partition by order_date, cat) as cat_cogs\n",
        "            FROM product_sales_order pso\n",
        "            JOIN sales_orders so ON so.id = pso.sales_order_id   \n",
        "            JOIN COHORT_PRICING_CHANGES cpc on cpc.id = pso.cohort_pricing_change_id\n",
        "            JOIN products on products.id = pso.product_id\n",
        "            JOIN brands on products.brand_id = brands.id \n",
        "            JOIN categories ON products.category_id = categories.id\n",
        "            JOIN finance.all_cogs f ON f.product_id = pso.product_id\n",
        "                AND f.from_date::date <= so.created_at::date AND f.to_date::date > so.created_at::date\n",
        "            WHERE so.created_at::date between \n",
        "                date_trunc('month', CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - 120) \n",
        "                and CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date\n",
        "                AND so.sales_order_status_id not in (7,12)\n",
        "                AND so.channel IN ('telesales','retailer')\n",
        "                AND pso.purchased_item_count <> 0\n",
        "            GROUP BY ALL\n",
        "        )\n",
        "    ) group by all \n",
        ")\n",
        "'''\n",
        "\n",
        "# =============================================================================\n",
        "# 7. TARGET MARGINS QUERY\n",
        "# =============================================================================\n",
        "TARGET_MARGINS_QUERY = f'''\n",
        "WITH cat_brand_target as (\n",
        "    SELECT DISTINCT cat, brand, margin as target_bm\n",
        "    FROM performance.commercial_targets cplan\n",
        "    QUALIFY CASE \n",
        "        WHEN DATE_TRUNC('month', MAX(DATE) OVER()) = DATE_TRUNC('month', CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date) \n",
        "        THEN DATE_TRUNC('month', CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date)\n",
        "        ELSE DATE_TRUNC('month', CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - INTERVAL '1 month') \n",
        "    END = DATE_TRUNC('month', date)\n",
        "),\n",
        "cat_target as (\n",
        "    select cat, sum(target_bm * (target_nmv/cat_total)) as cat_target_margin\n",
        "    from (\n",
        "        select *, sum(target_nmv) over(partition by cat) as cat_total\n",
        "        from (\n",
        "            select cat, brand, avg(target_bm) as target_bm, sum(target_nmv) as target_nmv\n",
        "            from (\n",
        "                SELECT DISTINCT date, city as region, cat, brand, margin as target_bm, nmv as target_nmv\n",
        "                FROM performance.commercial_targets cplan\n",
        "                QUALIFY CASE \n",
        "                    WHEN DATE_TRUNC('month', MAX(DATE) OVER()) = DATE_TRUNC('month', CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date) \n",
        "                    THEN DATE_TRUNC('month', CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date)\n",
        "                    ELSE DATE_TRUNC('month', CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - INTERVAL '1 month') \n",
        "                END = DATE_TRUNC('month', date)\n",
        "            ) group by all\n",
        "        )\n",
        "    ) group by all \n",
        ")\n",
        "SELECT DISTINCT cbt.cat, cbt.brand, cbt.target_bm, ct.cat_target_margin\n",
        "FROM cat_brand_target cbt\n",
        "LEFT JOIN cat_target ct ON ct.cat = cbt.cat\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Execute All Queries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Execute all queries\n",
        "# =============================================================================\n",
        "print(\"Loading data from Snowflake...\")\n",
        "\n",
        "# 1. Ben Soliman Prices\n",
        "print(\"  1. Loading Ben Soliman prices...\")\n",
        "df_ben_soliman = query_snowflake(BEN_SOLIMAN_QUERY)\n",
        "df_ben_soliman = convert_to_numeric(df_ben_soliman)\n",
        "print(f\"     Loaded {len(df_ben_soliman)} Ben Soliman price records\")\n",
        "\n",
        "# 2. Marketplace Prices\n",
        "print(\"  2. Loading marketplace prices...\")\n",
        "df_marketplace = query_snowflake(MARKETPLACE_PRICES_QUERY)\n",
        "df_marketplace = convert_to_numeric(df_marketplace)\n",
        "print(f\"     Loaded {len(df_marketplace)} marketplace price records\")\n",
        "\n",
        "# 3. Scrapped Data\n",
        "print(\"  3. Loading scrapped data...\")\n",
        "df_scrapped = query_snowflake(SCRAPPED_DATA_QUERY)\n",
        "df_scrapped = convert_to_numeric(df_scrapped)\n",
        "print(f\"     Loaded {len(df_scrapped)} scrapped price records\")\n",
        "\n",
        "# 4. Product Base Data (product_id, sku, brand, cat, wac1, wac_p, current_price)\n",
        "print(\"  4. Loading product base data...\")\n",
        "df_product_base = query_snowflake(PRODUCT_BASE_QUERY)\n",
        "df_product_base = convert_to_numeric(df_product_base)\n",
        "print(f\"     Loaded {len(df_product_base)} product base records\")\n",
        "\n",
        "# 5. Sales Data\n",
        "print(\"  5. Loading sales data...\")\n",
        "df_sales = query_snowflake(SALES_QUERY)\n",
        "df_sales = convert_to_numeric(df_sales)\n",
        "print(f\"     Loaded {len(df_sales)} sales records\")\n",
        "\n",
        "# 6. Margin Stats\n",
        "print(\"  6. Loading margin stats...\")\n",
        "df_margin_stats = query_snowflake(MARGIN_STATS_QUERY)\n",
        "df_margin_stats = convert_to_numeric(df_margin_stats)\n",
        "print(f\"     Loaded {len(df_margin_stats)} margin stat records\")\n",
        "\n",
        "# 7. Target Margins\n",
        "print(\"  7. Loading target margins...\")\n",
        "df_targets = query_snowflake(TARGET_MARGINS_QUERY)\n",
        "df_targets = convert_to_numeric(df_targets)\n",
        "print(f\"     Loaded {len(df_targets)} target margin records\")\n",
        "\n",
        "# 8. Product Groups (from PostgreSQL)\n",
        "print(\"  8. Loading product groups...\")\n",
        "df_groups = setup_environment_2.dwh_pg_query(\n",
        "    \"SELECT * FROM materialized_views.sku_commercial_groups\", \n",
        "    columns=['product_id', 'group']\n",
        ")\n",
        "df_groups.columns = df_groups.columns.str.lower()\n",
        "df_groups = convert_to_numeric(df_groups)\n",
        "print(f\"     Loaded {len(df_groups)} group records\")\n",
        "\n",
        "print(\"\\nAll queries completed!\")\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"df_product_base DataFrame available with columns:\")\n",
        "print(\"  - region, cohort_id, product_id, sku, brand, cat, wac1, wac_p, current_price\")\n",
        "print(f\"{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Build base pricing_data DataFrame - Start with PRODUCT_BASE (all our SKUs)\n",
        "# =============================================================================\n",
        "print(\"Building pricing_data DataFrame...\")\n",
        "\n",
        "# =============================================================================\n",
        "# Step 1: Start with df_product_base as the MAIN dataframe (all our SKUs)\n",
        "# =============================================================================\n",
        "print(\"  Step 1: Starting with product base (all SKUs)...\")\n",
        "pricing_data = df_product_base.copy()\n",
        "print(f\"     Product base: {len(pricing_data)} records\")\n",
        "\n",
        "# =============================================================================\n",
        "# Step 2: Add warehouse mapping (warehouse_id and warehouse name)\n",
        "# =============================================================================\n",
        "print(\"  Step 2: Adding warehouse mapping...\")\n",
        "warehouse_df = get_warehouse_df()\n",
        "pricing_data = pricing_data.merge(\n",
        "    warehouse_df[['cohort_id', 'warehouse_id', 'warehouse']], \n",
        "    on='cohort_id', \n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# =============================================================================\n",
        "# Step 3: LEFT JOIN market price data (marketplace, scrapped, ben_soliman)\n",
        "# =============================================================================\n",
        "print(\"  Step 3: Left joining market price data...\")\n",
        "\n",
        "# LEFT JOIN marketplace prices (by region + product_id)\n",
        "pricing_data = pricing_data.merge(\n",
        "    df_marketplace, \n",
        "    on=['region', 'product_id'], \n",
        "    how='left'\n",
        ")\n",
        "print(f\"     After marketplace join: {len(pricing_data)} records\")\n",
        "\n",
        "# LEFT JOIN scrapped data (by region + product_id)\n",
        "pricing_data = pricing_data.merge(\n",
        "    df_scrapped, \n",
        "    on=['region', 'product_id'], \n",
        "    how='left'\n",
        ")\n",
        "print(f\"     After scrapped join: {len(pricing_data)} records\")\n",
        "\n",
        "# LEFT JOIN Ben Soliman prices (by product_id only - same price for all regions)\n",
        "pricing_data = pricing_data.merge(\n",
        "    df_ben_soliman, \n",
        "    on='product_id', \n",
        "    how='left'\n",
        ")\n",
        "print(f\"     After Ben Soliman join: {len(pricing_data)} records\")\n",
        "\n",
        "# =============================================================================\n",
        "# Step 4: LEFT JOIN supporting data (sales, margins, targets, groups)\n",
        "# =============================================================================\n",
        "print(\"  Step 4: Left joining supporting data...\")\n",
        "\n",
        "# Merge sales data (nmv only)\n",
        "pricing_data = pricing_data.merge(\n",
        "    df_sales[['cohort_id', 'product_id', 'nmv']], \n",
        "    on=['cohort_id', 'product_id'], \n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Fill nmv with 0 for products without sales\n",
        "pricing_data['nmv'] = pricing_data['nmv'].fillna(0)\n",
        "\n",
        "# Merge margin statistics (by cohort_id + product_id)\n",
        "pricing_data = pricing_data.merge(df_margin_stats, on=['cohort_id', 'product_id'], how='left')\n",
        "\n",
        "# Merge target margins (by brand + cat)\n",
        "pricing_data = pricing_data.merge(df_targets, on=['brand', 'cat'], how='left')\n",
        "\n",
        "# Calculate target_margin (use brand target first, then category target)\n",
        "pricing_data['target_margin'] = pricing_data['target_bm'].fillna(pricing_data['cat_target_margin']).fillna(0)\n",
        "pricing_data = pricing_data.drop(columns=['target_bm', 'cat_target_margin'], errors='ignore')\n",
        "\n",
        "# Fill NaN values with defaults\n",
        "pricing_data['std'] = pricing_data['std'].fillna(0.01)\n",
        "pricing_data['avg_margin'] = pricing_data['avg_margin'].fillna(0)\n",
        "\n",
        "# Merge product groups\n",
        "pricing_data = pricing_data.merge(df_groups, on='product_id', how='left')\n",
        "\n",
        "# Remove duplicates\n",
        "pricing_data = pricing_data.drop_duplicates(subset=['cohort_id', 'product_id'])\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"PRICING DATA BASE COMPLETE\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Total records: {len(pricing_data)}\")\n",
        "print(f\"\\nRecords with market data:\")\n",
        "print(f\"  - With marketplace prices: {len(pricing_data[~pricing_data['final_min_price'].isna()])}\")\n",
        "print(f\"  - With scrapped prices: {len(pricing_data[~pricing_data['min_scrapped'].isna()])}\")\n",
        "print(f\"  - With Ben Soliman prices: {len(pricing_data[~pricing_data['ben_soliman_price'].isna()])}\")\n",
        "print(f\"\\nRecords with sales (nmv > 0): {len(pricing_data[pricing_data['nmv'] > 0])}\")\n",
        "print(f\"Records without sales (nmv = 0): {len(pricing_data[pricing_data['nmv'] == 0])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Group Processing - Fill Missing Prices with Group Medians"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Group Processing - Calculate group-level aggregated prices\n",
        "# =============================================================================\n",
        "\n",
        "# Calculate group-level aggregated prices for products with group assignments\n",
        "groups_data = pricing_data[~pricing_data['group'].isna()].copy()\n",
        "groups_data['group_nmv'] = groups_data.groupby(['group', 'cohort_id'])['nmv'].transform('sum')\n",
        "groups_data['cntrb'] = (groups_data['nmv'] / groups_data['group_nmv']).fillna(1)\n",
        "\n",
        "# Flag if any price/scrapped column is non-NaN\n",
        "price_cols = [\n",
        "    'ben_soliman_price', 'final_min_price', 'final_max_price', 'final_mod_price', 'final_true_min', 'final_true_max',\n",
        "    'min_scrapped', 'scrapped25', 'scrapped50', 'scrapped75', 'max_scrapped'\n",
        "]\n",
        "groups_data['flag_non_nan'] = groups_data[price_cols].notna().any(axis=1).astype(int)\n",
        "\n",
        "# Weighted Median Function\n",
        "def weighted_median(series, weights):\n",
        "    valid = ~series.isna() & ~weights.isna()\n",
        "    s = series[valid]\n",
        "    w = weights[valid]\n",
        "    if len(s) == 0:\n",
        "        return np.nan\n",
        "    order = np.argsort(s)\n",
        "    s, w = s.iloc[order], w.iloc[order]\n",
        "    return s.iloc[np.searchsorted(np.cumsum(w), w.sum() / 2)]\n",
        "\n",
        "# Perform Weighted Aggregation\n",
        "groups_agg = (\n",
        "    groups_data[groups_data['flag_non_nan'] == 1]\n",
        "    .groupby(['group', 'cohort_id'])\n",
        "    .apply(lambda g: pd.Series({\n",
        "        col: weighted_median(g[col], g['cntrb']) for col in price_cols\n",
        "    }))\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Fill missing prices with group-level prices\n",
        "merged = pricing_data.merge(groups_agg, on=['group', 'cohort_id'], how='left', suffixes=('', '_group'))\n",
        "for col in price_cols:\n",
        "    merged[col] = merged[col].fillna(merged[f'{col}_group'])\n",
        "\n",
        "pricing_data = merged.drop(columns=[f'{c}_group' for c in price_cols])\n",
        "\n",
        "print(f\"Pricing data after group processing: {len(pricing_data)} records\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Price Coverage Filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Price Coverage Filtering - Filter products with sufficient price data\n",
        "# =============================================================================\n",
        "\n",
        "# Score price coverage\n",
        "pricing_data['ben'] = 0\n",
        "pricing_data['MP'] = 0\n",
        "pricing_data['sp'] = 0\n",
        "\n",
        "# Ben Soliman: 1 point if present\n",
        "pricing_data.loc[~pricing_data['ben_soliman_price'].isna(), 'ben'] = 1\n",
        "\n",
        "# Marketplace: 1 point if single price, 3 points if range\n",
        "pricing_data.loc[(pricing_data['final_min_price'] == pricing_data['final_max_price']) & \n",
        "                (~pricing_data['final_min_price'].isna()), 'MP'] = 1\n",
        "pricing_data.loc[(pricing_data['final_min_price'] != pricing_data['final_max_price']) & \n",
        "                (~pricing_data['final_min_price'].isna()), 'MP'] = 3\n",
        "\n",
        "# Scrapped: 1 point if single price, 5 points if range\n",
        "pricing_data.loc[(pricing_data['min_scrapped'] == pricing_data['max_scrapped']) & \n",
        "                (~pricing_data['min_scrapped'].isna()), 'sp'] = 1\n",
        "pricing_data.loc[(pricing_data['min_scrapped'] != pricing_data['max_scrapped']) & \n",
        "                (~pricing_data['min_scrapped'].isna()), 'sp'] = 5\n",
        "\n",
        "# Total price coverage score\n",
        "pricing_data['total_p'] = pricing_data['ben'] + pricing_data['MP'] + pricing_data['sp']\n",
        "\n",
        "# Filter: keep only products with total_p > 2\n",
        "pricing_data = pricing_data[pricing_data['total_p'] > 2]\n",
        "\n",
        "print(f\"Pricing data after price coverage filtering: {len(pricing_data)} records\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Price Analysis & Margin Calculation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Price Analysis Functions\n",
        "# =============================================================================\n",
        "\n",
        "def price_analysis(row):\n",
        "    \"\"\"Analyze prices and calculate percentiles for a product.\"\"\"\n",
        "    wac = row['wac_p']\n",
        "    avg_margin = row['avg_margin'] if row['avg_margin'] >= 0.01 else row['target_margin']\n",
        "    std = np.maximum(row['std'], 0.0025)\n",
        "    target_margin = row['target_margin']\n",
        "    max_marg = np.maximum(avg_margin, target_margin)\n",
        "    \n",
        "    # Collect all price points\n",
        "    price_list = [\n",
        "        row['ben_soliman_price'], row['final_min_price'], row['final_mod_price'],\n",
        "        row['final_max_price'], row['final_true_min'], row['final_true_max'],\n",
        "        row['min_scrapped'], row['scrapped25'], row['scrapped50'], row['scrapped75'], row['max_scrapped']\n",
        "    ]\n",
        "    \n",
        "    # Filter valid prices within acceptable range\n",
        "    valid_prices = sorted({\n",
        "        x for x in price_list \n",
        "        if x and not pd.isna(x) and x != 0 \n",
        "        and wac / (1 - (avg_margin - (10 * std))) <= x <= wac / (1 - (max_marg + 10 * std))\n",
        "        and x >= wac * 0.9\n",
        "    })\n",
        "    \n",
        "    if not valid_prices:\n",
        "        return np.nan, np.nan, np.nan, np.nan, np.nan\n",
        "    \n",
        "    return (\n",
        "        np.min(valid_prices),\n",
        "        np.percentile(valid_prices, 25),\n",
        "        np.percentile(valid_prices, 50),\n",
        "        np.percentile(valid_prices, 75),\n",
        "        np.max(valid_prices)\n",
        "    )\n",
        "\n",
        "\n",
        "def calculate_step_bounds(row):\n",
        "    \"\"\"Calculate below/above market bounds based on price steps.\"\"\"\n",
        "    wac = row['wac_p']\n",
        "    std = row['std']\n",
        "    prices = [row['minimum'], row['percentile_25'], row['percentile_50'], row['percentile_75'], row['maximum']]\n",
        "    \n",
        "    # Calculate valid steps between price points\n",
        "    valid_steps = []\n",
        "    for i in range(len(prices) - 1):\n",
        "        step = prices[i + 1] - prices[i]\n",
        "        if (step / wac) <= std * 1.2:\n",
        "            valid_steps.append(step)\n",
        "    \n",
        "    avg_step = np.mean(valid_steps) if valid_steps else min(2 * std, 0.2 * row['target_margin'])\n",
        "    \n",
        "    new_min = prices[0] - avg_step if (prices[0] - avg_step) >= wac else prices[0]\n",
        "    new_max = prices[-1] + avg_step if (prices[-1] + avg_step) >= wac else prices[-1]\n",
        "    \n",
        "    return new_min, new_max\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Apply Price Analysis & Margin Calculation\n",
        "# =============================================================================\n",
        "\n",
        "# Apply price analysis to calculate price percentiles\n",
        "pricing_data[['minimum', 'percentile_25', 'percentile_50', 'percentile_75', 'maximum']] = \\\n",
        "    pricing_data.apply(price_analysis, axis=1, result_type='expand')\n",
        "\n",
        "# Filter out records without valid price analysis\n",
        "pricing_data = pricing_data[~pricing_data['minimum'].isna()]\n",
        "\n",
        "# Calculate below/above market bounds\n",
        "pricing_data[['below_market', 'above_market']] = pricing_data.apply(calculate_step_bounds, axis=1, result_type='expand')\n",
        "\n",
        "print(f\"Pricing data after price analysis: {len(pricing_data)} records\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Convert prices to margins\n",
        "# =============================================================================\n",
        "\n",
        "pricing_data['below_market'] = (pricing_data['below_market'] - pricing_data['wac_p']) / pricing_data['below_market']\n",
        "pricing_data['market_min'] = (pricing_data['minimum'] - pricing_data['wac_p']) / pricing_data['minimum']\n",
        "pricing_data['market_25'] = (pricing_data['percentile_25'] - pricing_data['wac_p']) / pricing_data['percentile_25']\n",
        "pricing_data['market_50'] = (pricing_data['percentile_50'] - pricing_data['wac_p']) / pricing_data['percentile_50']\n",
        "pricing_data['market_75'] = (pricing_data['percentile_75'] - pricing_data['wac_p']) / pricing_data['percentile_75']\n",
        "pricing_data['market_max'] = (pricing_data['maximum'] - pricing_data['wac_p']) / pricing_data['maximum']\n",
        "pricing_data['above_market'] = (pricing_data['above_market'] - pricing_data['wac_p']) / pricing_data['above_market']\n",
        "\n",
        "# Calculate current margin from current_price\n",
        "pricing_data['current_margin'] = (pricing_data['current_price'] - pricing_data['wac_p']) / pricing_data['current_price']\n",
        "\n",
        "# Reorder columns to put product base info first\n",
        "final_columns = [\n",
        "    # Product Base Info\n",
        "    'cohort_id', 'product_id', 'region', 'warehouse_id', 'warehouse', 'sku', 'brand', 'cat',\n",
        "    # Cost & Price\n",
        "    'wac1', 'wac_p', 'current_price', 'current_margin',\n",
        "    # Sales\n",
        "    'nmv',\n",
        "    # Market Prices (raw)\n",
        "    'ben_soliman_price', \n",
        "    'final_min_price', 'final_max_price', 'final_mod_price', 'final_true_min', 'final_true_max',\n",
        "    'min_scrapped', 'scrapped25', 'scrapped50', 'scrapped75', 'max_scrapped',\n",
        "    # Price Percentiles\n",
        "    'minimum', 'percentile_25', 'percentile_50', 'percentile_75', 'maximum',\n",
        "    # Margin Tiers\n",
        "    'below_market', 'market_min', 'market_25', 'market_50', 'market_75', 'market_max', 'above_market',\n",
        "    # Supporting Data\n",
        "    'std', 'avg_margin', 'target_margin', 'group'\n",
        "]\n",
        "# Keep only columns that exist in the dataframe\n",
        "pricing_data = pricing_data[[c for c in final_columns if c in pricing_data.columns]]\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"PRICING DATA PROCESSING COMPLETE\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Total records: {len(pricing_data)}\")\n",
        "print(f\"\\nProduct Base columns:\")\n",
        "print(\"  - cohort_id, product_id, region, warehouse_id, warehouse, sku, brand, cat, wac1, wac_p, current_price, current_margin\")\n",
        "print(f\"\\nMargin tier columns:\")\n",
        "print(\"  - below_market, market_min, market_25, market_50, market_75, market_max, above_market\")\n",
        "print(f\"\\nSample data:\")\n",
        "pricing_data.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Discount Analysis - Price & Margin After Discount\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Discount Query - Get discount percentage by warehouse and product\n",
        "# =============================================================================\n",
        "DISCOUNT_QUERY = f'''\n",
        "SELECT warehouse_id, product_id, total_discount/total_nmv AS discount_perc\n",
        "FROM (\n",
        "    SELECT  \n",
        "        pso.warehouse_id,\n",
        "        pso.product_id,\n",
        "        SUM(pso.total_price) AS total_nmv,\n",
        "        SUM((ITEM_QUANTITY_DISCOUNT_VALUE * pso.purchased_item_count) + \n",
        "            (ITEM_DISCOUNT_VALUE * pso.purchased_item_count)) AS total_discount\n",
        "    FROM product_sales_order pso \n",
        "    JOIN sales_orders so ON so.id = pso.sales_order_id\n",
        "    WHERE so.created_at::DATE >= CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE - 1 \n",
        "        AND so.sales_order_status_id NOT IN (7, 12)\n",
        "        AND so.channel IN ('telesales', 'retailer')\n",
        "        AND pso.purchased_item_count <> 0\n",
        "    GROUP BY ALL\n",
        ")\n",
        "WHERE total_nmv > 0\n",
        "'''\n",
        "\n",
        "# Execute discount query\n",
        "print(\"Loading discount data...\")\n",
        "df_discount = query_snowflake(DISCOUNT_QUERY)\n",
        "df_discount = convert_to_numeric(df_discount)\n",
        "print(f\"Loaded {len(df_discount)} discount records\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Create pricing_with_discount DataFrame\n",
        "# =============================================================================\n",
        "print(\"Creating pricing_with_discount DataFrame...\")\n",
        "\n",
        "# Copy pricing_data\n",
        "pricing_with_discount = pricing_data.copy()\n",
        "\n",
        "# Merge discount data (by warehouse_id + product_id)\n",
        "pricing_with_discount = pricing_with_discount.merge(\n",
        "    df_discount[['warehouse_id', 'product_id', 'discount_perc']], \n",
        "    on=['warehouse_id', 'product_id'], \n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Fill missing discount_perc with 0 (no discount)\n",
        "pricing_with_discount['discount_perc'] = pricing_with_discount['discount_perc'].fillna(0)\n",
        "\n",
        "# =============================================================================\n",
        "# Calculate price and margin after discount\n",
        "# =============================================================================\n",
        "# Price after discount = current_price * (1 - discount_perc)\n",
        "pricing_with_discount['price_after_discount'] = (\n",
        "    pricing_with_discount['current_price'] * (1 - pricing_with_discount['discount_perc'])\n",
        ")\n",
        "\n",
        "# Margin after discount = (price_after_discount - wac_p) / price_after_discount\n",
        "pricing_with_discount['margin_after_discount'] = (\n",
        "    (pricing_with_discount['price_after_discount'] - pricing_with_discount['wac_p']) / \n",
        "    pricing_with_discount['price_after_discount']\n",
        ")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"PRICING WITH DISCOUNT DATA COMPLETE\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Total records: {len(pricing_with_discount)}\")\n",
        "print(f\"Records with discount (discount_perc > 0): {len(pricing_with_discount[pricing_with_discount['discount_perc'] > 0])}\")\n",
        "print(f\"Records without discount: {len(pricing_with_discount[pricing_with_discount['discount_perc'] == 0])}\")\n",
        "print(f\"\\nNew columns added:\")\n",
        "print(\"  - discount_perc: discount percentage from sales\")\n",
        "print(\"  - price_after_discount: current_price * (1 - discount_perc)\")\n",
        "print(\"  - margin_after_discount: (price_after_discount - wac_p) / price_after_discount\")\n",
        "print(f\"\\nSample data with discounts:\")\n",
        "pricing_with_discount[pricing_with_discount['discount_perc'] > 0][\n",
        "    ['product_id', 'warehouse_id', 'current_price', 'current_margin', \n",
        "     'discount_perc', 'price_after_discount', 'margin_after_discount']\n",
        "].head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Price Position - Determine where price_after_discount falls in market tiers\n",
        "# =============================================================================\n",
        "\n",
        "def get_price_position(row):\n",
        "    \"\"\"Determine the price position relative to market price tiers.\"\"\"\n",
        "    price = row['price_after_discount']\n",
        "    wac = row['wac_p']\n",
        "    \n",
        "    # Check if we have market data (minimum price exists)\n",
        "    if pd.isna(row['minimum']) or pd.isna(price):\n",
        "        return \"No Market Data\"\n",
        "    \n",
        "    # Get price tiers\n",
        "    min_price = row['minimum']\n",
        "    p25 = row['percentile_25']\n",
        "    p50 = row['percentile_50']\n",
        "    p75 = row['percentile_75']\n",
        "    max_price = row['maximum']\n",
        "    \n",
        "    # Calculate below_market and above_market prices from margins\n",
        "    # margin = (price - wac) / price  =>  price = wac / (1 - margin)\n",
        "    below_market_margin = row['below_market']\n",
        "    above_market_margin = row['above_market']\n",
        "    \n",
        "    below_market_price = wac / (1 - below_market_margin) if below_market_margin < 1 else min_price\n",
        "    above_market_price = wac / (1 - above_market_margin) if above_market_margin < 1 else max_price\n",
        "    \n",
        "    # Determine position based on price tiers\n",
        "    if price < below_market_price:\n",
        "        return \"Below Market\"\n",
        "    elif price < min_price:\n",
        "        return \"Below Min\"\n",
        "    elif price < p25:\n",
        "        return \"At Min\"\n",
        "    elif price < p50:\n",
        "        return \"At 25th\"\n",
        "    elif price < p75:\n",
        "        return \"At 50th\"\n",
        "    elif price < max_price:\n",
        "        return \"At 75th\"\n",
        "    elif price < above_market_price:\n",
        "        return \"At Max\"\n",
        "    else:\n",
        "        return \"Above Market\"\n",
        "\n",
        "# Apply price position function\n",
        "pricing_with_discount['price_position'] = pricing_with_discount.apply(get_price_position, axis=1)\n",
        "\n",
        "# Summary of price positions\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"PRICE POSITION ANALYSIS\")\n",
        "print(f\"{'='*60}\")\n",
        "print(\"\\nPrice Position Distribution:\")\n",
        "print(pricing_with_discount['price_position'].value_counts().to_string())\n",
        "print(f\"\\nPrice Position Percentages:\")\n",
        "print((pricing_with_discount['price_position'].value_counts(normalize=True) * 100).round(2).astype(str) + '%')\n",
        "\n",
        "# Sample data showing price position\n",
        "print(f\"\\nSample data with price position:\")\n",
        "pricing_with_discount[\n",
        "    ['product_id', 'warehouse_id', 'sku', 'current_price', 'discount_perc', \n",
        "     'price_after_discount', 'minimum', 'maximum', 'price_position']\n",
        "].head(15)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Stock Query - Get available stock by warehouse and product\n",
        "# =============================================================================\n",
        "STOCK_QUERY = '''\n",
        "SELECT \n",
        "    pw.warehouse_id,\n",
        "    pw.product_id,\n",
        "    pw.available_stock::INTEGER AS stocks\n",
        "FROM product_warehouse pw\n",
        "WHERE pw.warehouse_id NOT IN (6, 9, 10)\n",
        "    AND pw.is_basic_unit = 1\n",
        "'''\n",
        "\n",
        "# Execute stock query\n",
        "print(\"Loading stock data...\")\n",
        "df_stocks = query_snowflake(STOCK_QUERY)\n",
        "df_stocks = convert_to_numeric(df_stocks)\n",
        "print(f\"Loaded {len(df_stocks)} stock records\")\n",
        "\n",
        "# Merge stock data with pricing_with_discount\n",
        "pricing_with_discount = pricing_with_discount.merge(\n",
        "    df_stocks[['warehouse_id', 'product_id', 'stocks']], \n",
        "    on=['warehouse_id', 'product_id'], \n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Fill missing stocks with 0\n",
        "pricing_with_discount['stocks'] = pricing_with_discount['stocks'].fillna(0).astype(int)\n",
        "\n",
        "print(f\"\\nStock data merged!\")\n",
        "print(f\"Records with stock (stocks > 0): {len(pricing_with_discount[pricing_with_discount['stocks'] > 0])}\")\n",
        "print(f\"Records without stock (stocks = 0): {len(pricing_with_discount[pricing_with_discount['stocks'] == 0])}\")\n",
        "print(f\"\\nSample data with stocks:\")\n",
        "pricing_with_discount[\n",
        "    ['product_id', 'warehouse_id', 'sku', 'stocks', 'price_after_discount', 'price_position']\n",
        "].head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Zero Demand Query - Identify SKUs with zero/low demand\n",
        "# =============================================================================\n",
        "ZERO_DEMAND_QUERY = f'''\n",
        "WITH last_oss AS (\n",
        "    SELECT product_id, warehouse_id, TIMESTAMP AS last_in_stock_day\n",
        "    FROM (\n",
        "        SELECT *, ROW_NUMBER() OVER(PARTITION BY product_id, warehouse_id ORDER BY TIMESTAMP DESC) AS rnk \n",
        "        FROM materialized_views.STOCK_DAY_CLOSE\n",
        "        WHERE AVAILABLE_STOCK = 0 \n",
        "            AND TIMESTAMP >= CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE - 120\n",
        "        QUALIFY rnk = 1 \n",
        "    )\n",
        "),\n",
        "\n",
        "current_stocks AS (\n",
        "    SELECT product_id, warehouse_id, AVAILABLE_STOCK, activation\n",
        "    FROM PRODUCT_WAREHOUSE\n",
        "    WHERE IS_BASIC_UNIT = 1\n",
        "        AND CASE WHEN product_id = 1309 THEN packing_unit_id <> 23 ELSE TRUE END\n",
        "),\n",
        "\n",
        "prs AS (\n",
        "    SELECT DISTINCT \n",
        "        product_purchased_receipts.product_id,\n",
        "        purchased_receipts.warehouse_id,\n",
        "        purchased_receipts.date::DATE AS date,\n",
        "        product_purchased_receipts.purchased_item_count * product_purchased_receipts.basic_unit_count AS purchase_min_count\n",
        "    FROM product_purchased_receipts\n",
        "    JOIN purchased_receipts ON purchased_receipts.id = product_purchased_receipts.purchased_receipt_id\n",
        "    JOIN last_oss lo ON product_purchased_receipts.product_id = lo.product_id \n",
        "        AND lo.warehouse_id = purchased_receipts.warehouse_id \n",
        "        AND purchased_receipts.date > lo.last_in_stock_day \n",
        "    WHERE product_purchased_receipts.purchased_item_count <> 0\n",
        "        AND purchased_receipts.purchased_receipt_status_id IN (4, 5, 7)\n",
        "        AND purchased_receipts.date::DATE >= CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE - 120\n",
        "),\n",
        "\n",
        "main AS (\n",
        "    SELECT \n",
        "        prs.product_id, \n",
        "        prs.warehouse_id, \n",
        "        MIN(date) AS first_order_date, \n",
        "        SUM(purchase_min_count) AS total_recieved, \n",
        "        cs.AVAILABLE_STOCK, \n",
        "        cs.activation\n",
        "    FROM prs \n",
        "    JOIN current_stocks cs ON cs.product_id = prs.product_id AND prs.warehouse_id = cs.warehouse_id\n",
        "    GROUP BY prs.product_id, prs.warehouse_id, cs.AVAILABLE_STOCK, cs.activation\n",
        "),\n",
        "\n",
        "sold_days AS (\n",
        "    SELECT product_id, warehouse_id, COUNT(DISTINCT o_date) AS sales_days\n",
        "    FROM (\n",
        "        SELECT DISTINCT\n",
        "            so.created_at::DATE AS o_date,\n",
        "            pso.warehouse_id,\n",
        "            pso.product_id,\n",
        "            SUM(pso.purchased_item_count * basic_unit_count) AS daily_qty\n",
        "        FROM product_sales_order pso\n",
        "        JOIN sales_orders so ON so.id = pso.sales_order_id\n",
        "        JOIN main m ON m.product_id = pso.product_id \n",
        "            AND m.warehouse_id = pso.warehouse_id \n",
        "            AND so.created_at::DATE >= m.first_order_date\n",
        "        WHERE so.created_at::DATE BETWEEN CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE - 120 \n",
        "            AND CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE\n",
        "            AND so.sales_order_status_id NOT IN (7, 12)\n",
        "            AND so.channel IN ('telesales', 'retailer')\n",
        "            AND pso.purchased_item_count <> 0\n",
        "        GROUP BY o_date, pso.warehouse_id, pso.product_id\n",
        "    )\n",
        "    GROUP BY product_id, warehouse_id\n",
        ")\n",
        "\n",
        "SELECT DISTINCT warehouse_id, product_id\n",
        "FROM (\n",
        "    SELECT m.product_id, m.warehouse_id, m.first_order_date, m.activation,\n",
        "        COALESCE(sd.sales_days, 0) AS sales_days,\n",
        "        COALESCE(sd.sales_days, 0)::FLOAT / NULLIF((CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE - 1) - m.first_order_date, 0) AS perc_days\n",
        "    FROM main m \n",
        "    LEFT JOIN sold_days sd ON sd.product_id = m.product_id AND sd.warehouse_id = m.warehouse_id\n",
        "    WHERE m.first_order_date < CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE - 10\n",
        ")\n",
        "WHERE perc_days <= 0.3\n",
        "    AND activation = 'true'\n",
        "'''\n",
        "\n",
        "# Execute zero demand query\n",
        "print(\"Loading zero demand SKUs...\")\n",
        "df_zero_demand = query_snowflake(ZERO_DEMAND_QUERY)\n",
        "df_zero_demand = convert_to_numeric(df_zero_demand)\n",
        "print(f\"Loaded {len(df_zero_demand)} zero demand SKU records\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Add Zero Demand Flag to pricing_with_discount\n",
        "# =============================================================================\n",
        "\n",
        "# Add a marker column to identify zero demand SKUs\n",
        "df_zero_demand['zero_demand'] = 1\n",
        "\n",
        "# Merge with pricing_with_discount\n",
        "pricing_with_discount = pricing_with_discount.merge(\n",
        "    df_zero_demand[['warehouse_id', 'product_id', 'zero_demand']], \n",
        "    on=['warehouse_id', 'product_id'], \n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Fill missing values with 0 (not zero demand)\n",
        "pricing_with_discount['zero_demand'] = pricing_with_discount['zero_demand'].fillna(0).astype(int)\n",
        "\n",
        "print(f\"Zero demand flag added!\")\n",
        "print(f\"SKUs flagged as zero demand: {len(pricing_with_discount[pricing_with_discount['zero_demand'] == 1])}\")\n",
        "print(f\"SKUs with normal demand: {len(pricing_with_discount[pricing_with_discount['zero_demand'] == 0])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# OOS Yesterday Query - Identify SKUs out of stock yesterday\n",
        "# =============================================================================\n",
        "OOS_YESTERDAY_QUERY = f'''\n",
        "SELECT DISTINCT product_id, warehouse_id,\n",
        "    CASE WHEN opening_stocks = 0 AND closing_stocks = 0 THEN 1\n",
        "         ELSE 0 \n",
        "    END AS oos_yesterday\n",
        "FROM (\n",
        "    SELECT \n",
        "        timestamp,\n",
        "        product_id,\n",
        "        warehouse_id, \n",
        "        AVAILABLE_STOCK AS closing_stocks,\n",
        "        LAG(AVAILABLE_STOCK) OVER (PARTITION BY product_id, warehouse_id ORDER BY TIMESTAMP) AS opening_stocks\n",
        "    FROM materialized_views.stock_day_close\n",
        "    WHERE timestamp::DATE >= CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE - 2\n",
        "    QUALIFY opening_stocks IS NOT NULL\n",
        ")\n",
        "WHERE oos_yesterday = 1\n",
        "'''\n",
        "\n",
        "# Execute OOS yesterday query\n",
        "print(\"Loading OOS yesterday data...\")\n",
        "df_oos_yesterday = query_snowflake(OOS_YESTERDAY_QUERY)\n",
        "df_oos_yesterday = convert_to_numeric(df_oos_yesterday)\n",
        "print(f\"Loaded {len(df_oos_yesterday)} OOS yesterday records\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Add OOS Yesterday Flag to pricing_with_discount\n",
        "# =============================================================================\n",
        "\n",
        "# Merge with pricing_with_discount\n",
        "pricing_with_discount = pricing_with_discount.merge(\n",
        "    df_oos_yesterday[['warehouse_id', 'product_id', 'oos_yesterday']], \n",
        "    on=['warehouse_id', 'product_id'], \n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Fill missing values with 0 (not OOS yesterday)\n",
        "pricing_with_discount['oos_yesterday'] = pricing_with_discount['oos_yesterday'].fillna(0).astype(int)\n",
        "\n",
        "print(f\"OOS yesterday flag added!\")\n",
        "print(f\"SKUs out of stock yesterday: {len(pricing_with_discount[pricing_with_discount['oos_yesterday'] == 1])}\")\n",
        "print(f\"SKUs in stock yesterday: {len(pricing_with_discount[pricing_with_discount['oos_yesterday'] == 0])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Running Rate Query - Get in-stock running rate by warehouse and product\n",
        "# =============================================================================\n",
        "RUNNING_RATE_QUERY = f'''\n",
        "WITH params AS (\n",
        "    SELECT\n",
        "        CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE AS run_date,\n",
        "        DATEADD(month, -3, CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE) AS history_start\n",
        "),\n",
        "\n",
        "-- Daily sales aggregation\n",
        "sales_base AS (\n",
        "    SELECT\n",
        "        pso.product_id,\n",
        "        pso.warehouse_id,\n",
        "        DATE_TRUNC('day', pso.created_at)::DATE AS date,\n",
        "        SUM(pso.purchased_item_count * pso.basic_unit_count) AS sold_units,\n",
        "        SUM(pso.purchased_item_count * pso.basic_unit_count * pso.item_price)\n",
        "            / NULLIF(SUM(pso.purchased_item_count * pso.basic_unit_count), 0) AS avg_selling_price,\n",
        "        COUNT(DISTINCT so.retailer_id) AS retailer_count\n",
        "    FROM product_sales_order pso\n",
        "    JOIN sales_orders so ON pso.sales_order_id = so.id\n",
        "    WHERE DATE_TRUNC('day', pso.created_at)::DATE >= (SELECT history_start FROM params)\n",
        "    GROUP BY 1, 2, 3\n",
        "),\n",
        "\n",
        "-- Stock daily metrics\n",
        "stock_daily AS (\n",
        "    SELECT\n",
        "        product_id,\n",
        "        warehouse_id,\n",
        "        DATE_TRUNC('day', TIMESTAMP)::DATE AS date,\n",
        "        MAX_BY(available_stock, TIMESTAMP) AS stock_closing,\n",
        "        24 * SUM(CASE WHEN activation = FALSE OR available_stock = 0 THEN 1 ELSE 0 END)::FLOAT \n",
        "            / NULLIF(COUNT(*), 0) AS oos_hours,\n",
        "        MAX(CASE WHEN activation = TRUE AND available_stock > 0 THEN 1 ELSE 0 END) AS in_stock_flag\n",
        "    FROM materialized_views.STOCK_SNAP_SHOTS_RECENT\n",
        "    WHERE product_id IS NOT NULL\n",
        "    GROUP BY product_id, warehouse_id, date\n",
        "),\n",
        "\n",
        "-- Join sales + stock + WAC (only in-stock days)\n",
        "base_data AS (\n",
        "    SELECT\n",
        "        sb.product_id,\n",
        "        sb.warehouse_id,\n",
        "        sb.date,\n",
        "        sb.sold_units,\n",
        "        sb.avg_selling_price,\n",
        "        sb.retailer_count,\n",
        "        sd.oos_hours,\n",
        "        sd.in_stock_flag,\n",
        "        ac.wac_p AS wac,\n",
        "        CASE WHEN DAYOFWEEKISO(sb.date) IN (5, 6) THEN 1 ELSE 0 END AS is_weekend\n",
        "    FROM sales_base sb\n",
        "    LEFT JOIN stock_daily sd ON sb.product_id = sd.product_id \n",
        "        AND sb.warehouse_id = sd.warehouse_id AND sb.date = sd.date\n",
        "    LEFT JOIN finance.ALL_COGS ac ON sb.product_id = ac.product_id \n",
        "        AND sb.date BETWEEN ac.from_date AND ac.to_date\n",
        "    WHERE sd.in_stock_flag = 1\n",
        "),\n",
        "\n",
        "-- Stats per SKU x Warehouse\n",
        "sku_wh_stats AS (\n",
        "    SELECT\n",
        "        product_id, warehouse_id,\n",
        "        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY sold_units) AS med_units,\n",
        "        PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY sold_units) AS pct95_units,\n",
        "        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY retailer_count) AS med_retailers,\n",
        "        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY \n",
        "            CASE WHEN avg_selling_price IS NULL OR avg_selling_price = 0 THEN 0 \n",
        "            ELSE (avg_selling_price - COALESCE(wac, 0)) / NULLIF(avg_selling_price, 0) END\n",
        "        ) AS med_margin\n",
        "    FROM base_data\n",
        "    GROUP BY product_id, warehouse_id\n",
        "),\n",
        "\n",
        "-- Cap outliers and adjust for retailer spikes\n",
        "adjusted AS (\n",
        "    SELECT\n",
        "        b.product_id, b.warehouse_id, b.date, b.in_stock_flag, b.oos_hours, b.is_weekend,\n",
        "        b.avg_selling_price, b.wac, s.med_margin,\n",
        "        CASE \n",
        "            WHEN b.retailer_count > GREATEST(2, s.med_retailers * 2) \n",
        "                AND b.retailer_count > 0 AND s.med_retailers IS NOT NULL\n",
        "            THEN ROUND(LEAST(b.sold_units, s.pct95_units) * (s.med_retailers::FLOAT / NULLIF(b.retailer_count::FLOAT, 0)), 0)\n",
        "            ELSE LEAST(b.sold_units, s.pct95_units)\n",
        "        END AS units_adjusted\n",
        "    FROM base_data b\n",
        "    LEFT JOIN sku_wh_stats s ON b.product_id = s.product_id AND b.warehouse_id = s.warehouse_id\n",
        "),\n",
        "\n",
        "-- Apply weights (recency, stock availability, weekend, margin)\n",
        "weighted AS (\n",
        "    SELECT\n",
        "        product_id, warehouse_id, date, units_adjusted,\n",
        "        (\n",
        "            -- Recency weight\n",
        "            CASE WHEN date >= DATEADD(day, -21, (SELECT run_date FROM params)) THEN 1.5\n",
        "                 WHEN date >= DATEADD(day, -90, (SELECT run_date FROM params)) THEN 1.0\n",
        "                 ELSE 0.5 END\n",
        "            -- In-stock weight\n",
        "            * CASE WHEN in_stock_flag = 1 AND COALESCE(oos_hours, 0) < 12 THEN 1.4\n",
        "                   WHEN in_stock_flag = 1 AND COALESCE(oos_hours, 0) >= 12 THEN 0.9\n",
        "                   ELSE 0.6 END\n",
        "            -- Weekend weight\n",
        "            * CASE WHEN is_weekend = 1 THEN 0.7 ELSE 1.0 END\n",
        "            -- Margin weight\n",
        "            * CASE WHEN avg_selling_price IS NULL OR avg_selling_price = 0 OR med_margin IS NULL THEN 1.0\n",
        "                   WHEN ((avg_selling_price - COALESCE(wac, 0)) / NULLIF(avg_selling_price, 0)) < med_margin\n",
        "                   THEN 1.0 + LEAST((med_margin - ((avg_selling_price - COALESCE(wac, 0)) / NULLIF(avg_selling_price, 0))) * 2.0, 0.6)\n",
        "                   WHEN ((avg_selling_price - COALESCE(wac, 0)) / NULLIF(avg_selling_price, 0)) > med_margin\n",
        "                   THEN 1.0 - LEAST((((avg_selling_price - COALESCE(wac, 0)) / NULLIF(avg_selling_price, 0)) - med_margin) * 2.0, 0.4)\n",
        "                   ELSE 1.0 END\n",
        "        ) AS final_weight\n",
        "    FROM adjusted\n",
        "    WHERE units_adjusted IS NOT NULL\n",
        "),\n",
        "\n",
        "-- Weighted average forecast\n",
        "forecast_base AS (\n",
        "    SELECT\n",
        "        product_id, warehouse_id,\n",
        "        SUM(units_adjusted * final_weight) / NULLIF(SUM(final_weight), 0) AS weighted_avg_units\n",
        "    FROM weighted\n",
        "    GROUP BY product_id, warehouse_id\n",
        "),\n",
        "\n",
        "-- Zero-sales last 4 days (with stock) exclusion flag\n",
        "last4_flag AS (\n",
        "    SELECT product_id, warehouse_id,\n",
        "        CASE WHEN COUNT(*) = 4 \n",
        "             AND SUM(CASE WHEN COALESCE(sold_units, 0) = 0 AND in_stock_flag = 1 THEN 1 ELSE 0 END) = 4\n",
        "        THEN 1 ELSE 0 END AS exclude_flag\n",
        "    FROM base_data\n",
        "    WHERE date >= DATEADD(day, -4, (SELECT run_date FROM params)) \n",
        "        AND date < (SELECT run_date FROM params)\n",
        "    GROUP BY product_id, warehouse_id\n",
        "),\n",
        "\n",
        "-- Zero sales excluded (in stock but no sales)\n",
        "zero_sales_excluded AS (\n",
        "    SELECT DISTINCT s.warehouse_id, s.product_id\n",
        "    FROM (\n",
        "        SELECT pw.warehouse_id, pw.product_id, SUM(pw.available_stock)::INT AS stocks\n",
        "        FROM product_warehouse pw\n",
        "        WHERE pw.warehouse_id NOT IN (6, 9, 10) AND pw.is_basic_unit = 1 AND pw.available_stock > 0\n",
        "        GROUP BY pw.warehouse_id, pw.product_id\n",
        "    ) s\n",
        "    LEFT JOIN (\n",
        "        SELECT pso.product_id, pso.warehouse_id, SUM(pso.total_price) AS nmv\n",
        "        FROM product_sales_order pso\n",
        "        JOIN sales_orders so ON so.id = pso.sales_order_id\n",
        "        WHERE so.created_at::date BETWEEN CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE - 5 \n",
        "            AND CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE - 1\n",
        "            AND so.sales_order_status_id NOT IN (7, 12) AND so.channel IN ('telesales', 'retailer')\n",
        "            AND pso.purchased_item_count <> 0\n",
        "        GROUP BY pso.product_id, pso.warehouse_id\n",
        "    ) md ON md.product_id = s.product_id AND md.warehouse_id = s.warehouse_id\n",
        "    LEFT JOIN finance.all_cogs f ON f.product_id = s.product_id\n",
        "        AND f.from_date::date <= CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE\n",
        "        AND f.to_date::date > CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE\n",
        "    LEFT JOIN (\n",
        "        SELECT pr.warehouse_id, ppr.product_id, SUM(ppr.final_price) AS total_prs\n",
        "        FROM product_purchased_receipts ppr\n",
        "        JOIN purchased_receipts pr ON pr.id = ppr.purchased_receipt_id\n",
        "        WHERE pr.date::date >= CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::DATE - 4\n",
        "            AND pr.is_actual = 'true' AND pr.purchased_receipt_status_id IN (4, 5, 7)\n",
        "            AND ppr.purchased_item_count <> 0\n",
        "        GROUP BY pr.warehouse_id, ppr.product_id\n",
        "    ) prs ON prs.product_id = s.product_id AND prs.warehouse_id = s.warehouse_id\n",
        "    WHERE COALESCE(md.nmv, 0) = 0 \n",
        "        AND COALESCE(prs.total_prs, 0) < 0.7 * (COALESCE(f.wac_p, 0) * s.stocks)\n",
        "),\n",
        "\n",
        "-- First sale date for new products\n",
        "first_sale AS (\n",
        "    SELECT product_id, warehouse_id, MIN(date) AS first_sale_date\n",
        "    FROM base_data WHERE sold_units > 0\n",
        "    GROUP BY product_id, warehouse_id\n",
        ")\n",
        "\n",
        "-- Final output: running rate per warehouse/product\n",
        "SELECT\n",
        "    fb.warehouse_id,\n",
        "    fb.product_id,\n",
        "    CASE\n",
        "        WHEN l4.exclude_flag = 1 THEN 0\n",
        "        WHEN fs.first_sale_date >= DATEADD(day, -2, (SELECT run_date FROM params))\n",
        "        THEN GREATEST(CEIL(fb.weighted_avg_units), 1)\n",
        "        ELSE CEIL(fb.weighted_avg_units)\n",
        "    END AS In_stock_rr\n",
        "FROM forecast_base fb\n",
        "LEFT JOIN last4_flag l4 ON fb.product_id = l4.product_id AND fb.warehouse_id = l4.warehouse_id\n",
        "LEFT JOIN first_sale fs ON fb.product_id = fs.product_id AND fb.warehouse_id = fs.warehouse_id\n",
        "LEFT JOIN zero_sales_excluded zse ON fb.product_id = zse.product_id AND fb.warehouse_id = zse.warehouse_id\n",
        "WHERE zse.product_id IS NULL\n",
        "'''\n",
        "\n",
        "# Execute running rate query\n",
        "print(\"Loading running rate data (this may take a moment)...\")\n",
        "df_running_rate = query_snowflake(RUNNING_RATE_QUERY)\n",
        "df_running_rate = convert_to_numeric(df_running_rate)\n",
        "print(f\"Loaded {len(df_running_rate)} running rate records\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Merge Running Rate and Calculate DOH (Days on Hand)\n",
        "# =============================================================================\n",
        "\n",
        "# Merge running rate data with pricing_with_discount\n",
        "pricing_with_discount = pricing_with_discount.merge(\n",
        "    df_running_rate[['warehouse_id', 'product_id', 'in_stock_rr']], \n",
        "    on=['warehouse_id', 'product_id'], \n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Fill missing running rate with 0\n",
        "pricing_with_discount['in_stock_rr'] = pricing_with_discount['in_stock_rr'].fillna(0)\n",
        "\n",
        "# Calculate DOH (Days on Hand) = stocks / in_stock_rr\n",
        "# Handle division by zero - if running rate is 0, DOH is infinite (use 999)\n",
        "pricing_with_discount['doh'] = np.where(\n",
        "    pricing_with_discount['in_stock_rr'] > 0,\n",
        "    pricing_with_discount['stocks'] / pricing_with_discount['in_stock_rr'],\n",
        "    999\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
