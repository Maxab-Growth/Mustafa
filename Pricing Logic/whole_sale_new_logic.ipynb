{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a793f96",
   "metadata": {},
   "source": [
    "# Wholesale Pricing Logic\n",
    "\n",
    "This notebook handles wholesale pricing calculations and uploads for different regions.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup & Dependencies](#setup)\n",
    "2. [Configuration](#config)\n",
    "3. [Helper Functions](#functions)\n",
    "4. [Data Loading](#data-loading)\n",
    "5. [Data Processing](#data-processing)\n",
    "6. [Export & Upload](#export-upload)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeb00cf",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Dependencies <a id='setup'></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed0aab1c-ca86-4f46-8dac-684b0062f85d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:07:23.178917Z",
     "iopub.status.busy": "2026-02-18T08:07:23.178719Z",
     "iopub.status.idle": "2026-02-18T08:07:46.117557Z",
     "shell.execute_reply": "2026-02-18T08:07:46.116536Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# =============================================================================\n",
    "# Package Installation\n",
    "# =============================================================================\n",
    "\n",
    "# Core\n",
    "!pip install --upgrade pip\n",
    "\n",
    "# Database Connectivity\n",
    "!pip install psycopg2-binary\n",
    "!pip install snowflake-connector-python==3.15.0\n",
    "!pip install snowflake-sqlalchemy\n",
    "!pip install sqlalchemy==1.4.46\n",
    "\n",
    "# AWS & API\n",
    "!pip install boto3\n",
    "!pip install requests\n",
    "!pip install keyring==23.11.0\n",
    "\n",
    "# Google Sheets\n",
    "!pip install oauth2client\n",
    "!pip install gspread==5.9.0\n",
    "!pip install gspread_dataframe\n",
    "!pip install google.cloud\n",
    "\n",
    "# Data Manipulation\n",
    "!pip install pandas==2.2.1\n",
    "!pip install numpy\n",
    "!pip install polars\n",
    "!pip install openpyxl\n",
    "!pip install xlsxwriter\n",
    "\n",
    "# Utilities\n",
    "!pip install tqdm\n",
    "!pip install warnings\n",
    "!pip install --upgrade datetime\n",
    "!pip install python-time\n",
    "!pip install --upgrade pytz\n",
    "!pip install db-dtypes\n",
    "!pip install import-ipynb\n",
    "\n",
    "# Analytics\n",
    "!pip install statsmodels\n",
    "!pip install scikit-learn\n",
    "!pip install pulp\n",
    "!pip install slack_sdk\n",
    "!pip install slack\n",
    "!pip install aiohttp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7d174c3-4449-497d-bbdc-d00c2335e221",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:07:46.119739Z",
     "iopub.status.busy": "2026-02-18T08:07:46.119505Z",
     "iopub.status.idle": "2026-02-18T08:07:49.527649Z",
     "shell.execute_reply": "2026-02-18T08:07:49.526939Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.12/site-packages/snowflake/connector/options.py:104: UserWarning: You have an incompatible version of 'pyarrow' installed (20.0.0), please install a version that adheres to: 'pyarrow<19.0.0; extra == \"pandas\"'\n",
      "  warn_incompatible_dep(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.Renviron\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/service_account_key.json\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Imports\n",
    "# =============================================================================\n",
    "\n",
    "# Standard Library\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import base64\n",
    "import warnings\n",
    "import calendar\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "# Data Processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Database\n",
    "import snowflake.connector\n",
    "\n",
    "# AWS\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# HTTP & API\n",
    "import requests\n",
    "from requests import get\n",
    "\n",
    "# Google Sheets\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "# Utilities\n",
    "from tqdm import tqdm\n",
    "import import_ipynb\n",
    "\n",
    "# Custom Environment Setup\n",
    "import setup_environment_2\n",
    "\n",
    "# =============================================================================\n",
    "# Configuration\n",
    "# =============================================================================\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "importlib.reload(setup_environment_2)\n",
    "setup_environment_2.initialize_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ed07ae",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Configuration <a id='config'></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cb42395",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:07:49.530194Z",
     "iopub.status.busy": "2026-02-18T08:07:49.529963Z",
     "iopub.status.idle": "2026-02-18T08:07:49.535073Z",
     "shell.execute_reply": "2026-02-18T08:07:49.534339Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Constants & Mappings\n",
    "# =============================================================================\n",
    "\n",
    "# All cohort IDs used in queries\n",
    "ALL_COHORT_IDS = [700, 701, 702, 703, 704, 696, 695, 698, 697, 699, 1123, 1124, 1125, 1126]\n",
    "\n",
    "# Warehouse mappings (region, warehouse_name, warehouse_id, cohort_id)\n",
    "WAREHOUSE_MAPPING = [\n",
    "    ('Cairo', 'Mostorod', 1, 700),\n",
    "    ('Giza', 'Barageel', 236, 701),\n",
    "    ('Delta West', 'El-Mahala', 337, 703),\n",
    "    ('Delta West', 'Tanta', 8, 703),\n",
    "    ('Delta East', 'Mansoura FC', 339, 704),\n",
    "    ('Delta East', 'Sharqya', 170, 704),\n",
    "    ('Upper Egypt', 'Assiut FC', 501, 1124),\n",
    "    ('Upper Egypt', 'Bani sweif', 401, 1126),\n",
    "    ('Upper Egypt', 'Menya Samalot', 703, 1123),\n",
    "    ('Upper Egypt', 'Sohag', 632, 1125),\n",
    "    ('Alexandria', 'Khorshed Alex', 797, 702),\n",
    "    ('Giza', 'Sakkarah', 962, 701)\n",
    "]\n",
    "\n",
    "# Region to Cohort mapping for uploads\n",
    "REGION_COHORT_MAPPING = {\n",
    "    'Greater Cairo': 1156,\n",
    "    'Upper Egypt': 1190,\n",
    "    'Delta': 1222,\n",
    "    'Alexandria': 1223\n",
    "}\n",
    "\n",
    "# Tier-based buffer for price calculations\n",
    "TIER_BUFFER_MAP = {\n",
    "    1: 0.7,\n",
    "    2: 0.75,\n",
    "    3: 0.8,\n",
    "    4: 0.85\n",
    "}\n",
    "\n",
    "# Products to exclude from upload\n",
    "EXCLUDED_PRODUCT_IDS = [4541, 12973]\n",
    "\n",
    "# Brands with special handling\n",
    "ADDITIONAL_BRANDS_REDUCE = ['هارفست فوودز', 'البوادي']\n",
    "BRANDS_TO_REMOVE_FROM_REDUCE = ['فيوري']\n",
    "# Upload chunk sizes\n",
    "CHUNK_SIZE_DEFAULT = 4000\n",
    "CHUNK_SIZE_SPECIAL = 2000  # For cohort 61\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dc40b1",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Helper Functions <a id='functions'></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a89df83b-343f-44e9-96f7-2b7fea6aea82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:07:49.536846Z",
     "iopub.status.busy": "2026-02-18T08:07:49.536655Z",
     "iopub.status.idle": "2026-02-18T08:07:49.542685Z",
     "shell.execute_reply": "2026-02-18T08:07:49.541974Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Database Functions\n",
    "# =============================================================================\n",
    "\n",
    "def query_snowflake(query: str, columns: list = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Execute a query on Snowflake and return results as DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        query: SQL query string to execute\n",
    "        columns: Optional list of column names for the result DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with query results\n",
    "    \"\"\"\n",
    "    columns = columns or []\n",
    "    \n",
    "    con = snowflake.connector.connect(\n",
    "        user=os.environ[\"SNOWFLAKE_USERNAME\"],\n",
    "        account=os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "        password=os.environ[\"SNOWFLAKE_PASSWORD\"],\n",
    "        database=os.environ[\"SNOWFLAKE_DATABASE\"]\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        cur = con.cursor()\n",
    "        cur.execute(\"USE WAREHOUSE COMPUTE_WH\")\n",
    "        cur.execute(query)\n",
    "        \n",
    "        data = np.array(cur.fetchall())\n",
    "        if len(columns) == 0:\n",
    "            return pd.DataFrame(data)\n",
    "        return pd.DataFrame(data, columns=columns)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    finally:\n",
    "        cur.close()\n",
    "        con.close()\n",
    "\n",
    "\n",
    "def get_snowflake_timezone() -> str:\n",
    "    \"\"\"Get the current timezone setting from Snowflake.\"\"\"\n",
    "    query = \"SHOW PARAMETERS LIKE 'TIMEZONE'\"\n",
    "    result = query_snowflake(query)\n",
    "    return result[1].values[0]\n",
    "\n",
    "\n",
    "def convert_columns_to_numeric(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert all columns to numeric where possible.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with numeric columns where conversion was possible\n",
    "    \"\"\"\n",
    "    df.columns = df.columns.str.lower()\n",
    "    for col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='ignore')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8994ad8-b430-4445-9ff3-1425fff31010",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:07:49.544620Z",
     "iopub.status.busy": "2026-02-18T08:07:49.544428Z",
     "iopub.status.idle": "2026-02-18T08:07:49.549750Z",
     "shell.execute_reply": "2026-02-18T08:07:49.549004Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# AWS & API Functions\n",
    "# =============================================================================\n",
    "\n",
    "def get_secret(secret_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve a secret from AWS Secrets Manager.\n",
    "    \n",
    "    Args:\n",
    "        secret_name: Name of the secret to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        Secret string or decoded binary\n",
    "    \"\"\"\n",
    "    region_name = \"us-east-1\"\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(service_name='secretsmanager', region_name=region_name)\n",
    "\n",
    "    try:\n",
    "        response = client.get_secret_value(SecretId=secret_name)\n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        error_messages = {\n",
    "            'DecryptionFailureException': \"Can't decrypt secret using provided KMS key\",\n",
    "            'InternalServiceErrorException': \"Server-side error occurred\",\n",
    "            'InvalidParameterException': \"Invalid parameter value provided\",\n",
    "            'InvalidRequestException': \"Invalid request for current resource state\",\n",
    "            'ResourceNotFoundException': \"Requested resource not found\"\n",
    "        }\n",
    "        if error_code in error_messages:\n",
    "            print(f\"AWS Error: {error_messages[error_code]}\")\n",
    "        raise e\n",
    "    \n",
    "    if 'SecretString' in response:\n",
    "        return response['SecretString']\n",
    "    return base64.b64decode(response['SecretBinary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33b35fc5-07ba-4ee8-915a-19421c73df34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:07:49.551651Z",
     "iopub.status.busy": "2026-02-18T08:07:49.551465Z",
     "iopub.status.idle": "2026-02-18T08:07:49.557114Z",
     "shell.execute_reply": "2026-02-18T08:07:49.556390Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_access_token(url: str, client_id: str, client_secret: str) -> str:\n",
    "    \"\"\"\n",
    "    Get OAuth access token for MaxAB API.\n",
    "    \n",
    "    Args:\n",
    "        url: Token endpoint URL\n",
    "        client_id: OAuth client ID\n",
    "        client_secret: OAuth client secret\n",
    "    \n",
    "    Returns:\n",
    "        Access token string\n",
    "    \"\"\"\n",
    "    response = requests.post(\n",
    "        url,\n",
    "        data={\n",
    "            \"grant_type\": \"password\",\n",
    "            \"username\": API_USERNAME,\n",
    "            \"password\": API_PASSWORD\n",
    "        },\n",
    "        auth=(client_id, client_secret),\n",
    "    )\n",
    "    return response.json()[\"access_token\"]\n",
    "\n",
    "\n",
    "def _get_api_token() -> str:\n",
    "    \"\"\"Get fresh API token for MaxAB requests.\"\"\"\n",
    "    return get_access_token(\n",
    "        'https://sso.maxab.info/auth/realms/maxab/protocol/openid-connect/token',\n",
    "        'main-system-externals',\n",
    "        API_SECRET\n",
    "    )\n",
    "\n",
    "\n",
    "def post_prices(cohort_id: int, file_name: str) -> requests.Response:\n",
    "    \"\"\"\n",
    "    Upload pricing sheet to MaxAB API for a specific cohort.\n",
    "    \n",
    "    Args:\n",
    "        cohort_id: Target cohort ID\n",
    "        file_name: Path to Excel file with pricing data\n",
    "    \n",
    "    Returns:\n",
    "        API response object\n",
    "    \"\"\"\n",
    "    token = _get_api_token()\n",
    "    url = f\"https://api.maxab.info/main-system/api/admin-portal/cohorts/{cohort_id}/pricing\"\n",
    "    \n",
    "    files = [('sheet', (file_name, open(file_name, 'rb'), \n",
    "              'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'))]\n",
    "    headers = {'Authorization': f'bearer {token}'}\n",
    "    \n",
    "    return requests.post(url, headers=headers, data={}, files=files)\n",
    "\n",
    "\n",
    "def post_cart_rules(cohort_id: int, file_name: str) -> requests.Response:\n",
    "    \"\"\"\n",
    "    Upload cart rules sheet to MaxAB API for a specific cohort.\n",
    "    \n",
    "    Args:\n",
    "        cohort_id: Target cohort ID\n",
    "        file_name: Path to Excel file with cart rules\n",
    "    \n",
    "    Returns:\n",
    "        API response object\n",
    "    \"\"\"\n",
    "    token = _get_api_token()\n",
    "    url = f\"https://api.maxab.info/main-system/api/admin-portal/cohorts/{cohort_id}/cart-rules\"\n",
    "    \n",
    "    files = [('sheet', (file_name, open(file_name, 'rb'),\n",
    "              'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'))]\n",
    "    headers = {'Authorization': f'bearer {token}'}\n",
    "    \n",
    "    return requests.post(url, headers=headers, data={}, files=files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "123694fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:07:49.558808Z",
     "iopub.status.busy": "2026-02-18T08:07:49.558602Z",
     "iopub.status.idle": "2026-02-18T08:07:49.567886Z",
     "shell.execute_reply": "2026-02-18T08:07:49.567144Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Pricing Logic Functions\n",
    "# =============================================================================\n",
    "\n",
    "# Minimum margin floor (1%)\n",
    "MIN_MARGIN = -0.05\n",
    "\n",
    "def calculate_final_wac(row: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Calculate final WAC (Weighted Average Cost) considering price ups.\n",
    "    \"\"\"\n",
    "    if pd.isna(row['new_pp']):\n",
    "        return row['wac_p']\n",
    "    else:\n",
    "        diff = (row['new_pp']-row['wac1']) / row['wac1']\n",
    "        if diff > 0:\n",
    "            wac_p = ((0.3*diff)+1)*row['wac_p']\n",
    "        else:\n",
    "            wac_p = row['wac_p']  \n",
    "        return wac_p\n",
    "\n",
    "\n",
    "def select_price(row: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the wholesale price based on various factors including:\n",
    "    - Brand-specific rules\n",
    "    - Category-specific rules  \n",
    "    - Tier-based pricing\n",
    "    - Value-to-weight status\n",
    "    - Minimum margin floor of 1%\n",
    "    \n",
    "    Args:\n",
    "        row: DataFrame row with product and pricing data\n",
    "    \n",
    "    Returns:\n",
    "        Calculated wholesale price\n",
    "    \"\"\"\n",
    "    # Calculate base WAC\n",
    "    final_wac = calculate_final_wac(row)\n",
    "    # Adjust target margin for price ups\n",
    "    target_margin = row['target_margin']\n",
    "    \n",
    "    # Brand-specific pricing rules\n",
    "    if row['brand'] in FORCED_BRAND_LIST:\n",
    "        if row['brand'] in ['كوكا كولا', 'شويبس']:\n",
    "            margin_factor = np.maximum(row['margin'] * 0.75, MIN_MARGIN)\n",
    "            return np.minimum(final_wac / np.maximum((1 - margin_factor), (0.25 * target_margin)),row['price']*0.995)\n",
    "        elif row['brand'] == 'جود كير':\n",
    "            margin_factor = np.maximum(row['margin'], MIN_MARGIN)\n",
    "            return np.minimum(final_wac / np.maximum((1 - margin_factor), (0.5 * target_margin)),row['price'])\n",
    "        elif row['brand'] == 'بيتي عصاير':\n",
    "            margin_factor = np.maximum(row['margin'] * 0.9, MIN_MARGIN)\n",
    "            return np.minimum(final_wac / np.maximum((1 - margin_factor), (0.85 * target_margin)),row['price']*0.995)    \n",
    "        else:\n",
    "            margin_factor = np.maximum(np.maximum(row['margin'] * 0.85, MIN_MARGIN),target_margin * 0.85)\n",
    "            return np.minimum(final_wac / (1 - margin_factor),row['price']*0.995)\n",
    "    \n",
    "    # Special brand handling\n",
    "    if row['brand'] == 'فيوري':\n",
    "        margin_factor = np.maximum(row['margin'] * 0.9, MIN_MARGIN)\n",
    "        return np.minimum(final_wac / (1 - margin_factor),row['price']*0.995)\n",
    "    if row['brand'] in ['فريش فارم','ماى ميت','ستار']:\n",
    "        margin_factor = np.maximum(row['margin'], MIN_MARGIN)\n",
    "        return np.minimum(final_wac / (1 - margin_factor),row['price']*0.995)\n",
    "    \n",
    "    # Category-specific rules\n",
    "    if row['cat'] in ['ورقيات','حفاضات أطفال']:\n",
    "        margin_factor = np.maximum(np.minimum(np.maximum(0.6 * target_margin, 0.015), target_margin), MIN_MARGIN)\n",
    "        return np.minimum(final_wac / (1 - margin_factor),row['price']*0.995)\n",
    "    if row['brand'] == 'ريد بل':\n",
    "        margin_factor = np.maximum(np.minimum(np.maximum(0.5 * target_margin, 0.015), target_margin), MIN_MARGIN)\n",
    "        return np.minimum(final_wac / (1 - margin_factor),row['price']*0.995)\n",
    "    # Tier-based pricing with VTW adjustment\n",
    "    vtw_multiplier = 1 if row['vtw_status'] else 1.3\n",
    "    \n",
    "    tier_configs = {\n",
    "        1: (0.20, 0.0125),\n",
    "        2: (0.25, 0.015),\n",
    "        3: (0.40, 0.0175),\n",
    "        4: (0.60, 0.0175)\n",
    "    }\n",
    "    \n",
    "    tier = row['tier']\n",
    "    if tier in tier_configs:\n",
    "        factor, min_tier_margin = tier_configs[tier]\n",
    "        margin_factor = np.minimum(\n",
    "        np.minimum(\n",
    "            np.maximum((factor * vtw_multiplier) * target_margin, min_tier_margin),\n",
    "            target_margin\n",
    "        ),\n",
    "        row['margin']*0.75\n",
    "        )\n",
    "        # Ensure minimum 1% margin\n",
    "        margin_factor = np.maximum(margin_factor, MIN_MARGIN)\n",
    "        return np.minimum(final_wac / (1 - margin_factor),row['price']*0.995)\n",
    "    \n",
    "    # Default case - ensure minimum 1% margin\n",
    "    margin_factor = np.maximum(target_margin, MIN_MARGIN)\n",
    "    return np.minimum(final_wac / (1 - margin_factor),row['price']*0.995)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6b64e9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:07:49.569586Z",
     "iopub.status.busy": "2026-02-18T08:07:49.569360Z",
     "iopub.status.idle": "2026-02-18T08:07:50.554577Z",
     "shell.execute_reply": "2026-02-18T08:07:50.553704Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using timezone: America/Los_Angeles\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Initialize API Credentials & Timezone\n",
    "# =============================================================================\n",
    "\n",
    "# Load API credentials\n",
    "pricing_api_secret = json.loads(get_secret(\"prod/pricing/api/\"))\n",
    "API_USERNAME = pricing_api_secret[\"egypt_username\"]\n",
    "API_PASSWORD = pricing_api_secret[\"egypt_password\"]\n",
    "API_SECRET = pricing_api_secret[\"egypt_secret\"]\n",
    "\n",
    "# Get Snowflake timezone\n",
    "TIMEZONE = get_snowflake_timezone()\n",
    "print(f\"Using timezone: {TIMEZONE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cdb69b",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Data Loading <a id='data-loading'></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b86c495-a75e-4836-887a-c46fe52f5c97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:07:50.556639Z",
     "iopub.status.busy": "2026-02-18T08:07:50.556415Z",
     "iopub.status.idle": "2026-02-18T08:07:54.806689Z",
     "shell.execute_reply": "2026-02-18T08:07:54.805733Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 102,758 wholesale price records\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Load SKU Prices\n",
    "# =============================================================================\n",
    "\n",
    "cohort_ids_str = ','.join(map(str, ALL_COHORT_IDS))\n",
    "\n",
    "prices_query = f'''\n",
    "WITH skus_prices AS (\n",
    "    WITH local_prices AS (\n",
    "        SELECT  \n",
    "            CASE \n",
    "                WHEN cpu.cohort_id IN (700, 695) THEN 'Cairo'\n",
    "                WHEN cpu.cohort_id IN (701) THEN 'Giza'\n",
    "                WHEN cpu.cohort_id IN (704, 698) THEN 'Delta East'\n",
    "                WHEN cpu.cohort_id IN (703, 697) THEN 'Delta West'\n",
    "                WHEN cpu.cohort_id IN (696, 1123, 1124, 1125, 1126) THEN 'Upper Egypt'\n",
    "                WHEN cpu.cohort_id IN (702, 699) THEN 'Alexandria'\n",
    "            END AS region,\n",
    "            cohort_id,\n",
    "            pu.product_id,\n",
    "            pu.packing_unit_id,\n",
    "            pu.basic_unit_count,\n",
    "            AVG(cpu.price) AS price\n",
    "        FROM cohort_product_packing_units cpu\n",
    "        JOIN PACKING_UNIT_PRODUCTS pu ON pu.id = cpu.product_packing_unit_id\n",
    "        WHERE cpu.cohort_id IN ({cohort_ids_str})\n",
    "            AND cpu.created_at::date <> '2023-07-31'\n",
    "            AND cpu.is_customized = TRUE\n",
    "        GROUP BY ALL\n",
    "    ),\n",
    "    \n",
    "    live_prices AS (\n",
    "        SELECT \n",
    "            region, cohort_id, product_id, \n",
    "            pu_id AS packing_unit_id, \n",
    "            buc AS basic_unit_count, \n",
    "            NEW_PRICE AS price\n",
    "        FROM materialized_views.DBDP_PRICES\n",
    "        WHERE created_at = CURRENT_DATE\n",
    "            AND DATE_PART('hour', CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())) \n",
    "                BETWEEN SPLIT_PART(time_slot, '-', 1)::int AND (SPLIT_PART(time_slot, '-', 1)::int) + 1\n",
    "            AND cohort_id IN ({cohort_ids_str})\n",
    "    ),\n",
    "    \n",
    "    prices AS (\n",
    "        SELECT *\n",
    "        FROM (\n",
    "            SELECT *, 1 AS priority FROM live_prices\n",
    "            UNION ALL\n",
    "            SELECT *, 2 AS priority FROM local_prices\n",
    "        )\n",
    "        QUALIFY ROW_NUMBER() OVER (PARTITION BY region, cohort_id, product_id, packing_unit_id ORDER BY priority) = 1\n",
    "    )\n",
    "    \n",
    "    SELECT region, cohort_id, product_id, price\n",
    "    FROM prices\n",
    "    WHERE basic_unit_count = 1\n",
    "        AND ((product_id = 1309 AND packing_unit_id = 2) OR (product_id <> 1309))\n",
    ")\n",
    "\n",
    "SELECT distinct\n",
    "    region, cohort_id, p.product_id,\n",
    "    CONCAT(products.name_ar, ' ', products.size, ' ', product_units.name_ar) AS sku,\n",
    "    b.name_ar AS brand,\n",
    "    cat.name_ar AS cat,\n",
    "    wac1, wac_p, p.price\n",
    "FROM skus_prices p\n",
    "JOIN finance.all_cogs c ON c.product_id = p.product_id \n",
    "    AND CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP()) BETWEEN c.from_date AND c.to_date\n",
    "JOIN products ON products.id = p.product_id\n",
    "JOIN categories cat ON cat.id = products.category_id\n",
    "JOIN brands b ON b.id = products.brand_id\n",
    "JOIN product_units ON product_units.id = products.unit_id\n",
    "WHERE wac1 > 0 AND wac_p > 0\n",
    "GROUP BY ALL\n",
    "'''\n",
    "\n",
    "whole_sale = query_snowflake(\n",
    "    prices_query, \n",
    "    columns=['region', 'cohort_id', 'product_id', 'sku', 'brand', 'cat', 'wac1', 'wac_p', 'price']\n",
    ")\n",
    "whole_sale = convert_columns_to_numeric(whole_sale).drop_duplicates()\n",
    "\n",
    "print(f\"Loaded {len(whole_sale):,} wholesale price records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b5376f2-6dee-462f-98e5-26deff2a271f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:07:54.808624Z",
     "iopub.status.busy": "2026-02-18T08:07:54.808401Z",
     "iopub.status.idle": "2026-02-18T08:07:54.927075Z",
     "shell.execute_reply": "2026-02-18T08:07:54.926210Z"
    }
   },
   "outputs": [],
   "source": [
    "whole_sale = whole_sale.groupby(['region', 'cohort_id', 'product_id', 'sku', 'brand', 'cat', 'price']).agg({'wac1':'max','wac_p':'max'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f738d2e8-c94b-4bae-8b25-148150672199",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:07:54.929202Z",
     "iopub.status.busy": "2026-02-18T08:07:54.928967Z",
     "iopub.status.idle": "2026-02-18T08:07:57.138067Z",
     "shell.execute_reply": "2026-02-18T08:07:57.137184Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 471 brand targets and 76 category targets\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Load Margin Targets\n",
    "# =============================================================================\n",
    "\n",
    "# Brand-Category level targets\n",
    "brand_target_query = '''\n",
    "SELECT DISTINCT cat, brand, margin AS target_bm\n",
    "FROM performance.commercial_targets cplan\n",
    "QUALIFY \n",
    "    CASE \n",
    "        WHEN DATE_TRUNC('month', MAX(DATE) OVER()) = DATE_TRUNC('month', CURRENT_DATE) \n",
    "        THEN DATE_TRUNC('month', CURRENT_DATE)\n",
    "        ELSE DATE_TRUNC('month', CURRENT_DATE - INTERVAL '1 month') \n",
    "    END = DATE_TRUNC('month', date)\n",
    "'''\n",
    "\n",
    "brand_cat_target = query_snowflake(brand_target_query, columns=['cat', 'brand', 'target_bm'])\n",
    "brand_cat_target['target_bm'] = pd.to_numeric(brand_cat_target['target_bm'])\n",
    "\n",
    "# Category level targets (weighted average)\n",
    "cat_target_query = '''\n",
    "SELECT cat, SUM(target_bm * (target_nmv / cat_total)) AS cat_target_margin\n",
    "FROM (\n",
    "    SELECT *, SUM(target_nmv) OVER(PARTITION BY cat) AS cat_total\n",
    "    FROM (\n",
    "        SELECT cat, brand, AVG(target_bm) AS target_bm, SUM(target_nmv) AS target_nmv\n",
    "        FROM (\n",
    "            SELECT DISTINCT date, city AS region, cat, brand, margin AS target_bm, nmv AS target_nmv\n",
    "            FROM performance.commercial_targets cplan\n",
    "            QUALIFY \n",
    "                CASE \n",
    "                    WHEN DATE_TRUNC('month', MAX(DATE) OVER()) = DATE_TRUNC('month', CURRENT_DATE) \n",
    "                    THEN DATE_TRUNC('month', CURRENT_DATE)\n",
    "                    ELSE DATE_TRUNC('month', CURRENT_DATE - INTERVAL '1 month') \n",
    "                END = DATE_TRUNC('month', date)\n",
    "        )\n",
    "        GROUP BY ALL\n",
    "    )\n",
    ")\n",
    "GROUP BY ALL\n",
    "'''\n",
    "\n",
    "cat_target = query_snowflake(cat_target_query, columns=['cat', 'cat_target_margin'])\n",
    "cat_target['cat_target_margin'] = pd.to_numeric(cat_target['cat_target_margin'])\n",
    "\n",
    "print(f\"Loaded {len(brand_cat_target):,} brand targets and {len(cat_target):,} category targets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e2999cf-a506-4a97-8bdf-b5c7715d9fe7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:07:57.140036Z",
     "iopub.status.busy": "2026-02-18T08:07:57.139814Z",
     "iopub.status.idle": "2026-02-18T08:07:58.689051Z",
     "shell.execute_reply": "2026-02-18T08:07:58.688232Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9,128 sales records\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Load Sales Data\n",
    "# =============================================================================\n",
    "\n",
    "sales_query = '''\n",
    "with cohort_data as (\n",
    "SELECT DISTINCT\n",
    "    cpc.cohort_id,\n",
    "    pso.product_id,\n",
    "    SUM(pso.total_price) AS nmv\n",
    "FROM product_sales_order pso\n",
    "JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "JOIN COHORT_PRICING_CHANGES cpc ON cpc.id = pso.COHORT_PRICING_CHANGE_ID and cpc.cohort_id  in (1156,1190,1222,1223)\n",
    "WHERE so.created_at::date BETWEEN  CURRENT_DATE -60 AND CURRENT_DATE-1\n",
    "    AND so.sales_order_status_id NOT IN (7, 12)\n",
    "    AND so.channel IN ('telesales', 'retailer')\n",
    "    AND pso.purchased_item_count <> 0\n",
    "GROUP BY ALL\n",
    ")\n",
    "select cohort_id_pricing as cohort_id,product_id,nmv \n",
    "from (\n",
    "select cd.*, c.FALLBACK_COHORT_ID,c2.id as cohort_id_pricing\n",
    "from cohort_data cd \n",
    "join cohorts c on c.id = cd.cohort_id\n",
    "join cohorts c2 on c2.FALLBACK_COHORT_ID = c.FALLBACK_COHORT_ID and c2.id in (700,701,702,703,704,1123,1124,1125,1126)\n",
    ")\n",
    "'''\n",
    "\n",
    "sales = query_snowflake(sales_query, columns=['cohort_id', 'product_id', 'nmv'])\n",
    "sales = convert_columns_to_numeric(sales)\n",
    "\n",
    "print(f\"Loaded {len(sales):,} sales records\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b51bea60-893e-4efb-922a-7898226d8771",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:07:58.690979Z",
     "iopub.status.busy": "2026-02-18T08:07:58.690749Z",
     "iopub.status.idle": "2026-02-18T08:07:58.749529Z",
     "shell.execute_reply": "2026-02-18T08:07:58.748641Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1,601 commercial groups\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Load Reference Data (Groups, Packing Units, Region Mapping)\n",
    "# =============================================================================\n",
    "\n",
    "# Commercial Groups\n",
    "groups_query = 'SELECT * FROM materialized_views.sku_commercial_groups'\n",
    "groups = setup_environment_2.dwh_pg_query(groups_query, columns=['product_id', 'group'])\n",
    "groups = convert_columns_to_numeric(groups)\n",
    "\n",
    "print(f\"Loaded {len(groups):,} commercial groups\")      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5af468a1-f6e1-41ad-97f5-c88c82a73725",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:07:58.751354Z",
     "iopub.status.busy": "2026-02-18T08:07:58.751144Z",
     "iopub.status.idle": "2026-02-18T08:07:59.833324Z",
     "shell.execute_reply": "2026-02-18T08:07:59.832501Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 14,778 stock records\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Load Stock Data\n",
    "# =============================================================================\n",
    "\n",
    "# Build warehouse values from config\n",
    "whs_values = ', '.join([f\"('{r}', '{w}', {wid}, {cid})\" for r, w, wid, cid in WAREHOUSE_MAPPING])\n",
    "\n",
    "stocks_query = f'''\n",
    "WITH whs AS (\n",
    "    SELECT * FROM (VALUES {whs_values}) x(region, wh, warehouse_id, cohort_id)\n",
    ")\n",
    "\n",
    "SELECT cohort_id, product_id, SUM(stocks) AS stocks\n",
    "FROM (\n",
    "    SELECT DISTINCT \n",
    "        whs.region,\n",
    "        cohort_id,\n",
    "        whs.wh,\n",
    "        product_warehouse.product_id,\n",
    "        product_warehouse.available_stock::integer AS stocks\n",
    "    FROM whs\n",
    "    JOIN product_warehouse ON product_warehouse.warehouse_id = whs.warehouse_id\n",
    "    JOIN products ON product_warehouse.product_id = products.id\n",
    "    JOIN product_units ON products.unit_id = product_units.id\n",
    "    WHERE product_warehouse.warehouse_id NOT IN (6, 9, 10)\n",
    "        AND product_warehouse.is_basic_unit = 1\n",
    "        AND product_warehouse.available_stock > 0\n",
    ")\n",
    "GROUP BY ALL\n",
    "'''\n",
    "\n",
    "stocks = query_snowflake(stocks_query, columns=['cohort_id', 'product_id', 'stocks'])\n",
    "stocks = convert_columns_to_numeric(stocks)\n",
    "\n",
    "print(f\"Loaded {len(stocks):,} stock records\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c7ace89-645d-4ee3-9031-f3407acd26bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:07:59.835322Z",
     "iopub.status.busy": "2026-02-18T08:07:59.835015Z",
     "iopub.status.idle": "2026-02-18T08:08:00.816841Z",
     "shell.execute_reply": "2026-02-18T08:08:00.816045Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 35,312 packing units\n"
     ]
    }
   ],
   "source": [
    "# Packing Units\n",
    "pu_query = '''\n",
    "SELECT product_id, PACKING_UNIT_id, basic_unit_count\n",
    "FROM PACKING_UNIT_PRODUCTS\n",
    "WHERE deleted_at IS NULL\n",
    "ORDER BY product_id, basic_unit_count\n",
    "'''\n",
    "\n",
    "pu = query_snowflake(pu_query, columns=['product_id', 'pu_id', 'buc'])\n",
    "pu = convert_columns_to_numeric(pu)\n",
    "\n",
    "print(f\"Loaded {len(pu):,} packing units\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72ebc880-886f-4e6a-9bef-a1a6f04c5a55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:08:00.818735Z",
     "iopub.status.busy": "2026-02-18T08:08:00.818507Z",
     "iopub.status.idle": "2026-02-18T08:08:01.783506Z",
     "shell.execute_reply": "2026-02-18T08:08:01.782741Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 142 price up records\n"
     ]
    }
   ],
   "source": [
    "# Price Ups\n",
    "price_ups_query = '''\n",
    "SELECT product_id, new_pp, forecasted_date\n",
    "FROM materialized_views.DBDP_PRICE_UPS\n",
    "WHERE region = 'Cairo'\n",
    "'''\n",
    "\n",
    "price_ups = query_snowflake(price_ups_query, columns=['product_id', 'new_pp', 'forcasted_date'])\n",
    "price_ups = convert_columns_to_numeric(price_ups)\n",
    "\n",
    "print(f\"Loaded {len(price_ups):,} price up records\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7c68b9a-dec0-4d84-b1a9-0e40fb9f32e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:08:01.785624Z",
     "iopub.status.busy": "2026-02-18T08:08:01.785310Z",
     "iopub.status.idle": "2026-02-18T08:08:02.566162Z",
     "shell.execute_reply": "2026-02-18T08:08:02.565343Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7 region mappings\n"
     ]
    }
   ],
   "source": [
    "# Region Mapping\n",
    "region_query = '''\n",
    "SELECT DISTINCT \n",
    "    CASE WHEN r.name_en LIKE '%Delta%' THEN 'Delta' ELSE r.name_en END AS main_region,\n",
    "    CASE WHEN r.id = 2 THEN s.name_en ELSE r.name_en END AS region\n",
    "FROM regions r\n",
    "JOIN states s ON s.region_id = r.id\n",
    "'''\n",
    "\n",
    "region_mapping = query_snowflake(region_query, columns=['main_region', 'region'])\n",
    "region_mapping.columns = region_mapping.columns.str.lower()\n",
    "\n",
    "print(f\"Loaded {len(region_mapping):,} region mappings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b8298fb-306a-47b8-8f37-5793298ea048",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:08:02.568180Z",
     "iopub.status.busy": "2026-02-18T08:08:02.567859Z",
     "iopub.status.idle": "2026-02-18T08:08:05.253282Z",
     "shell.execute_reply": "2026-02-18T08:08:05.252553Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 13,831 VTW records\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Load Value-to-Weight (VTW) Data\n",
    "# =============================================================================\n",
    "\n",
    "vtw_query = '''\n",
    "WITH whs AS (\n",
    "    SELECT * FROM (VALUES\n",
    "        ('Cairo', 'El-Marg', 38), ('Cairo', 'Mostorod', 1),\n",
    "        ('Giza', 'Barageel', 236), ('Giza', 'Basatin', 39),\n",
    "        ('Delta West', 'El-Mahala', 337), ('Delta West', 'Tanta', 8),\n",
    "        ('Delta East', 'Mansoura FC', 339), ('Delta East', 'Sharqya', 170),\n",
    "        ('Upper Egypt', 'Assiut FC', 501), ('Upper Egypt', 'Bani sweif', 401),\n",
    "        ('Upper Egypt', 'Menya Samalot', 703), ('Upper Egypt', 'Sohag', 632),\n",
    "        ('Alexandria', 'Khorshed Alex', 797)\n",
    "    ) x(region, wh, warehouse_id)\n",
    "),\n",
    "\n",
    "region_vtw AS (\n",
    "    SELECT region, nmv/weight AS r_vtw\n",
    "    FROM (\n",
    "        SELECT whs.region, \n",
    "            SUM(product_sales_order.total_price) AS nmv,\n",
    "            SUM((packing_unit_products.weight * product_sales_order.PURCHASED_ITEM_COUNT) / 1000.00) AS weight\n",
    "        FROM sales_orders\n",
    "        JOIN product_sales_order ON product_sales_order.sales_order_id = sales_orders.id\n",
    "        JOIN whs ON whs.warehouse_id = product_sales_order.warehouse_id\n",
    "        JOIN packing_unit_products ON product_sales_order.product_id = packing_unit_products.product_id \n",
    "            AND product_sales_order.packing_unit_id = packing_unit_products.packing_unit_id\n",
    "        JOIN products ON products.id = product_sales_order.product_id\n",
    "        JOIN packing_units ON packing_units.id = product_sales_order.packing_unit_id\n",
    "        JOIN product_units ON product_units.id = products.unit_id\n",
    "        JOIN categories ON categories.id = products.category_id\n",
    "        JOIN sections ON categories.section_id = sections.id\n",
    "        JOIN brands ON brands.id = products.brand_id\n",
    "        WHERE sales_orders.CREATED_AT::date >= CURRENT_DATE - 30\n",
    "            AND sales_orders.sales_order_status_id NOT IN (7, 12)\n",
    "        GROUP BY ALL\n",
    "    )\n",
    "),\n",
    "\n",
    "product_vtw AS (\n",
    "    SELECT region, product_id, SUM(p_vtw * cntrb) AS p_vtw\n",
    "    FROM (\n",
    "        SELECT *, nmv / SUM(nmv) OVER(PARTITION BY region, product_id) AS cntrb\n",
    "        FROM (\n",
    "            SELECT region, product_id, packing_unit_id, nmv, nmv/weight AS p_vtw\n",
    "            FROM (\n",
    "                SELECT whs.region, product_sales_order.product_id, product_sales_order.packing_unit_id,\n",
    "                    SUM(product_sales_order.total_price) AS nmv,\n",
    "                    SUM((packing_unit_products.weight * product_sales_order.PURCHASED_ITEM_COUNT) / 1000.00) AS weight\n",
    "                FROM sales_orders\n",
    "                JOIN product_sales_order ON product_sales_order.sales_order_id = sales_orders.id\n",
    "                JOIN whs ON whs.warehouse_id = product_sales_order.warehouse_id\n",
    "                JOIN packing_unit_products ON product_sales_order.product_id = packing_unit_products.product_id \n",
    "                    AND product_sales_order.packing_unit_id = packing_unit_products.packing_unit_id\n",
    "                JOIN products ON products.id = product_sales_order.product_id\n",
    "                JOIN packing_units ON packing_units.id = product_sales_order.packing_unit_id\n",
    "                JOIN product_units ON product_units.id = products.unit_id\n",
    "                JOIN categories ON categories.id = products.category_id\n",
    "                JOIN sections ON categories.section_id = sections.id\n",
    "                JOIN brands ON brands.id = products.brand_id\n",
    "                WHERE sales_orders.CREATED_AT::date >= CURRENT_DATE - 30\n",
    "                    AND sales_orders.sales_order_status_id NOT IN (7, 12)\n",
    "                GROUP BY ALL\n",
    "            )\n",
    "            WHERE weight > 0\n",
    "        )\n",
    "    )\n",
    "    GROUP BY ALL\n",
    ")\n",
    "\n",
    "SELECT pv.*, rv.r_vtw\n",
    "FROM product_vtw pv\n",
    "JOIN region_vtw rv ON rv.region = pv.region\n",
    "'''\n",
    "\n",
    "vtw = query_snowflake(vtw_query, columns=['region', 'product_id', 'p_vtw', 'r_vtw'])\n",
    "vtw = convert_columns_to_numeric(vtw)\n",
    "\n",
    "print(f\"Loaded {len(vtw):,} VTW records\")         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3aa51fd-f3f2-4c30-a0d0-2dea2fc156cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:08:05.255279Z",
     "iopub.status.busy": "2026-02-18T08:08:05.255071Z",
     "iopub.status.idle": "2026-02-18T08:08:07.619148Z",
     "shell.execute_reply": "2026-02-18T08:08:07.618320Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 127 brands for reduction\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Load Google Sheets Data (Brands & Categories Overrides)\n",
    "# =============================================================================\n",
    "\n",
    "# Initialize Google Sheets client\n",
    "GSHEET_SCOPE = [\n",
    "    \"https://spreadsheets.google.com/feeds\",\n",
    "    'https://www.googleapis.com/auth/spreadsheets',\n",
    "    \"https://www.googleapis.com/auth/drive.file\",\n",
    "    \"https://www.googleapis.com/auth/drive\"\n",
    "]\n",
    "\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_dict(\n",
    "    json.loads(setup_environment_2.get_secret(\"prod/maxab-sheets\")), \n",
    "    GSHEET_SCOPE\n",
    ")\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "# Load campaign brands for reduction\n",
    "brands_list = client.open('Anniversary Campaign 2025 (Final)').worksheet('Suppliers Brands')\n",
    "brands_df = pd.DataFrame(brands_list.get_all_records())[['Brands']].drop_duplicates()\n",
    "brands_reduce = list(brands_df['Brands']) + ADDITIONAL_BRANDS_REDUCE\n",
    "\n",
    "for brand in BRANDS_TO_REMOVE_FROM_REDUCE:\n",
    "    if brand in brands_reduce:\n",
    "        brands_reduce.remove(brand)\n",
    "\n",
    "print(f\"Loaded {len(brands_reduce)} brands for reduction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2cb0a8a-7d62-48d6-a489-28e8410ba46d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:08:07.620966Z",
     "iopub.status.busy": "2026-02-18T08:08:07.620762Z",
     "iopub.status.idle": "2026-02-18T08:08:07.624508Z",
     "shell.execute_reply": "2026-02-18T08:08:07.623758Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_numeric_columns(df):\n",
    "    \"\"\"Convert all columns to numeric where possible.\"\"\"\n",
    "    for col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='ignore')\n",
    "    return df\n",
    "\n",
    "def convert_sku_id(row):\n",
    "    \"\"\"Convert SKU string to integer ID.\"\"\"\n",
    "    try:\n",
    "        return int(str(row.SKU).replace(\",\", \"\"))\n",
    "    except:\n",
    "        return row.SKU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cd42b98-c5f5-40d9-9f1e-915d37d8884a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:08:07.626153Z",
     "iopub.status.busy": "2026-02-18T08:08:07.625967Z",
     "iopub.status.idle": "2026-02-18T08:08:10.608430Z",
     "shell.execute_reply": "2026-02-18T08:08:10.607672Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forced brands: 14, Forced categories: 1\n"
     ]
    }
   ],
   "source": [
    "# Load forced brands and categories from execution sheet\n",
    "force_brands_ws = client.open('Wholesales_exec').worksheet('brands')\n",
    "force_cats_ws = client.open('Wholesales_exec').worksheet('cats')\n",
    "force_cart_rules = client.open('Wholesales_exec').worksheet('Cart rules')\n",
    "\n",
    "force_brands_df = pd.DataFrame(force_brands_ws.get_all_records())\n",
    "force_cats_df = pd.DataFrame(force_cats_ws.get_all_records())\n",
    "force_cart_rules = pd.DataFrame(force_cart_rules.get_all_records())\n",
    "\n",
    "FORCED_BRAND_LIST = list(force_brands_df.brand.unique()) if not force_brands_df.empty else []\n",
    "FORCED_CAT_LIST = list(force_cats_df.cat.unique()) if not force_cats_df.empty else []\n",
    "\n",
    "print(f\"Forced brands: {len(FORCED_BRAND_LIST)}, Forced categories: {len(FORCED_CAT_LIST)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9d354b-1251-4f63-91a2-832344ced2aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## 5. Data Processing <a id='data-processing'></a>\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93287cf6-58ed-497d-acaf-0e9bac9d2741",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:08:10.610482Z",
     "iopub.status.busy": "2026-02-18T08:08:10.610279Z",
     "iopub.status.idle": "2026-02-18T08:08:11.812520Z",
     "shell.execute_reply": "2026-02-18T08:08:11.811702Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data shape: (30482, 18)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Merge All Data Sources & Calculate Base Metrics\n",
    "# =============================================================================\n",
    "\n",
    "# Merge targets\n",
    "wholesale_data = whole_sale.merge(brand_cat_target, on=['cat', 'brand'], how='left')\n",
    "wholesale_data = wholesale_data.merge(cat_target, on=['cat'], how='left')\n",
    "\n",
    "# Merge operational data\n",
    "wholesale_data = wholesale_data.merge(stocks, on=['product_id', 'cohort_id'], how='left')\n",
    "wholesale_data = wholesale_data.merge(vtw, on=['product_id', 'region'], how='left')\n",
    "wholesale_data = wholesale_data.fillna(0)\n",
    "\n",
    "# Calculate VTW status and margins\n",
    "wholesale_data['vtw_status'] = wholesale_data['p_vtw'] >= wholesale_data['r_vtw']\n",
    "wholesale_data['stocks'] = wholesale_data['stocks'].fillna(0)\n",
    "wholesale_data['margin'] = (wholesale_data['price'] - wholesale_data['wac_p']) / wholesale_data['price']\n",
    "wholesale_data['target_margin'] = (\n",
    "    wholesale_data['target_bm']\n",
    "    .fillna(wholesale_data['cat_target_margin'])\n",
    "    .fillna(wholesale_data['margin'])\n",
    ")\n",
    "\n",
    "# Merge sales and region mapping\n",
    "wholesale_data = wholesale_data.merge(sales, on=['product_id', 'cohort_id'], how='left')\n",
    "wholesale_data['nmv'] = wholesale_data['nmv'].fillna(0)\n",
    "wholesale_data = region_mapping.merge(wholesale_data, on=['region'])\n",
    "\n",
    "# Aggregate to main region level\n",
    "aggregation = {'price': 'mean', 'stocks': 'sum', 'margin': 'mean', 'nmv': 'sum', 'vtw_status': 'max'}\n",
    "wholesale_data = wholesale_data.groupby(\n",
    "    ['main_region', 'product_id', 'sku', 'brand', 'cat', 'target_margin', 'wac1', 'wac_p']\n",
    ").agg(aggregation).reset_index()\n",
    "\n",
    "# Merge price ups and calculate contribution\n",
    "wholesale_data = wholesale_data.merge(price_ups, on=['product_id'], how='left')\n",
    "wholesale_data['total_nmv'] = wholesale_data.groupby('main_region')['nmv'].transform('sum')\n",
    "wholesale_data['cntrb'] = wholesale_data['nmv'] / wholesale_data['total_nmv']\n",
    "wholesale_data = wholesale_data.sort_values(['main_region', 'nmv'], ascending=[True, False])\n",
    "wholesale_data['nmv_cumulative_cntrb'] = wholesale_data.groupby('main_region')['cntrb'].cumsum()\n",
    "\n",
    "print(f\"Processed data shape: {wholesale_data.shape}\")\n",
    "\n",
    "cond = [wholesale_data['nmv_cumulative_cntrb'] < 0.3 ,\n",
    "        (wholesale_data['nmv_cumulative_cntrb'] >= 0.3)&(wholesale_data['nmv_cumulative_cntrb'] < 0.6),\n",
    "        (wholesale_data['nmv_cumulative_cntrb'] >= 0.6)&(wholesale_data['nmv_cumulative_cntrb'] < 0.8),\n",
    "        wholesale_data['nmv_cumulative_cntrb'] >= 0.8\n",
    "       ] \n",
    "cho = [1,2,3,4]\n",
    "\n",
    "wholesale_data['tier'] = np.select(cond,cho,default = 4)\n",
    "\n",
    "# wholesale_data.loc[wholesale_data['brand'].isin(brands_reduce),'tier']=np.maximum(wholesale_data['tier']-1,1)\n",
    "# wholesale_data.loc[wholesale_data['brand'].isin(ADDITIONAL_BRANDS_REDUCE),'tier']=1\n",
    "#wholesale_data=wholesale_data.merge(tgtg_long[['main_region','product_id','TGTG_f']],on=['main_region','product_id'],how='left')\n",
    "#wholesale_data.loc[~wholesale_data['TGTG_f'].isna(),'tier']=1\n",
    "wholesale_data['base_price'] =  wholesale_data.apply(select_price,axis=1)\n",
    "wholesale_data['new_margin'] =  (wholesale_data['base_price']-wholesale_data['wac_p'])/wholesale_data['base_price']\n",
    "wholesale_data['drop_margin'] = ((wholesale_data['new_margin']-wholesale_data['margin'])/wholesale_data['margin'])*-1\n",
    "buffer_map = {\n",
    "    1: 0.7,   \n",
    "    2: 0.7,  \n",
    "    3: 0.7,   \n",
    "    4: 0.7  \n",
    "}\n",
    "# You can change these numbers to be more/less aggressive.\n",
    "\n",
    "wholesale_data['buffer_B'] = wholesale_data['tier'].map(buffer_map)\n",
    "wholesale_data['allowed_discount_fraction'] = wholesale_data['margin'] * wholesale_data['buffer_B']\n",
    "wholesale_data['wholesale_min_price'] = wholesale_data['price'] * (1 - wholesale_data['allowed_discount_fraction'])\n",
    "wholesale_data['min_margin'] = (wholesale_data['wholesale_min_price'] -wholesale_data['wac_p']) /wholesale_data['wholesale_min_price']  \n",
    "wholesale_data['selected_margin'] = np.maximum(wholesale_data['min_margin'],wholesale_data['new_margin'])\n",
    "wholesale_data['selected_price'] = wholesale_data['wac_p']/(1-wholesale_data['selected_margin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9158017-ccd2-4f8e-a149-a46fb86ea330",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:08:11.814466Z",
     "iopub.status.busy": "2026-02-18T08:08:11.814256Z",
     "iopub.status.idle": "2026-02-18T08:08:11.844837Z",
     "shell.execute_reply": "2026-02-18T08:08:11.844051Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Apply Group-Based Pricing\n",
    "# =============================================================================\n",
    "\n",
    "wholesale_data['price_diff'] = (wholesale_data['selected_price'] - wholesale_data['price']) / wholesale_data['price']\n",
    "wholesale_data = wholesale_data.merge(groups, on=['product_id'], how='left')\n",
    "\n",
    "# Calculate group-weighted prices\n",
    "wholesale_data['new_group_nmv'] = wholesale_data['nmv']\n",
    "wholesale_data.loc[wholesale_data['stocks'] == 0, 'new_group_nmv'] = wholesale_data['nmv'] * 0.1\n",
    "\n",
    "wholesale_data['total_group_nmv'] = wholesale_data.groupby(['main_region', 'group'])['new_group_nmv'].transform('sum')\n",
    "wholesale_data['price_cntrb'] = wholesale_data['selected_price'] * (wholesale_data['new_group_nmv'] / wholesale_data['total_group_nmv'])\n",
    "wholesale_data['final_group_price'] = wholesale_data.groupby(['main_region', 'group'])['price_cntrb'].transform('sum')\n",
    "\n",
    "# Set final price with fallbacks\n",
    "wholesale_data['final_price'] = wholesale_data['selected_price']\n",
    "wholesale_data['final_price'] = np.ceil(wholesale_data['final_price'] * 4) / 4  # Round to nearest 0.25\n",
    "\n",
    "# Handle edge cases\n",
    "wholesale_data.loc[wholesale_data['final_price'] == 0, 'final_price'] = wholesale_data['selected_price']\n",
    "wholesale_data.loc[wholesale_data['final_price'] == 0, 'final_price'] = wholesale_data['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2793a600-7980-4b23-99e1-492b6c05f273",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:08:11.846642Z",
     "iopub.status.busy": "2026-02-18T08:08:11.846424Z",
     "iopub.status.idle": "2026-02-18T08:08:11.851576Z",
     "shell.execute_reply": "2026-02-18T08:08:11.850892Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final pricing complete. Records: 30,482\n"
     ]
    }
   ],
   "source": [
    "# Apply forced category pricing (keep original price for specified categories)\n",
    "wholesale_data.loc[wholesale_data['cat'].isin(FORCED_CAT_LIST), 'final_price'] = wholesale_data['price']\n",
    "\n",
    "print(f\"Final pricing complete. Records: {len(wholesale_data):,}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0526e487-0daf-4eda-b1e1-1fcdfd476ef4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:08:11.853638Z",
     "iopub.status.busy": "2026-02-18T08:08:11.853436Z",
     "iopub.status.idle": "2026-02-18T08:08:22.908297Z",
     "shell.execute_reply": "2026-02-18T08:08:22.907553Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported: Wholesales_new_price_list.xlsx\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Export Full Data to Excel\n",
    "# =============================================================================\n",
    "\n",
    "wholesale_data.to_excel('Wholesales_new_price_list.xlsx', index=False)\n",
    "print(\"Exported: Wholesales_new_price_list.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcceaeee",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Export & Upload <a id='export-upload'></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3dcfebb5-6bdc-489f-9575-3dbc89ab9888",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:08:22.910384Z",
     "iopub.status.busy": "2026-02-18T08:08:22.910192Z",
     "iopub.status.idle": "2026-02-18T08:08:22.981607Z",
     "shell.execute_reply": "2026-02-18T08:08:22.980881Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload data prepared. Records: 45,691\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Prepare Upload Data\n",
    "# =============================================================================\n",
    "\n",
    "# Prepare final data with packing units\n",
    "final_data = wholesale_data[['main_region', 'product_id', 'sku', 'brand', 'cat','wac_p', 'final_price', 'tier']]\n",
    "final_data = final_data.drop_duplicates()\n",
    "final_data = final_data.merge(pu, on='product_id')\n",
    "final_data['new_price'] = final_data['final_price'] * final_data['buc']\n",
    "final_data['wac_p'] = final_data['wac_p'] * final_data['buc']\n",
    "final_data['margin'] = (final_data['new_price']-final_data['wac_p'])/final_data['new_price']\n",
    "\n",
    "# Handle packing unit indexing\n",
    "final_data['ind'] = 1\n",
    "final_data['ind'] = final_data.groupby(['main_region', 'product_id']).ind.cumsum()\n",
    "\n",
    "# Load and apply minimum PU removal rules\n",
    "remove_min_pu = pd.read_csv('skus_to_remove_min.csv')\n",
    "remove_min_pu['remove_min'] = 1\n",
    "final_data = final_data.merge(remove_min_pu[['product_id', 'remove_min']], on='product_id', how='left')\n",
    "\n",
    "final_data['max_ind'] = final_data.groupby(['product_id', 'main_region'])['ind'].transform('max')\n",
    "final_data.loc[(final_data['max_ind'] > 1) & (final_data['ind'] == 1), 'remove_min'] = 1\n",
    "\n",
    "print(f\"Upload data prepared. Records: {len(final_data):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29d0a965-4428-473d-b37b-cb1e8d7d9693",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:08:22.983623Z",
     "iopub.status.busy": "2026-02-18T08:08:22.983417Z",
     "iopub.status.idle": "2026-02-18T08:08:23.000612Z",
     "shell.execute_reply": "2026-02-18T08:08:22.999870Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cart rules prepared. Records: 45,691\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Prepare Cart Rules Data\n",
    "# =============================================================================\n",
    "\n",
    "cart_rules_data = final_data[final_data['new_price'] > 0].copy()\n",
    "\n",
    "# Calculate allowed quantities based on tier\n",
    "cart_rules_data['half_allowed_quantity'] = 25000 / cart_rules_data['new_price']\n",
    "cart_rules_data.loc[cart_rules_data['tier'] == 1, 'half_allowed_quantity'] = 20000 / cart_rules_data['new_price']\n",
    "cart_rules_data.loc[cart_rules_data['margin'] <= 0, 'half_allowed_quantity'] = 10000 / cart_rules_data['new_price']\n",
    "\n",
    "cart_rules_data['Cart_rules'] = np.ceil(cart_rules_data['half_allowed_quantity'])\n",
    "\n",
    "# Brand-specific overrides\n",
    "cart_rules_data.loc[cart_rules_data['brand'].isin(['بست', 'فيوري']), 'Cart_rules'] = 10\n",
    "\n",
    "print(f\"Cart rules prepared. Records: {len(cart_rules_data):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a166de2d-2db1-427d-bb81-cd34cf8ad98d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:08:23.002457Z",
     "iopub.status.busy": "2026-02-18T08:08:23.002253Z",
     "iopub.status.idle": "2026-02-18T08:08:23.030903Z",
     "shell.execute_reply": "2026-02-18T08:08:23.030148Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Finalize Upload DataFrames\n",
    "# =============================================================================\n",
    "\n",
    "# Prepare price upload data\n",
    "to_upload = final_data[['product_id', 'sku', 'pu_id', 'new_price', 'main_region', 'ind', 'remove_min']]\n",
    "to_upload = to_upload.drop_duplicates()\n",
    "to_upload = to_upload.dropna(subset=['new_price'])\n",
    "to_upload = to_upload[(to_upload['new_price'] > 1)].reset_index(drop=True)\n",
    "\n",
    "# Region to cohort mapping (using config)\n",
    "mapping_cc = pd.DataFrame(REGION_COHORT_MAPPING.items(), columns=['main_region', 'new_cohort_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c1a6f747-19c8-4d3b-a2cb-585f4e43e34c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:08:23.033181Z",
     "iopub.status.busy": "2026-02-18T08:08:23.032964Z",
     "iopub.status.idle": "2026-02-18T08:08:23.064516Z",
     "shell.execute_reply": "2026-02-18T08:08:23.063794Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply cohort mapping\n",
    "to_upload = to_upload.merge(mapping_cc, on='main_region')\n",
    "cart_rules_data = cart_rules_data.merge(mapping_cc, on='main_region')\n",
    "\n",
    "to_upload['cohort_id'] = to_upload['new_cohort_id']\n",
    "cart_rules_data['cohort_id'] = cart_rules_data['new_cohort_id']\n",
    "\n",
    "to_upload = to_upload.drop(columns=['new_cohort_id', 'main_region'])\n",
    "cart_rules_data = cart_rules_data.drop(columns=['new_cohort_id', 'main_region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d772f2cd-04bf-4500-b733-8266c1de6651",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:08:23.066692Z",
     "iopub.status.busy": "2026-02-18T08:08:23.066474Z",
     "iopub.status.idle": "2026-02-18T08:08:23.083939Z",
     "shell.execute_reply": "2026-02-18T08:08:23.083240Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To upload: 45,671 price records\n",
      "Cart rules: 45,679 records\n",
      "Cohorts: [1223 1222 1156 1190]\n"
     ]
    }
   ],
   "source": [
    "# Exclude specific products\n",
    "for pid in EXCLUDED_PRODUCT_IDS:\n",
    "    to_upload = to_upload[to_upload['product_id'] != pid]\n",
    "    cart_rules_data = cart_rules_data[cart_rules_data['product_id'] != pid]\n",
    "\n",
    "# Finalize cart rules columns\n",
    "cart_rules_data = cart_rules_data[['cohort_id', 'product_id', 'pu_id', 'Cart_rules']]\n",
    "\n",
    "print(f\"To upload: {len(to_upload):,} price records\")\n",
    "print(f\"Cart rules: {len(cart_rules_data):,} records\")\n",
    "print(f\"Cohorts: {to_upload.cohort_id.unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a07c1567-0298-4a00-9229-854f8ddc429f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:08:23.085970Z",
     "iopub.status.busy": "2026-02-18T08:08:23.085745Z",
     "iopub.status.idle": "2026-02-18T08:08:23.088885Z",
     "shell.execute_reply": "2026-02-18T08:08:23.088075Z"
    }
   },
   "outputs": [],
   "source": [
    "# rem_min = pd.read_csv('skus_to_remove_min.csv')\n",
    "# to_upload = pd.read_excel('Fixed_prices.xlsx')\n",
    "# to_upload['remove_min'] = np.nan\n",
    "# to_upload.loc[to_upload['product_id'].isin(rem_min.product_id.unique()), 'remove_min']=1\n",
    "# to_upload = to_upload[to_upload['product_id']!=11794]\n",
    "# to_upload=to_upload[~to_upload['new_price'].isna()]\n",
    "# df_mapping = pd.DataFrame({\n",
    "#     \"cohort_id\": [700, 701, 702, 703, 704, 1123],\n",
    "#     \"new_cohort\": [61, 695, 699, 697, 698, 696]\n",
    "# })\n",
    "\n",
    "# to_upload_ext = to_upload.merge(df_mapping,on='cohort_id')\n",
    "# to_upload_ext=to_upload_ext.drop(columns = 'cohort_id')\n",
    "# to_upload_ext=to_upload_ext.rename(columns = {'new_cohort':'cohort_id'})\n",
    "# to_upload = pd.concat([to_upload,to_upload_ext])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0dfcb22d-7b84-478d-b769-bc477d35794a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:08:23.090646Z",
     "iopub.status.busy": "2026-02-18T08:08:23.090438Z",
     "iopub.status.idle": "2026-02-18T08:08:31.167402Z",
     "shell.execute_reply": "2026-02-18T08:08:31.166667Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Processing cohort: 1223\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 3 chunks (size: 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving chunks:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving chunks:  33%|███▎      | 1/3 [00:00<00:00,  3.87it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving chunks:  67%|██████▋   | 2/3 [00:00<00:00,  3.86it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving chunks: 100%|██████████| 3/3 [00:00<00:00,  4.14it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving chunks: 100%|██████████| 3/3 [00:00<00:00,  4.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✗ ERROR chunk 1\n",
      "    Response: b'{\"success\":false,\"message\":\"Product ID, Packing Unit ID in rows 1145, 1146 are equal!, Product ID, Packing Unit ID in rows 1147, 1148 are equal!, Product ID, Packing Unit ID in rows 1401, 1402 are equal!, Product ID, Packing Unit ID in rows 1403, 1404 are equal!, Product ID, Packing Unit ID in rows 1405, 1406 are equal!, Product ID, Packing Unit ID in rows 1407, 1408 are equal!, Product ID, Packing Unit ID in rows 1409, 1410 are equal!, Product ID, Packing Unit ID in rows 1411, 1412 are equal!, Product ID, Packing Unit ID in rows 1660, 1661 are equal!, Product ID, Packing Unit ID in rows 1662, 1663 are equal!, Product ID, Packing Unit ID in rows 3021, 3022 are equal!, Product ID, Packing Unit ID in rows 3023, 3024 are equal!, Product ID, Packing Unit ID in rows 3025, 3026 are equal!, Product ID, Packing Unit ID in rows 3027, 3028 are equal!, Product ID, Packing Unit ID in rows 3029, 3030 are equal!, Product ID, Packing Unit ID in rows 3031, 3032 are equal!, Product ID, Packing Unit ID in rows 3065, 3066 are equal!, Product ID, Packing Unit ID in rows 3067, 3068 are equal!, Product ID, Packing Unit ID in rows 3070, 3071 are equal!, Product ID, Packing Unit ID in rows 3073, 3074 are equal!, Product ID, Packing Unit ID in rows 3075, 3076 are equal!\",\"data\":[],\"type\":null}'\n",
      "Upload failed for cohort 1223. Stopping.\n",
      "\n",
      "Price upload complete!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Upload Prices to API\n",
    "# =============================================================================\n",
    "\n",
    "for cohort in to_upload.cohort_id.unique():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing cohort: {cohort}\")\n",
    "    print('='*50)\n",
    "    \n",
    "    # Prepare cohort data\n",
    "    upload = to_upload[to_upload['cohort_id'] == cohort].copy()\n",
    "    out = upload[['product_id', 'sku', 'pu_id', 'new_price', 'ind', 'remove_min']].copy()\n",
    "    out.columns = ['Product ID', 'Product Name', 'Packing Unit ID', 'Price', 'ind', 'remove_min']\n",
    "    \n",
    "    # Set visibility\n",
    "    out['Visibility (YES/NO)'] = 'YES'\n",
    "    out.loc[(out['ind'] == 1) & (out['remove_min'] == 1), 'Visibility (YES/NO)'] = 'NO'\n",
    "    out = out.drop(columns=['ind', 'remove_min']).drop_duplicates()\n",
    "    \n",
    "    # Add required columns\n",
    "    out['Execute At (format:dd/mm/yyyy HH:mm)'] = None\n",
    "    out['Tags'] = None\n",
    "    \n",
    "    # Save full file\n",
    "    file_name_ = f'WS_uploads/1_new_{cohort}.xlsx'.replace(' ', '_')\n",
    "    out.to_excel(file_name_, index=False, engine='xlsxwriter')\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Split into chunks for upload\n",
    "    chunk_size = CHUNK_SIZE_SPECIAL if cohort == 61 else CHUNK_SIZE_DEFAULT\n",
    "    chunks = [out[i:i + chunk_size] for i in range(0, len(out), chunk_size)]\n",
    "    print(f\"Split into {len(chunks)} chunks (size: {chunk_size})\")\n",
    "    \n",
    "    # Save chunk files\n",
    "    fileslist = []\n",
    "    for i, chunk in tqdm(enumerate(chunks), total=len(chunks), desc=\"Saving chunks\"):\n",
    "        output_file = f'WS_uploads/manual/output_{cohort}_chunk_{i + 1}.xlsx'\n",
    "        fileslist.append(output_file)\n",
    "        chunk.to_excel(output_file, index=False, engine='xlsxwriter')\n",
    "    \n",
    "    # Upload chunks\n",
    "    print(\"Uploading...\")\n",
    "    upload_success = True\n",
    "    \n",
    "    for file in fileslist:\n",
    "        chunk_num = file.split('chunk_')[1].split('.xls')[0]\n",
    "        response = post_prices(cohort, file)\n",
    "        \n",
    "        if '\"success\":true' in str(response.content).lower():\n",
    "            print(f\"  ✓ Chunk {chunk_num} uploaded successfully\")\n",
    "        else:\n",
    "            print(f\"  ✗ ERROR chunk {chunk_num}\")\n",
    "            print(f\"    Response: {response.content}\")\n",
    "            upload_success = False\n",
    "            break\n",
    "    \n",
    "    if not upload_success:\n",
    "        print(f\"Upload failed for cohort {cohort}. Stopping.\")\n",
    "        break\n",
    "\n",
    "print(\"\\nPrice upload complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "417f6712-b0db-424e-83e4-481b59bbff47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:08:31.169421Z",
     "iopub.status.busy": "2026-02-18T08:08:31.169214Z",
     "iopub.status.idle": "2026-02-18T08:08:37.811235Z",
     "shell.execute_reply": "2026-02-18T08:08:37.810500Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading Cart Rules...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✗ ERROR cohort 1223\n",
      "    Response: b'{\"status\":false,\"message\":\"Product ID, Packing Unit ID in rows 1145, 1146 are equal!, Product ID, Packing Unit ID in rows 1147, 1148 are equal!, Product ID, Packing Unit ID in rows 1401, 1402 are equal!, Product ID, Packing Unit ID in rows 1403, 1404 are equal!, Product ID, Packing Unit ID in rows 1405, 1406 are equal!, Product ID, Packing Unit ID in rows 1407, 1408 are equal!, Product ID, Packing Unit ID in rows 1409, 1410 are equal!, Product ID, Packing Unit ID in rows 1411, 1412 are equal!, Product ID, Packing Unit ID in rows 1660, 1661 are equal!, Product ID, Packing Unit ID in rows 1662, 1663 are equal!, Product ID, Packing Unit ID in rows 3021, 3022 are equal!, Product ID, Packing Unit ID in rows 3023, 3024 are equal!, Product ID, Packing Unit ID in rows 3025, 3026 are equal!, Product ID, Packing Unit ID in rows 3027, 3028 are equal!, Product ID, Packing Unit ID in rows 3029, 3030 are equal!, Product ID, Packing Unit ID in rows 3031, 3032 are equal!, Product ID, Packing Unit ID in rows 3065, 3066 are equal!, Product ID, Packing Unit ID in rows 3067, 3068 are equal!, Product ID, Packing Unit ID in rows 3070, 3071 are equal!, Product ID, Packing Unit ID in rows 3073, 3074 are equal!, Product ID, Packing Unit ID in rows 3075, 3076 are equal!, Product ID, Packing Unit ID in rows 5733, 5734 are equal!, Product ID, Packing Unit ID in rows 7226, 7227 are equal!, Product ID, Packing Unit ID in rows 7228, 7229 are equal!, Product ID, Packing Unit ID in rows 7230, 7231 are equal!, Product ID, Packing Unit ID in rows 7993, 7994 are equal!, Product ID, Packing Unit ID in rows 7995, 7996 are equal!, Product ID, Packing Unit ID in rows 7997, 7998 are equal!, Product ID, Packing Unit ID in rows 9624, 9625 are equal!, Product ID, Packing Unit ID in rows 9626, 9627 are equal!, Product ID, Packing Unit ID in rows 9628, 9629 are equal!, Product ID, Packing Unit ID in rows 9631, 9632 are equal!, Product ID, Packing Unit ID in rows 9634, 9635 are equal!, Product ID, Packing Unit ID in rows 9745, 9746 are equal!, Product ID, Packing Unit ID in rows 9747, 9748 are equal!, Product ID, Packing Unit ID in rows 9749, 9750 are equal!, Product ID, Packing Unit ID in rows 10239, 10240 are equal!, Product ID, Packing Unit ID in rows 10241, 10242 are equal!, Product ID, Packing Unit ID in rows 10243, 10244 are equal!, Product ID, Packing Unit ID in rows 10245, 10246 are equal!, Product ID, Packing Unit ID in rows 10247, 10248 are equal!, Product ID, Packing Unit ID in rows 10250, 10251 are equal!, Product ID, Packing Unit ID in rows 10252, 10253 are equal!, Product ID, Packing Unit ID in rows 10254, 10255 are equal!, Product ID, Packing Unit ID in rows 10256, 10257 are equal!, Product ID, Packing Unit ID in rows 10258, 10259 are equal!, Product ID, Packing Unit ID in rows 10261, 10262 are equal!, Product ID, Packing Unit ID in rows 10263, 10264 are equal!, Product ID, Packing Unit ID in rows 10265, 10266 are equal!, Product ID, Packing Unit ID in rows 10267, 10268 are equal!\"}'\n",
      "\n",
      "Cart rules upload complete!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Upload Cart Rules to API\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nUploading Cart Rules...\")\n",
    "print('='*50)\n",
    "\n",
    "for cohort in cart_rules_data.cohort_id.unique():\n",
    "    req_data = cart_rules_data[cart_rules_data['cohort_id'] == cohort]\n",
    "    \n",
    "    if len(req_data) == 0:\n",
    "        print(f\"  No cart rules for cohort {cohort}\")\n",
    "        continue\n",
    "    \n",
    "    # Prepare and save cart rules file\n",
    "    req_data = req_data[['product_id', 'pu_id', 'Cart_rules']]\n",
    "    req_data.columns = ['Product ID', 'Packing Unit ID', 'Cart Rules']\n",
    "    \n",
    "    file_name = f'WS_uploads/Cart_rules/CartRules_{cohort}.xlsx'\n",
    "    req_data.to_excel(file_name, index=False, engine='xlsxwriter')\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Upload\n",
    "    response = post_cart_rules(cohort, file_name)\n",
    "    \n",
    "    if response.ok:\n",
    "        print(f\"  ✓ Cohort {cohort}: Cart rules uploaded successfully\")\n",
    "    else:\n",
    "        print(f\"  ✗ ERROR cohort {cohort}\")\n",
    "        print(f\"    Response: {response.content}\")\n",
    "        break\n",
    "\n",
    "print(\"\\nCart rules upload complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319d94fb-67e3-49b0-ace8-12bb6834edaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb8c44d-2566-46e5-9711-71eefb047b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
