{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0aab1c-ca86-4f46-8dac-684b0062f85d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Upgrade pip\n",
    "!pip install --upgrade pip\n",
    "# Connectivity\n",
    "!pip install psycopg2-binary  # PostgreSQL adapter\n",
    "# !pip install snowflake-connector-python  # Snowflake connector\n",
    "!pip install snowflake-connector-python==3.15.0 # Snowflake connector Older Version\n",
    "!pip install snowflake-sqlalchemy  # Snowflake SQLAlchemy connector\n",
    "!pip install warnings # Warnings management\n",
    "# !pip install pyarrow # Serialization\n",
    "!pip install keyring==23.11.0 # Key management\n",
    "!pip install sqlalchemy==1.4.46 # SQLAlchemy\n",
    "!pip install requests # HTTP requests\n",
    "!pip install boto3 # AWS SDK\n",
    "# !pip install slackclient # Slack API\n",
    "!pip install oauth2client # Google Sheets API\n",
    "!pip install gspread==5.9.0 # Google Sheets API\n",
    "!pip install gspread_dataframe # Google Sheets API\n",
    "!pip install google.cloud # Google Cloud\n",
    "# Data manipulation and analysis\n",
    "!pip install polars\n",
    "!pip install pandas==2.2.1\n",
    "!pip install numpy\n",
    "# !pip install fastparquet\n",
    "!pip install openpyxl # Excel file handling\n",
    "!pip install xlsxwriter # Excel file handling\n",
    "# Linear programming\n",
    "!pip install pulp\n",
    "# Date and time handling\n",
    "!pip install --upgrade datetime\n",
    "!pip install python-time\n",
    "!pip install --upgrade pytz\n",
    "# Progress bar\n",
    "!pip install tqdm\n",
    "# Database data types\n",
    "!pip install db-dtypes\n",
    "# Geospatial data handling\n",
    "# !pip install geopandas\n",
    "# !pip install shapely\n",
    "# !pip install fiona\n",
    "# !pip install haversine\n",
    "# Plotting\n",
    "\n",
    "# Modeling\n",
    "!pip install statsmodels\n",
    "!pip install scikit-learn\n",
    "\n",
    "!pip install import-ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d174c3-4449-497d-bbdc-d00c2335e221",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "import json\n",
    "from datetime import date, timedelta\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import setup_environment_2\n",
    "import importlib\n",
    "import import_ipynb\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "importlib.reload(setup_environment_2)\n",
    "setup_environment_2.initialize_env()\n",
    "import gspread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89df83b-343f-44e9-96f7-2b7fea6aea82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_snowflake(query, columns=[]):\n",
    "    import os\n",
    "    import snowflake.connector\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    con = snowflake.connector.connect(\n",
    "        user =  os.environ[\"SNOWFLAKE_USERNAME\"],\n",
    "        account= os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "        password= os.environ[\"SNOWFLAKE_PASSWORD\"],\n",
    "        database =os.environ[\"SNOWFLAKE_DATABASE\"]\n",
    "    )\n",
    "    try:\n",
    "        cur = con.cursor()\n",
    "        cur.execute(\"USE WAREHOUSE COMPUTE_WH\")\n",
    "        cur.execute(query)\n",
    "        if len(columns) == 0:\n",
    "            out = pd.DataFrame(np.array(cur.fetchall()))\n",
    "        else:\n",
    "            out = pd.DataFrame(np.array(cur.fetchall()),columns=columns)\n",
    "        return out\n",
    "    except Exception as e:\n",
    "        print(\"Error: \", e)\n",
    "    finally:\n",
    "        cur.close()\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8994ad8-b430-4445-9ff3-1425fff31010",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "SHOW PARAMETERS LIKE 'TIMEZONE'\n",
    "'''\n",
    "x  = query_snowflake(query)\n",
    "zone_to_use = x[1].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b35fc5-07ba-4ee8-915a-19421c73df34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import base64\n",
    "from botocore.exceptions import ClientError\n",
    "import json\n",
    "from requests import get\n",
    "from pathlib import Path\n",
    "import requests\n",
    "    \n",
    "def get_secret(secret_name):\n",
    "    region_name = \"us-east-1\"\n",
    "\n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "    # In this sample we only handle the specific exceptions for the 'GetSecretValue' API.\n",
    "    # See https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html\n",
    "    # We rethrow the exception by default.\n",
    "\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(SecretId=secret_name)\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'DecryptionFailureException':\n",
    "            # Secrets Manager can't decrypt the protected secret text using the provided KMS key.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InternalServiceErrorException':\n",
    "            # An error occurred on the server side.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidParameterException':\n",
    "            # You provided an invalid value for a parameter.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidRequestException':\n",
    "            # You provided a parameter value that is not valid for the current state of the resource.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'ResourceNotFoundException':\n",
    "            # We can't find the resource that you asked for.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "    else:\n",
    "        # Decrypts secret using the associated KMS CMK.\n",
    "        # Depending on whether the secret is a string or binary, one of these fields will be populated.\n",
    "        if 'SecretString' in get_secret_value_response:\n",
    "            return get_secret_value_response['SecretString']\n",
    "        else:\n",
    "            return base64.b64decode(get_secret_value_response['SecretBinary'])\n",
    "\n",
    "        \n",
    "pricing_api_secret = json.loads(get_secret(\"prod/pricing/api/\"))\n",
    "username = pricing_api_secret[\"egypt_username\"]\n",
    "password = pricing_api_secret[\"egypt_password\"]\n",
    "secret = pricing_api_secret[\"egypt_secret\"]\n",
    "\n",
    "# get access token\n",
    "def get_access_token(url, client_id, client_secret):\n",
    "    \"\"\"\n",
    "    get_access_token function takes three parameters and returns a session token\n",
    "    to connect to MaxAB APIs\n",
    "\n",
    "    :param url: production MaxAB token URL\n",
    "    :param client_id: client ID\n",
    "    :param client_secret: client sercret\n",
    "    :return: session token\n",
    "    \"\"\"\n",
    "    response = requests.post(\n",
    "        url,\n",
    "        data={\"grant_type\": \"password\",\n",
    "              \"username\": username,\n",
    "              \"password\": password},\n",
    "        auth=(client_id, client_secret),\n",
    "    )\n",
    "    return response.json()[\"access_token\"]\n",
    "\n",
    "\n",
    "def post_prices(id_,file_name):\n",
    "    token = get_access_token('https://sso.maxab.info/auth/realms/maxab/protocol/openid-connect/token',\n",
    "                             'main-system-externals',\n",
    "                             secret)\n",
    "    url = \"https://api.maxab.info/main-system/api/admin-portal/cohorts/{}/pricing\".format(id_)\n",
    "    payload={}\n",
    "    files=[\n",
    "      ('sheet',(file_name,open(file_name,'rb'),'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'))\n",
    "    ]\n",
    "    headers = {\n",
    "      'Authorization': 'bearer {}'.format(token)}\n",
    "\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload, files=files)\n",
    "    return response\n",
    "\n",
    "def post_cart_rules(id_,file_name):\n",
    "    token = get_access_token('https://sso.maxab.info/auth/realms/maxab/protocol/openid-connect/token',\n",
    "                             'main-system-externals',\n",
    "                             secret)\n",
    "    url = \"https://api.maxab.info/main-system/api/admin-portal/cohorts/{}/cart-rules\".format(id_)\n",
    "    payload={}\n",
    "    files=[\n",
    "      ('sheet',(file_name,open(file_name,'rb'),'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'))\n",
    "    ]\n",
    "    headers = {\n",
    "      'Authorization': 'bearer {}'.format(token)}\n",
    "\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload, files=files)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b86c495-a75e-4836-887a-c46fe52f5c97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f'''\n",
    "with skus_prices as (\n",
    "with local_prices as (\n",
    "SELECT  case when cpu.cohort_id in (700,695) then 'Cairo'\n",
    "             when cpu.cohort_id in (701) then 'Giza'\n",
    "             when cpu.cohort_id in (704,698) then 'Delta East'\n",
    "             when cpu.cohort_id in (703,697) then 'Delta West'\n",
    "             when cpu.cohort_id in (696,1123,1124,1125,1126) then 'Upper Egypt'\n",
    "             when cpu.cohort_id in (702,699) then 'Alexandria'\n",
    "        end as region,\n",
    "\t\tcohort_id,\n",
    "        pu.product_id,\n",
    "\t\tpu.packing_unit_id as packing_unit_id,\n",
    "\t\tpu.basic_unit_count,\n",
    "        avg(cpu.price) as price\n",
    "FROM    cohort_product_packing_units cpu\n",
    "join    PACKING_UNIT_PRODUCTS pu on pu.id = cpu.product_packing_unit_id\n",
    "WHERE   cpu.cohort_id in (700,701,702,703,704,696,695,698,697,699,1123,1124,1125,1126)\n",
    "    and cpu.created_at::date<>'2023-07-31'\n",
    "    and cpu.is_customized = true\n",
    "\tgroup by all \n",
    "),\n",
    "live_prices as (\n",
    "select region,cohort_id,product_id,pu_id as packing_unit_id,buc as basic_unit_count,NEW_PRICE as price\n",
    "from materialized_views.DBDP_PRICES\n",
    "where created_at = current_date\n",
    "and DATE_PART('hour',CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMEstamp())) BETWEEN SPLIT_PART(time_slot, '-', 1)::int AND SPLIT_PART(time_slot, '-', 2)::int\n",
    "and cohort_id in (700,701,702,703,704,696,695,698,697,699,1123,1124,1125,1126)\n",
    "),\n",
    "prices as (\n",
    "select *\n",
    "from (\n",
    "    SELECT *, 1 AS priority FROM live_prices\n",
    "    UNION ALL\n",
    "    SELECT *, 2 AS priority FROM local_prices\n",
    ")\n",
    "QUALIFY ROW_NUMBER() OVER (PARTITION BY region,cohort_id,product_id,packing_unit_id ORDER BY priority) = 1\n",
    ")\n",
    "select region,cohort_id,product_id,price \n",
    "from prices \n",
    "where basic_unit_count = 1\n",
    "AND (\n",
    "        (product_id = 1309 AND packing_unit_id = 2)\n",
    "        OR (product_id <> 1309)\n",
    "      )\n",
    ")\n",
    "select cohort_id,p.product_id,CONCAT(products.name_ar,' ',products.size,' ',product_units.name_ar) as sku,b.name_ar as brand,cat.name_ar as cat,wac1,wac_p,p.price\n",
    "from skus_prices p \n",
    "join finance.all_cogs c on c.product_id = p.product_id and CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMEstamp()) between c.from_date and c.to_date\n",
    "join products  on products.id = p.product_id \n",
    "join categories cat on cat.id = products.category_id\n",
    "join brands b on b.id = products.brand_id\n",
    "JOIN product_units ON product_units.id = products.unit_id \n",
    "where wac1 > 0 and wac_p > 0 \n",
    "group by all\n",
    "\n",
    "'''\n",
    "whole_sale = query_snowflake(query, columns = ['cohort_id','product_id','sku','brand','cat','wac1','wac_p','price'])\n",
    "whole_sale.columns = whole_sale.columns.str.lower()\n",
    "for col in whole_sale.columns:\n",
    "    whole_sale[col] = pd.to_numeric(whole_sale[col], errors='ignore')   \n",
    "whole_sale = whole_sale.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f738d2e8-c94b-4bae-8b25-148150672199",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT DISTINCT cat, brand, margin as target_bm\n",
    "FROM    performance.commercial_targets cplan\n",
    "QUALIFY CASE WHEN DATE_TRUNC('month', MAX(DATE)OVER()) = DATE_TRUNC('month', CURRENT_DATE) THEN DATE_TRUNC('month', CURRENT_DATE)\n",
    "ELSE DATE_TRUNC('month', CURRENT_DATE - INTERVAL '1 month') END = DATE_TRUNC('month', date)\n",
    "'''\n",
    "brand_cat_target  = query_snowflake(query, columns = ['cat','brand','target_bm'])\n",
    "brand_cat_target.target_bm=pd.to_numeric(brand_cat_target.target_bm)\n",
    "\n",
    "query = '''\n",
    "select cat,sum(target_bm *(target_nmv/cat_total)) as cat_target_margin\n",
    "from (\n",
    "select *,sum(target_nmv)over(partition by cat) as cat_total\n",
    "from (\n",
    "select cat,brand,avg(target_bm) as target_bm , sum(target_nmv) as target_nmv\n",
    "from (\n",
    "SELECT DISTINCT date,city as region,cat, brand, margin as target_bm,nmv as target_nmv\n",
    "FROM    performance.commercial_targets cplan\n",
    "QUALIFY CASE WHEN DATE_TRUNC('month', MAX(DATE)OVER()) = DATE_TRUNC('month', CURRENT_DATE) THEN DATE_TRUNC('month', CURRENT_DATE)\n",
    "ELSE DATE_TRUNC('month', CURRENT_DATE - INTERVAL '1 month') END = DATE_TRUNC('month', date)\n",
    ")\n",
    "group by all\n",
    ")\n",
    ")\n",
    "group by all \n",
    "'''\n",
    "cat_target  = query_snowflake(query, columns = ['cat','cat_target_margin'])\n",
    "cat_target.cat_target_margin=pd.to_numeric(cat_target.cat_target_margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2999cf-a506-4a97-8bdf-b5c7715d9fe7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT  DISTINCT\n",
    "\t\tcpc.cohort_id,  \n",
    "\t\tpso.product_id,\n",
    "        sum(pso.total_price) as nmv,\n",
    "FROM product_sales_order pso\n",
    "JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "join COHORT_PRICING_CHANGES cpc on cpc.id = pso.COHORT_PRICING_CHANGE_ID\n",
    "WHERE so.created_at::date between date_trunc('month', current_date- interval '3 months') and  current_date\n",
    "    AND so.sales_order_status_id not in (7,12)\n",
    "    AND so.channel IN ('telesales','retailer')\n",
    "    AND pso.purchased_item_count <> 0\n",
    "\n",
    "GROUP BY ALL\n",
    "'''\n",
    "sales  = query_snowflake(query, columns = ['cohort_id','product_id','nmv'])\n",
    "sales.columns = sales.columns.str.lower()\n",
    "for col in sales.columns:\n",
    "    sales[col] = pd.to_numeric(sales[col], errors='ignore')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51bea60-893e-4efb-922a-7898226d8771",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "select * \n",
    "from materialized_views.sku_commercial_groups\n",
    "'''\n",
    "groups  = setup_environment_2.dwh_pg_query(query, columns = ['product_id','group'])\n",
    "groups.columns = groups.columns.str.lower()\n",
    "for col in groups.columns:\n",
    "    groups[col] = pd.to_numeric(groups[col], errors='ignore')      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af468a1-f6e1-41ad-97f5-c88c82a73725",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "\n",
    "WITH whs as (SELECT *\n",
    "             FROM   (values\n",
    "                            ('Cairo', 'Mostorod', 1,700),\n",
    "                            ('Giza', 'Barageel', 236,701),\n",
    "                            ('Delta West', 'El-Mahala', 337,703),\n",
    "                            ('Delta West', 'Tanta', 8,703),\n",
    "                            ('Delta East', 'Mansoura FC', 339,704),\n",
    "                            ('Delta East', 'Sharqya', 170,704),\n",
    "                            ('Upper Egypt', 'Assiut FC', 501,1124),\n",
    "                            ('Upper Egypt', 'Bani sweif', 401,1126),\n",
    "                            ('Upper Egypt', 'Menya Samalot', 703,1123),\n",
    "                            ('Upper Egypt', 'Sohag', 632,1125),\n",
    "                            ('Alexandria', 'Khorshed Alex', 797,702),\n",
    "\t\t\t\t\t\t\t('Giza', 'Sakkarah', 962,701)\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t)\n",
    "                    x(region, wh, warehouse_id,cohort_id))\n",
    "\t\t\t\t\t\n",
    "select cohort_id,product_id,sum(stocks) as stocks \n",
    "from (\n",
    "\t\tSELECT DISTINCT whs.region,\n",
    "\t\t\t\tcohort_id,\t\n",
    "                whs.wh,\n",
    "                product_warehouse.product_id,\n",
    "                (product_warehouse.available_stock)::integer as stocks,\n",
    "\n",
    "        from whs\n",
    "        JOIN product_warehouse ON product_warehouse.warehouse_id = whs.warehouse_id\n",
    "        JOIN products on product_warehouse.product_id = products.id\n",
    "        JOIN product_units ON products.unit_id = product_units.id\n",
    "\n",
    "        where   product_warehouse.warehouse_id not in (6,9,10)\n",
    "            AND product_warehouse.is_basic_unit = 1\n",
    "\t\t\tand product_warehouse.available_stock > 0 \n",
    "\n",
    ")\n",
    "group by all\n",
    "\n",
    "'''\n",
    "stocks  = query_snowflake(query, columns = ['cohort_id','product_id','stocks'])\n",
    "stocks.columns = stocks.columns.str.lower()\n",
    "for col in stocks.columns:\n",
    "    stocks[col] = pd.to_numeric(stocks[col], errors='ignore')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7ace89-645d-4ee3-9031-f3407acd26bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "command_string = '''\n",
    "select \n",
    "product_id,\n",
    "PACKING_UNIT_id,\n",
    "basic_unit_count,\n",
    "from PACKING_UNIT_PRODUCTS\n",
    "where PACKING_UNIT_PRODUCTS.deleted_at is null\n",
    "order by product_id,basic_unit_count'''\n",
    "pu = query_snowflake(command_string, columns = ['product_id','pu_id', 'buc'])\n",
    "pu.product_id = pd.to_numeric(pu.product_id)\n",
    "pu.pu_id = pd.to_numeric(pu.pu_id)\n",
    "pu.buc = pd.to_numeric(pu.buc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ebc880-886f-4e6a-9bef-a1a6f04c5a55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query ='''\n",
    "select product_id,new_pp,forecasted_date\n",
    "from materialized_views.DBDP_PRICE_UPS\n",
    "where region = 'Cairo'\n",
    "'''\n",
    "price_ups  = query_snowflake(query, columns = ['product_id','new_pp','forcasted_date'])\n",
    "price_ups.columns = price_ups.columns.str.lower()\n",
    "for col in price_ups.columns:\n",
    "    price_ups[col] = pd.to_numeric(price_ups[col], errors='ignore')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d94e0d5-7d5d-460d-b017-1a2fd634da4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def select_price(x):\n",
    "    new_pp = x['new_pp']\n",
    "    if x['top']:\n",
    "        if pd.isna(new_pp):\n",
    "            return x['wac_p'] / (1-np.minimum(np.maximum((0.2*x['target_margin']),0.012),x['target_margin']))\n",
    "        else:\n",
    "            return x['wac_p'] / (1-(0.75*x['margin']))\n",
    "        \n",
    "    else:\n",
    "        if pd.isna(new_pp):\n",
    "            return x['wac_p'] / (1-np.minimum(np.maximum((0.4*x['target_margin']),0.015),x['target_margin']))\n",
    "        else:\n",
    "            return x['wac_p'] / (1-(0.85*x['margin']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9158017-ccd2-4f8e-a149-a46fb86ea330",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wholesale_data  = whole_sale.merge(brand_cat_target,on =['cat','brand'],how='left')\n",
    "wholesale_data  = wholesale_data.merge(cat_target,on =['cat'],how='left')\n",
    "wholesale_data  = wholesale_data.merge(price_ups,on =['product_id'],how='left')\n",
    "wholesale_data = wholesale_data.merge(stocks,on=['product_id','cohort_id'],how='left')\n",
    "wholesale_data['stocks'] = wholesale_data['stocks'].fillna(0)\n",
    "wholesale_data['margin'] = (wholesale_data['price'] - wholesale_data['wac_p'])/wholesale_data['price']\n",
    "wholesale_data['target_margin'] = (wholesale_data['target_bm'].fillna(wholesale_data['cat_target_margin'])).fillna(wholesale_data['margin'])\n",
    "wholesale_data = wholesale_data.merge(sales,on=['product_id','cohort_id'],how='left')\n",
    "wholesale_data['nmv'] = wholesale_data['nmv'].fillna(0)\n",
    "wholesale_data=wholesale_data[wholesale_data['cohort_id']==700]\n",
    "wholesale_data['total_nmv'] = wholesale_data.groupby('cohort_id')['nmv'].transform(sum)\n",
    "wholesale_data['cntrb'] = wholesale_data['nmv']/wholesale_data['total_nmv'] \n",
    "wholesale_data = wholesale_data.sort_values(['cohort_id', 'nmv'], ascending=[True, False])\n",
    "wholesale_data['nmv_cumulative_cntrb'] = wholesale_data.groupby('cohort_id')['cntrb'].cumsum()\n",
    "wholesale_data['top'] = wholesale_data['nmv_cumulative_cntrb'] <= 0.5\n",
    "wholesale_data['selected_price'] =  wholesale_data.apply(select_price,axis=1)\n",
    "wholesale_data['new_margin'] =  (wholesale_data['selected_price']-wholesale_data['wac_p'])/wholesale_data['selected_price']\n",
    "wholesale_data['price_diff'] = (wholesale_data['selected_price']-wholesale_data['price'])/wholesale_data['price']\n",
    "wholesale_data = wholesale_data.merge(groups,on=['product_id'],how='left')\n",
    "wholesale_data['new_group_nmv'] = wholesale_data['nmv']\n",
    "wholesale_data.loc[wholesale_data['stocks'] == 0 ,'new_group_nmv'] = wholesale_data['nmv']*0.1\n",
    "wholesale_data['total_group_nmv'] =wholesale_data.groupby(['cohort_id','group'])['new_group_nmv'].transform(sum)\n",
    "wholesale_data['price_cntrb'] = wholesale_data['selected_price'] * (wholesale_data['new_group_nmv']/wholesale_data['total_group_nmv'])\n",
    "wholesale_data['final_group_price'] = wholesale_data.groupby(['cohort_id','group'])['price_cntrb'].transform(sum) \n",
    "wholesale_data['final_price'] = wholesale_data['final_group_price'].fillna(wholesale_data['selected_price'])\n",
    "wholesale_data['final_price']=round(wholesale_data['final_price']*4)/4\n",
    "wholesale_data.loc[wholesale_data['final_price']==0,'final_price'] = wholesale_data['selected_price']\n",
    "wholesale_data.loc[wholesale_data['final_price']==0,'final_price'] = wholesale_data['price']\n",
    "wholesale_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76c9534-1af2-4a1e-8958-a82dad7956b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scope = [\"https://spreadsheets.google.com/feeds\",\n",
    "         'https://www.googleapis.com/auth/spreadsheets',\n",
    "         \"https://www.googleapis.com/auth/drive.file\",\n",
    "         \"https://www.googleapis.com/auth/drive\"]\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_dict(json.loads(setup_environment_2.get_secret(\"prod/maxab-sheets\")), scope)\n",
    "client = gspread.authorize(creds)\n",
    "force_brands = client.open('Wholesales_exec').worksheet('brands')\n",
    "force_cats = client.open('Wholesales_exec').worksheet('cats')\n",
    "force_brands_df = pd.DataFrame(force_brands.get_all_records())\n",
    "force_cats_df = pd.DataFrame(force_cats.get_all_records())\n",
    "if force_brands_df.empty:\n",
    "    forced_brand_list = []\n",
    "else:    \n",
    "    forced_brand_list = force_brands_df.brand.unique()\n",
    "    \n",
    "if force_cats_df.empty:\n",
    "    forced_cat_list = []  \n",
    "else:    \n",
    "    forced_cat_list = force_cats_df.cat.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2793a600-7980-4b23-99e1-492b6c05f273",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wholesale_data.loc[wholesale_data['brand'].isin(forced_brand_list),'final_price'] = wholesale_data['price'] \n",
    "wholesale_data.loc[wholesale_data['cat'].isin(forced_cat_list),'final_price'] = wholesale_data['price'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0526e487-0daf-4eda-b1e1-1fcdfd476ef4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wholesale_data.to_excel('Wholesales_new_price_list.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcfebb5-6bdc-489f-9575-3dbc89ab9888",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_data = wholesale_data[['cohort_id', 'product_id', 'sku', 'brand', 'cat','final_price']]\n",
    "final_data=final_data.drop_duplicates()\n",
    "final_data = final_data.merge(pu, on='product_id')\n",
    "final_data['new_price'] = final_data['final_price'] * final_data['buc']\n",
    "final_data['ind'] = 1\n",
    "final_data['ind'] = final_data.groupby(['cohort_id', 'product_id']).ind.cumsum()\n",
    "remove_min_pu = pd.read_csv('skus_to_remove_min.csv')\n",
    "remove_min_pu['remove_min'] = 1\n",
    "final_data = final_data.merge(remove_min_pu[['product_id','remove_min']], on='product_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d0a965-4428-473d-b37b-cb1e8d7d9693",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cart_rules_data = final_data.copy()\n",
    "cart_rules_data = cart_rules_data[cart_rules_data['new_price']>0]\n",
    "cart_rules_data['half_allowed_quantity'] = 25000/(cart_rules_data['new_price'])\n",
    "cart_rules_data['Cart_rules'] = np.ceil(cart_rules_data['half_allowed_quantity'])\n",
    "cart_rules_data.loc[cart_rules_data['brand'].isin(['بست','ريد بل','فيوري']),'Cart_rules']=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a166de2d-2db1-427d-bb81-cd34cf8ad98d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "to_upload = final_data[['product_id','sku','pu_id','new_price','cohort_id','ind','remove_min']]\n",
    "to_upload=to_upload.drop_duplicates()\n",
    "to_upload.dropna(subset=['new_price'], inplace=True)\n",
    "to_upload = to_upload[to_upload.new_price > 0].drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d623ef-0945-4bb8-908f-44a0c7eb44b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "to_upload = to_upload[to_upload['new_price']>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1161f8-c761-41f0-8873-3a487cbe4164",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "to_upload['cohort_id'] =1156\n",
    "cart_rules_data['cohort_id'] =1156"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0953c8-7be9-412d-9a33-c81effc4d299",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cart_rules_data = cart_rules_data[['cohort_id','product_id','pu_id','Cart_rules']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417f6712-b0db-424e-83e4-481b59bbff47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for cohort in to_upload.cohort_id.unique():\n",
    "        upload = to_upload[to_upload['cohort_id']==cohort]\n",
    "        out=upload[['product_id', 'sku', 'pu_id', 'new_price', 'ind', 'remove_min']].copy()\n",
    "        out.columns = ['Product ID','Product Name','Packing Unit ID','Price','ind','remove_min']\n",
    "        out['Visibility (YES/NO)'] = 'YES'\n",
    "        out.loc[(out['ind'] == 1) & (out['remove_min'] == 1), 'Visibility (YES/NO)'] = 'NO'\n",
    "        out.drop(columns=['ind','remove_min'], inplace=True)\n",
    "        out = out.drop_duplicates()\n",
    "        out['Execute At (format:dd/mm/yyyy HH:mm)'] = None\n",
    "        out['Tags'] = None\n",
    "        file_name_ = 'uploads/1_new_{}.xlsx'.format(cohort).replace(' ','_')\n",
    "        out.to_excel(file_name_,index = False,engine = 'xlsxwriter')\n",
    "        time.sleep(5)\n",
    "        ################### Loop to avoid file limit ######################\n",
    "        # split file into chunks\n",
    "        print('Spliting file into chunks...')\n",
    "        if cohort == 61:\n",
    "            chunks = [out[i:i + 2000] for i in range(0, len(out), 2000)]\n",
    "        else:\n",
    "            chunks = [out[i:i + 4000] for i in range(0, len(out), 4000)]\n",
    "        print(f'len chunks = {len(chunks)}')\n",
    "        fileslist = []\n",
    "        for i, chunk in tqdm(enumerate(chunks), total=len(chunks)):\n",
    "            fileslist.append(f'manual/output_{cohort}_chunk_{i + 1}.xlsx')\n",
    "            output_file_path = f'manual/output_{cohort}_chunk_{i + 1}.xlsx'\n",
    "            chunk.to_excel(output_file_path, index=False, engine='xlsxwriter')\n",
    "        # Loop over chunks and upload\n",
    "        print('Uploading...')\n",
    "        for file in fileslist:\n",
    "            chunk = file.split('chunk_')[1].split('.xls')[0]\n",
    "            x = post_prices(cohort, file)\n",
    "            # print(str(x.content))\n",
    "            if ('\"success\":true' in str(x.content).lower()):\n",
    "                print(f\"Prices are upoladed successfuly cohort: {cohort}, chunk: {chunk}\")\n",
    "            else:\n",
    "                print(f\"ERROR cohort: {cohort}, chunk: {chunk}\")\n",
    "                print(x.content)\n",
    "                final_status = False\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c262d3-a258-4e98-b82f-c65c0e566bc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for cohort in cart_rules_data.cohort_id.unique():\n",
    "    req_data = cart_rules_data[cart_rules_data['cohort_id']==cohort]\n",
    "    if len(req_data) > 0 :\n",
    "        req_data = req_data[['product_id','pu_id','Cart_rules']]\n",
    "        req_data.columns = ['Product ID','Packing Unit ID','Cart Rules']\n",
    "        req_data.to_excel(f'CartRules_{cohort}.xlsx', index=False, engine='xlsxwriter')\n",
    "        time.sleep(5)\n",
    "        x =  post_cart_rules(cohort,f'CartRules_{cohort}.xlsx')\n",
    "        if x.ok:\n",
    "            print(f\"success_{cohort}\")\n",
    "        else:\n",
    "            print(f\"ERROR_{cohort}\")\n",
    "            print(x.content)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e7b914-94dc-4ae8-bfd2-b43153e9e607",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
