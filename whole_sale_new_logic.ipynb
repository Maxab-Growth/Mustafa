{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed0aab1c-ca86-4f46-8dac-684b0062f85d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Upgrade pip\n",
    "!pip install --upgrade pip\n",
    "# Connectivity\n",
    "!pip install psycopg2-binary  # PostgreSQL adapter\n",
    "# !pip install snowflake-connector-python  # Snowflake connector\n",
    "!pip install snowflake-connector-python==3.15.0 # Snowflake connector Older Version\n",
    "!pip install snowflake-sqlalchemy  # Snowflake SQLAlchemy connector\n",
    "!pip install warnings # Warnings management\n",
    "# !pip install pyarrow # Serialization\n",
    "!pip install keyring==23.11.0 # Key management\n",
    "!pip install sqlalchemy==1.4.46 # SQLAlchemy\n",
    "!pip install requests # HTTP requests\n",
    "!pip install boto3 # AWS SDK\n",
    "# !pip install slackclient # Slack API\n",
    "!pip install oauth2client # Google Sheets API\n",
    "!pip install gspread==5.9.0 # Google Sheets API\n",
    "!pip install gspread_dataframe # Google Sheets API\n",
    "!pip install google.cloud # Google Cloud\n",
    "# Data manipulation and analysis\n",
    "!pip install polars\n",
    "!pip install pandas==2.2.1\n",
    "!pip install numpy\n",
    "# !pip install fastparquet\n",
    "!pip install openpyxl # Excel file handling\n",
    "!pip install xlsxwriter # Excel file handling\n",
    "# Linear programming\n",
    "!pip install pulp\n",
    "# Date and time handling\n",
    "!pip install --upgrade datetime\n",
    "!pip install python-time\n",
    "!pip install --upgrade pytz\n",
    "# Progress bar\n",
    "!pip install tqdm\n",
    "# Database data types\n",
    "!pip install db-dtypes\n",
    "# Geospatial data handling\n",
    "# !pip install geopandas\n",
    "# !pip install shapely\n",
    "# !pip install fiona\n",
    "# !pip install haversine\n",
    "# Plotting\n",
    "\n",
    "# Modeling\n",
    "!pip install statsmodels\n",
    "!pip install scikit-learn\n",
    "\n",
    "!pip install import-ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7d174c3-4449-497d-bbdc-d00c2335e221",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.Renviron\n",
      "/home/ec2-user/service_account_key.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "import json\n",
    "from datetime import date, timedelta\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import setup_environment_2\n",
    "import importlib\n",
    "import import_ipynb\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "importlib.reload(setup_environment_2)\n",
    "setup_environment_2.initialize_env()\n",
    "import gspread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a89df83b-343f-44e9-96f7-2b7fea6aea82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_snowflake(query, columns=[]):\n",
    "    import os\n",
    "    import snowflake.connector\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    con = snowflake.connector.connect(\n",
    "        user =  os.environ[\"SNOWFLAKE_USERNAME\"],\n",
    "        account= os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "        password= os.environ[\"SNOWFLAKE_PASSWORD\"],\n",
    "        database =os.environ[\"SNOWFLAKE_DATABASE\"]\n",
    "    )\n",
    "    try:\n",
    "        cur = con.cursor()\n",
    "        cur.execute(\"USE WAREHOUSE COMPUTE_WH\")\n",
    "        cur.execute(query)\n",
    "        if len(columns) == 0:\n",
    "            out = pd.DataFrame(np.array(cur.fetchall()))\n",
    "        else:\n",
    "            out = pd.DataFrame(np.array(cur.fetchall()),columns=columns)\n",
    "        return out\n",
    "    except Exception as e:\n",
    "        print(\"Error: \", e)\n",
    "    finally:\n",
    "        cur.close()\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8994ad8-b430-4445-9ff3-1425fff31010",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "SHOW PARAMETERS LIKE 'TIMEZONE'\n",
    "'''\n",
    "x  = query_snowflake(query)\n",
    "zone_to_use = x[1].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33b35fc5-07ba-4ee8-915a-19421c73df34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import base64\n",
    "from botocore.exceptions import ClientError\n",
    "import json\n",
    "from requests import get\n",
    "from pathlib import Path\n",
    "import requests\n",
    "    \n",
    "def get_secret(secret_name):\n",
    "    region_name = \"us-east-1\"\n",
    "\n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "    # In this sample we only handle the specific exceptions for the 'GetSecretValue' API.\n",
    "    # See https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html\n",
    "    # We rethrow the exception by default.\n",
    "\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(SecretId=secret_name)\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'DecryptionFailureException':\n",
    "            # Secrets Manager can't decrypt the protected secret text using the provided KMS key.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InternalServiceErrorException':\n",
    "            # An error occurred on the server side.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidParameterException':\n",
    "            # You provided an invalid value for a parameter.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidRequestException':\n",
    "            # You provided a parameter value that is not valid for the current state of the resource.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'ResourceNotFoundException':\n",
    "            # We can't find the resource that you asked for.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "    else:\n",
    "        # Decrypts secret using the associated KMS CMK.\n",
    "        # Depending on whether the secret is a string or binary, one of these fields will be populated.\n",
    "        if 'SecretString' in get_secret_value_response:\n",
    "            return get_secret_value_response['SecretString']\n",
    "        else:\n",
    "            return base64.b64decode(get_secret_value_response['SecretBinary'])\n",
    "\n",
    "        \n",
    "pricing_api_secret = json.loads(get_secret(\"prod/pricing/api/\"))\n",
    "username = pricing_api_secret[\"egypt_username\"]\n",
    "password = pricing_api_secret[\"egypt_password\"]\n",
    "secret = pricing_api_secret[\"egypt_secret\"]\n",
    "\n",
    "# get access token\n",
    "def get_access_token(url, client_id, client_secret):\n",
    "    \"\"\"\n",
    "    get_access_token function takes three parameters and returns a session token\n",
    "    to connect to MaxAB APIs\n",
    "\n",
    "    :param url: production MaxAB token URL\n",
    "    :param client_id: client ID\n",
    "    :param client_secret: client sercret\n",
    "    :return: session token\n",
    "    \"\"\"\n",
    "    response = requests.post(\n",
    "        url,\n",
    "        data={\"grant_type\": \"password\",\n",
    "              \"username\": username,\n",
    "              \"password\": password},\n",
    "        auth=(client_id, client_secret),\n",
    "    )\n",
    "    return response.json()[\"access_token\"]\n",
    "\n",
    "\n",
    "def post_prices(id_,file_name):\n",
    "    token = get_access_token('https://sso.maxab.info/auth/realms/maxab/protocol/openid-connect/token',\n",
    "                             'main-system-externals',\n",
    "                             secret)\n",
    "    url = \"https://api.maxab.info/main-system/api/admin-portal/cohorts/{}/pricing\".format(id_)\n",
    "    payload={}\n",
    "    files=[\n",
    "      ('sheet',(file_name,open(file_name,'rb'),'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'))\n",
    "    ]\n",
    "    headers = {\n",
    "      'Authorization': 'bearer {}'.format(token)}\n",
    "\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload, files=files)\n",
    "    return response\n",
    "\n",
    "def post_cart_rules(id_,file_name):\n",
    "    token = get_access_token('https://sso.maxab.info/auth/realms/maxab/protocol/openid-connect/token',\n",
    "                             'main-system-externals',\n",
    "                             secret)\n",
    "    url = \"https://api.maxab.info/main-system/api/admin-portal/cohorts/{}/cart-rules\".format(id_)\n",
    "    payload={}\n",
    "    files=[\n",
    "      ('sheet',(file_name,open(file_name,'rb'),'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'))\n",
    "    ]\n",
    "    headers = {\n",
    "      'Authorization': 'bearer {}'.format(token)}\n",
    "\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload, files=files)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b86c495-a75e-4836-887a-c46fe52f5c97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f'''\n",
    "with skus_prices as (\n",
    "with local_prices as (\n",
    "SELECT  case when cpu.cohort_id in (700,695) then 'Cairo'\n",
    "             when cpu.cohort_id in (701) then 'Giza'\n",
    "             when cpu.cohort_id in (704,698) then 'Delta East'\n",
    "             when cpu.cohort_id in (703,697) then 'Delta West'\n",
    "             when cpu.cohort_id in (696,1123,1124,1125,1126) then 'Upper Egypt'\n",
    "             when cpu.cohort_id in (702,699) then 'Alexandria'\n",
    "        end as region,\n",
    "\t\tcohort_id,\n",
    "        pu.product_id,\n",
    "\t\tpu.packing_unit_id as packing_unit_id,\n",
    "\t\tpu.basic_unit_count,\n",
    "        avg(cpu.price) as price\n",
    "FROM    cohort_product_packing_units cpu\n",
    "join    PACKING_UNIT_PRODUCTS pu on pu.id = cpu.product_packing_unit_id\n",
    "WHERE   cpu.cohort_id in (700,701,702,703,704,696,695,698,697,699,1123,1124,1125,1126)\n",
    "    and cpu.created_at::date<>'2023-07-31'\n",
    "    and cpu.is_customized = true\n",
    "\tgroup by all \n",
    "),\n",
    "live_prices as (\n",
    "select region,cohort_id,product_id,pu_id as packing_unit_id,buc as basic_unit_count,NEW_PRICE as price\n",
    "from materialized_views.DBDP_PRICES\n",
    "where created_at = current_date\n",
    "and DATE_PART('hour',CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMEstamp())) BETWEEN SPLIT_PART(time_slot, '-', 1)::int AND (SPLIT_PART(time_slot, '-', 1)::int)+1\n",
    "and cohort_id in (700,701,702,703,704,696,695,698,697,699,1123,1124,1125,1126)\n",
    "),\n",
    "prices as (\n",
    "select *\n",
    "from (\n",
    "    SELECT *, 1 AS priority FROM live_prices\n",
    "    UNION ALL\n",
    "    SELECT *, 2 AS priority FROM local_prices\n",
    ")\n",
    "QUALIFY ROW_NUMBER() OVER (PARTITION BY region,cohort_id,product_id,packing_unit_id ORDER BY priority) = 1\n",
    ")\n",
    "select region,cohort_id,product_id,price \n",
    "from prices \n",
    "where basic_unit_count = 1\n",
    "AND (\n",
    "        (product_id = 1309 AND packing_unit_id = 2)\n",
    "        OR (product_id <> 1309)\n",
    "      )\n",
    ")\n",
    "select region,cohort_id,p.product_id,CONCAT(products.name_ar,' ',products.size,' ',product_units.name_ar) as sku,b.name_ar as brand,cat.name_ar as cat,wac1,wac_p,p.price\n",
    "from skus_prices p \n",
    "join finance.all_cogs c on c.product_id = p.product_id and CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMEstamp()) between c.from_date and c.to_date\n",
    "join products  on products.id = p.product_id \n",
    "join categories cat on cat.id = products.category_id\n",
    "join brands b on b.id = products.brand_id\n",
    "JOIN product_units ON product_units.id = products.unit_id \n",
    "where wac1 > 0 and wac_p > 0 \n",
    "group by all\n",
    "\n",
    "'''\n",
    "whole_sale = query_snowflake(query, columns = ['region','cohort_id','product_id','sku','brand','cat','wac1','wac_p','price'])\n",
    "whole_sale.columns = whole_sale.columns.str.lower()\n",
    "for col in whole_sale.columns:\n",
    "    whole_sale[col] = pd.to_numeric(whole_sale[col], errors='ignore')   \n",
    "whole_sale = whole_sale.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f738d2e8-c94b-4bae-8b25-148150672199",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT DISTINCT cat, brand, margin as target_bm\n",
    "FROM    performance.commercial_targets cplan\n",
    "QUALIFY CASE WHEN DATE_TRUNC('month', MAX(DATE)OVER()) = DATE_TRUNC('month', CURRENT_DATE) THEN DATE_TRUNC('month', CURRENT_DATE)\n",
    "ELSE DATE_TRUNC('month', CURRENT_DATE - INTERVAL '1 month') END = DATE_TRUNC('month', date)\n",
    "'''\n",
    "brand_cat_target  = query_snowflake(query, columns = ['cat','brand','target_bm'])\n",
    "brand_cat_target.target_bm=pd.to_numeric(brand_cat_target.target_bm)\n",
    "\n",
    "query = '''\n",
    "select cat,sum(target_bm *(target_nmv/cat_total)) as cat_target_margin\n",
    "from (\n",
    "select *,sum(target_nmv)over(partition by cat) as cat_total\n",
    "from (\n",
    "select cat,brand,avg(target_bm) as target_bm , sum(target_nmv) as target_nmv\n",
    "from (\n",
    "SELECT DISTINCT date,city as region,cat, brand, margin as target_bm,nmv as target_nmv\n",
    "FROM    performance.commercial_targets cplan\n",
    "QUALIFY CASE WHEN DATE_TRUNC('month', MAX(DATE)OVER()) = DATE_TRUNC('month', CURRENT_DATE) THEN DATE_TRUNC('month', CURRENT_DATE)\n",
    "ELSE DATE_TRUNC('month', CURRENT_DATE - INTERVAL '1 month') END = DATE_TRUNC('month', date)\n",
    ")\n",
    "group by all\n",
    ")\n",
    ")\n",
    "group by all \n",
    "'''\n",
    "cat_target  = query_snowflake(query, columns = ['cat','cat_target_margin'])\n",
    "cat_target.cat_target_margin=pd.to_numeric(cat_target.cat_target_margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e2999cf-a506-4a97-8bdf-b5c7715d9fe7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT  DISTINCT\n",
    "\t\tcpc.cohort_id,  \n",
    "\t\tpso.product_id,\n",
    "        sum(pso.total_price) as nmv,\n",
    "FROM product_sales_order pso\n",
    "JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "join COHORT_PRICING_CHANGES cpc on cpc.id = pso.COHORT_PRICING_CHANGE_ID\n",
    "WHERE so.created_at::date between date_trunc('month', current_date- interval '3 months') and  current_date\n",
    "    AND so.sales_order_status_id not in (7,12)\n",
    "    AND so.channel IN ('telesales','retailer')\n",
    "    AND pso.purchased_item_count <> 0\n",
    "\n",
    "GROUP BY ALL\n",
    "'''\n",
    "sales  = query_snowflake(query, columns = ['cohort_id','product_id','nmv'])\n",
    "sales.columns = sales.columns.str.lower()\n",
    "for col in sales.columns:\n",
    "    sales[col] = pd.to_numeric(sales[col], errors='ignore')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b51bea60-893e-4efb-922a-7898226d8771",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "select * \n",
    "from materialized_views.sku_commercial_groups\n",
    "'''\n",
    "groups  = setup_environment_2.dwh_pg_query(query, columns = ['product_id','group'])\n",
    "groups.columns = groups.columns.str.lower()\n",
    "for col in groups.columns:\n",
    "    groups[col] = pd.to_numeric(groups[col], errors='ignore')      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5af468a1-f6e1-41ad-97f5-c88c82a73725",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "\n",
    "WITH whs as (SELECT *\n",
    "             FROM   (values\n",
    "                            ('Cairo', 'Mostorod', 1,700),\n",
    "                            ('Giza', 'Barageel', 236,701),\n",
    "                            ('Delta West', 'El-Mahala', 337,703),\n",
    "                            ('Delta West', 'Tanta', 8,703),\n",
    "                            ('Delta East', 'Mansoura FC', 339,704),\n",
    "                            ('Delta East', 'Sharqya', 170,704),\n",
    "                            ('Upper Egypt', 'Assiut FC', 501,1124),\n",
    "                            ('Upper Egypt', 'Bani sweif', 401,1126),\n",
    "                            ('Upper Egypt', 'Menya Samalot', 703,1123),\n",
    "                            ('Upper Egypt', 'Sohag', 632,1125),\n",
    "                            ('Alexandria', 'Khorshed Alex', 797,702),\n",
    "\t\t\t\t\t\t\t('Giza', 'Sakkarah', 962,701)\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t)\n",
    "                    x(region, wh, warehouse_id,cohort_id))\n",
    "\t\t\t\t\t\n",
    "select cohort_id,product_id,sum(stocks) as stocks \n",
    "from (\n",
    "\t\tSELECT DISTINCT whs.region,\n",
    "\t\t\t\tcohort_id,\t\n",
    "                whs.wh,\n",
    "                product_warehouse.product_id,\n",
    "                (product_warehouse.available_stock)::integer as stocks,\n",
    "\n",
    "        from whs\n",
    "        JOIN product_warehouse ON product_warehouse.warehouse_id = whs.warehouse_id\n",
    "        JOIN products on product_warehouse.product_id = products.id\n",
    "        JOIN product_units ON products.unit_id = product_units.id\n",
    "\n",
    "        where   product_warehouse.warehouse_id not in (6,9,10)\n",
    "            AND product_warehouse.is_basic_unit = 1\n",
    "\t\t\tand product_warehouse.available_stock > 0 \n",
    "\n",
    ")\n",
    "group by all\n",
    "\n",
    "'''\n",
    "stocks  = query_snowflake(query, columns = ['cohort_id','product_id','stocks'])\n",
    "stocks.columns = stocks.columns.str.lower()\n",
    "for col in stocks.columns:\n",
    "    stocks[col] = pd.to_numeric(stocks[col], errors='ignore')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c7ace89-645d-4ee3-9031-f3407acd26bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "command_string = '''\n",
    "select \n",
    "product_id,\n",
    "PACKING_UNIT_id,\n",
    "basic_unit_count,\n",
    "from PACKING_UNIT_PRODUCTS\n",
    "where PACKING_UNIT_PRODUCTS.deleted_at is null\n",
    "order by product_id,basic_unit_count'''\n",
    "pu = query_snowflake(command_string, columns = ['product_id','pu_id', 'buc'])\n",
    "pu.product_id = pd.to_numeric(pu.product_id)\n",
    "pu.pu_id = pd.to_numeric(pu.pu_id)\n",
    "pu.buc = pd.to_numeric(pu.buc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72ebc880-886f-4e6a-9bef-a1a6f04c5a55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query ='''\n",
    "select product_id,new_pp,forecasted_date\n",
    "from materialized_views.DBDP_PRICE_UPS\n",
    "where region = 'Cairo'\n",
    "'''\n",
    "price_ups  = query_snowflake(query, columns = ['product_id','new_pp','forcasted_date'])\n",
    "price_ups.columns = price_ups.columns.str.lower()\n",
    "for col in price_ups.columns:\n",
    "    price_ups[col] = pd.to_numeric(price_ups[col], errors='ignore')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7c68b9a-dec0-4d84-b1a9-0e40fb9f32e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "select distinct case when r.name_en like '%Delta%' then 'Delta' else r.name_en end as main_region, case when r.id = 2 then s.name_en else r.name_en end as region\n",
    "from regions r \n",
    "join states s on s.region_id = r.id\n",
    "'''\n",
    " \n",
    "region_mapping = query_snowflake(query, columns = ['main_region','region'])\n",
    "region_mapping.columns = region_mapping.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b8298fb-306a-47b8-8f37-5793298ea048",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>product_id</th>\n",
       "      <th>p_vtw</th>\n",
       "      <th>r_vtw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Upper Egypt</td>\n",
       "      <td>2424</td>\n",
       "      <td>128.184369</td>\n",
       "      <td>36.746557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cairo</td>\n",
       "      <td>24126</td>\n",
       "      <td>269.239418</td>\n",
       "      <td>43.702525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cairo</td>\n",
       "      <td>23859</td>\n",
       "      <td>174.181752</td>\n",
       "      <td>43.702525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Delta East</td>\n",
       "      <td>7706</td>\n",
       "      <td>23.343598</td>\n",
       "      <td>43.107765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Giza</td>\n",
       "      <td>11846</td>\n",
       "      <td>124.799371</td>\n",
       "      <td>47.893861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13708</th>\n",
       "      <td>Giza</td>\n",
       "      <td>5560</td>\n",
       "      <td>27.331724</td>\n",
       "      <td>47.893861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13709</th>\n",
       "      <td>Giza</td>\n",
       "      <td>512</td>\n",
       "      <td>158.985801</td>\n",
       "      <td>47.893861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13710</th>\n",
       "      <td>Alexandria</td>\n",
       "      <td>13063</td>\n",
       "      <td>48.717949</td>\n",
       "      <td>38.572451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13711</th>\n",
       "      <td>Upper Egypt</td>\n",
       "      <td>12219</td>\n",
       "      <td>279.583333</td>\n",
       "      <td>36.746557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13712</th>\n",
       "      <td>Alexandria</td>\n",
       "      <td>11897</td>\n",
       "      <td>110.175439</td>\n",
       "      <td>38.572451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13713 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            region  product_id       p_vtw      r_vtw\n",
       "0      Upper Egypt        2424  128.184369  36.746557\n",
       "1            Cairo       24126  269.239418  43.702525\n",
       "2            Cairo       23859  174.181752  43.702525\n",
       "3       Delta East        7706   23.343598  43.107765\n",
       "4             Giza       11846  124.799371  47.893861\n",
       "...            ...         ...         ...        ...\n",
       "13708         Giza        5560   27.331724  47.893861\n",
       "13709         Giza         512  158.985801  47.893861\n",
       "13710   Alexandria       13063   48.717949  38.572451\n",
       "13711  Upper Egypt       12219  279.583333  36.746557\n",
       "13712   Alexandria       11897  110.175439  38.572451\n",
       "\n",
       "[13713 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query ='''\n",
    "WITH whs as (SELECT *\n",
    "             FROM   (values\n",
    "                            ('Cairo', 'El-Marg', 38),\n",
    "                            ('Cairo', 'Mostorod', 1),\n",
    "                            ('Giza', 'Barageel', 236),\n",
    "                            ('Giza', 'Basatin', 39),\n",
    "                            ('Delta West', 'El-Mahala', 337),\n",
    "                            ('Delta West', 'Tanta', 8),\n",
    "                            ('Delta East', 'Mansoura FC', 339),\n",
    "                            ('Delta East', 'Sharqya', 170),\n",
    "                            ('Upper Egypt', 'Assiut FC', 501),\n",
    "                            ('Upper Egypt', 'Bani sweif', 401),\n",
    "                            ('Upper Egypt', 'Menya Samalot', 703),\n",
    "                            ('Upper Egypt', 'Sohag', 632),\n",
    "                            ('Alexandria', 'Khorshed Alex', 797))\n",
    "                    x(region, wh, warehouse_id)),\n",
    "\n",
    "region_vtw as (\n",
    "select region, nmv/weight as r_vtw\n",
    "from (\n",
    "SELECT\n",
    "        whs.region as region,\n",
    "        sum(product_sales_order.total_price) as nmv,\n",
    "        sum((packing_unit_products.weight*product_sales_order.PURCHASED_ITEM_COUNT)/1000.00) AS weight\n",
    "\n",
    "    \n",
    "    \n",
    "    From\n",
    "        sales_orders\n",
    "    \n",
    "        JOIN product_sales_order ON product_sales_order.sales_order_id = sales_orders.id\n",
    "        join whs on whs.warehouse_id = product_sales_order.warehouse_id\n",
    "         JOIN packing_unit_products ON product_sales_order.product_id = packing_unit_products.product_id AND product_sales_order.packing_unit_id = packing_unit_products.packing_unit_id\n",
    "        JOIN products ON products.id = product_sales_order.product_id\n",
    "        JOIN packing_units ON packing_units.id = product_sales_order.packing_unit_id\n",
    "        JOIN product_units ON product_units.id = products.unit_id\n",
    "        JOIN categories ON categories.id = products.category_id \n",
    "        JOIN sections ON categories.section_id= sections.id \n",
    "        JOIN brands ON brands.id = products.brand_id\n",
    "\n",
    "    \n",
    "    WHERE sales_orders.CREATED_AT::date>=current_date-30\n",
    "    AND sales_orders.sales_order_status_id NOT IN (7,12)\n",
    "    \n",
    "    GROUP BY all\n",
    ")\n",
    "),\n",
    "product_vtw as (\n",
    "select region,product_id,sum(p_vtw*cntrb) as p_vtw\n",
    "from (\n",
    "select *,nmv/(sum(nmv) over(partition by region,product_id)) as cntrb\n",
    "from (\n",
    "select region,product_id,packing_unit_id,nmv,nmv/weight as p_vtw \n",
    "from (\n",
    "SELECT\n",
    "        whs.region as region,\n",
    "\t\tproduct_sales_order.product_id,\n",
    "\t\tproduct_sales_order.packing_unit_id,\n",
    "        sum(product_sales_order.total_price) as nmv,\n",
    "        sum((packing_unit_products.weight*product_sales_order.PURCHASED_ITEM_COUNT)/1000.00) AS weight\n",
    "\n",
    "    \n",
    "    \n",
    "    From\n",
    "        sales_orders\n",
    "    \n",
    "        JOIN product_sales_order ON product_sales_order.sales_order_id = sales_orders.id\n",
    "        join whs on whs.warehouse_id = product_sales_order.warehouse_id\n",
    "         JOIN packing_unit_products ON product_sales_order.product_id = packing_unit_products.product_id AND product_sales_order.packing_unit_id = packing_unit_products.packing_unit_id\n",
    "        JOIN products ON products.id = product_sales_order.product_id\n",
    "        JOIN packing_units ON packing_units.id = product_sales_order.packing_unit_id\n",
    "        JOIN product_units ON product_units.id = products.unit_id\n",
    "        JOIN categories ON categories.id = products.category_id \n",
    "        JOIN sections ON categories.section_id= sections.id \n",
    "        JOIN brands ON brands.id = products.brand_id\n",
    "\n",
    "    \n",
    "    WHERE sales_orders.CREATED_AT::date>=current_date-30\n",
    "    AND sales_orders.sales_order_status_id NOT IN (7,12)\n",
    "    \n",
    "    GROUP BY all\n",
    "\n",
    ")\n",
    "where weight > 0 \n",
    ")\n",
    ")\n",
    "group by all \n",
    ")\n",
    "select pv.*,rv.r_vtw\n",
    "from product_vtw pv \n",
    "join region_vtw rv on rv.region = pv.region\n",
    "\n",
    "'''\n",
    "vtw  = query_snowflake(query, columns = ['region','product_id','p_vtw','r_vtw'])\n",
    "vtw.columns = vtw.columns.str.lower()\n",
    "for col in vtw.columns:\n",
    "    vtw[col] = pd.to_numeric(vtw[col], errors='ignore')         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3aa51fd-f3f2-4c30-a0d0-2dea2fc156cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scope = [\"https://spreadsheets.google.com/feeds\",\n",
    "         'https://www.googleapis.com/auth/spreadsheets',\n",
    "         \"https://www.googleapis.com/auth/drive.file\",\n",
    "         \"https://www.googleapis.com/auth/drive\"]\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_dict(json.loads(setup_environment_2.get_secret(\"prod/maxab-sheets\")), scope)\n",
    "client = gspread.authorize(creds)\n",
    "brands_list =  client.open('Anniversary Campaign 2025 (Final)').worksheet('Suppliers Brands')\n",
    "brands_df = pd.DataFrame(brands_list.get_all_records())[['Brands']].drop_duplicates()\n",
    "brands_reduce = [brand  for brand in brands_df['Brands']]+['البوادي','هارفست فوودز','هاينز']\n",
    "brands_reduce.remove('فيوري')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cd42b98-c5f5-40d9-9f1e-915d37d8884a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "force_brands = client.open('Wholesales_exec').worksheet('brands')\n",
    "force_cats = client.open('Wholesales_exec').worksheet('cats')\n",
    "force_brands_df = pd.DataFrame(force_brands.get_all_records())\n",
    "force_cats_df = pd.DataFrame(force_cats.get_all_records())\n",
    "if force_brands_df.empty:\n",
    "    forced_brand_list = []\n",
    "else:    \n",
    "    forced_brand_list = force_brands_df.brand.unique()\n",
    "    \n",
    "if force_cats_df.empty:\n",
    "    forced_cat_list = []  \n",
    "else:    \n",
    "    forced_cat_list = force_cats_df.cat.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b6436d7-f3cd-4c7b-a403-8fa4ca9aedc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def select_price(x):\n",
    "    new_pp = x['new_pp']\n",
    "    wac_p = x['wac_p']\n",
    "    wac1 = x['wac1']\n",
    "    if pd.isna(new_pp):\n",
    "        final_wac = wac_p \n",
    "    else:\n",
    "        final_wac =  np.minimum(((wac_p/wac1))*new_pp,wac_p)\n",
    "        x['target_margin'] = x['margin']*0.9\n",
    "        \n",
    "    if x['brand'] in  forced_brand_list:    \n",
    "        if x['brand'] in ['كوكا كولا', 'شويبس']:\n",
    "            return np.maximum(final_wac/(1-(x['margin']*0.65)),(0.25*x['target_margin']))\n",
    "        elif x['brand'] == 'جود كير':\n",
    "            return np.maximum(final_wac/(1-(x['margin']*0.5)),(0.25*x['target_margin']))\n",
    "        else:\n",
    "            return final_wac/(1-(x['margin']*0.8))\n",
    "    elif x['brand'] == 'فيوري':\n",
    "        return final_wac/(1-(x['margin']*0.9))\n",
    "    elif x['cat'] == 'ورقيات':\n",
    "        return final_wac / (1-np.minimum(np.maximum((0.6*x['target_margin']),0.015),x['target_margin']))\n",
    "    else:\n",
    "        if x['vtw_status']:\n",
    "            mult = 1\n",
    "        else: \n",
    "            mult = 1.1\n",
    "            \n",
    "        if x['tier'] == 1 :\n",
    "                return final_wac/ (1-np.minimum(np.maximum(((0.2*mult)*x['target_margin']),0.01),x['target_margin']))\n",
    "        elif x['tier']== 2 :  \n",
    "            return final_wac / (1-np.minimum(np.maximum(((0.25*mult)*x['target_margin']),0.015),x['target_margin']))\n",
    "        elif x['tier']== 3 :  \n",
    "            return final_wac / (1-np.minimum(np.maximum(((0.4*mult)*x['target_margin']),0.015),x['target_margin']))    \n",
    "        else:\n",
    "            return final_wac / (1-np.minimum(np.maximum(((0.6*mult)*x['target_margin']),0.015),x['target_margin']))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93287cf6-58ed-497d-acaf-0e9bac9d2741",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wholesale_data  = whole_sale.merge(brand_cat_target,on =['cat','brand'],how='left')\n",
    "wholesale_data  = wholesale_data.merge(cat_target,on =['cat'],how='left')\n",
    "wholesale_data = wholesale_data.merge(stocks,on=['product_id','cohort_id'],how='left')\n",
    "wholesale_data = wholesale_data.merge(vtw,on=['product_id','region'],how='left')\n",
    "wholesale_data=wholesale_data.fillna(0)\n",
    "wholesale_data['vtw_status'] = wholesale_data['p_vtw'] >= wholesale_data['r_vtw']\n",
    "\n",
    "wholesale_data['stocks'] = wholesale_data['stocks'].fillna(0)\n",
    "wholesale_data['margin'] = (wholesale_data['price'] - wholesale_data['wac_p'])/wholesale_data['price']\n",
    "wholesale_data['target_margin'] = (wholesale_data['target_bm'].fillna(wholesale_data['cat_target_margin'])).fillna(wholesale_data['margin'])\n",
    "wholesale_data = wholesale_data.merge(sales,on=['product_id','cohort_id'],how='left')\n",
    "wholesale_data['nmv'] = wholesale_data['nmv'].fillna(0)\n",
    "wholesale_data = region_mapping.merge(wholesale_data,on=['region'])\n",
    "wholesale_data = wholesale_data.groupby(['main_region','product_id','sku','brand','cat','target_margin','wac1','wac_p']).agg({'price':'mean','stocks':'sum','margin':'mean','nmv':'sum','vtw_status':'max'}).reset_index()\n",
    "wholesale_data  = wholesale_data.merge(price_ups,on =['product_id'],how='left')\n",
    "wholesale_data['total_nmv'] = wholesale_data.groupby('main_region')['nmv'].transform(sum)\n",
    "wholesale_data['cntrb'] = wholesale_data['nmv']/wholesale_data['total_nmv'] \n",
    "wholesale_data = wholesale_data.sort_values(['main_region', 'nmv'], ascending=[True, False])\n",
    "wholesale_data['nmv_cumulative_cntrb'] = wholesale_data.groupby('main_region')['cntrb'].cumsum()\n",
    "\n",
    "cond = [wholesale_data['nmv_cumulative_cntrb'] < 0.4 ,\n",
    "        (wholesale_data['nmv_cumulative_cntrb'] >= 0.4)&(wholesale_data['nmv_cumulative_cntrb'] < 0.6),\n",
    "        (wholesale_data['nmv_cumulative_cntrb'] >= 0.6)&(wholesale_data['nmv_cumulative_cntrb'] < 0.8),\n",
    "        wholesale_data['nmv_cumulative_cntrb'] >= 0.8\n",
    "       ] \n",
    "cho = [1,2,3,4]\n",
    "\n",
    "wholesale_data['tier'] = np.select(cond,cho,default = 4)\n",
    "\n",
    "wholesale_data.loc[wholesale_data['brand'].isin([brands_reduce]),'tier']=np.maximum(wholesale_data['tier']-2,1)\n",
    "wholesale_data['base_price'] =  wholesale_data.apply(select_price,axis=1)\n",
    "wholesale_data['new_margin'] =  (wholesale_data['base_price']-wholesale_data['wac_p'])/wholesale_data['base_price']\n",
    "wholesale_data['drop_margin'] = ((wholesale_data['new_margin']-wholesale_data['margin'])/wholesale_data['margin'])*-1\n",
    "buffer_map = {\n",
    "    1: 0.7,   \n",
    "    2: 0.75,  \n",
    "    3: 0.8,   \n",
    "    4: 0.85  \n",
    "}\n",
    "# You can change these numbers to be more/less aggressive.\n",
    "\n",
    "wholesale_data['buffer_B'] = wholesale_data['tier'].map(buffer_map)\n",
    "wholesale_data['allowed_discount_fraction'] = wholesale_data['margin'] * wholesale_data['buffer_B']\n",
    "wholesale_data['wholesale_min_price'] = wholesale_data['price'] * (1 - wholesale_data['allowed_discount_fraction'])\n",
    "wholesale_data['min_margin'] = (wholesale_data['wholesale_min_price'] -wholesale_data['wac_p']) /wholesale_data['wholesale_min_price']  \n",
    "wholesale_data['selected_margin'] = np.maximum(wholesale_data['min_margin'],wholesale_data['new_margin'])\n",
    "wholesale_data['selected_price'] = wholesale_data['wac_p']/(1-wholesale_data['selected_margin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9158017-ccd2-4f8e-a149-a46fb86ea330",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wholesale_data['price_diff'] = (wholesale_data['selected_price']-wholesale_data['price'])/wholesale_data['price']\n",
    "wholesale_data = wholesale_data.merge(groups,on=['product_id'],how='left')\n",
    "wholesale_data['new_group_nmv'] = wholesale_data['nmv']\n",
    "wholesale_data.loc[wholesale_data['stocks'] == 0 ,'new_group_nmv'] = wholesale_data['nmv']*0.1\n",
    "wholesale_data['total_group_nmv'] =wholesale_data.groupby(['main_region','group'])['new_group_nmv'].transform(sum)\n",
    "wholesale_data['price_cntrb'] = wholesale_data['selected_price'] * (wholesale_data['new_group_nmv']/wholesale_data['total_group_nmv'])\n",
    "wholesale_data['final_group_price'] = wholesale_data.groupby(['main_region','group'])['price_cntrb'].transform(sum) \n",
    "wholesale_data['final_price'] = wholesale_data['final_group_price'].fillna(wholesale_data['selected_price'])\n",
    "wholesale_data['final_price']=np.ceil(wholesale_data['final_price']*4)/4\n",
    "wholesale_data.loc[wholesale_data['final_price']==0,'final_price'] = wholesale_data['selected_price']\n",
    "wholesale_data.loc[wholesale_data['final_price']==0,'final_price'] = wholesale_data['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2793a600-7980-4b23-99e1-492b6c05f273",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wholesale_data.loc[wholesale_data['cat'].isin(forced_cat_list),'final_price'] = wholesale_data['price'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0526e487-0daf-4eda-b1e1-1fcdfd476ef4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wholesale_data.to_excel('Wholesales_new_price_list.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3dcfebb5-6bdc-489f-9575-3dbc89ab9888",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_data = wholesale_data[['main_region', 'product_id', 'sku', 'brand', 'cat','final_price','tier']]\n",
    "final_data=final_data.drop_duplicates()\n",
    "final_data = final_data.merge(pu, on='product_id')\n",
    "final_data['new_price'] = final_data['final_price'] * final_data['buc']\n",
    "final_data['ind'] = 1\n",
    "final_data['ind'] = final_data.groupby(['main_region', 'product_id']).ind.cumsum()\n",
    "remove_min_pu = pd.read_csv('skus_to_remove_min.csv')\n",
    "remove_min_pu['remove_min'] = 1\n",
    "final_data = final_data.merge(remove_min_pu[['product_id','remove_min']], on='product_id', how='left')\n",
    "final_data['max_ind'] = final_data.groupby(['product_id','main_region'])['ind'].transform(max)\n",
    "final_data.loc[(final_data['max_ind']>1)&(final_data['ind']==1),'remove_min']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29d0a965-4428-473d-b37b-cb1e8d7d9693",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cart_rules_data = final_data.copy()\n",
    "cart_rules_data = cart_rules_data[cart_rules_data['new_price']>0]\n",
    "cart_rules_data['half_allowed_quantity'] = 25000/(cart_rules_data['new_price'])\n",
    "cart_rules_data.loc[cart_rules_data['tier']==1,'half_allowed_quantity'] = 20000/(cart_rules_data['new_price'])\n",
    "#cart_rules_data.loc[cart_rules_data['cat'].isin(['ألبان']),'half_allowed_quantity']=15000/(cart_rules_data['new_price'])\n",
    "cart_rules_data['Cart_rules'] = np.ceil(cart_rules_data['half_allowed_quantity'])\n",
    "cart_rules_data.loc[cart_rules_data['brand'].isin(['بست','فيوري']),'Cart_rules']=10\n",
    "cart_rules_data.loc[cart_rules_data['brand'].isin(['ريد بل']),'Cart_rules']=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a166de2d-2db1-427d-bb81-cd34cf8ad98d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "to_upload = final_data[['product_id','sku','pu_id','new_price','main_region','ind','remove_min']]\n",
    "to_upload=to_upload.drop_duplicates()\n",
    "to_upload.dropna(subset=['new_price'], inplace=True)\n",
    "to_upload = to_upload[to_upload.new_price > 0].drop_duplicates().reset_index(drop=True)\n",
    "to_upload = to_upload[to_upload['new_price']>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1a6f747-19c8-4d3b-a2cb-585f4e43e34c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mapping_cc = pd.DataFrame({\n",
    "    'main_region': ['Greater Cairo', 'Upper Egypt','Delta','Alexandria'],\n",
    "    'new_cohort_id': [1156, 1190,1222,1223]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d772f2cd-04bf-4500-b733-8266c1de6651",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "to_upload = to_upload.merge(mapping_cc,on='main_region')\n",
    "cart_rules_data = cart_rules_data.merge(mapping_cc,on='main_region')\n",
    "to_upload['cohort_id'] =to_upload['new_cohort_id']\n",
    "cart_rules_data['cohort_id'] =cart_rules_data['new_cohort_id']\n",
    "to_upload=to_upload.drop(columns= ['new_cohort_id','main_region'])\n",
    "cart_rules_data=cart_rules_data.drop(columns= ['new_cohort_id','main_region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0dfcb22d-7b84-478d-b769-bc477d35794a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "to_upload=to_upload[to_upload['product_id']!=4541]\n",
    "cart_rules_data=cart_rules_data[cart_rules_data['product_id']!=4541]\n",
    "to_upload=to_upload[to_upload['product_id']!=12973]\n",
    "cart_rules_data=cart_rules_data[cart_rules_data['product_id']!=12973]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "417f6712-b0db-424e-83e4-481b59bbff47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spliting file into chunks...\n",
      "len chunks = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading...\n",
      "Prices are upoladed successfuly cohort: 1223, chunk: 1\n",
      "Prices are upoladed successfuly cohort: 1223, chunk: 2\n",
      "Prices are upoladed successfuly cohort: 1223, chunk: 3\n",
      "Spliting file into chunks...\n",
      "len chunks = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  3.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading...\n",
      "Prices are upoladed successfuly cohort: 1222, chunk: 1\n",
      "Prices are upoladed successfuly cohort: 1222, chunk: 2\n",
      "Prices are upoladed successfuly cohort: 1222, chunk: 3\n",
      "Spliting file into chunks...\n",
      "len chunks = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading...\n",
      "Prices are upoladed successfuly cohort: 1156, chunk: 1\n",
      "Prices are upoladed successfuly cohort: 1156, chunk: 2\n",
      "Prices are upoladed successfuly cohort: 1156, chunk: 3\n",
      "Spliting file into chunks...\n",
      "len chunks = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading...\n",
      "Prices are upoladed successfuly cohort: 1190, chunk: 1\n",
      "Prices are upoladed successfuly cohort: 1190, chunk: 2\n",
      "Prices are upoladed successfuly cohort: 1190, chunk: 3\n"
     ]
    }
   ],
   "source": [
    "cart_rules_data = cart_rules_data[['cohort_id','product_id','pu_id','Cart_rules']]\n",
    "for cohort in to_upload.cohort_id.unique():\n",
    "        upload = to_upload[to_upload['cohort_id']==cohort]\n",
    "        out=upload[['product_id', 'sku', 'pu_id', 'new_price', 'ind', 'remove_min']].copy()\n",
    "        out.columns = ['Product ID','Product Name','Packing Unit ID','Price','ind','remove_min']\n",
    "        out['Visibility (YES/NO)'] = 'YES'\n",
    "        out.loc[(out['ind'] == 1) & (out['remove_min'] == 1), 'Visibility (YES/NO)'] = 'NO'\n",
    "        out.drop(columns=['ind','remove_min'], inplace=True)\n",
    "        out = out.drop_duplicates()\n",
    "        out['Execute At (format:dd/mm/yyyy HH:mm)'] = None\n",
    "        out['Tags'] = None\n",
    "        file_name_ = 'uploads/1_new_{}.xlsx'.format(cohort).replace(' ','_')\n",
    "        out.to_excel(file_name_,index = False,engine = 'xlsxwriter')\n",
    "        time.sleep(5)\n",
    "        ################### Loop to avoid file limit ######################\n",
    "        # split file into chunks\n",
    "        print('Spliting file into chunks...')\n",
    "        if cohort == 61:\n",
    "            chunks = [out[i:i + 2000] for i in range(0, len(out), 2000)]\n",
    "        else:\n",
    "            chunks = [out[i:i + 4000] for i in range(0, len(out), 4000)]\n",
    "        print(f'len chunks = {len(chunks)}')\n",
    "        fileslist = []\n",
    "        for i, chunk in tqdm(enumerate(chunks), total=len(chunks)):\n",
    "            fileslist.append(f'manual/output_{cohort}_chunk_{i + 1}.xlsx')\n",
    "            output_file_path = f'manual/output_{cohort}_chunk_{i + 1}.xlsx'\n",
    "            chunk.to_excel(output_file_path, index=False, engine='xlsxwriter')\n",
    "        # Loop over chunks and upload\n",
    "        print('Uploading...')\n",
    "        for file in fileslist:\n",
    "            chunk = file.split('chunk_')[1].split('.xls')[0]\n",
    "            x = post_prices(cohort, file)\n",
    "            if ('\"success\":true' in str(x.content).lower()):\n",
    "                print(f\"Prices are upoladed successfuly cohort: {cohort}, chunk: {chunk}\")\n",
    "            else:\n",
    "                print(f\"ERROR cohort: {cohort}, chunk: {chunk}\")\n",
    "                print(x.content)\n",
    "                final_status = False\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "12c262d3-a258-4e98-b82f-c65c0e566bc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success_1223\n",
      "success_1222\n",
      "success_1156\n",
      "success_1190\n"
     ]
    }
   ],
   "source": [
    "for cohort in cart_rules_data.cohort_id.unique():\n",
    "    req_data = cart_rules_data[cart_rules_data['cohort_id']==cohort]\n",
    "    if len(req_data) > 0 :\n",
    "        req_data = req_data[['product_id','pu_id','Cart_rules']]\n",
    "        req_data.columns = ['Product ID','Packing Unit ID','Cart Rules']\n",
    "        req_data.to_excel(f'CartRules_{cohort}.xlsx', index=False, engine='xlsxwriter')\n",
    "        time.sleep(5)\n",
    "        x =  post_cart_rules(cohort,f'CartRules_{cohort}.xlsx')\n",
    "        if x.ok:\n",
    "            print(f\"success_{cohort}\")\n",
    "        else:\n",
    "            print(f\"ERROR_{cohort}\")\n",
    "            print(x.content)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d8cf4a-6469-4137-a639-2a5f547727bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b514ff07-7d25-414d-a07d-a953865910a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
