{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "371c6017-a418-42a9-9fa7-1eef1ce8cfc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Upgrade pip\n",
    "!pip install --upgrade pip\n",
    "# Connectivity\n",
    "!pip install psycopg2-binary  # PostgreSQL adapter\n",
    "# !pip install snowflake-connector-python  # Snowflake connector\n",
    "!pip install snowflake-connector-python==3.15.0 # Snowflake connector Older Version\n",
    "!pip install snowflake-sqlalchemy  # Snowflake SQLAlchemy connector\n",
    "!pip install warnings # Warnings management\n",
    "# !pip install pyarrow # Serialization\n",
    "!pip install keyring==23.11.0 # Key management\n",
    "!pip install sqlalchemy==1.4.46 # SQLAlchemy\n",
    "!pip install requests # HTTP requests\n",
    "!pip install boto3 # AWS SDK\n",
    "# !pip install slackclient # Slack API\n",
    "!pip install oauth2client # Google Sheets API\n",
    "!pip install gspread==5.9.0 # Google Sheets API\n",
    "!pip install gspread_dataframe # Google Sheets API\n",
    "!pip install google.cloud # Google Cloud\n",
    "# Data manipulation and analysis\n",
    "!pip install polars\n",
    "!pip install pandas==2.2.1\n",
    "!pip install numpy\n",
    "# !pip install fastparquet\n",
    "!pip install openpyxl # Excel file handling\n",
    "!pip install xlsxwriter # Excel file handling\n",
    "# Linear programming\n",
    "!pip install pulp\n",
    "# Date and time handling\n",
    "!pip install --upgrade datetime\n",
    "!pip install python-time\n",
    "!pip install --upgrade pytz\n",
    "# Progress bar\n",
    "!pip install tqdm\n",
    "# Database data types\n",
    "!pip install db-dtypes\n",
    "# Geospatial data handling\n",
    "# !pip install geopandas\n",
    "# !pip install shapely\n",
    "# !pip install fiona\n",
    "# !pip install haversine\n",
    "# Plotting\n",
    "\n",
    "# Modeling\n",
    "!pip install statsmodels\n",
    "!pip install scikit-learn\n",
    "\n",
    "!pip install import-ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "007e8be5-f084-46b8-9ab9-e6dcec8662a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.Renviron\n",
      "/home/ec2-user/service_account_key.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "import json\n",
    "from datetime import date, timedelta\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import setup_environment_2\n",
    "import importlib\n",
    "import import_ipynb\n",
    "import warnings\n",
    "import boto3\n",
    "import requests\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "importlib.reload(setup_environment_2)\n",
    "setup_environment_2.initialize_env()\n",
    "import os\n",
    "import time\n",
    "import pytz\n",
    "import gspread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95799e78-611c-4a58-873e-de86b15e2cc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_snowflake(query, columns=[]):\n",
    "    import os\n",
    "    import snowflake.connector\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    con = snowflake.connector.connect(\n",
    "        user =  os.environ[\"SNOWFLAKE_USERNAME\"],\n",
    "        account= os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "        password= os.environ[\"SNOWFLAKE_PASSWORD\"],\n",
    "        database =os.environ[\"SNOWFLAKE_DATABASE\"]\n",
    "    )\n",
    "    try:\n",
    "        cur = con.cursor()\n",
    "        cur.execute(\"USE WAREHOUSE COMPUTE_WH\")\n",
    "        cur.execute(query)\n",
    "        if len(columns) == 0:\n",
    "            out = pd.DataFrame(np.array(cur.fetchall()))\n",
    "        else:\n",
    "            out = pd.DataFrame(np.array(cur.fetchall()),columns=columns)\n",
    "        return out\n",
    "    except Exception as e:\n",
    "        print(\"Error: \", e)\n",
    "    finally:\n",
    "        cur.close()\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4995eba-902b-42d6-9f9b-59aab8007f1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "SHOW PARAMETERS LIKE 'TIMEZONE'\n",
    "'''\n",
    "x  = query_snowflake(query)\n",
    "zone_to_use = x[1].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "221cfa1c-8664-4ac5-a695-fd932b050608",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scope = [\"https://spreadsheets.google.com/feeds\",\n",
    "         'https://www.googleapis.com/auth/spreadsheets',\n",
    "         \"https://www.googleapis.com/auth/drive.file\",\n",
    "         \"https://www.googleapis.com/auth/drive\"]\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_dict(json.loads(setup_environment_2.get_secret(\"prod/maxab-sheets\")), scope)\n",
    "client = gspread.authorize(creds)\n",
    "included_brand = client.open('QD_brands').worksheet('Happy Hour push brands')\n",
    "included_brand_df = pd.DataFrame(included_brand.get_all_records())\n",
    "for col in included_brand_df.columns:\n",
    "    included_brand_df[col] = pd.to_numeric(included_brand_df[col], errors='ignore')\n",
    "try:    \n",
    "    b_list = [brand  for brand in included_brand_df['brand']]\n",
    "except:\n",
    "    b_list= [] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f35f29-f58b-401a-a201-c3aa6fdc12c4",
   "metadata": {},
   "source": [
    "## Prodcut Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25c3b4fe-f7f8-4e8c-8ecb-87cd3536059a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "command_string = f'''\n",
    "with last_update as (\n",
    "select  DATE_PART('hour', max_date) * 60 + DATE_PART('minute', max_date) AS total_minutes\n",
    "from (\n",
    "select max(created_at) as max_date from sales_orders\n",
    ")\n",
    "\n",
    "),\n",
    " predicted_rr  as (\n",
    "select product_id,warehouse_id,rr,date\n",
    "from Finance.PREDICTED_RUNNING_RATES\n",
    "where date >= CURRENT_DATE\n",
    "qualify date = max(date)over(partition by product_id,warehouse_id)\n",
    "),\n",
    "days_stocks as (\n",
    "select timestamp::date as date ,product_id,warehouse_id,avg(in_stock) as in_stock_perc,avg(case when date_part('hour',timestamp) =date_part('hour',current_timestamp)-1 then  in_stock end) as last_hour_stocks\n",
    "from (\n",
    "select timestamp,product_id,warehouse_id,case when AVAILABLE_STOCK > 0 then 1 else 0 end as in_stock\n",
    "from materialized_views.STOCK_SNAP_SHOTS_RECENT sss\n",
    "where sss.timestamp::date >= date_trunc('month',current_date - 90)\n",
    "and date_part('hour',sss.timestamp)<date_part('hour',CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMEstamp())) \n",
    "and warehouse_id in (1,8,170,236,337,339,401,501,632,703,797,962)\n",
    ")\n",
    "group by all \n",
    "),\n",
    "base as (\n",
    "select *, row_number()over(partition by retailer_id order by priority) as rnk \n",
    "from (\n",
    "select x.*,TAGGABLE_ID as retailer_id \n",
    "from (\n",
    "select id as cohort_id,name as cohort_name,priority,dynamic_tag_id \n",
    "from cohorts \n",
    "where is_active = 'true'\n",
    "and id in (700,701,702,703,704,1123,1124,1125,1126)\n",
    ") x \n",
    "join DYNAMIC_TAGgables dt on x.dynamic_tag_id = dt.dynamic_tag_id and  dt.dynamic_tag_id <> 3038\n",
    ")\n",
    "qualify rnk = 1 \n",
    "),\n",
    "sales_data as (\n",
    "SELECT  DISTINCT\n",
    "\t\tso.created_at::date as date,\n",
    "\t\tpso.warehouse_id as warehouse_id,\n",
    "\t\tpso.product_id,\n",
    "\t\tCONCAT(products.name_ar,' ',products.size,' ',product_units.name_ar) as sku,\n",
    "\t\tbrands.name_ar as brand, \n",
    "\t\tcategories.name_ar as cat,\n",
    "        sum(pso.total_price) as all_day_nmv,\n",
    "\t\tsum(case when (date_part('hour',so.created_at)*60 + DATE_PART('minute', so.created_at))< (select * from last_update) then pso.total_price end) as uth_nmv,\n",
    "\t\tsum(case when (date_part('hour',so.created_at)*60 + DATE_PART('minute', so.created_at))\n",
    "\t\tbetween (select * from last_update) -60 \n",
    "\t\tand (select * from last_update)\n",
    "\t\tthen pso.total_price end) as last_hour_nmv,\n",
    "\n",
    "\n",
    "FROM product_sales_order pso\n",
    "JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "JOIN products on products.id=pso.product_id\n",
    "JOIN brands on products.brand_id = brands.id \n",
    "JOIN categories ON products.category_id = categories.id\n",
    "JOIN finance.all_cogs f  ON f.product_id = pso.product_id\n",
    "                        AND f.from_date::date <= so.created_at ::date\n",
    "                        AND f.to_date::date > so.created_at ::date\n",
    "JOIN product_units ON product_units.id = products.unit_id \n",
    "JOIN materialized_views.retailer_polygon on materialized_views.retailer_polygon.retailer_id=so.retailer_id\n",
    "JOIN districts on districts.id=materialized_views.retailer_polygon.district_id\n",
    "JOIN cities on cities.id=districts.city_id\n",
    "join states on states.id=cities.state_id\n",
    "join regions on regions.id=states.region_id  \n",
    "\n",
    "WHERE   True\n",
    "    AND so.created_at ::date >= date_trunc('month',current_date - 90)\n",
    "    AND so.sales_order_status_id not in (7,12)\n",
    "    AND so.channel IN ('telesales','retailer')\n",
    "    AND pso.purchased_item_count <> 0\n",
    "\n",
    "GROUP BY ALL\n",
    "order by date desc\n",
    "),\n",
    "data as (\n",
    "select * , 1/nullif((0.3*week_distance+0.1*month_distance+0.6*day_distance),0) as distance\n",
    "from (\n",
    "select * ,\n",
    "floor((DATE_PART('day', date) - 1) / 7 + 1) AS week_of_month,\n",
    "DATE_PART('month', date) as month,\n",
    "DATE_PART('DOW', date) AS day_number,\n",
    "\n",
    "abs(floor((DATE_PART('day', current_date) - 1) / 7 + 1) - week_of_month)  as week_distance ,\n",
    "abs(DATE_PART('month', current_date)- month) as month_distance,\n",
    "abs(DATE_PART('DOW', current_date)- day_number) as day_distance\n",
    "from (\n",
    "select *, max(case when date = CURRENT_DATE then last_hour_stocks end) over(partition by product_id,warehouse_id) as current_stocks \n",
    "from (\n",
    "select ds.*, all_day_nmv,\n",
    "uth_nmv,\n",
    "last_hour_nmv\n",
    "from days_stocks ds\n",
    "left join sales_data sd  on ds.product_id = sd.product_id and ds.warehouse_id = sd.warehouse_id and ds.date= sd.date\n",
    ")\n",
    ")\n",
    "where current_stocks <> 0 \n",
    "and (in_stock_perc = 1 or date = CURRENT_DATE)\n",
    ")\n",
    "),\n",
    "current_state as (\n",
    "select product_id,warehouse_id,AVAILABLE_STOCK,activation\n",
    "from PRODUCT_WAREHOUSE\n",
    "where IS_BASIC_UNIT = 1\n",
    "and case when product_id = 1309 then packing_unit_id <> 23 else true end\n",
    ")\n",
    "select x.*,\n",
    "cs.AVAILABLE_STOCK,\n",
    "cs.activation,\n",
    "coalesce(prr.rr,0) as rr,\n",
    "case when coalesce(prr.rr,0) <>0 then cs.AVAILABLE_STOCK/coalesce(prr.rr,0) else cs.AVAILABLE_STOCK end  as doh ,\n",
    "cs.AVAILABLE_STOCK*f.wac1  as stock_value\n",
    " from (\n",
    "select product_id,warehouse_id,\n",
    "coalesce(max(case when state = 'prev' then all_day_nmv end),0) as prev_all_day,\n",
    "coalesce(max(case when state = 'prev' then uth_nmv end),0)  as prev_uth,\n",
    "coalesce(max(case when state = 'prev' then last_hour_nmv end),0)  as prev_last_hour,\n",
    "\n",
    "coalesce(max(case when state = 'current' then all_day_nmv end),0)  as current_all_day,\n",
    "coalesce(max(case when state = 'current' then uth_nmv end),0)  as current_uth,\n",
    "coalesce(max(case when state = 'current' then last_hour_nmv end),0)  as current_last_hour\n",
    "\n",
    "from (\n",
    "select 'current' as state,product_id,warehouse_id,all_day_nmv,uth_nmv,last_hour_nmv\n",
    "from data\n",
    "where date = CURRENT_DATE\n",
    "union all \n",
    "(\n",
    "select state,product_id,warehouse_id,\n",
    "sum(all_day_nmv*distance)/sum(distance) as all_day_nmv,\n",
    "sum(uth_nmv*distance)/sum(distance) as uth_nmv,\n",
    "sum(last_hour_nmv*distance)/sum(distance) as last_hour_nmv\n",
    "from(\n",
    "select 'prev' as state,product_id,warehouse_id,all_day_nmv,uth_nmv,last_hour_nmv,distance\n",
    "from data \n",
    "where date <> CURRENT_DATE\n",
    ")\n",
    "group by all \n",
    ")\n",
    ")\n",
    "group by all \n",
    ")x \n",
    "join current_state cs on x.product_id = cs.product_id and x.warehouse_id = cs.warehouse_id\n",
    "left join predicted_rr prr on x.product_id = prr.product_id and x.warehouse_id = prr.warehouse_id \n",
    "join products p on p.id = x.product_id\n",
    "join finance.all_cogs f on f.product_id = x.product_id and CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMEstamp()) between f.from_date and f.to_date \n",
    "where doh > 1 \n",
    "and p.activation ='true'\n",
    "and cs.activation = 'true'\n",
    "and cs.AVAILABLE_STOCK * f.wac1 >= 1000\n",
    "and prev_uth > 0\n",
    "'''\n",
    "product_data = query_snowflake(command_string, columns = ['product_id','warehouse_id','prev_all_day','prev_uth','prev_last_hour','current_all_day','current_uth','current_last_hour','available_stock','activation','rr','doh','stock_value'])\n",
    "product_data.product_id = pd.to_numeric(product_data.product_id)\n",
    "product_data.warehouse_id = pd.to_numeric(product_data.warehouse_id)\n",
    "product_data.prev_all_day = pd.to_numeric(product_data.prev_all_day)\n",
    "product_data.prev_uth = pd.to_numeric(product_data.prev_uth)\n",
    "product_data.prev_last_hour = pd.to_numeric(product_data.prev_last_hour)\n",
    "product_data.current_all_day = pd.to_numeric(product_data.current_all_day)\n",
    "product_data.current_uth = pd.to_numeric(product_data.current_uth)\n",
    "product_data.current_last_hour = pd.to_numeric(product_data.current_last_hour)\n",
    "product_data.available_stock = pd.to_numeric(product_data.available_stock)\n",
    "product_data.rr = pd.to_numeric(product_data.rr)\n",
    "product_data.doh = pd.to_numeric(product_data.doh)\n",
    "product_data.stock_value = pd.to_numeric(product_data.stock_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89169236-0e28-4be4-8800-96377ec58584",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f'''\n",
    "with last_update as (\n",
    "select  DATE_PART('hour', max_date) * 60 + DATE_PART('minute', max_date) AS total_minutes\n",
    "from (\n",
    "select max(created_at) as max_date from sales_orders\n",
    ")\n",
    "),\n",
    "base as (\n",
    "select *, row_number()over(partition by retailer_id order by priority) as rnk \n",
    "from (\n",
    "select x.*,TAGGABLE_ID as retailer_id \n",
    "from (\n",
    "select id as cohort_id,name as cohort_name,priority,dynamic_tag_id \n",
    "from cohorts \n",
    "where is_active = 'true'\n",
    "and id in (700,701,702,703,704,1123,1124,1125,1126)\n",
    ") x \n",
    "join DYNAMIC_TAGgables dt on x.dynamic_tag_id = dt.dynamic_tag_id and dt.dynamic_tag_id <> 3038\n",
    "\n",
    ")\n",
    "qualify rnk = 1 \n",
    "),\n",
    "sales as (\n",
    "SELECT \n",
    "\t\tso.created_at::date as date,\n",
    "\t\tpso.warehouse_id as warehouse_id,\n",
    "        sum(pso.total_price) as all_day_nmv,\n",
    "\t\tsum(case when (date_part('hour',so.created_at)*60 + DATE_PART('minute', so.created_at))< (select * from last_update) then pso.total_price end) as uth_nmv,\n",
    "\t\tsum(case when (date_part('hour',so.created_at)*60 + DATE_PART('minute', so.created_at))\n",
    "\t\tbetween (select * from last_update) -60 \n",
    "\t\tand (select * from last_update)\n",
    "\t\tthen pso.total_price end) as last_hour_nmv,\n",
    "\n",
    "\n",
    "FROM product_sales_order pso\n",
    "JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "JOIN products on products.id=pso.product_id\n",
    "JOIN brands on products.brand_id = brands.id \n",
    "JOIN categories ON products.category_id = categories.id\n",
    "JOIN finance.all_cogs f  ON f.product_id = pso.product_id\n",
    "                        AND f.from_date::date <= so.created_at ::date\n",
    "                        AND f.to_date::date > so.created_at ::date\n",
    "JOIN product_units ON product_units.id = products.unit_id \n",
    "JOIN materialized_views.retailer_polygon on materialized_views.retailer_polygon.retailer_id=so.retailer_id\n",
    "JOIN districts on districts.id=materialized_views.retailer_polygon.district_id\n",
    "JOIN cities on cities.id=districts.city_id\n",
    "join states on states.id=cities.state_id\n",
    "join regions on regions.id=states.region_id  \n",
    "\n",
    "WHERE   True\n",
    "    AND so.created_at ::date between date_trunc('month',current_date - 60) and current_date -1 \n",
    "    AND so.sales_order_status_id not in (7,12)\n",
    "    AND so.channel IN ('telesales','retailer')\n",
    "    AND pso.purchased_item_count <> 0\n",
    "\n",
    "GROUP BY ALL\n",
    "order by date desc\n",
    ")\n",
    "select warehouse_id,sum(uth_cntrb*distance)/sum(distance) as uth_cntrb\n",
    "from (\n",
    "select *, 1/nullif((0.3*week_distance+0.1*month_distance+0.6*day_distance),0) as distance\n",
    "from(\n",
    "select * ,uth_nmv/all_day_nmv as uth_cntrb,\n",
    "floor((DATE_PART('day', date) - 1) / 7 + 1) AS week_of_month,\n",
    "DATE_PART('month', date) as month,\n",
    "DATE_PART('DOW', date) AS day_number,\n",
    "abs(floor((DATE_PART('day', current_date) - 1) / 7 + 1) - week_of_month)  as week_distance ,\n",
    "abs(DATE_PART('month', current_date)- month) as month_distance,\n",
    "abs(DATE_PART('DOW', current_date)- day_number) as day_distance\n",
    "\n",
    "from sales \n",
    ")\n",
    ")\n",
    "group by all \n",
    "'''\n",
    "uth_cntrb = query_snowflake(query, columns = ['warehouse_id','uth_cntrb'])\n",
    "uth_cntrb.warehouse_id = pd.to_numeric(uth_cntrb.warehouse_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17bfde16-bbc0-4c0d-9c6c-2569a5dddf3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f'''\n",
    "WITH whs as (SELECT *\n",
    "             FROM   (values\n",
    "                            ('Cairo', 'El-Marg', 38,700),\n",
    "                            ('Cairo', 'Mostorod', 1,700),\n",
    "                            ('Giza', 'Barageel', 236,701),\n",
    "                            ('Delta West', 'El-Mahala', 337,703),\n",
    "                            ('Delta West', 'Tanta', 8,703),\n",
    "                            ('Delta East', 'Mansoura FC', 339,704),\n",
    "                            ('Delta East', 'Sharqya', 170,704),\n",
    "                            ('Upper Egypt', 'Assiut FC', 501,1124),\n",
    "                            ('Upper Egypt', 'Bani sweif', 401,1126),\n",
    "                            ('Upper Egypt', 'Menya Samalot', 703,1123),\n",
    "                            ('Upper Egypt', 'Sohag', 632,1125),\n",
    "                            ('Alexandria', 'Khorshed Alex', 797,702),\n",
    "\t\t\t\t\t\t\t('Giza', 'Sakkarah', 962,701)\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t)\n",
    "                    x(region, wh, warehouse_id,cohort_id)),\n",
    "\n",
    "\n",
    "local_prices as (\n",
    "SELECT  case when cpu.cohort_id in (700,695) then 'Cairo'\n",
    "             when cpu.cohort_id in (701) then 'Giza'\n",
    "             when cpu.cohort_id in (704,698) then 'Delta East'\n",
    "             when cpu.cohort_id in (703,697) then 'Delta West'\n",
    "             when cpu.cohort_id in (696,1123,1124,1125,1126) then 'Upper Egypt'\n",
    "             when cpu.cohort_id in (702,699) then 'Alexandria'\n",
    "        end as region,\n",
    "\t\tcohort_id,\n",
    "        pu.product_id,\n",
    "\t\tpu.packing_unit_id as packing_unit_id,\n",
    "\t\tpu.basic_unit_count,\n",
    "        avg(cpu.price) as price\n",
    "FROM    cohort_product_packing_units cpu\n",
    "join    PACKING_UNIT_PRODUCTS pu on pu.id = cpu.product_packing_unit_id\n",
    "WHERE   cpu.cohort_id in (700,701,702,703,704,696,695,698,697,699,1123,1124,1125,1126)\n",
    "    and cpu.created_at::date<>'2023-07-31'\n",
    "    and cpu.is_customized = true\n",
    "\tgroup by all \n",
    "),\n",
    "live_prices as (\n",
    "select region,cohort_id,product_id,pu_id as packing_unit_id,buc as basic_unit_count,NEW_PRICE as price\n",
    "from materialized_views.DBDP_PRICES\n",
    "where created_at = current_date\n",
    "and DATE_PART('hour',CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMEstamp())) BETWEEN SPLIT_PART(time_slot, '-', 1)::int AND (SPLIT_PART(time_slot, '-', 1)::int)+1\n",
    "and cohort_id in (700,701,702,703,704,696,695,698,697,699,1123,1124,1125,1126)\n",
    "),\n",
    "prices as (\n",
    "select *\n",
    "from (\n",
    "    SELECT *, 1 AS priority FROM live_prices\n",
    "    UNION ALL\n",
    "    SELECT *, 2 AS priority FROM local_prices\n",
    ")\n",
    "QUALIFY ROW_NUMBER() OVER (PARTITION BY region,cohort_id,product_id,packing_unit_id ORDER BY priority) = 1\n",
    ")\n",
    "select warehouse_id,product_id,price \n",
    "from prices \n",
    "join whs on prices.cohort_id = whs.cohort_id\n",
    "and basic_unit_count = 1 \n",
    "and case when product_id = 1309 then packing_unit_id <> 23 else true end\n",
    "\n",
    "'''\n",
    "product_warehouse_price = query_snowflake(query, columns = ['warehouse_id','product_id','price'])\n",
    "product_warehouse_price.warehouse_id = pd.to_numeric(product_warehouse_price.warehouse_id)\n",
    "product_warehouse_price.product_id = pd.to_numeric(product_warehouse_price.product_id)\n",
    "product_warehouse_price.price = pd.to_numeric(product_warehouse_price.price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f15810cc-6e26-4f5a-93a8-3f60efb52644",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f'''\n",
    "WITH whs as (SELECT *\n",
    "             FROM   (values\n",
    "                            ('Cairo', 'El-Marg', 38,700),\n",
    "                            ('Cairo', 'Mostorod', 1,700),\n",
    "                            ('Giza', 'Barageel', 236,701),\n",
    "                            ('Delta West', 'El-Mahala', 337,703),\n",
    "                            ('Delta West', 'Tanta', 8,703),\n",
    "                            ('Delta East', 'Mansoura FC', 339,704),\n",
    "                            ('Delta East', 'Sharqya', 170,704),\n",
    "                            ('Upper Egypt', 'Assiut FC', 501,1124),\n",
    "                            ('Upper Egypt', 'Bani sweif', 401,1126),\n",
    "                            ('Upper Egypt', 'Menya Samalot', 703,1123),\n",
    "                            ('Upper Egypt', 'Sohag', 632,1125),\n",
    "                            ('Alexandria', 'Khorshed Alex', 797,702),\n",
    "\t\t\t\t\t\t\t('Giza', 'Sakkarah', 962,701)\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t)\n",
    "                    x(region, wh, warehouse_id,cohort_id)),\n",
    "full_data as (\n",
    "select products.id as product_id, region,warehouse_id\n",
    "from products , whs \n",
    "where activation = 'true'\n",
    "),\t\t\t\t\n",
    "\n",
    "MP as (\n",
    "select region,product_id,\n",
    "min(min_price) as min_price,\n",
    "min(max_price) as max_price,\n",
    "min(mod_price) as mod_price,\n",
    "min(true_min) as true_min,\n",
    "min(true_max) as true_max\n",
    "\n",
    "from (\n",
    "select mp.region,mp.product_id,mp.pu_id,\n",
    "min_price/BASIC_UNIT_COUNT as min_price,\n",
    "max_price/BASIC_UNIT_COUNT as max_price,\n",
    "mod_price/BASIC_UNIT_COUNT as mod_price,\n",
    "TRUE_MIN_PRICE/BASIC_UNIT_COUNT as true_min,\n",
    "TRUE_MAX_PRICE/BASIC_UNIT_COUNT as true_max\n",
    "from materialized_views.marketplace_prices mp \n",
    "join packing_unit_products pup on pup.product_id = mp.product_id and pup.packing_unit_id = mp.pu_id\n",
    "join finance.all_cogs f on f.product_id = mp.product_id and CURRENT_TIMESTAMP between f.from_date and f.to_date\n",
    "where  least(min_price,mod_price) between wac_p*0.9 and wac_p*1.3 \n",
    ")\n",
    "group by all \n",
    "),\n",
    "region_mapping AS (\n",
    "    SELECT * \n",
    "\tFROM \n",
    "\t(\tVALUES\n",
    "        ('Delta East', 'Delta West'),\n",
    "        ('Delta West', 'Delta East'),\n",
    "        ('Alexandria', 'Cairo'),\n",
    "        ('Alexandria', 'Giza'),\n",
    "        ('Upper Egypt', 'Cairo'),\n",
    "        ('Upper Egypt', 'Giza'),\n",
    "\t\t('Cairo','Giza'),\n",
    "\t\t('Giza','Cairo'),\n",
    "\t\t('Delta West', 'Cairo'),\n",
    "\t\t('Delta East', 'Cairo'),\n",
    "\t\t('Delta West', 'Giza'),\n",
    "\t\t('Delta East', 'Giza')\n",
    "\t\t)\n",
    "    AS region_mapping(region, fallback_region)\n",
    ")\n",
    "\n",
    "\n",
    "select region,warehouse_id,product_id,\n",
    "min(final_min_price) as final_min_price,\n",
    "min(final_max_price) as final_max_price,\n",
    "min(final_mod_price) as final_mod_price,\n",
    "min(final_true_min) as final_true_min,\n",
    "min(final_true_max) as final_true_max\n",
    "\n",
    "from (\n",
    "SELECT\n",
    "distinct \n",
    "\tw.region,\n",
    "    w.warehouse_id,\n",
    "\tw.product_id,\n",
    "    COALESCE(m1.min_price, m2.min_price) AS final_min_price,\n",
    "    COALESCE(m1.max_price, m2.max_price) AS final_max_price,\n",
    "    COALESCE(m1.mod_price, m2.mod_price) AS final_mod_price,\n",
    "\tCOALESCE(m1.true_min, m2.true_min) AS final_true_min,\n",
    "\tCOALESCE(m1.true_max, m2.true_max) AS final_true_max,\n",
    "FROM full_data w\n",
    "LEFT JOIN MP m1\n",
    "    ON w.region = m1.region and w.product_id = m1.product_id\n",
    "JOIN region_mapping rm\n",
    "    ON w.region = rm.region\n",
    "LEFT JOIN MP m2\n",
    "    ON rm.fallback_region = m2.region\n",
    "   AND w.product_id = m2.product_id\n",
    ")\n",
    "where final_min_price is not null \n",
    "group by all \n",
    "'''\n",
    "marketplace = query_snowflake(query, columns = ['REGION','WAREHOUSE_ID','PRODUCT_ID','FINAL_MIN_PRICE','FINAL_MAX_PRICE','FINAL_MOD_PRICE','FINAL_TRUE_MIN','FINAL_TRUE_MAX'])\n",
    "marketplace.columns = marketplace.columns.str.lower()\n",
    "for col in marketplace.columns:\n",
    "    marketplace[col] = pd.to_numeric(marketplace[col], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de23bc1f-2b20-4b28-9132-a9fa6a125f93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f'''\n",
    "select z.* \n",
    "from (\n",
    "select maxab_product_id as product_id,avg(bs_final_price) as ben_soliman_price\n",
    "from (\n",
    "select * , row_number()over(partition by maxab_product_id order by diff) as rnk_2\n",
    "from (\n",
    "select *,(bs_final_price-wac_p)/wac_p as diff_2\n",
    "from (\n",
    "select * ,bs_price/maxab_basic_unit_count as bs_final_price\n",
    "from (\n",
    "select *,row_number()over(partition by maxab_product_id,maxab_pu order by diff) as rnk \n",
    "from (\n",
    "select sm.* ,max(INJECTION_DATE::date)over(partition by maxab_product_id,maxab_pu) as max_date,wac1,wac_p,abs(bs_price-(wac_p*maxab_basic_unit_count))/(wac_p*maxab_basic_unit_count) as diff \n",
    "from materialized_views.savvy_mapping sm \n",
    "join finance.all_cogs f on f.product_id = sm.maxab_product_id and current_timestamp between f.from_Date and f.to_date\n",
    "where bs_price is not null \n",
    "and INJECTION_DATE::date >= CURRENT_DATE- 5\n",
    "qualify INJECTION_DATE::date = max_date\n",
    ")\n",
    "qualify rnk = 1 \n",
    ")\n",
    ")\n",
    "where diff_2 between -0.5 and 0.5 \n",
    ")\n",
    "qualify rnk_2 = 1 \n",
    ")\n",
    "group by all\n",
    ")z \n",
    "join finance.all_cogs f on f.product_id = z.product_id and current_timestamp between f.from_Date and f.to_date\n",
    "\n",
    "where ben_soliman_price between f.wac_p*0.9 and f.wac_p*1.3\n",
    "'''\n",
    "\n",
    "bensoliman = query_snowflake(query, columns = ['product_id','ben_soliman_basic_price'])\n",
    "bensoliman.columns = bensoliman.columns.str.lower()\n",
    "for col in bensoliman.columns:\n",
    "    bensoliman[col] = pd.to_numeric(bensoliman[col], errors='ignore')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ade96b0-3200-4a77-826a-dd49f0dd4e77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f'''\n",
    "WITH whs as (SELECT *\n",
    "             FROM   (values\n",
    "                            ('Cairo', 'El-Marg', 38,700),\n",
    "                            ('Cairo', 'Mostorod', 1,700),\n",
    "                            ('Giza', 'Barageel', 236,701),\n",
    "                            ('Delta West', 'El-Mahala', 337,703),\n",
    "                            ('Delta West', 'Tanta', 8,703),\n",
    "                            ('Delta East', 'Mansoura FC', 339,704),\n",
    "                            ('Delta East', 'Sharqya', 170,704),\n",
    "                            ('Upper Egypt', 'Assiut FC', 501,1124),\n",
    "                            ('Upper Egypt', 'Bani sweif', 401,1126),\n",
    "                            ('Upper Egypt', 'Menya Samalot', 703,1123),\n",
    "                            ('Upper Egypt', 'Sohag', 632,1125),\n",
    "                            ('Alexandria', 'Khorshed Alex', 797,702),\n",
    "\t\t\t\t\t\t\t('Giza', 'Sakkarah', 962,701)\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t)\n",
    "                    x(region, wh, warehouse_id,cohort_id))\n",
    "select product_id,x.region,warehouse_id,min(MARKET_PRICE) as min_scrapped,max(MARKET_PRICE) as max_scrapped,median(MARKET_PRICE) as median_scrapped\n",
    "from (\n",
    "select MATERIALIZED_VIEWS.CLEANED_MARKET_PRICES.*,max(date)over(partition by region,MATERIALIZED_VIEWS.CLEANED_MARKET_PRICES.product_id,competitor) as max_date\n",
    "from MATERIALIZED_VIEWS.CLEANED_MARKET_PRICES\n",
    "join finance.all_cogs f on f.product_id = MATERIALIZED_VIEWS.CLEANED_MARKET_PRICES.product_id and CURRENT_TIMESTAMP between f.from_date and f.to_date \n",
    "where date>= current_date -5\n",
    "and MARKET_PRICE between f.wac_p * 0.9 and wac_p*1.3\n",
    "qualify date = max_date \n",
    ") x \n",
    "left join whs on whs.region = x.region\n",
    "group by all \n",
    "'''\n",
    "try:\n",
    "    scrapped_prices = query_snowflake(query, columns = ['product_id','region','warehouse_id','min_scrapped','max_scrapped','median_scrapped'])\n",
    "    scrapped_prices.columns = scrapped_prices.columns.str.lower()\n",
    "    for col in scrapped_prices.columns:\n",
    "        scrapped_prices[col] = pd.to_numeric(scrapped_prices[col], errors='ignore')   \n",
    "except: \n",
    "    scrapped_prices = pd.DataFrame(columns = ['product_id','region','warehouse_id','min_scrapped','max_scrapped','median_scrapped'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbba3668-3267-4142-a7c2-3666cef3226d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f'''\n",
    "select region,product_id,optimal_bm,MIN_BOUNDARY,MAX_BOUNDARY,MEDIAN_BM\n",
    "from (\n",
    "select region,product_id,target_bm,optimal_bm,MIN_BOUNDARY,MAX_BOUNDARY,MEDIAN_BM,max(created_at) over(partition by product_id,region) as max_date,created_at\n",
    "from materialized_views.PRODUCT_STATISTICS\n",
    "where created_at::date >= date_trunc('month',current_date - 60)\n",
    "qualify max_date = created_at\n",
    ")\n",
    "\n",
    "'''\n",
    " \n",
    "stats = query_snowflake(query, columns = ['region','product_id','optimal_bm','MIN_BOUNDARY','MAX_BOUNDARY','MEDIAN_BM'])\n",
    "stats.columns = stats.columns.str.lower()\n",
    "for col in stats.columns:\n",
    "    stats[col] = pd.to_numeric(stats[col], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1329532-37a0-47a8-8835-813a3c9cfadf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f'''\n",
    "select warehouse_id,region\n",
    "from (\n",
    "select * ,row_number()over(partition by warehouse_id order by nmv desc) as rnk \n",
    "from (\n",
    "SELECT case when regions.id = 2 then cities.name_en else regions.name_en end as region,\n",
    "\t   pso.warehouse_id,\n",
    "        sum(pso.total_price) as nmv\n",
    "\n",
    "\n",
    "\n",
    "FROM product_sales_order pso\n",
    "JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "JOIN materialized_views.retailer_polygon on materialized_views.retailer_polygon.retailer_id=so.retailer_id\n",
    "JOIN districts on districts.id=materialized_views.retailer_polygon.district_id\n",
    "JOIN cities on cities.id=districts.city_id\n",
    "join states on states.id=cities.state_id\n",
    "join regions on regions.id=states.region_id             \n",
    "\n",
    "WHERE   True\n",
    "    AND so.created_at ::date between current_date-31 and CURRENT_DATE-1\n",
    "    AND so.sales_order_status_id not in (7,12)\n",
    "    AND so.channel IN ('telesales','retailer')\n",
    "    AND pso.purchased_item_count <> 0\n",
    "\n",
    "GROUP BY ALL\n",
    ")\n",
    "qualify rnk = 1 \n",
    ")\n",
    "'''\n",
    "warehouse_region = query_snowflake(query, columns = ['warehouse_id','region'])\n",
    "warehouse_region.columns = warehouse_region.columns.str.lower()\n",
    "for col in warehouse_region.columns:\n",
    "    warehouse_region[col] = pd.to_numeric(warehouse_region[col], errors='ignore')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e94e1fbe-ccb3-45b8-9f1e-dde8ba79d5cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f'''\n",
    "SELECT DISTINCT cat, brand, margin as target_bm\n",
    "FROM    performance.commercial_targets cplan\n",
    "QUALIFY CASE WHEN DATE_TRUNC('month', MAX(DATE)OVER()) = DATE_TRUNC('month', CURRENT_DATE) THEN DATE_TRUNC('month', CURRENT_DATE)\n",
    "ELSE DATE_TRUNC('month', CURRENT_DATE - INTERVAL '1 month') END = DATE_TRUNC('month', date)\n",
    "'''\n",
    "brand_cat_target  = query_snowflake(query, columns = ['cat','brand','target_bm'])\n",
    "brand_cat_target.target_bm=pd.to_numeric(brand_cat_target.target_bm)\n",
    "\n",
    "query = f'''\n",
    "select cat,sum(target_bm *(target_nmv/cat_total)) as cat_target_margin\n",
    "from (\n",
    "select *,sum(target_nmv)over(partition by cat) as cat_total\n",
    "from (\n",
    "select cat,brand,avg(target_bm) as target_bm , sum(target_nmv) as target_nmv\n",
    "from (\n",
    "SELECT DISTINCT date,city as region,cat, brand, margin as target_bm,nmv as target_nmv\n",
    "FROM    performance.commercial_targets cplan\n",
    "QUALIFY CASE WHEN DATE_TRUNC('month', MAX(DATE)OVER()) = DATE_TRUNC('month', CURRENT_DATE) THEN DATE_TRUNC('month', CURRENT_DATE)\n",
    "ELSE DATE_TRUNC('month', CURRENT_DATE - INTERVAL '1 month') END = DATE_TRUNC('month', date)\n",
    ")\n",
    "group by all\n",
    ")\n",
    ")\n",
    "group by all \n",
    "'''\n",
    "cat_target  = query_snowflake(query, columns = ['cat','cat_target_margin'])\n",
    "cat_target.cat_target_margin=pd.to_numeric(cat_target.cat_target_margin)\n",
    "\n",
    "query = f'''\n",
    "SELECT  DIStinct  \n",
    "\t\tproducts.id as product_id,\n",
    "\t\tCONCAT(products.name_ar,' ',products.size,' ',product_units.name_ar) as sku,\n",
    "\t\tbrands.name_ar as brand, \n",
    "\t\tcategories.name_ar as cat,\n",
    "\t\tf.wac_p\n",
    "from products \n",
    "JOIN brands on products.brand_id = brands.id \n",
    "JOIN categories ON products.category_id = categories.id\n",
    "JOIN finance.all_cogs f  ON f.product_id = products.id and CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMEstamp()) between f.from_date and f.to_date \n",
    "JOIN product_units ON product_units.id = products.unit_id \n",
    "'''\n",
    "sku_info  = query_snowflake(query, columns = ['product_id','sku','brand','cat','wac_p'])\n",
    "sku_info.product_id=pd.to_numeric(sku_info.product_id)\n",
    "sku_info.wac_p=pd.to_numeric(sku_info.wac_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8722c17b-0da0-43a6-ae4d-12b81b9760d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "select warehouse_id,product_id,nmv,\n",
    "coalesce(sku_dis_nmv,0)/nmv as sku_disc_cntrb,\n",
    "coalesce(quantity_nmv,0)/nmv as quant_disc_cntrb,\n",
    "sku_disc_price,\n",
    "quantity_price,\n",
    "from (\n",
    "SELECT  DISTINCT\n",
    "\t\tpso.warehouse_id,\n",
    "\t\tpso.product_id,\n",
    "\t\tsum(pso.total_price) as nmv,\n",
    "\t\tavg(item_price/basic_unit_count) as item_price,\n",
    "\t\tsum(case when ITEM_DISCOUNT_value > 0 then pso.total_price end) as sku_dis_nmv,\n",
    "\t\tsum(case when ITEM_quantity_DISCOUNT_value > 0 then pso.total_price end) as quantity_nmv,\n",
    "\t\tavg(case when ITEM_DISCOUNT_value > 0 then (item_price/BASIC_UNIT_COUNT)-(ITEM_DISCOUNT_value/BASIC_UNIT_COUNT)end) as sku_disc_price,\n",
    "\t\tavg(case when ITEM_quantity_DISCOUNT_value > 0 then (item_price/BASIC_UNIT_COUNT)-(ITEM_quantity_DISCOUNT_value/BASIC_UNIT_COUNT)end) as quantity_price,\n",
    "\n",
    "\n",
    "FROM product_sales_order pso \n",
    "JOIN sales_orders so ON so.id = pso.sales_order_id and so.retailer_id not in (select taggable_id from dynamic_taggables where dynamic_tag_id = 3038)\n",
    "WHERE so.created_at::date = current_date -1 \n",
    "    AND so.sales_order_status_id not in (7,12)\n",
    "    AND so.channel IN ('telesales','retailer')\n",
    "    AND pso.purchased_item_count <> 0\n",
    "GROUP BY ALL\n",
    ")\n",
    "order by nmv desc\n",
    "'''\n",
    "last_day =  query_snowflake(query, columns = ['warehouse_id','product_id','last_d_nmv','sku_disc_cntrb','quant_disc_cntrb','sku_disc_price','quantity_price'])\n",
    "last_day.columns = last_day.columns.str.lower()\n",
    "for col in last_day.columns:\n",
    "     last_day[col] = pd.to_numeric(last_day[col], errors='ignore')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d4da0bd-7018-4315-a7a1-770795bf1992",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "\n",
    "with main_data  as (\n",
    "SELECT  DISTINCT\n",
    "\t\tpso.warehouse_id,\n",
    "\t\tpso.product_id,\n",
    "\t\tCONCAT(products.name_ar,' ',products.size,' ',product_units.name_ar) as sku,\n",
    "\t\tbrands.name_ar as brand, \n",
    "\t\tcategories.name_ar as cat,\n",
    "\t\t\n",
    "        sum(pso.total_price) as nmv,\n",
    "       sum(COALESCE(f.wac_p,0) * pso.purchased_item_count * pso.basic_unit_count) as cogs_p,\n",
    "\t   ((nmv-cogs_p)/nmv) as bm_p,\n",
    "\n",
    "\n",
    "FROM product_sales_order pso\n",
    "JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "--join COHORT_PRICING_CHANGES cpc on cpc.id = pso.COHORT_PRICING_CHANGE_id\n",
    "JOIN products on products.id=pso.product_id\n",
    "JOIN brands on products.brand_id = brands.id \n",
    "JOIN categories ON products.category_id = categories.id\n",
    "JOIN finance.all_cogs f  ON f.product_id = pso.product_id\n",
    "                        AND f.from_date::date <= so.created_at ::date\n",
    "                        AND f.to_date::date > so.created_at ::date\n",
    "JOIN product_units ON product_units.id = products.unit_id \n",
    "JOIN materialized_views.retailer_polygon on materialized_views.retailer_polygon.retailer_id=so.retailer_id\n",
    "JOIN districts on districts.id=materialized_views.retailer_polygon.district_id\n",
    "JOIN cities on cities.id=districts.city_id\n",
    "join states on states.id=cities.state_id\n",
    "join regions on regions.id=states.region_id             \n",
    "\n",
    "WHERE   True\n",
    "    AND so.created_at ::date between  current_date - 5 and current_date -1 \n",
    "    AND so.sales_order_status_id not in (7,12)\n",
    "    AND so.channel IN ('telesales','retailer')\n",
    "    AND pso.purchased_item_count <> 0\n",
    "\n",
    "GROUP BY ALL\n",
    "),\n",
    "cp as (\n",
    "select cat,brand,sum(nmv) as target_nmv ,avg(margin) as target_margin\n",
    "from performance.commercial_targets \n",
    "where date  between '2025-10-01' and current_date - 1\n",
    "group by all \n",
    "),\n",
    "stocks as (\t\t\t\t\t\n",
    "select warehouse_id,warehouse,product_id,sum(stocks) as stocks\n",
    "from (\n",
    "\t\tSELECT DISTINCT product_warehouse.warehouse_id,w.name as warehouse,\n",
    "                product_warehouse.product_id,\n",
    "                (product_warehouse.available_stock)::integer as stocks\n",
    "\n",
    "        from  product_warehouse \n",
    "        JOIN products on product_warehouse.product_id = products.id\n",
    "        JOIN product_units ON products.unit_id = product_units.id\n",
    "\t\tjoin warehouses w on w.id = product_warehouse.warehouse_id\n",
    "\n",
    "        where   product_warehouse.warehouse_id not in (6,9,10)\n",
    "            AND product_warehouse.is_basic_unit = 1\n",
    "\t\t\tand product_warehouse.available_stock > 0 \n",
    "\n",
    ")\n",
    "group by all\n",
    "),\n",
    "prs AS (\n",
    "SELECT DISTINCT product_purchased_receipts.purchased_receipt_id,\n",
    "                purchased_receipts.purchased_order_id,\n",
    "                DATE_PART('Day', purchased_receipts.date::date) AS DAY,\n",
    "                DATE_PART('month', purchased_receipts.date::date) AS MONTH,\n",
    "                DATE_Part('year', purchased_receipts.date::date) AS YEAR,\n",
    "                products.id AS product_id,\n",
    "                CONCAT(products.name_ar, ' ', products.size, ' ', product_units.name_ar) AS sku,\n",
    "                brands.name_ar AS Brand,\n",
    "                categories.name_ar as category,\n",
    "                products.description,\n",
    "                purchased_receipts.warehouse_id AS warehouse_id,\n",
    "                warehouses.name as warehouse,\n",
    "                packing_units.name_ar AS packing_unit,\n",
    "                purchased_receipts.discount AS Total_discount,\n",
    "                purchased_receipts.return_orders_discount,\n",
    "                purchased_receipts.discount_type_id,\n",
    "                suppliers.id AS supplier_id,\n",
    "                suppliers.name AS supplier_name,\n",
    "                purchased_receipt_statuses.name_ar AS PR_status,\n",
    "                product_purchased_receipts.basic_unit_count,\n",
    "                product_purchased_receipts.purchased_item_count AS purchase_count,\n",
    "                product_purchased_receipts.purchased_item_count*product_purchased_receipts.basic_unit_count AS purchase_min_count,\n",
    "                product_purchased_receipts.item_price,\n",
    "                product_purchased_receipts.final_price/product_purchased_receipts.purchased_item_count AS final_item_price,\n",
    "                product_purchased_receipts.total_price AS purchase_price,\n",
    "                CASE WHEN product_purchased_receipts.vat = 'true' THEN product_purchased_receipts.total_price * 0.14\n",
    "                     ELSE CASE WHEN product_purchased_receipts.vat = 'false' THEN product_purchased_receipts.total_price * 0\n",
    "                               END\n",
    "                END AS vat,\n",
    "                CASE WHEN purchased_receipts.discount_type_id = 2 THEN (product_purchased_receipts.discount/100) * product_purchased_receipts.total_price\n",
    "                     ELSE product_purchased_receipts.discount\n",
    "                END AS SKU_discount,\n",
    "                purchased_receipts.total_price AS pr_value,\n",
    "                CASE\n",
    "                    WHEN product_purchased_receipts.t_tax_id = 1 THEN product_purchased_receipts.total_price * 0.05\n",
    "                    ELSE CASE\n",
    "                             WHEN product_purchased_receipts.t_tax_id = 2 THEN product_purchased_receipts.total_price * 0.08\n",
    "                             ELSE CASE\n",
    "                                      WHEN product_purchased_receipts.t_tax_id = 3 THEN product_purchased_receipts.total_price * 0.1\n",
    "                                      ELSE 0\n",
    "                                  END\n",
    "                         END\n",
    "                END AS table_tax,\n",
    "                product_purchased_receipts.final_price AS Final_Price,\n",
    "                product_purchased_receipts.product_type_id,\n",
    "                purchased_receipts.debt_note_value as credit_note,\n",
    "                purchased_receipts.tips,\n",
    "                purchased_receipts.delivery_fees,\n",
    "                case when purchased_receipts.is_actual = 'true' then 'Real' \n",
    "                     else 'Virtual' \n",
    "                     end as is_actual\n",
    "                     \n",
    "FROM product_purchased_receipts\n",
    "LEFT JOIN products ON products.id = product_purchased_receipts.product_id\n",
    "LEFT JOIN packing_unit_products ON packing_unit_products.product_id = products.id\n",
    "LEFT JOIN purchased_receipts ON purchased_receipts.id = product_purchased_receipts.purchased_receipt_id\n",
    "LEFT JOIN purchased_receipt_statuses ON purchased_receipt_statuses.id = purchased_receipts.purchased_receipt_status_id\n",
    "LEFT JOIN packing_units ON packing_units.id = product_purchased_receipts.packing_unit_id\n",
    "LEFT JOIN product_units ON products.unit_id = product_units.id\n",
    "LEFT JOIN suppliers ON suppliers.id = purchased_receipts.supplier_id\n",
    "LEFT JOIN brands ON brands.id = products.brand_id\n",
    "left join categories on categories.id = products.category_id\n",
    "left join warehouses on warehouses.id = purchased_receipts.warehouse_id\n",
    "WHERE product_purchased_receipts.purchased_item_count <> 0\n",
    "      AND purchased_receipts.purchased_receipt_status_id IN (4,5,7)\n",
    "      AND purchased_receipts.date::date >= current_date - 4\n",
    "    AND purchased_receipts.is_actual = 'true'\n",
    "     \n",
    "     \n",
    "    ),\n",
    "prs_data as (\n",
    "select warehouse_id , product_id,sum(final_price) as total_prs \n",
    "from prs \n",
    "group by all\n",
    ")\n",
    "\n",
    "select warehouse_id,product_id,1 as zero_rr\n",
    "from (\n",
    "select s.*,\n",
    "CONCAT(products.name_ar,' ',products.size,' ',product_units.name_ar) as sku,\n",
    "brands.name_ar as brand, \n",
    "categories.name_ar as cat,\n",
    "coalesce(md.nmv,0) as sales,wac1,\n",
    "wac1*stocks as stock_value,\n",
    "coalesce(total_prs,0) as prs_data\n",
    "from stocks s\n",
    "left join main_data md on md.product_id =s.product_id and md.warehouse_id = s.warehouse_id\n",
    "JOIN finance.all_cogs f  ON f.product_id = s.product_id\n",
    "                        AND f.from_date::date <= current_date \n",
    "                        AND f.to_date::date > current_date\n",
    "JOIN products on products.id=s.product_id\n",
    "JOIN brands on products.brand_id = brands.id \n",
    "JOIN categories ON products.category_id = categories.id\n",
    "JOIN product_units ON product_units.id = products.unit_id\n",
    "left join prs_data on prs_data.product_id =s.product_id and prs_data.warehouse_id = s.warehouse_id \n",
    "where stocks > 0 and sales = 0 \n",
    "and prs_data < 0.7*stock_value\n",
    "order by wac1* stocks desc \n",
    ")\n",
    "'''\n",
    "zerorr =  query_snowflake(query, columns = ['warehouse_id','product_id','zero_rr'])\n",
    "zerorr.columns = zerorr.columns.str.lower()\n",
    "for col in zerorr.columns:\n",
    "    zerorr[col] = pd.to_numeric(zerorr[col], errors='ignore')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e9d8f4d-8ec6-4fb3-bd10-8f52c50993be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "product_data = product_data.merge(product_warehouse_price,on=['product_id','warehouse_id'])\n",
    "product_data = product_data.merge(uth_cntrb[['warehouse_id','uth_cntrb']],on='warehouse_id')\n",
    "product_data['product_UTH_growth'] =(product_data['current_uth'] -product_data['prev_uth'])/product_data['prev_uth']\n",
    "product_data['product_LH_growth'] =(product_data['current_last_hour'] -product_data['prev_last_hour'])/product_data['prev_last_hour']\n",
    "product_data[['product_UTH_growth','product_LH_growth']] =product_data[['product_UTH_growth','product_LH_growth']].fillna(0) \n",
    "product_data = product_data.replace([np.inf, -np.inf], 1)\n",
    "product_data['product_closing_growth'] = (product_data['product_UTH_growth']*product_data['uth_cntrb'])+(product_data['product_LH_growth']*(1-product_data['uth_cntrb']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20e2675c-61d8-4c74-9b8d-3d6508a061c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warehouse_data = product_data.groupby('warehouse_id')[['prev_all_day', 'prev_uth','prev_last_hour', 'current_all_day', 'current_uth', 'current_last_hour']].sum().reset_index()\n",
    "warehouse_data['UTH_growth'] =(warehouse_data['current_uth'] -warehouse_data['prev_uth'])/warehouse_data['prev_uth']\n",
    "warehouse_data['LH_growth'] =(warehouse_data['current_last_hour'] -warehouse_data['prev_last_hour'])/warehouse_data['prev_last_hour']\n",
    "warehouse_data = warehouse_data.merge(uth_cntrb,on='warehouse_id')\n",
    "warehouse_data['Closing_growth'] = (warehouse_data['UTH_growth']*warehouse_data['uth_cntrb'])+(warehouse_data['LH_growth']*(1-warehouse_data['uth_cntrb']))\n",
    "dropping_whs = warehouse_data[warehouse_data['Closing_growth']<0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1fb16c40-bee2-4050-b08d-da0069012b33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "growing_products  = product_data.merge(warehouse_data[['warehouse_id','UTH_growth','LH_growth','Closing_growth']],on='warehouse_id')\n",
    "#needs edit\n",
    "growing_products = growing_products[growing_products['product_closing_growth']>=np.maximum(growing_products['Closing_growth'],0.1)]\n",
    "growing_products['max_closing'] = growing_products.groupby('product_id')['product_closing_growth'].transform(sum)\n",
    "growing_products=growing_products[growing_products['max_closing']==growing_products['product_closing_growth']]\n",
    "growing_products = growing_products.groupby(['product_id'])['price'].mean().reset_index()\n",
    "growing_products.columns = ['product_id','maxab_good_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b870a73b-e9ca-46c0-a3f8-02d41cd513c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_products = product_data.merge(sku_info,on=['product_id'])\n",
    "selected_products = selected_products[selected_products['brand'].isin(b_list)]\n",
    "selected_products = selected_products.merge(warehouse_data[['warehouse_id','UTH_growth','LH_growth','Closing_growth']],on='warehouse_id')\n",
    "selected_products=selected_products.drop(columns=['cat','brand','sku','wac_p'])\n",
    "selected_products = selected_products[selected_products['product_closing_growth'] <selected_products['Closing_growth']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b97e09f-7b97-4e02-9dff-089e8b8c0ef7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dropping_products = product_data.merge(dropping_whs[['warehouse_id','UTH_growth','LH_growth','Closing_growth']],on='warehouse_id')\n",
    "dropping_products = dropping_products[dropping_products['product_closing_growth'] < 0]\n",
    "dropping_products = pd.concat([dropping_products,selected_products])\n",
    "dropping_products=dropping_products.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ffc8ac4-093a-4445-9fb8-f20e80bddde4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dropping_products = dropping_products.merge(sku_info,on=['product_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8612be22-6e29-4c14-8034-13b798c07d9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delta_wh = [8,170,337,339]\n",
    "rem_brands = ['','','','']\n",
    "dropping_products = dropping_products[~((dropping_products['brand'].isin(rem_brands)) & (dropping_products['warehouse_id'].isin(delta_wh)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61f2f629-8e42-49a5-a6b7-c0fd215cbc96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dropping_products = dropping_products.sort_values(by='prev_all_day',ascending = False)\n",
    "dropping_products = dropping_products.merge(growing_products,on='product_id',how='left')\n",
    "dropping_products = dropping_products.merge(marketplace,on=['product_id','warehouse_id'],how='left')\n",
    "dropping_products = dropping_products.merge(bensoliman[['product_id','ben_soliman_basic_price']],on=['product_id'],how='left')\n",
    "dropping_products = dropping_products.drop(columns = 'region')\n",
    "dropping_products = dropping_products.merge(scrapped_prices,on=['product_id','warehouse_id'],how='left')\n",
    "dropping_products = dropping_products.drop(columns = 'region')\n",
    "dropping_products = dropping_products.merge(zerorr,on=['product_id','warehouse_id'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea0f1062-ebf9-4006-b727-671c546ab895",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dropping_products = dropping_products.merge(warehouse_region,on=['warehouse_id'])\n",
    "dropping_products = dropping_products.merge(stats,on=['product_id','region'],how='left')\n",
    "dropping_products = dropping_products.merge(brand_cat_target,on=['brand','cat'],how='left')\n",
    "dropping_products = dropping_products.merge(cat_target,on=['cat'],how='left')\n",
    "dropping_products['Target_margin'] = dropping_products['target_bm'].fillna(dropping_products['cat_target_margin'])\n",
    "dropping_products = dropping_products[[ 'warehouse_id','product_id','sku','brand','cat', 'prev_all_day', 'prev_uth',\n",
    "       'prev_last_hour', 'current_all_day', 'current_uth', 'current_last_hour','product_UTH_growth', 'product_LH_growth',\n",
    "       'product_closing_growth','doh','wac_p','price','maxab_good_price', 'final_min_price', 'final_max_price',\n",
    "       'final_mod_price', 'final_true_min', 'final_true_max',\n",
    "       'ben_soliman_basic_price','optimal_bm', 'min_boundary',\n",
    "       'max_boundary', 'median_bm','Target_margin','min_scrapped','max_scrapped','median_scrapped','zero_rr']]\n",
    "dropping_products = dropping_products.merge(last_day,on=['product_id','warehouse_id'],how='left')\n",
    "dropping_products[['last_d_nmv','sku_disc_cntrb','quant_disc_cntrb','sku_disc_price','quantity_price']] = dropping_products[['last_d_nmv','sku_disc_cntrb','quant_disc_cntrb','sku_disc_price','quantity_price']].fillna(0)\n",
    "dropping_products=dropping_products.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39e7ab85-01f8-447a-b214-28dca3e83b3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def select_price(product_UTH_growth,product_LH_growth,product_closing_growth,remaining_prices,price,wac,Target_margin,min_boundary,optimal_bm,zero_rr,doh):\n",
    "    target_price = 0 \n",
    "    min_price = 0\n",
    "    max_price = 0 \n",
    "    acceptable = []\n",
    "    source = ''\n",
    "    current_margin = (price-wac)/price\n",
    "    stocks_pricing_list = remaining_prices+[wac/(1-(Target_margin*0.5))]\n",
    "    stocks_pricing_list.sort()\n",
    "    if not np.isnan(zero_rr) or doh >45:\n",
    "        if doh >45 and np.isnan(zero_rr): \n",
    "            source = 'OS'\n",
    "        else:\n",
    "            source = 'Zero_rr' \n",
    "            \n",
    "        for i in range(0,len(stocks_pricing_list)):\n",
    "            new_price = stocks_pricing_list[i]\n",
    "            diff = (new_price-price)/price\n",
    "            if new_price >= wac and  diff <= -0.05:\n",
    "                target_price = new_price\n",
    "                break\n",
    "        if target_price == 0 and current_margin> Target_margin and current_margin - Target_margin > 0.0025:\n",
    "                target_price = wac/(1-Target_margin)\n",
    "        elif target_price == 0 and current_margin> min_boundary and current_margin - min_boundary > 0.0025:\n",
    "                target_price = wac/(1-min_boundary)\n",
    "        elif target_price == 0 and current_margin> Target_margin/2 and current_margin - Target_margin/2 > 0.0025: \n",
    "                target_price = wac/(1-(Target_margin/2))\n",
    "\n",
    "    else:\n",
    "        for i in range(len(remaining_prices)-1,-1,-1):\n",
    "            new_price = remaining_prices[i]\n",
    "            diff = (new_price-price)/price\n",
    "            current_margin = (price-wac)/price\n",
    "            new_margin = (new_price-wac)/new_price\n",
    "            if new_margin >= min_boundary*0.9 and new_margin >= 0.3*Target_margin and new_margin >0  and diff <= -0.0025:\n",
    "                target_price = new_price\n",
    "                source = 'Listed'\n",
    "                break\n",
    "        if target_price == 0:\n",
    "\n",
    "            for j in range(0,len(remaining_prices)):\n",
    "                new_price = remaining_prices[j]\n",
    "                diff = (new_price-price)/price\n",
    "                current_margin = (price-wac)/price\n",
    "                new_margin = (new_price-wac)/new_price\n",
    "                if new_margin >0 :\n",
    "                    acceptable.append(new_price)\n",
    "            if(len(acceptable) > 1):\n",
    "                distance_arr = []\n",
    "                for k in range(0,len(acceptable)):\n",
    "                    new_price = acceptable[k]\n",
    "                    diff = 1/abs(price-new_price)\n",
    "                    distance_arr.append(diff)\n",
    "\n",
    "                total_array = sum(distance_arr)\n",
    "                normalized = [x / total_array for x in distance_arr]\n",
    "                final_value = 0 \n",
    "                for v in range(0,len(normalized)):\n",
    "                    w = normalized[v]\n",
    "                    p = acceptable[v]\n",
    "                    final_value+= (w*p)\n",
    "                target_price = np.maximum(final_value,wac/(1-(0.3*Target_margin)))\n",
    "                source = 'induced_1'\n",
    "\n",
    "            elif (len(acceptable) > 0):\n",
    "                new_price = acceptable[0]\n",
    "                final_value = (0.3*new_price)+(0.7*price)\n",
    "                target_price = np.maximum(final_value,wac/(1-(0.3*Target_margin)))\n",
    "                source = 'induced_2'\n",
    "\n",
    "              \n",
    "               \n",
    "    return target_price,source    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa9eeee1-ea7f-429b-8156-8a68a4924ebf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 7316/7316 [00:29<00:00, 248.81it/s]\n"
     ]
    }
   ],
   "source": [
    "product_final_df = pd.DataFrame(columns = ['warehouse_id', 'product_id', 'sku', 'brand', 'cat', 'prev_all_day',\n",
    "       'prev_uth', 'prev_last_hour', 'current_all_day', 'current_uth',\n",
    "       'current_last_hour', 'product_UTH_growth', 'product_LH_growth',\n",
    "       'product_closing_growth', 'doh', 'wac_p', 'price', 'maxab_good_price',\n",
    "       'final_min_price', 'final_max_price', 'final_mod_price',\n",
    "       'final_true_min', 'final_true_max', 'ben_soliman_basic_price',\n",
    "       'optimal_bm', 'min_boundary', 'max_boundary', 'median_bm','min_scrapped','max_scrapped','median_scrapped','zero_rr',\n",
    "       'Target_margin','last_d_nmv','sku_disc_cntrb','quant_disc_cntrb','sku_disc_price','quantity_price' ,'selected_price','source'])\n",
    "\n",
    "for _,row in tqdm(dropping_products.iterrows(), total=len(dropping_products)):\n",
    "    wac = row['wac_p']\n",
    "    price = row['price']\n",
    "    doh = row['doh']\n",
    "    maxab_good_price = row['maxab_good_price']\n",
    "    final_min_price = row['final_min_price']\n",
    "    final_max_price = row['final_max_price']\n",
    "    final_mod_price = row['final_mod_price']\n",
    "    final_true_min = row['final_true_min']\n",
    "    final_true_max = row['final_true_max']\n",
    "    ben_soliman_basic_price= row['ben_soliman_basic_price']\n",
    "    scrapped_min = row['min_scrapped']\n",
    "    scrapped_median = row['median_scrapped']\n",
    "    scrapped_max = row['max_scrapped']\n",
    "    optimal_price = wac/(1-row['optimal_bm'])\n",
    "    min_b_price = wac/(1-row['min_boundary'])\n",
    "    max_b_price = wac/(1-row['max_boundary'])\n",
    "    median_price = wac/(1-row['median_bm'])\n",
    "    target_price = wac/(1-row['Target_margin'])\n",
    "    product_UTH_growth = row['product_UTH_growth']\n",
    "    product_LH_growth = row['product_LH_growth']\n",
    "    product_closing_growth=row['product_closing_growth']\n",
    "    qd_cntrb = row['quant_disc_cntrb']\n",
    "    sd_cntrb = row['sku_disc_cntrb']\n",
    "    qd_price = row['quantity_price']\n",
    "    sd_price = row['sku_disc_price']\n",
    "    ld_nmv = row['last_d_nmv']\n",
    "    prev_nmv = row['prev_all_day']\n",
    "    \n",
    "    qd_discount = ((qd_price-price)/price)*-1 \n",
    "    sku_discount = ((sd_price-price)/price)*-1\n",
    "    \n",
    "    prices_list = [maxab_good_price,final_min_price,final_max_price,final_mod_price,final_true_min,final_true_max,ben_soliman_basic_price,optimal_price,min_b_price,max_b_price,median_price,target_price,scrapped_min,scrapped_max,scrapped_median]\n",
    "    cleaned_prices = list({x for x in prices_list if x not in [0, np.nan] and not pd.isna(x)})\n",
    "    if ld_nmv > (prev_nmv*1.1) and (((qd_cntrb > 0)and (qd_cntrb > sd_cntrb) and (qd_discount < row['Target_margin']*0.25)) or ((sd_cntrb > 0) and (qd_cntrb < sd_cntrb) and (sku_discount < row['Target_margin']*0.25))):\n",
    "        if qd_cntrb > sd_cntrb and qd_cntrb > 0 and qd_price >0 and qd_price > wac: \n",
    "            row['selected_price'] = qd_price\n",
    "            row['source'] = 'Prev_disc'\n",
    "        elif qd_cntrb < sd_cntrb and sd_cntrb > 0 and sd_price > 0 and sd_price > wac:\n",
    "            row['selected_price'] = sd_price\n",
    "            row['source'] = 'Prev_disc'\n",
    "        elif  (sd_price > wac) or (qd_price > wac):\n",
    "            row['selected_price'] = np.maximum(qd_price,sd_price)\n",
    "            row['source'] = 'Prev_disc'\n",
    "        \n",
    "    else:\n",
    "        if qd_cntrb > sd_cntrb and qd_price >0 and qd_discount <= row['Target_margin']*0.35 :\n",
    "            remaining_prices = [x for x in cleaned_prices if (x < qd_price) and ( x < price )]\n",
    "        elif qd_cntrb < sd_cntrb and sd_price >0 and sku_discount <= row['Target_margin']*0.35:\n",
    "            remaining_prices = [x for x in cleaned_prices if (x < sd_price) and (x < price)]\n",
    "        else:\n",
    "            remaining_prices = [x for x in cleaned_prices if x < price]\n",
    "        remaining_prices.sort()\n",
    "        new_price,source = select_price(product_UTH_growth,product_LH_growth,product_closing_growth,remaining_prices,price,wac,row['Target_margin'],row['min_boundary'],row['optimal_bm'],row['zero_rr'],doh)\n",
    "        row['selected_price'] = new_price\n",
    "        row['source'] = source\n",
    "        \n",
    "        \n",
    "    product_final_df = pd.concat([product_final_df, row.to_frame().T], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1621a7d-28e5-459c-bf2f-ed32ea7cd170",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "product_final_df['discount'] = abs((product_final_df['selected_price']-product_final_df['price'])/product_final_df['price'])\n",
    "product_final_df = product_final_df[(product_final_df['discount'] > 0.0025)&(product_final_df['selected_price']>0)]\n",
    "product_final_df['discount'] = product_final_df['discount']*100000\n",
    "product_final_df['discount'] = ((product_final_df['discount']//10)+1)/10000\n",
    "product_final_df['discount'] = np.minimum(product_final_df['discount'],0.05)\n",
    "product_final_df['discount']=product_final_df['discount']*100\n",
    "product_final_df['discount'] = product_final_df['discount'].apply(lambda x: f\"{x:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5fde937c-bcf2-416e-ad86-afa34c01ebb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "product_final_df = product_final_df[~product_final_df['cat'].isin([' '])]\n",
    "product_final_df = product_final_df[~product_final_df['brand'].isin(['',''])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc10e321-37e5-42da-9a82-67df9e825627",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "product_final_df.to_excel('Main_HH.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c02aedc7-0bf2-43ea-a23e-71c580ded71e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "product_final_df['tuple'] = product_final_df[[\"product_id\",'warehouse_id']].apply(tuple, axis=1)\n",
    "selected_skus_tuple = str(list(product_final_df['tuple']))[1:-1]\n",
    "product_final_df=product_final_df.drop(columns = 'tuple')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffcb5e8-eb24-42df-bb3b-00944d7a0267",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Retailers Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83b73dfe-46b7-4727-bf8a-44bac6c38ed8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2225"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = f'''\n",
    "with selected_prods as (\n",
    "select * \n",
    "from(\n",
    "VALUES\n",
    "{selected_skus_tuple}\n",
    ")x(product_id,warehouse_id)\n",
    "),\n",
    "selected_districts as (\n",
    "select distinct sp.warehouse_id , sp.product_id,dp.district_id  \n",
    "from WAREHOUSE_DISPATCHING_RULES wdr \n",
    "join DISPATCHING_POLYGONS dp on dp.id = wdr.DISPATCHING_POLYGON_ID\n",
    "join selected_prods sp on sp.product_id = wdr.product_id and wdr.warehouse_id = sp.warehouse_id\n",
    "),\n",
    "sales_before as (\n",
    "select retailer_id,product_id,warehouse_id,district_id,avg(nmv) as avg_nmv_before\n",
    "from (\n",
    "SELECT  DISTINCT\n",
    "so.id as order_id ,\n",
    "sd.district_id,\n",
    "sd.warehouse_id as warehouse_id,\n",
    "pso.product_id as product_id,\n",
    "so.retailer_id as retailer_id,\n",
    "sum(pso.total_price) as nmv \n",
    "\n",
    "\n",
    "FROM product_sales_order pso\n",
    "JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "JOIN materialized_views.retailer_polygon on materialized_views.retailer_polygon.retailer_id=so.retailer_id\n",
    "JOIN districts on districts.id=materialized_views.retailer_polygon.district_id\n",
    "join selected_districts sd on sd.product_id = pso.product_id and sd.district_id = districts.id\n",
    "           \n",
    "\n",
    "WHERE   True\n",
    "    AND so.created_at ::date between current_date - 120 and current_date - 31\n",
    "    AND so.sales_order_status_id not in (7,12)\n",
    "    AND so.channel IN ('telesales','retailer')\n",
    "    AND pso.purchased_item_count <> 0\n",
    "\t\n",
    "GROUP BY ALL\n",
    ")\n",
    "group by all \n",
    "),\n",
    "sales_after as (\n",
    "select retailer_id,product_id,warehouse_id,district_id,avg(nmv) as avg_nmv_after,max(order_date) as last_order\n",
    "from (\n",
    "SELECT  DISTINCT\n",
    "so.id as order_id ,\n",
    "so.created_at::date as order_date,\n",
    "sales_order_status_id, \n",
    "sd.district_id,\n",
    "sd.warehouse_id as warehouse_id,\n",
    "pso.product_id as product_id,\n",
    "so.retailer_id as retailer_id,\n",
    "sum(pso.total_price) as nmv \n",
    "\n",
    "\n",
    "FROM product_sales_order pso\n",
    "JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "JOIN materialized_views.retailer_polygon on materialized_views.retailer_polygon.retailer_id=so.retailer_id\n",
    "JOIN districts on districts.id=materialized_views.retailer_polygon.district_id\n",
    "join selected_districts sd on sd.product_id = pso.product_id and sd.district_id = districts.id\n",
    "           \n",
    "\n",
    "WHERE   True\n",
    "    AND so.created_at ::date > current_date - 31\n",
    "    AND so.sales_order_status_id not in (7,12)\n",
    "    AND so.channel IN ('telesales','retailer')\n",
    "    AND pso.purchased_item_count <> 0\n",
    "\t\n",
    "GROUP BY ALL\n",
    ")\n",
    "group by all \n",
    "\n",
    "),\n",
    "made_order as (\n",
    "select distinct so.retailer_id\n",
    "\n",
    "\n",
    "FROM  sales_orders so \n",
    "JOIN materialized_views.retailer_polygon on materialized_views.retailer_polygon.retailer_id=so.retailer_id\n",
    "JOIN districts on districts.id=materialized_views.retailer_polygon.district_id\n",
    "join selected_districts sd on sd.district_id = districts.id\n",
    "           \n",
    "\n",
    "WHERE   True\n",
    "    AND so.created_at ::date >= current_date - 60\n",
    "    AND so.sales_order_status_id not in (7,12)\n",
    "    AND so.channel IN ('telesales','retailer')\n",
    "\t\n",
    "GROUP BY ALL\n",
    ")\n",
    "\n",
    "select distinct retailer_id , product_id,warehouse_id \n",
    "from (\n",
    "select sb.* , coalesce(avg_nmv_after,0) as nmv_after,(nmv_after-avg_nmv_before)/avg_nmv_before as growth\n",
    "from sales_before sb \n",
    "left join sales_after sa on sb.retailer_id = sa.retailer_id and sb.product_id = sa.product_id\n",
    "left join made_order mo on mo.retailer_id = sa.retailer_id \n",
    "where growth < -0.6\n",
    "and (current_date - last_order >=5 or last_order is null)\n",
    "and mo.retailer_id is not null \n",
    ")\n",
    "'''\n",
    "churned_dropped =  query_snowflake(query, columns = ['retailer_id','product_id','warehouse_id'])\n",
    "churned_dropped.columns = churned_dropped.columns.str.lower()\n",
    "for col in churned_dropped.columns:\n",
    "    churned_dropped[col] = pd.to_numeric(churned_dropped[col], errors='ignore')  \n",
    "churned_dropped.retailer_id.nunique()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3099392-144d-42d0-8b00-e6c5fbccb5a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25940"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = f'''\n",
    "with selected_prods as (\n",
    "select * \n",
    "from(\n",
    "VALUES\n",
    "{selected_skus_tuple}\n",
    ")x(product_id,warehouse_id)\n",
    "),\n",
    "selected_districts as (\n",
    "select distinct sp.warehouse_id , sp.product_id,dp.district_id ,c.name_ar as cat ,b.name_ar as brand\n",
    "from WAREHOUSE_DISPATCHING_RULES wdr \n",
    "join DISPATCHING_POLYGONS dp on dp.id = wdr.DISPATCHING_POLYGON_ID\n",
    "join selected_prods sp on sp.product_id = wdr.product_id and wdr.warehouse_id = sp.warehouse_id\n",
    "join products p on p.id = sp.product_id\n",
    "join brands b on b.id = p.brand_id \n",
    "join categories c on c.id = p.category_id \n",
    "),\n",
    "selected_dis_cat_brand as (\n",
    "select distinct warehouse_id,district_id,cat\n",
    "from selected_districts\n",
    "),\n",
    "\n",
    "buy_cat as (\n",
    "SELECT  DISTINCT\n",
    "sd.district_id,\n",
    "sd.warehouse_id as warehouse_id,\n",
    "so.retailer_id as retailer_id,\n",
    "c.name_ar as cat,\n",
    "b.name_ar as brand,\n",
    "pso.product_id\n",
    "\n",
    "\n",
    "FROM product_sales_order pso\n",
    "JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "JOIN materialized_views.retailer_polygon on materialized_views.retailer_polygon.retailer_id=so.retailer_id\n",
    "JOIN districts on districts.id=materialized_views.retailer_polygon.district_id\n",
    "join products p on p.id = pso.product_id\n",
    "join brands b on b.id = p.brand_id \n",
    "join categories c on c.id = p.category_id \n",
    "join selected_dis_cat_brand sd on sd.cat = c.name_ar and sd.district_id = districts.id\n",
    "           \n",
    "\n",
    "WHERE   True\n",
    "    AND so.created_at::date >= current_date - 60\n",
    "    AND so.sales_order_status_id not in (7,12)\n",
    "    AND so.channel IN ('telesales','retailer')\n",
    "    AND pso.purchased_item_count <> 0\n",
    "\t\n",
    "\n",
    "),\n",
    "chosen_products as (\n",
    "select sp.*,c.name_ar as cat ,b.name_ar as brand\n",
    "from selected_prods sp \n",
    "join products p on p.id = sp.product_id\n",
    "join brands b on b.id = p.brand_id \n",
    "join categories c on c.id = p.category_id \n",
    "\n",
    ")\n",
    "select distinct retailer_id,selected_product_id,warehouse_id \n",
    "from (\n",
    "select warehouse_id,retailer_id,cat,brand,selected_product_id,max(flag) as flag\n",
    "from (\n",
    "select bc.* , cp.product_id as selected_product_id, case when cp.product_id = bc.product_id then 1 else 0 end as flag \n",
    "from buy_cat bc \n",
    "left join chosen_products cp on cp.warehouse_id = bc.warehouse_id  and cp.cat = bc.cat \n",
    ")\n",
    "group by all \n",
    ")\n",
    "where flag = 0 \n",
    "'''\n",
    "cat_not_product =  query_snowflake(query, columns = ['retailer_id','product_id','warehouse_id'])\n",
    "cat_not_product.columns = cat_not_product.columns.str.lower()\n",
    "for col in cat_not_product.columns:\n",
    "    cat_not_product[col] = pd.to_numeric(cat_not_product[col], errors='ignore') \n",
    "cat_not_product    \n",
    "cat_not_product.retailer_id.nunique()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f94759ec-f354-4d3c-a5da-a2114f8d2eb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1728"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query =f'''\n",
    "with selected_prods as (\n",
    "select * \n",
    "from(\n",
    "VALUES\n",
    "{selected_skus_tuple}\n",
    ")x(product_id,warehouse_id)\n",
    "),\n",
    "selected_districts as (\n",
    "select distinct sp.warehouse_id , sp.product_id,dp.district_id  \n",
    "from WAREHOUSE_DISPATCHING_RULES wdr \n",
    "join DISPATCHING_POLYGONS dp on dp.id = wdr.DISPATCHING_POLYGON_ID\n",
    "join selected_prods sp on sp.product_id = wdr.product_id and wdr.warehouse_id = sp.warehouse_id\n",
    ")\n",
    "select retailer_id,product_id,warehouse_id\n",
    "from (\n",
    "select * ,last_o_date+ floor(avg_cycle+(2.5*std))::int as next_order\n",
    "from(\n",
    "select retailer_id,product_id,warehouse_id,max(last_o_date) as last_o_date,sum(order_days * (w/all_w)) as avg_cycle,stddev(order_days) as std\n",
    "from (\n",
    "select *,\n",
    "max(order_num) over(partition by retailer_id , product_id) as max_orders,\n",
    "lag(o_date) over(partition by product_id,retailer_id order by o_date) as prev_order,\n",
    "o_date - prev_order as order_days,\n",
    "case when current_date- o_date = 0 then 1 else 1/(CURRENT_DATE-o_date) end as w,\n",
    "sum(w)over(partition by product_id , retailer_id) as all_w\n",
    "from (\n",
    "SELECT  DISTINCT\n",
    "so.id as order_id,\n",
    "so.created_at::date as o_date,\n",
    "sd.district_id,\n",
    "sd.warehouse_id as warehouse_id,\n",
    "pso.product_id as product_id,\n",
    "so.retailer_id as retailer_id,\n",
    "sum(pso.total_price) as nmv,\n",
    "row_number()over(partition by so.retailer_id , pso.product_id order by o_date desc) as order_num,\n",
    "max(o_date) over(partition by so.retailer_id , pso.product_id ) as last_o_date\n",
    "\n",
    "\n",
    "\n",
    "FROM product_sales_order pso\n",
    "JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "JOIN materialized_views.retailer_polygon on materialized_views.retailer_polygon.retailer_id=so.retailer_id\n",
    "JOIN districts on districts.id=materialized_views.retailer_polygon.district_id\n",
    "join selected_districts sd on sd.product_id = pso.product_id and sd.district_id = districts.id\n",
    "           \n",
    "\n",
    "WHERE  so.created_at ::date >= date_trunc('month',current_date- interval '1 year')\n",
    "    AND so.sales_order_status_id not in (7,12)\n",
    "    AND so.channel IN ('telesales','retailer')\n",
    "    AND pso.purchased_item_count <> 0\n",
    "GROUP BY 1,2,3,4,5,6\n",
    ")\n",
    "where last_o_date >= current_date - 60\n",
    "qualify max_orders >= 4\n",
    ")\n",
    "where prev_order is not null \n",
    "group by all\n",
    ")\n",
    "where CURRENT_DATE >= next_order\n",
    "\n",
    ")\n",
    "'''\n",
    "out_of_cycle =  query_snowflake(query, columns = ['retailer_id','product_id','warehouse_id'])\n",
    "out_of_cycle.columns = out_of_cycle.columns.str.lower()\n",
    "for col in out_of_cycle.columns:\n",
    "    out_of_cycle[col] = pd.to_numeric(out_of_cycle[col], errors='ignore')  \n",
    "out_of_cycle\n",
    "out_of_cycle.retailer_id.nunique()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d86d1f3a-57ae-465a-b871-5cde09728450",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14254"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = f'''\n",
    "with selected_prods as (\n",
    "select * \n",
    "from(\n",
    "VALUES\n",
    "{selected_skus_tuple}\n",
    ")x(product_id,warehouse_id)\n",
    "),\n",
    "selected_districts as (\n",
    "select distinct sp.warehouse_id ,dp.district_id,c.id as cat_id ,b.id as brand_id,sp.product_id\n",
    "from WAREHOUSE_DISPATCHING_RULES wdr \n",
    "join DISPATCHING_POLYGONS dp on dp.id = wdr.DISPATCHING_POLYGON_ID\n",
    "join selected_prods sp on sp.product_id = wdr.product_id and wdr.warehouse_id = sp.warehouse_id\n",
    "join products p on p.id = sp.product_id \n",
    "join brands b on b.id = p.brand_id \n",
    "join categories c on c.id = p.category_id\n",
    "),\n",
    "brand_open as (\n",
    "select        \n",
    "\t \t\tevent_date,\n",
    "            event_timestamp,\n",
    "            vb.retailer_id,\n",
    "\t\t\tvb.brand_id,\n",
    "\t\t\tvb.brand_name,\n",
    "\t\t\tvb.category_id,\n",
    "\t\t\tc.name_ar as cat_name\n",
    "\t\t\t\n",
    "\t\t\n",
    "\t\tFROM maxab_events.view_brand vb\n",
    "\t\tjoin categories c on c.id = vb.category_id\n",
    "          WHERE  event_timestamp::date between CURRENT_DATE -10  and CURRENT_DATE -3\n",
    "            AND country LIKE '%Egypt%'\n",
    "            AND user_id LIKE '%EG_retailers_%'\n",
    "\t\t\tand brand_id <> 'null'\n",
    "),\n",
    "add_to_cart as (\n",
    "\n",
    "          SELECT \n",
    "            event_date,\n",
    "            event_timestamp,\n",
    "            uc.retailer_id,\n",
    "            productsid AS product_id,\n",
    "\t\t\tb.id as brand_id\n",
    "          FROM maxab_events.update_cart uc\n",
    "\t\t  join products p on p.id = uc.productsid \n",
    "\t\t  join brands b on b.id = p.brand_id \n",
    "          WHERE  event_timestamp::date between CURRENT_DATE -10  and CURRENT_DATE -3\n",
    "            AND country LIKE '%Egypt%'\n",
    "            AND update_type = 'add'\n",
    "            AND user_id LIKE '%EG_retailers_%'\n",
    "            AND productsid REGEXP '^[0-9]+$'\n",
    "),\n",
    "in_stock_retailers as(\n",
    "select distinct retailer_id \n",
    "from sales_orders \n",
    "where sales_order_status_id = 6 \n",
    "and channel in ('retailer','telesales')\n",
    "and created_at::date >= date_trunc('month',current_date - interval '6 months')\n",
    "),\n",
    "sales_data as (\n",
    "select so.retailer_id,b.name_ar as brand,c.name_ar as cat,max(so.created_at::date) as o_date\n",
    "\n",
    "from sales_orders so\n",
    "join PRODUCT_SALES_ORDER pso on pso.sales_order_id = so.id \n",
    "join products p on p.id = pso.product_id\n",
    "join brands b on b.id = p.brand_id \n",
    "join categories c on c.id = p.category_id\n",
    "where so.created_at::date >= CURRENT_DATE -10  \n",
    "and sales_order_status_id not in (7,12)\n",
    "group by all\n",
    "\n",
    "),\n",
    "cat_brand as (\n",
    "select distinct c.id as cat , b.id as brand \n",
    "from sales_orders so\n",
    "join PRODUCT_SALES_ORDER pso on pso.sales_order_id = so.id \n",
    "join products p on p.id = pso.product_id\n",
    "join brands b on b.id = p.brand_id \n",
    "join categories c on c.id = p.category_id\n",
    "where so.created_at::date >= CURRENT_DATE -120 \n",
    "and sales_order_status_id not in (7,12)\n",
    "\n",
    "),\n",
    "main_cte as (\n",
    "select * \n",
    "from (\n",
    "select x.* , case when sd.retailer_id is not null then 1 else 0 end as ordered \n",
    "from (\n",
    "select *,max(event_date) over(partition by retailer_id,brand_id,category_id) as last_event\n",
    "from (\n",
    "select event_date,retailer_id,brand_id,brand_name,category_id,\n",
    "cat_name,sum(count_n) as total_count\n",
    "from (\n",
    "select bo.*,count(distinct atc.product_id) as count_n\n",
    "from brand_open bo \n",
    "join cat_brand cb on bo.category_id = cb.cat and bo.brand_id = cb.brand\n",
    "join in_stock_retailers isr on isr.retailer_id = bo.retailer_id \n",
    "left join add_to_cart atc on bo.retailer_id = atc.retailer_id and bo.brand_id = atc.brand_id and atc.event_timestamp >= bo.event_timestamp\n",
    "group by all \n",
    ")\n",
    "group by all \n",
    ")\n",
    "qualify event_date = last_event\n",
    ")x \n",
    "left join sales_data sd on sd.retailer_id = x.retailer_id and x.cat_name = sd.cat and x.brand_name = sd.brand and x.event_date <= sd.o_date\n",
    ")\n",
    "where ordered = 0 and total_count = 0 \n",
    ")\n",
    "select distinct m.retailer_id ,sd.product_id,sd.warehouse_id\n",
    "from main_cte m \n",
    "JOIN materialized_views.retailer_polygon on materialized_views.retailer_polygon.retailer_id=m.retailer_id\n",
    "JOIN districts on districts.id=materialized_views.retailer_polygon.district_id\n",
    "join selected_districts sd on sd.district_id = districts.id and sd.brand_id = m.brand_id and sd.cat_id = m.category_id\n",
    "'''\n",
    "view_no_orders =  query_snowflake(query, columns = ['retailer_id','product_id','warehouse_id'])\n",
    "view_no_orders.columns = view_no_orders.columns.str.lower()\n",
    "for col in view_no_orders.columns:\n",
    "    view_no_orders[col] = pd.to_numeric(view_no_orders[col], errors='ignore')  \n",
    "view_no_orders.retailer_id.nunique()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fb235fb7-ae1a-4ef5-9051-343bad49fa91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128386"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = '''\n",
    "select retailer_id\n",
    "from (\n",
    "SELECT  DISTINCT\n",
    "retailer_id,\n",
    "sales_order_status_id,\n",
    "created_at::date as o_date ,\n",
    "max(o_date)over(partition by retailer_id) as last_order\n",
    "from sales_orders so \n",
    "WHERE  so.created_at ::date >= current_date - 120\n",
    "AND so.sales_order_status_id not in (7,12)\n",
    "AND so.channel IN ('telesales','retailer')\n",
    "qualify o_date = last_order\n",
    ")\n",
    "where sales_order_status_id not in (6,9,12)\n",
    "\n",
    "union all \n",
    "\n",
    "select id as retailer_id \n",
    "from retailers \n",
    "where activation = 'false'\n",
    "\n",
    "union all \n",
    "\n",
    "select distinct dta.TAGGABLE_ID as retailer_id\n",
    "from DYNAMIC_TAGS dt \n",
    "join dynamic_taggables dta on dt.id = dta.dynamic_tag_id \n",
    "where name like '%whole_sale%'\n",
    "and dt.id > 3000\n",
    "\n",
    "'''\n",
    "exec_rets =  query_snowflake(query, columns = ['retailer_id'])\n",
    "exec_rets.columns = exec_rets.columns.str.lower()\n",
    "for col in exec_rets.columns:\n",
    "    exec_rets[col] = pd.to_numeric(exec_rets[col], errors='ignore') \n",
    "exec_rets =  exec_rets.retailer_id.unique() \n",
    "len(exec_rets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d0332ed-b0be-49f7-bd28-029df48d7e87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>tag_id</th>\n",
       "      <th>retailer_id</th>\n",
       "      <th>have_quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3301</td>\n",
       "      <td>558354</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3301</td>\n",
       "      <td>858862</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3301</td>\n",
       "      <td>876524</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3301</td>\n",
       "      <td>398102</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>3301</td>\n",
       "      <td>361040</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7659013</th>\n",
       "      <td>23920</td>\n",
       "      <td>3311</td>\n",
       "      <td>543436</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7659014</th>\n",
       "      <td>23920</td>\n",
       "      <td>3311</td>\n",
       "      <td>659488</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7659015</th>\n",
       "      <td>23920</td>\n",
       "      <td>3311</td>\n",
       "      <td>855668</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7659016</th>\n",
       "      <td>23920</td>\n",
       "      <td>3311</td>\n",
       "      <td>855775</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7659017</th>\n",
       "      <td>23920</td>\n",
       "      <td>3311</td>\n",
       "      <td>612628</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7659018 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         product_id  tag_id  retailer_id  have_quantity\n",
       "0                 3    3301       558354              1\n",
       "1                 3    3301       858862              1\n",
       "2                 3    3301       876524              1\n",
       "3                 3    3301       398102              1\n",
       "4                 3    3301       361040              1\n",
       "...             ...     ...          ...            ...\n",
       "7659013       23920    3311       543436              1\n",
       "7659014       23920    3311       659488              1\n",
       "7659015       23920    3311       855668              1\n",
       "7659016       23920    3311       855775              1\n",
       "7659017       23920    3311       612628              1\n",
       "\n",
       "[7659018 rows x 4 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    query = f'''\n",
    "    SELECT DISTINCT\n",
    "        qdv.product_id,\n",
    "        qd.dynamic_tag_id AS tag_id\n",
    "    FROM quantity_discounts qd\n",
    "    JOIN quantity_discount_values qdv \n",
    "        ON qd.id = qdv.quantity_discount_id\n",
    "    WHERE ((CURRENT_TIMESTAMP AT TIME ZONE 'Africa/Cairo'\n",
    "          BETWEEN qd.start_at AND qd.end_at) or ((qd.start_at::date = current_date) and (CURRENT_TIMESTAMP AT TIME ZONE 'Africa/Cairo' < qd.start_at)))\n",
    "    AND qd.active = TRUE\n",
    "    '''\n",
    "    quantity_data =  setup_environment_2.dwh_pg_query(query, columns = ['product_id','tag_id'])\n",
    "    quantity_data.columns = quantity_data.columns.str.lower()\n",
    "    for col in quantity_data.columns:\n",
    "        quantity_data[col] = pd.to_numeric(quantity_data[col], errors='ignore')     \n",
    "\n",
    "    qd_data = quantity_data.copy()[['tag_id']].drop_duplicates()\n",
    "    qd_data['tuple'] = \"(\"+qd_data['tag_id'].astype(str)+\")\"\n",
    "    qd_data = qd_data['tuple'].unique()\n",
    "    qd_list = ''\n",
    "    for c in qd_data:\n",
    "        qd_list = qd_list+c+\",\"\n",
    "    qd_list = qd_list[:-1]\n",
    "\n",
    "    query = f'''\n",
    "    with tags as (\n",
    "    select *\n",
    "    from(\n",
    "    values\n",
    "    {qd_list}\n",
    "    )x(dynamic_tag_id)\n",
    "\n",
    "    )\n",
    "\n",
    "    select tags.dynamic_tag_id as tag_id,taggable_id as retailer_id\n",
    "    from dynamic_taggables dt  \n",
    "    join tags on tags.dynamic_tag_id = dt.dynamic_tag_id\n",
    "    '''\n",
    "    qd_rets = query_snowflake(query, columns = ['tag_id','retailer_id'])\n",
    "    for col in qd_rets.columns:\n",
    "        qd_rets[col] = pd.to_numeric(qd_rets[col], errors='ignore')  \n",
    "\n",
    "    quantity_data=quantity_data.merge(qd_rets,on='tag_id')\n",
    "    quantity_data['have_quantity']=1\n",
    "except:\n",
    "    quantity_data = pd.DataFrame(columns= ['product_id','tag_id','retailer_id','have_quantity'])\n",
    "    \n",
    "quantity_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "267d24de-8a68-45ae-a9c6-f8d7a9c25385",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "select distinct product_id,packing_unit_id \n",
    "from packing_unit_products\n",
    "where product_id <> 1309 or (product_id = 1309 and packing_unit_id <> 23)\n",
    "'''\n",
    "pus = query_snowflake(query, columns = ['product_id','packing_unit_id'])\n",
    "for col in pus.columns:\n",
    "    pus[col] = pd.to_numeric(pus[col], errors='ignore')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7881b02e-8f82-45aa-8a92-c25a1fbdbde6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query ='''\n",
    "select retailer_id,warehouse_id,1 as last_wh \n",
    "from (\n",
    "SELECT  DISTINCT\n",
    "\t\tso.retailer_id,\n",
    "\t\tpso.warehouse_id,\n",
    "\t\tso.created_at::date as o_date,\n",
    "\t\tmax(so.created_at::date) over(partition by so.retailer_id) as max_date\n",
    "\n",
    "FROM product_sales_order pso\n",
    "JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "JOIN products on products.id=pso.product_id\n",
    "JOIN brands on products.brand_id = brands.id \n",
    "JOIN categories ON products.category_id = categories.id\n",
    "JOIN finance.all_cogs f  ON f.product_id = pso.product_id\n",
    "                        AND f.from_date::date <= so.created_at ::date\n",
    "                        AND f.to_date::date > so.created_at ::date\n",
    "JOIN product_units ON product_units.id = products.unit_id  \n",
    "\n",
    "\n",
    "WHERE  so.created_at::date >= current_date - 120 \n",
    "    AND so.sales_order_status_id not in (7,12)\n",
    "    AND so.channel IN ('telesales','retailer')\n",
    "    AND pso.purchased_item_count <> 0\n",
    "\tand pso.warehouse_id in (1,8,170,236,337,339,401,501,632,703,797,962)\n",
    "\n",
    "GROUP BY 1,2,3\n",
    "qualify o_date = max_date\n",
    ")\n",
    "'''\n",
    "ret_wh = query_snowflake(query, columns = ['retailer_id','warehouse_id','last_wh'])\n",
    "for col in ret_wh.columns:\n",
    "    ret_wh[col] = pd.to_numeric(ret_wh[col], errors='ignore')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3eaa5f5a-d837-4199-b51e-5104a50c66e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_retailers  = pd.concat([cat_not_product, churned_dropped]).drop_duplicates().reset_index(drop=True)\n",
    "all_retailers  = pd.concat([all_retailers, out_of_cycle]).drop_duplicates().reset_index(drop=True)\n",
    "all_retailers  = pd.concat([all_retailers, view_no_orders]).drop_duplicates().reset_index(drop=True)\n",
    "all_retailers = all_retailers.merge(ret_wh,on=['retailer_id','warehouse_id'],how='left')\n",
    "all_retailers=all_retailers.fillna(0)\n",
    "all_retailers['rank'] = all_retailers.groupby(['retailer_id'])['last_wh'].rank(method = 'dense',ascending=False).astype(int)\n",
    "all_retailers=all_retailers[all_retailers['rank']==1]\n",
    "all_retailers = all_retailers[~(all_retailers['retailer_id'].isin(exec_rets))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "763733c2-12c8-477b-9388-1ab5790674c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "product_final_df=product_final_df[['product_id','warehouse_id','discount']]\n",
    "all_retailers[['warehouse_id','product_id','retailer_id']]\n",
    "\n",
    "final_df = product_final_df.merge(all_retailers,on=['warehouse_id','product_id'])\n",
    "final_df=final_df.merge(quantity_data,on=['retailer_id','product_id'],how='left')\n",
    "final_df=final_df[final_df['have_quantity'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2d9212ae-72c8-4afc-ae47-b859f79c9425",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23742"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.retailer_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1c5a0150-0259-41e8-ae02-e718471e17b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_df = final_df.merge(pus,on='product_id')\n",
    "final_df['HH_data'] = '['+(final_df['product_id']).astype(str)+','+(final_df['packing_unit_id']).astype(str)+','+(final_df['discount']).astype(str)+']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5a1a9c82-66c1-4815-8a83-fef5855b2784",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/12/2025 12:25 11/12/2025 11:59\n"
     ]
    }
   ],
   "source": [
    "slots = ['0-12','13-17','18-23']\n",
    "local_tz = pytz.timezone('Africa/Cairo')\n",
    "current_hour = datetime.now(local_tz).hour\n",
    "chosen_slot = [np.nan,np.nan]\n",
    "\n",
    "for slot in slots:\n",
    "    parts = slot.split(\"-\")\n",
    "    if(current_hour >= int(parts[0]) and current_hour < int(parts[1])):\n",
    "        chosen_slot[0] = int(parts[0]) \n",
    "        chosen_slot[1] = int(parts[1]) \n",
    "        break\n",
    "    else:\n",
    "        chosen_slot[0] = 0\n",
    "        chosen_slot[1] = 0 \n",
    "        \n",
    "today = datetime.now(local_tz) \n",
    "start_hour = np.maximum(current_hour,chosen_slot[0])\n",
    "if(start_hour==current_hour):\n",
    "    start_mins =  (datetime.now(local_tz).minute) +10\n",
    "else:\n",
    "    start_mins = 30 \n",
    "    \n",
    "    \n",
    "start_date = (today.replace(hour=start_hour, minute=0, second=0, microsecond=0)+ timedelta(minutes=start_mins)).strftime('%d/%m/%Y %H:%M')\n",
    "end_date = ((today+ timedelta(days=1)).replace(hour=11, minute=59, second=0, microsecond=0)).strftime('%d/%m/%Y %H:%M')\n",
    "print(start_date,end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "11720849-a741-4bb3-b6b1-377981796a7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_df  = final_df.groupby('retailer_id')['HH_data'].apply(list).reset_index()\n",
    "output_df['Discounts']= output_df['HH_data'].astype(str).str.replace(\"'\",'').str.replace(' ','')\n",
    "output_df = output_df.groupby('Discounts')['retailer_id'].agg(list).reset_index()\n",
    "output_df['Arabic Offer Name']= ' '\n",
    "output_df['Start Date/Time'] = start_date\n",
    "output_df['End Date/Time'] = end_date\n",
    "output_df = output_df[['retailer_id','Start Date/Time','End Date/Time','Discounts','Arabic Offer Name']]\n",
    "output_df['French Offer Name']=np.nan\n",
    "output_df['English Offer Name']=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c800dca1-7dc2-480f-b84c-99f8a011fdf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for i,row in output_df.iterrows():\n",
    "    \n",
    "    start_date = row['Start Date/Time']\n",
    "    end_date = row['End Date/Time']\n",
    "    retailers = row['retailer_id']\n",
    "    discount = row['Discounts']\n",
    "    name = row['Arabic Offer Name'] \n",
    "    name_f = row['French Offer Name'] \n",
    "    name_e = row['English Offer Name'] \n",
    "    \n",
    "    length = len(retailers)\n",
    "    if(length>100):\n",
    "        iters = length//100\n",
    "        remaining = length%100\n",
    "        for j in range(0,iters+1):\n",
    "            if(j<=iters):\n",
    "                start = (j*100)\n",
    "                end = (j+1)*100\n",
    "                rets = retailers[start:end]\n",
    "                data.append({'Discounts':discount,'retailer_id':rets,'Start Date/Time':start_date,'End Date/Time':end_date\n",
    "                            ,'Arabic Offer Name':name,'French Offer Name':name_f,'English Offer Name':name_e})\n",
    "            else:\n",
    "                print(\"else new\")\n",
    "            \n",
    "    else:\n",
    "        data.append({'Discounts':discount,'retailer_id':retailers,'Start Date/Time':start_date,'End Date/Time':end_date\n",
    "                            ,'Arabic Offer Name':name,'French Offer Name':name_f,'English Offer Name':name_e})\n",
    "        \n",
    "dfx = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e3df721b-5e11-4f6d-92e8-c8753065a46b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfx['English Offer Name'] = 'Special Discounts'\n",
    "dfx['Swahili Offer Name'] = ''\n",
    "dfx['Rwandan Offer Name'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "215ee4e4-20f4-4a70-a7de-e7a359e64f23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_added = dfx.iloc[0, :].to_frame().T\n",
    "df_added['retailer_id'] = \"[111780,114210]\"\n",
    "dfx = pd.concat([dfx,df_added])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1848d7bc-675b-496f-bf88-e7514da46898",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Moved: o_happy_hour_2025-12-09_NO._16.xlsx\n",
      " Moved: o_happy_hour_2025-12-09_NO._18.xlsx\n",
      " Moved: o_happy_hour_2025-12-09_NO._25.xlsx\n",
      " Moved: o_happy_hour_2025-12-09_NO._20.xlsx\n",
      " Moved: o_happy_hour_2025-12-09_NO._19.xlsx\n",
      " Moved: o_happy_hour_2025-12-09_NO._4.xlsx\n",
      " Moved: o_happy_hour_2025-12-09_NO._22.xlsx\n",
      " Moved: o_happy_hour_2025-12-09_NO._12.xlsx\n",
      " Moved: o_happy_hour_2025-12-09_NO._21.xlsx\n",
      " Moved: o_happy_hour_2025-12-09_NO._14.xlsx\n",
      " Moved: o_happy_hour_2025-12-09_NO._6.xlsx\n",
      " Moved: o_happy_hour_2025-12-09_NO._1.xlsx\n",
      " Moved: o_happy_hour_2025-12-09_NO._11.xlsx\n",
      " Moved: o_happy_hour_2025-12-09_NO._13.xlsx\n",
      " Moved: o_happy_hour_2025-12-09_NO._3.xlsx\n",
      " Moved: o_happy_hour_2025-12-09_NO._17.xlsx\n",
      " Moved: o_happy_hour_2025-12-09_NO._10.xlsx\n",
      " Moved: o_happy_hour_2025-12-09_NO._2.xlsx\n",
      " Moved: o_happy_hour_2025-12-09_NO._15.xlsx\n",
      " Moved: o_happy_hour_2025-12-09_NO._26.xlsx\n",
      " Moved: o_happy_hour_2025-12-09_NO._8.xlsx\n",
      " Moved: o_happy_hour_2025-12-09_NO._7.xlsx\n",
      " Moved: o_happy_hour_2025-12-09_NO._23.xlsx\n",
      " Moved: o_happy_hour_2025-12-09_NO._24.xlsx\n",
      " Moved: o_happy_hour_2025-12-09_NO._9.xlsx\n",
      " Moved: o_happy_hour_2025-12-09_NO._0.xlsx\n",
      " Moved: o_happy_hour_2025-12-09_NO._5.xlsx\n",
      "\n",
      "Summary: 27 files moved, 0 errors\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def move_all_files(source_dir, dest_dir):\n",
    "    \"\"\"Copy files to destination and delete from source\"\"\"\n",
    "    # Create destination directory if it doesn't exist\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "    \n",
    "    source = Path(source_dir)\n",
    "    destination = Path(dest_dir)\n",
    "    \n",
    "    moved_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    for file in source.iterdir():\n",
    "        if file.is_file():\n",
    "            try:\n",
    "                # Copy file to destination\n",
    "                dest_file = destination / file.name\n",
    "                #shutil.copy2(file, dest_file)  # copy2 preserves metadata\n",
    "                \n",
    "                # Delete from source\n",
    "                file.unlink()\n",
    "                \n",
    "                print(f\" Moved: {file.name}\")\n",
    "                moved_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\" Error moving {file.name}: {e}\")\n",
    "                error_count += 1\n",
    "    \n",
    "    print(f\"\\nSummary: {moved_count} files moved, {error_count} errors\")\n",
    "\n",
    "# Usage\n",
    "move_all_files('HH_Sheets', 'HH_temp_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8ab3de1c-12ca-42f6-a0fd-b11c82835f9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 21/21 [00:07<00:00,  2.91it/s]\n"
     ]
    }
   ],
   "source": [
    "dfx.rename(columns={'retailer_id': 'Retailers List'}, inplace=True)\n",
    "dfx = dfx[['Retailers List','Start Date/Time','End Date/Time','Discounts','Arabic Offer Name','French Offer Name','English Offer Name','Swahili Offer Name','Rwandan Offer Name']]\n",
    "# 500 row per sheet \n",
    "final=dfx.reset_index().drop(columns='index')\n",
    "mino=final.index.min()\n",
    "maxo=final.index.max()\n",
    "ran = [i for i in range(mino,maxo,1000)]\n",
    "for i in tqdm(range(len(ran))):\n",
    "    if i+1 == len(ran):\n",
    "        val1 = ran[i]\n",
    "        val2 = maxo\n",
    "    else:\n",
    "        val1 = ran[i]\n",
    "        val2 = ran[i+1] - 1\n",
    "    x=final.loc[val1:val2,:]\n",
    "    x.to_excel(f'HH_Sheets/o_happy_hour_{str((datetime.now()).date())}_NO._{i}.xlsx'.format(i),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9aec4354-8439-44c2-8be7-09da1cc156dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_secret(secret_name):\n",
    "    region_name = \"us-east-1\"\n",
    "\n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "    # In this sample we only handle the specific exceptions for the 'GetSecretValue' API.\n",
    "    # See https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html\n",
    "    # We rethrow the exception by default.\n",
    "\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(SecretId=secret_name)\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'DecryptionFailureException':\n",
    "            # Secrets Manager can't decrypt the protected secret text using the provided KMS key.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InternalServiceErrorException':\n",
    "            # An error occurred on the server side.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidParameterException':\n",
    "            # You provided an invalid value for a parameter.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidRequestException':\n",
    "            # You provided a parameter value that is not valid for the current state of the resource.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'ResourceNotFoundException':\n",
    "            # We can't find the resource that you asked for.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "    else:\n",
    "        # Decrypts secret using the associated KMS CMK.\n",
    "        # Depending on whether the secret is a string or binary, one of these fields will be populated.\n",
    "        if 'SecretString' in get_secret_value_response:\n",
    "            return get_secret_value_response['SecretString']\n",
    "        else:\n",
    "            return base64.b64decode(get_secret_value_response['SecretBinary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "11c0205e-fd0f-4fe1-bd11-58030e5f68b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pricing_api_secret = json.loads(get_secret(\"prod/pricing/api/\"))\n",
    "username = pricing_api_secret[\"egypt_username\"]\n",
    "password = pricing_api_secret[\"egypt_password\"]\n",
    "secret = pricing_api_secret[\"egypt_secret\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a5e32466-b1b1-455c-a574-7e6351d79dcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_access_token(url, client_id, client_secret):\n",
    "    \"\"\"\n",
    "    get_access_token function takes three parameters and returns a session token\n",
    "    to connect to MaxAB APIs\n",
    "\n",
    "    :param url: production MaxAB token URL\n",
    "    :param client_id: client ID\n",
    "    :param client_secret: client sercret\n",
    "    :return: session token\n",
    "    \"\"\"\n",
    "    response = requests.post(\n",
    "        url,\n",
    "        data={\"grant_type\": \"password\",\n",
    "              \"username\": username,\n",
    "              \"password\": password},\n",
    "        auth=(client_id, client_secret),\n",
    "    )\n",
    "    return response.json()[\"access_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "58084f7e-13a5-4b66-86f9-ca72e08801c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preassigned_url():\n",
    "    token = get_access_token('https://sso.maxab.info/auth/realms/maxab/protocol/openid-connect/token',\n",
    "                             'main-system-externals',\n",
    "                             secret)\n",
    "    url = \"https://api.maxab.info/commerce/api/admins/v1/bulk-upload/presigned-url?type=SKU_DISCOUNTS\"\n",
    "    payload={}\n",
    "    headers = {\n",
    "      'Authorization': 'bearer {}'.format(token)}\n",
    "\n",
    "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "83ee4313-5124-47a7-a5ef-2d16d8833c4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def upload_sku_discount(file_name,new_url):\n",
    "    url = new_url\n",
    "    headers = {'Content-Type':'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'}\n",
    "    with open(file_name, 'rb') as f:\n",
    "        response = requests.put(new_url, data=f, headers=headers)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7747642f-865f-40bf-8949-2099d434782c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validate_skus_discount(key):\n",
    "    token = get_access_token('https://sso.maxab.info/auth/realms/maxab/protocol/openid-connect/token',\n",
    "                             'main-system-externals',\n",
    "                             secret)\n",
    "    url = 'https://api.maxab.info/commerce/api/admins/v1/bulk-upload/sheets/validate'\n",
    "    headers = {\n",
    "        'Authorization': 'bearer {}'.format(token),\n",
    "        'content-type':'application/json'\n",
    "\n",
    "       }\n",
    "    payload={\"fileName\":key,\"sheetType\":\"SKU_DISCOUNTS\"}\n",
    "    response = requests.request(\"POST\", url, headers=headers, json=payload)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8a92b90c-6cdf-411a-a280-cc08336bbeb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def proceed(key):\n",
    "    token = get_access_token('https://sso.maxab.info/auth/realms/maxab/protocol/openid-connect/token',\n",
    "                             'main-system-externals',\n",
    "                             secret)\n",
    "    url = f'https://api.maxab.info/commerce/api/admins/v1/bulk-upload/sheets/proceed/{key}?uploadType=SKU_DISCOUNTS'\n",
    "    headers = {\n",
    "        'Authorization': 'bearer {}'.format(token),\n",
    "        'content-type':'application/json'\n",
    "       }\n",
    "    response = requests.request(\"POST\", url, headers=headers)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bfd128ad-a322-4240-8881-d20fd9fe65b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def listing():\n",
    "    token = get_access_token('https://sso.maxab.info/auth/realms/maxab/protocol/openid-connect/token',\n",
    "                             'main-system-externals',\n",
    "                             secret)\n",
    "    url = 'https://api.maxab.info/commerce/api/admins/v1/bulk-upload/sheets?filter=sheetType=in=(SKU_DISCOUNTS,EDIT_SKU_DISCOUNTS);status!=DELETED&limit=1'\n",
    "    headers = {\n",
    "        'Authorization': 'bearer {}'.format(token),\n",
    "        'content-type':'application/json'\n",
    "       }\n",
    "    response = requests.request(\"GET\", url, headers=headers)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eff346a4-8a8f-4338-8273-915150d1a7bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def sku_discount_upload_func(file_name):\n",
    "#     pre_data = preassigned_url().json()\n",
    "#     key = pre_data['key']\n",
    "#     new_url = pre_data['preSignedUrl']\n",
    "#     upload_sku_discount(file_name,new_url)\n",
    "#     validation_data = validate_skus_discount(key)\n",
    "#     #print('validate: ',validation_data)\n",
    "#     proceed_data = proceed(key)\n",
    "#     # print('proceed:',proceed_data)\n",
    "#     try:\n",
    "#         if proceed_data.ok and validation_data.ok:\n",
    "#             print('Passed')\n",
    "#         else:\n",
    "#             print('Failed')\n",
    "#     except:\n",
    "#         print(\"error\")\n",
    "# files = [f for f in os.listdir('HH_Sheets') if os.path.isfile(os.path.join('HH_Sheets', f))]\n",
    "# for file in files:\n",
    "#     print(file)\n",
    "#     sku_discount_upload_func('HH_Sheets/'+file)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b8fdfafa-13eb-4d12-a3ac-c6359ce6526a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def update_delivery_fees(token, delivery_fees_data):\n",
    "\n",
    "#     url = 'https://api.maxab.info/commerce/api/admins/v1/delivery-fees'\n",
    "    \n",
    "#     headers = {\n",
    "#         'Authorization': f'Bearer {token}',\n",
    "#         'Content-Type': 'application/json'\n",
    "#     }\n",
    "    \n",
    "#     response = requests.post(url, headers=headers, json=delivery_fees_data)\n",
    "    \n",
    "#     return response\n",
    "\n",
    "\n",
    "# # Usage\n",
    "# token = get_access_token('https://sso.maxab.info/auth/realms/maxab/protocol/openid-connect/token',\n",
    "#                              'main-system-externals',\n",
    "#                              secret)\n",
    "\n",
    "# data = [\n",
    "#      {\n",
    "#     \"dynamic_tag_id\":3154 ,\n",
    "#     \"delivery_fees\": 349,\n",
    "#     \"ticket_size\": 1000000\n",
    "        \n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# response = update_delivery_fees(token, data)\n",
    "# print(f\"Status Code: {response.status_code}\")\n",
    "# print(f\"Response: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c6b4dc3d-7184-433d-ba5d-b9c377c7b766",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def delete_delivery_fees(token,data):\n",
    "\n",
    "#     url = 'https://api.maxab.info/commerce/api/admins/v1/delivery-fees'\n",
    "    \n",
    "#     headers = {\n",
    "#         'Authorization': f'Bearer {token}',\n",
    "#         'Content-Type': 'application/json'\n",
    "#     }\n",
    "    \n",
    "#     response = requests.delete(url, headers=headers, json=data)\n",
    "    \n",
    "#     return response\n",
    "# data = {\n",
    "#   \"deliveryFeesIds\": [268,269,273,272,270,271]\n",
    "# }\n",
    "# token = get_access_token('https://sso.maxab.info/auth/realms/maxab/protocol/openid-connect/token',\n",
    "#                              'main-system-externals',\n",
    "#                              secret)\n",
    "# response = delete_delivery_fees(token,data)\n",
    "# print(f\"Status Code: {response.status_code}\")\n",
    "# print(f\"Response: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "21097779-7ed5-42ef-9e47-84caa5750f98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "SKU DISCOUNT BATCH UPLOAD\n",
      "######################################################################\n",
      " Directory: HH_Sheets\n",
      " Started: 2025-12-10 10:16:10\n",
      " Found 21 file(s) to process\n",
      "\n",
      "\n",
      "\n",
      "Processing 1/21: o_happy_hour_2025-12-10_NO._3.xlsx\n",
      "\n",
      "\n",
      "======================================================================\n",
      " Processing file: o_happy_hour_2025-12-10_NO._3.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4]  Getting pre-signed upload URL...\n",
      "       Key: 2025-12-10-12-16-10-user-2642.xlsx\n",
      "\n",
      "[2/4]  Uploading file to S3...\n",
      "       Upload successful (Status: 200)\n",
      "\n",
      "[3/4]  Validating file data...\n",
      "       Validation passed (Status: 200)\n",
      "\n",
      "[4/4]   Processing discounts...\n",
      "       Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      " SUCCESS: o_happy_hour_2025-12-10_NO._3.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Processing 2/21: o_happy_hour_2025-12-10_NO._16.xlsx\n",
      "\n",
      "\n",
      "======================================================================\n",
      " Processing file: o_happy_hour_2025-12-10_NO._16.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4]  Getting pre-signed upload URL...\n",
      "       Key: 2025-12-10-12-16-13-user-2642.xlsx\n",
      "\n",
      "[2/4]  Uploading file to S3...\n",
      "       Upload successful (Status: 200)\n",
      "\n",
      "[3/4]  Validating file data...\n",
      "       Validation passed (Status: 200)\n",
      "\n",
      "[4/4]   Processing discounts...\n",
      "       Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      " SUCCESS: o_happy_hour_2025-12-10_NO._16.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Processing 3/21: o_happy_hour_2025-12-10_NO._6.xlsx\n",
      "\n",
      "\n",
      "======================================================================\n",
      " Processing file: o_happy_hour_2025-12-10_NO._6.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4]  Getting pre-signed upload URL...\n",
      "       Key: 2025-12-10-12-16-15-user-2642.xlsx\n",
      "\n",
      "[2/4]  Uploading file to S3...\n",
      "       Upload successful (Status: 200)\n",
      "\n",
      "[3/4]  Validating file data...\n",
      "       Validation passed (Status: 200)\n",
      "\n",
      "[4/4]   Processing discounts...\n",
      "       Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      " SUCCESS: o_happy_hour_2025-12-10_NO._6.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Processing 4/21: o_happy_hour_2025-12-10_NO._11.xlsx\n",
      "\n",
      "\n",
      "======================================================================\n",
      " Processing file: o_happy_hour_2025-12-10_NO._11.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4]  Getting pre-signed upload URL...\n",
      "       Key: 2025-12-10-12-16-17-user-2642.xlsx\n",
      "\n",
      "[2/4]  Uploading file to S3...\n",
      "       Upload successful (Status: 200)\n",
      "\n",
      "[3/4]  Validating file data...\n",
      "       Validation passed (Status: 200)\n",
      "\n",
      "[4/4]   Processing discounts...\n",
      "       Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      " SUCCESS: o_happy_hour_2025-12-10_NO._11.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Processing 5/21: o_happy_hour_2025-12-10_NO._20.xlsx\n",
      "\n",
      "\n",
      "======================================================================\n",
      " Processing file: o_happy_hour_2025-12-10_NO._20.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4]  Getting pre-signed upload URL...\n",
      "       Key: 2025-12-10-12-16-19-user-2642.xlsx\n",
      "\n",
      "[2/4]  Uploading file to S3...\n",
      "       Upload successful (Status: 200)\n",
      "\n",
      "[3/4]  Validating file data...\n",
      "       Validation passed (Status: 200)\n",
      "\n",
      "[4/4]   Processing discounts...\n",
      "       Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      " SUCCESS: o_happy_hour_2025-12-10_NO._20.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Processing 6/21: o_happy_hour_2025-12-10_NO._8.xlsx\n",
      "\n",
      "\n",
      "======================================================================\n",
      " Processing file: o_happy_hour_2025-12-10_NO._8.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4]  Getting pre-signed upload URL...\n",
      "       Key: 2025-12-10-12-16-22-user-2642.xlsx\n",
      "\n",
      "[2/4]  Uploading file to S3...\n",
      "       Upload successful (Status: 200)\n",
      "\n",
      "[3/4]  Validating file data...\n",
      "       Validation passed (Status: 200)\n",
      "\n",
      "[4/4]   Processing discounts...\n",
      "       Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      " SUCCESS: o_happy_hour_2025-12-10_NO._8.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Processing 7/21: o_happy_hour_2025-12-10_NO._5.xlsx\n",
      "\n",
      "\n",
      "======================================================================\n",
      " Processing file: o_happy_hour_2025-12-10_NO._5.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4]  Getting pre-signed upload URL...\n",
      "       Key: 2025-12-10-12-16-23-user-2642.xlsx\n",
      "\n",
      "[2/4]  Uploading file to S3...\n",
      "       Upload successful (Status: 200)\n",
      "\n",
      "[3/4]  Validating file data...\n",
      "       Validation passed (Status: 200)\n",
      "\n",
      "[4/4]   Processing discounts...\n",
      "       Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      " SUCCESS: o_happy_hour_2025-12-10_NO._5.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Processing 8/21: o_happy_hour_2025-12-10_NO._14.xlsx\n",
      "\n",
      "\n",
      "======================================================================\n",
      " Processing file: o_happy_hour_2025-12-10_NO._14.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4]  Getting pre-signed upload URL...\n",
      "       Key: 2025-12-10-12-16-27-user-2642.xlsx\n",
      "\n",
      "[2/4]  Uploading file to S3...\n",
      "       Upload successful (Status: 200)\n",
      "\n",
      "[3/4]  Validating file data...\n",
      "       Validation passed (Status: 200)\n",
      "\n",
      "[4/4]   Processing discounts...\n",
      "       Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      " SUCCESS: o_happy_hour_2025-12-10_NO._14.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Processing 9/21: o_happy_hour_2025-12-10_NO._12.xlsx\n",
      "\n",
      "\n",
      "======================================================================\n",
      " Processing file: o_happy_hour_2025-12-10_NO._12.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4]  Getting pre-signed upload URL...\n",
      "       Key: 2025-12-10-12-16-31-user-2642.xlsx\n",
      "\n",
      "[2/4]  Uploading file to S3...\n",
      "       Upload successful (Status: 200)\n",
      "\n",
      "[3/4]  Validating file data...\n",
      "       Validation passed (Status: 200)\n",
      "\n",
      "[4/4]   Processing discounts...\n",
      "       Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      " SUCCESS: o_happy_hour_2025-12-10_NO._12.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Processing 10/21: o_happy_hour_2025-12-10_NO._13.xlsx\n",
      "\n",
      "\n",
      "======================================================================\n",
      " Processing file: o_happy_hour_2025-12-10_NO._13.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4]  Getting pre-signed upload URL...\n",
      "       Key: 2025-12-10-12-16-33-user-2642.xlsx\n",
      "\n",
      "[2/4]  Uploading file to S3...\n",
      "       Upload successful (Status: 200)\n",
      "\n",
      "[3/4]  Validating file data...\n",
      "       Validation passed (Status: 200)\n",
      "\n",
      "[4/4]   Processing discounts...\n",
      "       Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      " SUCCESS: o_happy_hour_2025-12-10_NO._13.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Processing 11/21: o_happy_hour_2025-12-10_NO._4.xlsx\n",
      "\n",
      "\n",
      "======================================================================\n",
      " Processing file: o_happy_hour_2025-12-10_NO._4.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4]  Getting pre-signed upload URL...\n",
      "       Key: 2025-12-10-12-16-37-user-2642.xlsx\n",
      "\n",
      "[2/4]  Uploading file to S3...\n",
      "       Upload successful (Status: 200)\n",
      "\n",
      "[3/4]  Validating file data...\n",
      "       Validation passed (Status: 200)\n",
      "\n",
      "[4/4]   Processing discounts...\n",
      "       Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      " SUCCESS: o_happy_hour_2025-12-10_NO._4.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Processing 12/21: o_happy_hour_2025-12-10_NO._0.xlsx\n",
      "\n",
      "\n",
      "======================================================================\n",
      " Processing file: o_happy_hour_2025-12-10_NO._0.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4]  Getting pre-signed upload URL...\n",
      "       Key: 2025-12-10-12-16-42-user-2642.xlsx\n",
      "\n",
      "[2/4]  Uploading file to S3...\n",
      "       Upload successful (Status: 200)\n",
      "\n",
      "[3/4]  Validating file data...\n",
      "       Validation passed (Status: 200)\n",
      "\n",
      "[4/4]   Processing discounts...\n",
      "       Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      " SUCCESS: o_happy_hour_2025-12-10_NO._0.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Processing 13/21: o_happy_hour_2025-12-10_NO._19.xlsx\n",
      "\n",
      "\n",
      "======================================================================\n",
      " Processing file: o_happy_hour_2025-12-10_NO._19.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4]  Getting pre-signed upload URL...\n",
      "       Key: 2025-12-10-12-16-45-user-2642.xlsx\n",
      "\n",
      "[2/4]  Uploading file to S3...\n",
      "       Upload successful (Status: 200)\n",
      "\n",
      "[3/4]  Validating file data...\n",
      "       Validation passed (Status: 200)\n",
      "\n",
      "[4/4]   Processing discounts...\n",
      "       Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      " SUCCESS: o_happy_hour_2025-12-10_NO._19.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Processing 14/21: o_happy_hour_2025-12-10_NO._1.xlsx\n",
      "\n",
      "\n",
      "======================================================================\n",
      " Processing file: o_happy_hour_2025-12-10_NO._1.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4]  Getting pre-signed upload URL...\n",
      "       Key: 2025-12-10-12-16-47-user-2642.xlsx\n",
      "\n",
      "[2/4]  Uploading file to S3...\n",
      "       Upload successful (Status: 200)\n",
      "\n",
      "[3/4]  Validating file data...\n",
      "       Validation passed (Status: 200)\n",
      "\n",
      "[4/4]   Processing discounts...\n",
      "       Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      " SUCCESS: o_happy_hour_2025-12-10_NO._1.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Processing 15/21: o_happy_hour_2025-12-10_NO._9.xlsx\n",
      "\n",
      "\n",
      "======================================================================\n",
      " Processing file: o_happy_hour_2025-12-10_NO._9.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4]  Getting pre-signed upload URL...\n",
      "       Key: 2025-12-10-12-16-48-user-2642.xlsx\n",
      "\n",
      "[2/4]  Uploading file to S3...\n",
      "       Upload successful (Status: 200)\n",
      "\n",
      "[3/4]  Validating file data...\n",
      "       Validation passed (Status: 200)\n",
      "\n",
      "[4/4]   Processing discounts...\n",
      "       Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      " SUCCESS: o_happy_hour_2025-12-10_NO._9.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Processing 16/21: o_happy_hour_2025-12-10_NO._17.xlsx\n",
      "\n",
      "\n",
      "======================================================================\n",
      " Processing file: o_happy_hour_2025-12-10_NO._17.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4]  Getting pre-signed upload URL...\n",
      "       Key: 2025-12-10-12-16-51-user-2642.xlsx\n",
      "\n",
      "[2/4]  Uploading file to S3...\n",
      "       Upload successful (Status: 200)\n",
      "\n",
      "[3/4]  Validating file data...\n",
      "       Validation passed (Status: 200)\n",
      "\n",
      "[4/4]   Processing discounts...\n",
      "       Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      " SUCCESS: o_happy_hour_2025-12-10_NO._17.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Processing 17/21: o_happy_hour_2025-12-10_NO._2.xlsx\n",
      "\n",
      "\n",
      "======================================================================\n",
      " Processing file: o_happy_hour_2025-12-10_NO._2.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4]  Getting pre-signed upload URL...\n",
      "       Key: 2025-12-10-12-16-53-user-2642.xlsx\n",
      "\n",
      "[2/4]  Uploading file to S3...\n",
      "       Upload successful (Status: 200)\n",
      "\n",
      "[3/4]  Validating file data...\n",
      "       Validation passed (Status: 200)\n",
      "\n",
      "[4/4]   Processing discounts...\n",
      "       Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      " SUCCESS: o_happy_hour_2025-12-10_NO._2.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Processing 18/21: o_happy_hour_2025-12-10_NO._7.xlsx\n",
      "\n",
      "\n",
      "======================================================================\n",
      " Processing file: o_happy_hour_2025-12-10_NO._7.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4]  Getting pre-signed upload URL...\n",
      "       Key: 2025-12-10-12-16-55-user-2642.xlsx\n",
      "\n",
      "[2/4]  Uploading file to S3...\n",
      "       Upload successful (Status: 200)\n",
      "\n",
      "[3/4]  Validating file data...\n",
      "       Validation passed (Status: 200)\n",
      "\n",
      "[4/4]   Processing discounts...\n",
      "       Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      " SUCCESS: o_happy_hour_2025-12-10_NO._7.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Processing 19/21: o_happy_hour_2025-12-10_NO._18.xlsx\n",
      "\n",
      "\n",
      "======================================================================\n",
      " Processing file: o_happy_hour_2025-12-10_NO._18.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4]  Getting pre-signed upload URL...\n",
      "       Key: 2025-12-10-12-16-57-user-2642.xlsx\n",
      "\n",
      "[2/4]  Uploading file to S3...\n",
      "       Upload successful (Status: 200)\n",
      "\n",
      "[3/4]  Validating file data...\n",
      "       Validation passed (Status: 200)\n",
      "\n",
      "[4/4]   Processing discounts...\n",
      "       Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      " SUCCESS: o_happy_hour_2025-12-10_NO._18.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Processing 20/21: o_happy_hour_2025-12-10_NO._15.xlsx\n",
      "\n",
      "\n",
      "======================================================================\n",
      " Processing file: o_happy_hour_2025-12-10_NO._15.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4]  Getting pre-signed upload URL...\n",
      "       Key: 2025-12-10-12-16-59-user-2642.xlsx\n",
      "\n",
      "[2/4]  Uploading file to S3...\n",
      "       Upload successful (Status: 200)\n",
      "\n",
      "[3/4]  Validating file data...\n",
      "       Validation passed (Status: 200)\n",
      "\n",
      "[4/4]   Processing discounts...\n",
      "       Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      " SUCCESS: o_happy_hour_2025-12-10_NO._15.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Processing 21/21: o_happy_hour_2025-12-10_NO._10.xlsx\n",
      "\n",
      "\n",
      "======================================================================\n",
      " Processing file: o_happy_hour_2025-12-10_NO._10.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4]  Getting pre-signed upload URL...\n",
      "       Key: 2025-12-10-12-17-02-user-2642.xlsx\n",
      "\n",
      "[2/4]  Uploading file to S3...\n",
      "       Upload successful (Status: 200)\n",
      "\n",
      "[3/4]  Validating file data...\n",
      "       Validation passed (Status: 200)\n",
      "\n",
      "[4/4]   Processing discounts...\n",
      "       Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      " SUCCESS: o_happy_hour_2025-12-10_NO._10.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "\n",
      "######################################################################\n",
      "BATCH PROCESSING SUMMARY\n",
      "######################################################################\n",
      " Total files: 21\n",
      " Successful: 21\n",
      " Failed: 0\n",
      " Completed: 2025-12-10 10:17:04\n",
      "######################################################################\n",
      "\n",
      "\n",
      " All files processed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def sku_discount_upload_func(file_name):\n",
    "    \"\"\"\n",
    "    Upload SKU discount file and process through validation pipeline\n",
    "    \n",
    "    Args:\n",
    "        file_name: Path to the Excel file to upload\n",
    "    \n",
    "    Returns:\n",
    "        dict: Summary of upload and validation status\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\" Processing file: {os.path.basename(file_name)}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    results = {\n",
    "        'file': file_name,\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'steps': {}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Get pre-signed URL\n",
    "        print(\"\\n[1/4]  Getting pre-signed upload URL...\")\n",
    "        pre_data = preassigned_url().json()\n",
    "        key = pre_data['key']\n",
    "        new_url = pre_data['preSignedUrl']\n",
    "        print(f\"       Key: {key}\")\n",
    "        results['steps']['presigned_url'] = 'Success'\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"       Failed to get pre-signed URL: {e}\")\n",
    "        results['steps']['presigned_url'] = f'Failed: {e}'\n",
    "        return results\n",
    "    \n",
    "    try:\n",
    "        # Step 2: Upload file\n",
    "        print(\"\\n[2/4]  Uploading file to S3...\")\n",
    "        upload_response = upload_sku_discount(file_name, new_url)\n",
    "        \n",
    "        if upload_response.status_code in [200, 201, 204]:\n",
    "            print(f\"       Upload successful (Status: {upload_response.status_code})\")\n",
    "            results['steps']['upload'] = 'Success'\n",
    "        else:\n",
    "            print(f\"       Upload failed (Status: {upload_response.status_code})\")\n",
    "            print(f\"      Error: {upload_response.text}\")\n",
    "            results['steps']['upload'] = f'Failed: {upload_response.status_code}'\n",
    "            return results\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"       Upload error: {e}\")\n",
    "        results['steps']['upload'] = f'Failed: {e}'\n",
    "        return results\n",
    "    \n",
    "    try:\n",
    "        # Step 3: Validate file\n",
    "        print(\"\\n[3/4]  Validating file data...\")\n",
    "        validation_data = validate_skus_discount(key)\n",
    "        \n",
    "        if validation_data.ok:\n",
    "            print(f\"       Validation passed (Status: {validation_data.status_code})\")\n",
    "            results['steps']['validation'] = 'Success'\n",
    "            \n",
    "            # Try to parse validation response\n",
    "            try:\n",
    "                validation_response = validation_data.json()\n",
    "                if validation_response:\n",
    "                    print(f\"      Response: {validation_response}\")\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            print(f\"       Validation failed (Status: {validation_data.status_code})\")\n",
    "            print(f\"      Error: {validation_data.text[:200]}\")\n",
    "            results['steps']['validation'] = f'Failed: {validation_data.status_code}'\n",
    "            return results\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"       Validation error: {e}\")\n",
    "        results['steps']['validation'] = f'Failed: {e}'\n",
    "        return results\n",
    "    \n",
    "    try:\n",
    "        # Step 4: Proceed with processing\n",
    "        print(\"\\n[4/4]   Processing discounts...\")\n",
    "        proceed_data = proceed(key)\n",
    "        \n",
    "        if proceed_data.ok:\n",
    "            print(f\"       Processing completed (Status: {proceed_data.status_code})\")\n",
    "            results['steps']['proceed'] = 'Success'\n",
    "            results['status'] = 'SUCCESS'\n",
    "            \n",
    "            # Try to parse proceed response\n",
    "            try:\n",
    "                proceed_response = proceed_data.json()\n",
    "                if proceed_response:\n",
    "                    print(f\"      Response: {proceed_response}\")\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            print(f\"       Processing failed (Status: {proceed_data.status_code})\")\n",
    "            print(f\"      Error: {proceed_data.text[:200]}\")\n",
    "            results['steps']['proceed'] = f'Failed: {proceed_data.status_code}'\n",
    "            results['status'] = 'FAILED'\n",
    "            return results\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"       Processing error: {e}\")\n",
    "        results['steps']['proceed'] = f'Failed: {e}'\n",
    "        results['status'] = 'FAILED'\n",
    "        return results\n",
    "    \n",
    "    # Final status\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    if results.get('status') == 'SUCCESS':\n",
    "        print(f\" SUCCESS: {os.path.basename(file_name)} processed successfully!\")\n",
    "    else:\n",
    "        print(f\" FAILED: {os.path.basename(file_name)} processing failed\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Main execution with summary\n",
    "def process_all_discount_files(directory='HH_Sheets'):\n",
    "    \"\"\"\n",
    "    Process all discount files in the specified directory\n",
    "    \n",
    "    Args:\n",
    "        directory: Directory containing Excel files to process\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'#'*70}\")\n",
    "    print(f\"SKU DISCOUNT BATCH UPLOAD\")\n",
    "    print(f\"{'#'*70}\")\n",
    "    print(f\" Directory: {directory}\")\n",
    "    print(f\" Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # Get all files\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"\\n ERROR: Directory '{directory}' does not exist\")\n",
    "        return\n",
    "    \n",
    "    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "    excel_files = [f for f in files if f.endswith(('.xlsx', '.xls'))]\n",
    "    \n",
    "    if not excel_files:\n",
    "        print(f\"\\n  WARNING: No Excel files found in '{directory}'\")\n",
    "        return\n",
    "    \n",
    "    print(f\" Found {len(excel_files)} file(s) to process\\n\")\n",
    "    \n",
    "    # Process each file\n",
    "    all_results = []\n",
    "    success_count = 0\n",
    "    failed_count = 0\n",
    "    \n",
    "    for idx, file in enumerate(excel_files, 1):\n",
    "        file_path = os.path.join(directory, file)\n",
    "        print(f\"\\n{''*70}\")\n",
    "        print(f\"Processing {idx}/{len(excel_files)}: {file}\")\n",
    "        print(f\"{''*70}\")\n",
    "        \n",
    "        result = sku_discount_upload_func(file_path)\n",
    "        all_results.append(result)\n",
    "        \n",
    "        if result.get('status') == 'SUCCESS':\n",
    "            success_count += 1\n",
    "        else:\n",
    "            failed_count += 1\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n\\n{'#'*70}\")\n",
    "    print(f\"BATCH PROCESSING SUMMARY\")\n",
    "    print(f\"{'#'*70}\")\n",
    "    print(f\" Total files: {len(excel_files)}\")\n",
    "    print(f\" Successful: {success_count}\")\n",
    "    print(f\" Failed: {failed_count}\")\n",
    "    print(f\" Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"{'#'*70}\\n\")\n",
    "    \n",
    "    # Detailed results table\n",
    "    if failed_count > 0:\n",
    "        print(\"\\n FAILED FILES DETAILS:\")\n",
    "        print(f\"{''*70}\")\n",
    "        for result in all_results:\n",
    "            if result.get('status') != 'SUCCESS':\n",
    "                print(f\"\\n File: {os.path.basename(result['file'])}\")\n",
    "                for step, status in result['steps'].items():\n",
    "                    if 'Failed' in str(status):\n",
    "                        print(f\"    {step}: {status}\")\n",
    "    \n",
    "    print(\"\\n All files processed!\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "# Usage\n",
    "results = process_all_discount_files('HH_Sheets')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ca013c-1e72-4a5a-a5d0-dd3d0d38343e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51d2c5b-51de-423b-97d8-e2866eb1b27d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
