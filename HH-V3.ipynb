{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84094942",
   "metadata": {},
   "source": [
    "# üéØ Happy Hour Dynamic Pricing Engine (V3)\n",
    "\n",
    "This notebook implements an automated pricing optimization system for the Happy Hour promotion program. It identifies products with declining sales performance and calculates optimal discount prices to boost sales.\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "1. **Environment Setup** - Install dependencies and configure connections\n",
    "2. **Data Collection** - Fetch product, pricing, and sales data from Snowflake\n",
    "3. **Product Selection** - Identify underperforming products eligible for discounts\n",
    "4. **Price Optimization** - Calculate optimal discount prices using multiple data sources\n",
    "5. **Retailer Targeting** - Select retailers most likely to respond to discounts\n",
    "6. **Output Generation** - Create discount sheets for upload to the pricing system\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab523b97",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "### 1.1 Install Required Packages\n",
    "Install all necessary Python packages for database connectivity, data manipulation, and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "371c6017-a418-42a9-9fa7-1eef1ce8cfc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Upgrade pip\n",
    "!pip install --upgrade pip\n",
    "# Connectivity\n",
    "!pip install psycopg2-binary  # PostgreSQL adapter\n",
    "# !pip install snowflake-connector-python  # Snowflake connector\n",
    "!pip install snowflake-connector-python==3.15.0 # Snowflake connector Older Version\n",
    "!pip install snowflake-sqlalchemy  # Snowflake SQLAlchemy connector\n",
    "!pip install warnings # Warnings management\n",
    "# !pip install pyarrow # Serialization\n",
    "!pip install keyring==23.11.0 # Key management\n",
    "!pip install sqlalchemy==1.4.46 # SQLAlchemy\n",
    "!pip install requests # HTTP requests\n",
    "!pip install boto3 # AWS SDK\n",
    "# !pip install slackclient # Slack API\n",
    "!pip install oauth2client # Google Sheets API\n",
    "!pip install gspread==5.9.0 # Google Sheets API\n",
    "!pip install gspread_dataframe # Google Sheets API\n",
    "!pip install google.cloud # Google Cloud\n",
    "# Data manipulation and analysis\n",
    "!pip install polars\n",
    "!pip install pandas==2.2.1\n",
    "!pip install numpy\n",
    "# !pip install fastparquet\n",
    "!pip install openpyxl # Excel file handling\n",
    "!pip install xlsxwriter # Excel file handling\n",
    "# Linear programming\n",
    "!pip install pulp\n",
    "# Date and time handling\n",
    "!pip install --upgrade datetime\n",
    "!pip install python-time\n",
    "!pip install --upgrade pytz\n",
    "# Progress bar\n",
    "!pip install tqdm\n",
    "# Database data types\n",
    "!pip install db-dtypes\n",
    "# Geospatial data handling\n",
    "# !pip install geopandas\n",
    "# !pip install shapely\n",
    "# !pip install fiona\n",
    "# !pip install haversine\n",
    "# Plotting\n",
    "\n",
    "# Modeling\n",
    "!pip install statsmodels\n",
    "!pip install scikit-learn\n",
    "\n",
    "!pip install import-ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9fad94",
   "metadata": {},
   "source": [
    "### 1.2 Import Libraries and Initialize Environment\n",
    "Import required libraries and set up connections to Snowflake, Google Sheets, and AWS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "007e8be5-f084-46b8-9ab9-e6dcec8662a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.Renviron\n",
      "/home/ec2-user/service_account_key.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "import json\n",
    "from datetime import date, timedelta\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import setup_environment_2\n",
    "import importlib\n",
    "import import_ipynb\n",
    "import warnings\n",
    "import boto3\n",
    "import requests\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "importlib.reload(setup_environment_2)\n",
    "setup_environment_2.initialize_env()\n",
    "import os\n",
    "import time\n",
    "import pytz\n",
    "import gspread\n",
    "import snowflake.connector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261581cd",
   "metadata": {},
   "source": [
    "### 1.3 Database Helper Functions\n",
    "Define reusable functions for querying Snowflake database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95799e78-611c-4a58-873e-de86b15e2cc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def snowflake_query(country, query, warehouse=None, columns=[], conn=None):\n",
    "    \"\"\"\n",
    "    Execute a query against Snowflake and return results as DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        country: Country identifier (e.g., \"Egypt\")\n",
    "        query: SQL query string to execute\n",
    "        warehouse: Snowflake warehouse (optional)\n",
    "        columns: Custom column names (optional)\n",
    "        conn: Existing connection (optional)\n",
    "        \n",
    "    Returns:\n",
    "        pandas DataFrame with query results\n",
    "    \"\"\"\n",
    "    con = snowflake.connector.connect(\n",
    "        user     = os.environ[\"SNOWFLAKE_USERNAME\"],\n",
    "        account  = os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "        password = os.environ[\"SNOWFLAKE_PASSWORD\"],\n",
    "        database = os.environ[\"SNOWFLAKE_DATABASE\"]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        cur = con.cursor()\n",
    "        cur.execute(\"USE WAREHOUSE COMPUTE_WH\")\n",
    "        cur.execute(query)\n",
    "        \n",
    "        column_names = [col[0] for col in cur.description]\n",
    "        results = cur.fetchall()\n",
    "        \n",
    "        if not results:\n",
    "            out = pd.DataFrame(columns=[name.lower() for name in column_names])\n",
    "        else:\n",
    "            if len(columns) == 0:\n",
    "                out = pd.DataFrame(np.array(results), columns=column_names)\n",
    "                out.columns = out.columns.str.lower()\n",
    "            else:\n",
    "                out = pd.DataFrame(np.array(results), columns=columns)\n",
    "                out.columns = out.columns.str.lower()\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Query error: {e}\")\n",
    "        raise\n",
    "        \n",
    "    finally:\n",
    "        cur.close()\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "651794d1-bf49-40bb-adee-8f7880b5163a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warehouse Mapping:\n"
     ]
    }
   ],
   "source": [
    "def get_warehouse_mapping():\n",
    "    \"\"\"Define warehouse to region/cohort mapping.\"\"\"\n",
    "    whs_data = [\n",
    "        ('Cairo', 'Mostorod', 1, 700),\n",
    "        ('Giza', 'Barageel', 236, 701),\n",
    "        ('Delta West', 'El-Mahala', 337, 703),\n",
    "        ('Delta West', 'Tanta', 8, 703),\n",
    "        ('Delta East', 'Mansoura FC', 339, 704),\n",
    "        ('Delta East', 'Sharqya', 170, 704),\n",
    "        ('Upper Egypt', 'Assiut FC', 501, 1124),\n",
    "        ('Upper Egypt', 'Bani sweif', 401, 1126),\n",
    "        ('Upper Egypt', 'Menya Samalot', 703, 1123),\n",
    "        ('Upper Egypt', 'Sohag', 632, 1125),\n",
    "        ('Alexandria', 'Khorshed Alex', 797, 702),\n",
    "        ('Giza', 'Sakkarah', 962, 701)\n",
    "    ]\n",
    "    \n",
    "    df_whs = pd.DataFrame(whs_data, columns=['region', 'warehouse', 'warehouse_id', 'cohort_id'])\n",
    "    return df_whs\n",
    "\n",
    "# Get warehouse mapping\n",
    "df_whs = get_warehouse_mapping()\n",
    "print(\"Warehouse Mapping:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56776c0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Data Collection\n",
    "\n",
    "This section fetches all required data from Snowflake and Google Sheets for the pricing analysis.\n",
    "\n",
    "### 2.1 Configuration & Reference Data\n",
    "Get timezone settings and load brand inclusion lists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4995eba-902b-42d6-9f9b-59aab8007f1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Using timezone: America/Los_Angeles\n"
     ]
    }
   ],
   "source": [
    "# Get Snowflake timezone for consistent date/time handling\n",
    "query = \"SHOW PARAMETERS LIKE 'TIMEZONE'\"\n",
    "timezone_result = snowflake_query(\"Egypt\", query)\n",
    "zone_to_use = timezone_result['value'].values[0]\n",
    "print(f\"‚úì Using timezone: {zone_to_use}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "221cfa1c-8664-4ac5-a695-fd932b050608",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scope = [\"https://spreadsheets.google.com/feeds\",\n",
    "         'https://www.googleapis.com/auth/spreadsheets',\n",
    "         \"https://www.googleapis.com/auth/drive.file\",\n",
    "         \"https://www.googleapis.com/auth/drive\"]\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_dict(json.loads(setup_environment_2.get_secret(\"prod/maxab-sheets\")), scope)\n",
    "client = gspread.authorize(creds)\n",
    "included_brand = client.open('QD_brands').worksheet('Happy Hour push brands')\n",
    "included_brand_df = pd.DataFrame(included_brand.get_all_records())\n",
    "for col in included_brand_df.columns:\n",
    "    included_brand_df[col] = pd.to_numeric(included_brand_df[col], errors='ignore')\n",
    "try:    \n",
    "    b_list = [brand  for brand in included_brand_df['brand']]\n",
    "except:\n",
    "    b_list= [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a82b0b8c-a4ad-4271-9016-0322ad5baa47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>sku</th>\n",
       "      <th>warehouse</th>\n",
       "      <th>warehouse_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [product_id, sku, warehouse, warehouse_id]\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sku_to_add = client.open('QD_brands').worksheet('HH SKU PUSH')\n",
    "try:\n",
    "    sku_to_add_df = pd.DataFrame(sku_to_add.get_all_records())\n",
    "    sku_to_add_df = sku_to_add_df.merge(df_whs[['warehouse','warehouse_id']],on=['warehouse'])\n",
    "except: \n",
    "    sku_to_add_df = pd.DataFrame(columns=['product_id', 'sku', 'warehouse', 'warehouse_id'])\n",
    "sku_to_add_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f35f29-f58b-401a-a201-c3aa6fdc12c4",
   "metadata": {},
   "source": [
    "### 2.2 Product Performance Data\n",
    "\n",
    "Fetch comprehensive product sales and stock data to identify underperforming products:\n",
    "- **Sales metrics**: All-day NMV, until-the-hour NMV, last hour NMV\n",
    "- **Stock metrics**: Available stock, days on hand (DOH), running rates\n",
    "- **Growth metrics**: Compare current vs historical performance using weighted distance scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25c3b4fe-f7f8-4e8c-8ecb-87cd3536059a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "command_string = f'''\n",
    "with last_update as (\n",
    "    select DATE_PART('hour', max_date) * 60 + DATE_PART('minute', max_date) AS total_minutes\n",
    "    from (\n",
    "        select max(created_at) as max_date from sales_orders\n",
    "    )\n",
    "),\n",
    "\n",
    "predicted_rr as (\n",
    "    select product_id, warehouse_id, rr, date\n",
    "    from Finance.PREDICTED_RUNNING_RATES\n",
    "    where date >= CURRENT_DATE\n",
    "    qualify date = max(date) over(partition by product_id, warehouse_id)\n",
    "),\n",
    "\n",
    "days_stocks as (\n",
    "    select timestamp::date as date, product_id, warehouse_id,\n",
    "        avg(in_stock) as in_stock_perc,\n",
    "        avg(case when date_part('hour', timestamp) = date_part('hour', current_timestamp) - 1 then in_stock end) as last_hour_stocks\n",
    "    from (\n",
    "        select timestamp, product_id, warehouse_id, case when AVAILABLE_STOCK > 0 then 1 else 0 end as in_stock\n",
    "        from materialized_views.STOCK_SNAP_SHOTS_RECENT sss\n",
    "        where sss.timestamp::date >= date_trunc('month', current_date - 90)\n",
    "            and date_part('hour', sss.timestamp) < date_part('hour', CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMEstamp()))\n",
    "            and warehouse_id in (1, 8, 170, 236, 337, 339, 401, 501, 632, 703, 797, 962)\n",
    "    )\n",
    "    group by all\n",
    "),\n",
    "\n",
    "base as (\n",
    "    select *, row_number() over(partition by retailer_id order by priority) as rnk\n",
    "    from (\n",
    "        select x.*, TAGGABLE_ID as retailer_id\n",
    "        from (\n",
    "            select id as cohort_id, name as cohort_name, priority, dynamic_tag_id\n",
    "            from cohorts\n",
    "            where is_active = 'true'\n",
    "                and id in (700, 701, 702, 703, 704, 1123, 1124, 1125, 1126)\n",
    "        ) x\n",
    "        join DYNAMIC_TAGgables dt on x.dynamic_tag_id = dt.dynamic_tag_id and dt.dynamic_tag_id <> 3038\n",
    "    )\n",
    "    qualify rnk = 1\n",
    "),\n",
    "\n",
    "sales_data as (\n",
    "    SELECT DISTINCT\n",
    "        so.created_at::date as date,\n",
    "        pso.warehouse_id as warehouse_id,\n",
    "        districts.id as district_id,\n",
    "        districts.name_ar as district_name,\n",
    "        pso.product_id,\n",
    "        CONCAT(products.name_ar, ' ', products.size, ' ', product_units.name_ar) as sku,\n",
    "        brands.name_ar as brand,\n",
    "        categories.name_ar as cat,\n",
    "        sum(pso.total_price) as all_day_nmv,\n",
    "        sum(case when (date_part('hour', so.created_at) * 60 + DATE_PART('minute', so.created_at)) < (select * from last_update) then pso.total_price end) as uth_nmv,\n",
    "        sum(case when (date_part('hour', so.created_at) * 60 + DATE_PART('minute', so.created_at))\n",
    "            between (select * from last_update) - 60\n",
    "            and (select * from last_update)\n",
    "            then pso.total_price end) as last_hour_nmv\n",
    "\n",
    "    FROM product_sales_order pso\n",
    "    JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "    JOIN products on products.id = pso.product_id\n",
    "    JOIN brands on products.brand_id = brands.id\n",
    "    JOIN categories ON products.category_id = categories.id\n",
    "    JOIN finance.all_cogs f ON f.product_id = pso.product_id\n",
    "        AND f.from_date::date <= so.created_at::date\n",
    "        AND f.to_date::date > so.created_at::date\n",
    "    JOIN product_units ON product_units.id = products.unit_id\n",
    "    JOIN materialized_views.retailer_polygon on materialized_views.retailer_polygon.retailer_id = so.retailer_id\n",
    "    JOIN districts on districts.id = materialized_views.retailer_polygon.district_id\n",
    "    JOIN cities on cities.id = districts.city_id\n",
    "    join states on states.id = cities.state_id\n",
    "    join regions on regions.id = states.region_id\n",
    "\n",
    "    WHERE True\n",
    "        AND so.created_at::date >= date_trunc('month', current_date - 90)\n",
    "        AND so.sales_order_status_id not in (7, 12)\n",
    "        AND so.channel IN ('telesales', 'retailer')\n",
    "        AND pso.purchased_item_count <> 0\n",
    "        and products.id <> 7630\n",
    "\n",
    "    GROUP BY ALL\n",
    "    order by date desc\n",
    "),\n",
    "\n",
    "data as (\n",
    "    select *, 1 / nullif((0.3 * week_distance + 0.1 * month_distance + 0.6 * day_distance), 0) as distance\n",
    "    from (\n",
    "        select *,\n",
    "            floor((DATE_PART('day', date) - 1) / 7 + 1) AS week_of_month,\n",
    "            DATE_PART('month', date) as month,\n",
    "            DATE_PART('DOW', date) AS day_number,\n",
    "            abs(floor((DATE_PART('day', current_date) - 1) / 7 + 1) - week_of_month) as week_distance,\n",
    "            abs(DATE_PART('month', current_date) - month) as month_distance,\n",
    "            abs(DATE_PART('DOW', current_date) - day_number) as day_distance\n",
    "        from (\n",
    "            select *,\n",
    "                max(case when date = CURRENT_DATE then last_hour_stocks end) over(partition by product_id, warehouse_id) as current_stocks\n",
    "            from (\n",
    "                select ds.date, ds.product_id, ds.warehouse_id, ds.in_stock_perc, ds.last_hour_stocks,\n",
    "                    sd.district_id, sd.district_name,\n",
    "                    sd.all_day_nmv, sd.uth_nmv, sd.last_hour_nmv\n",
    "                from days_stocks ds\n",
    "                left join sales_data sd on ds.product_id = sd.product_id\n",
    "                    and ds.warehouse_id = sd.warehouse_id\n",
    "                    and ds.date = sd.date\n",
    "            )\n",
    "        )\n",
    "        where current_stocks <> 0\n",
    "            and (in_stock_perc = 1 or date = CURRENT_DATE)\n",
    "    )\n",
    "),\n",
    "\n",
    "current_state as (\n",
    "    select product_id, warehouse_id, AVAILABLE_STOCK, activation\n",
    "    from PRODUCT_WAREHOUSE\n",
    "    where IS_BASIC_UNIT = 1\n",
    "        and case when product_id = 1309 then packing_unit_id <> 23 else true end\n",
    ")\n",
    "\n",
    "select x.*,\n",
    "    cs.AVAILABLE_STOCK,\n",
    "    cs.activation,\n",
    "    coalesce(prr.rr, 0) as rr,\n",
    "    case when coalesce(prr.rr, 0) <> 0 then cs.AVAILABLE_STOCK / coalesce(prr.rr, 0) else cs.AVAILABLE_STOCK end as doh,\n",
    "    cs.AVAILABLE_STOCK * f.wac1 as stock_value\n",
    "from (\n",
    "    select product_id, warehouse_id, district_id, district_name,\n",
    "        coalesce(max(case when state = 'prev' then all_day_nmv end), 0) as prev_all_day,\n",
    "        coalesce(max(case when state = 'prev' then uth_nmv end), 0) as prev_uth,\n",
    "        coalesce(max(case when state = 'prev' then last_hour_nmv end), 0) as prev_last_hour,\n",
    "\n",
    "        coalesce(max(case when state = 'current' then all_day_nmv end), 0) as current_all_day,\n",
    "        coalesce(max(case when state = 'current' then uth_nmv end), 0) as current_uth,\n",
    "        coalesce(max(case when state = 'current' then last_hour_nmv end), 0) as current_last_hour\n",
    "\n",
    "    from (\n",
    "        select 'current' as state, product_id, warehouse_id, district_id, district_name, all_day_nmv, uth_nmv, last_hour_nmv\n",
    "        from data\n",
    "        where date = CURRENT_DATE\n",
    "        \n",
    "        union all\n",
    "        \n",
    "        (\n",
    "            select state, product_id, warehouse_id, district_id, district_name,\n",
    "                sum(all_day_nmv * distance) / sum(distance) as all_day_nmv,\n",
    "                sum(uth_nmv * distance) / sum(distance) as uth_nmv,\n",
    "                sum(last_hour_nmv * distance) / sum(distance) as last_hour_nmv\n",
    "            from (\n",
    "                select 'prev' as state, product_id, warehouse_id, district_id, district_name, all_day_nmv, uth_nmv, last_hour_nmv, distance\n",
    "                from data\n",
    "                where date <> CURRENT_DATE\n",
    "            )\n",
    "            group by all\n",
    "        )\n",
    "    )\n",
    "    group by all\n",
    ") x\n",
    "join current_state cs on x.product_id = cs.product_id and x.warehouse_id = cs.warehouse_id\n",
    "left join predicted_rr prr on x.product_id = prr.product_id and x.warehouse_id = prr.warehouse_id\n",
    "join products p on p.id = x.product_id\n",
    "join finance.all_cogs f on f.product_id = x.product_id and CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMEstamp()) between f.from_date and f.to_date\n",
    "where doh > 1\n",
    "    and p.activation = 'true'\n",
    "    and cs.activation = 'true'\n",
    "    and cs.AVAILABLE_STOCK * f.wac1 >= 1000\n",
    "    and prev_uth > 0\n",
    "'''\n",
    "\n",
    "product_data = snowflake_query(\"Egypt\", command_string)\n",
    "for col in product_data.columns:\n",
    "    product_data[col] = pd.to_numeric(product_data[col], errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176e6341",
   "metadata": {},
   "source": [
    "### 2.3 Price Reference Data\n",
    "\n",
    "Fetch pricing data from multiple sources for comparison:\n",
    "- **UTH Contribution**: Historical until-the-hour sales contribution by district\n",
    "- **Current Prices**: Live and local cohort prices per warehouse\n",
    "- **Marketplace Prices**: External market price benchmarks\n",
    "- **Competitor Prices**: Ben Soliman and scraped competitor prices\n",
    "- **Historical Stats**: Optimal margin boundaries and targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89169236-0e28-4be4-8800-96377ec58584",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f'''\n",
    "with last_update as (\n",
    "    select DATE_PART('hour', max_date) * 60 + DATE_PART('minute', max_date) AS total_minutes\n",
    "    from (\n",
    "        select max(created_at) as max_date from sales_orders\n",
    "    )\n",
    "),\n",
    "\n",
    "base as (\n",
    "    select *, row_number() over(partition by retailer_id order by priority) as rnk\n",
    "    from (\n",
    "        select x.*, TAGGABLE_ID as retailer_id\n",
    "        from (\n",
    "            select id as cohort_id, name as cohort_name, priority, dynamic_tag_id\n",
    "            from cohorts\n",
    "            where is_active = 'true'\n",
    "                and id in (700, 701, 702, 703, 704, 1123, 1124, 1125, 1126)\n",
    "        ) x\n",
    "        join DYNAMIC_TAGgables dt on x.dynamic_tag_id = dt.dynamic_tag_id and dt.dynamic_tag_id <> 3038\n",
    "    )\n",
    "    qualify rnk = 1\n",
    "),\n",
    "\n",
    "sales as (\n",
    "    SELECT\n",
    "        so.created_at::date as date,\n",
    "        pso.warehouse_id as warehouse_id,\n",
    "        districts.id as district_id,\n",
    "        districts.name_ar as district_name,\n",
    "        sum(pso.total_price) as all_day_nmv,\n",
    "        sum(case when (date_part('hour', so.created_at) * 60 + DATE_PART('minute', so.created_at)) < (select * from last_update) then pso.total_price end) as uth_nmv,\n",
    "        sum(case when (date_part('hour', so.created_at) * 60 + DATE_PART('minute', so.created_at))\n",
    "            between (select * from last_update) - 60\n",
    "            and (select * from last_update)\n",
    "            then pso.total_price end) as last_hour_nmv\n",
    "    FROM product_sales_order pso\n",
    "    JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "    JOIN products on products.id = pso.product_id\n",
    "    JOIN brands on products.brand_id = brands.id\n",
    "    JOIN categories ON products.category_id = categories.id\n",
    "    JOIN finance.all_cogs f ON f.product_id = pso.product_id\n",
    "        AND f.from_date::date <= so.created_at::date\n",
    "        AND f.to_date::date > so.created_at::date\n",
    "    JOIN product_units ON product_units.id = products.unit_id\n",
    "    JOIN materialized_views.retailer_polygon on materialized_views.retailer_polygon.retailer_id = so.retailer_id\n",
    "    JOIN districts on districts.id = materialized_views.retailer_polygon.district_id\n",
    "    JOIN cities on cities.id = districts.city_id\n",
    "    join states on states.id = cities.state_id\n",
    "    join regions on regions.id = states.region_id\n",
    "    WHERE True\n",
    "        AND so.created_at::date between date_trunc('month', current_date - 60) and current_date - 1\n",
    "        AND so.sales_order_status_id not in (7, 12)\n",
    "        AND so.channel IN ('telesales', 'retailer')\n",
    "        AND pso.purchased_item_count <> 0\n",
    "    GROUP BY ALL\n",
    "    order by date desc\n",
    ")\n",
    "\n",
    "select warehouse_id, district_id, district_name, sum(uth_cntrb * distance) / sum(distance) as uth_cntrb\n",
    "from (\n",
    "    select *, 1 / nullif((0.3 * week_distance + 0.1 * month_distance + 0.6 * day_distance), 0) as distance\n",
    "    from (\n",
    "        select *,\n",
    "            uth_nmv / all_day_nmv as uth_cntrb,\n",
    "            floor((DATE_PART('day', date) - 1) / 7 + 1) AS week_of_month,\n",
    "            DATE_PART('month', date) as month,\n",
    "            DATE_PART('DOW', date) AS day_number,\n",
    "            abs(floor((DATE_PART('day', current_date) - 1) / 7 + 1) - week_of_month) as week_distance,\n",
    "            abs(DATE_PART('month', current_date) - month) as month_distance,\n",
    "            abs(DATE_PART('DOW', current_date) - day_number) as day_distance\n",
    "        from sales\n",
    "    )\n",
    ")\n",
    "group by all\n",
    "'''\n",
    "\n",
    "uth_cntrb = snowflake_query(\"Egypt\", query)\n",
    "for col in uth_cntrb.columns:\n",
    "    uth_cntrb[col] = pd.to_numeric(uth_cntrb[col], errors='ignore')\n",
    "uth_cntrb['uth_cntrb'] = uth_cntrb.groupby('warehouse_id')['uth_cntrb'].transform(\n",
    "    lambda x: x.fillna(x.mean())\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17bfde16-bbc0-4c0d-9c6c-2569a5dddf3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f'''\n",
    "WITH whs as (SELECT *\n",
    "             FROM   (values\n",
    "                            ('Cairo', 'El-Marg', 38,700),\n",
    "                            ('Cairo', 'Mostorod', 1,700),\n",
    "                            ('Giza', 'Barageel', 236,701),\n",
    "                            ('Delta West', 'El-Mahala', 337,703),\n",
    "                            ('Delta West', 'Tanta', 8,703),\n",
    "                            ('Delta East', 'Mansoura FC', 339,704),\n",
    "                            ('Delta East', 'Sharqya', 170,704),\n",
    "                            ('Upper Egypt', 'Assiut FC', 501,1124),\n",
    "                            ('Upper Egypt', 'Bani sweif', 401,1126),\n",
    "                            ('Upper Egypt', 'Menya Samalot', 703,1123),\n",
    "                            ('Upper Egypt', 'Sohag', 632,1125),\n",
    "                            ('Alexandria', 'Khorshed Alex', 797,702),\n",
    "\t\t\t\t\t\t\t('Giza', 'Sakkarah', 962,701)\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t)\n",
    "                    x(region, wh, warehouse_id,cohort_id)),\n",
    "\n",
    "\n",
    "local_prices as (\n",
    "SELECT  case when cpu.cohort_id in (700,695) then 'Cairo'\n",
    "             when cpu.cohort_id in (701) then 'Giza'\n",
    "             when cpu.cohort_id in (704,698) then 'Delta East'\n",
    "             when cpu.cohort_id in (703,697) then 'Delta West'\n",
    "             when cpu.cohort_id in (696,1123,1124,1125,1126) then 'Upper Egypt'\n",
    "             when cpu.cohort_id in (702,699) then 'Alexandria'\n",
    "        end as region,\n",
    "\t\tcohort_id,\n",
    "        pu.product_id,\n",
    "\t\tpu.packing_unit_id as packing_unit_id,\n",
    "\t\tpu.basic_unit_count,\n",
    "        avg(cpu.price) as price\n",
    "FROM    cohort_product_packing_units cpu\n",
    "join    PACKING_UNIT_PRODUCTS pu on pu.id = cpu.product_packing_unit_id\n",
    "WHERE   cpu.cohort_id in (700,701,702,703,704,696,695,698,697,699,1123,1124,1125,1126)\n",
    "    and cpu.created_at::date<>'2023-07-31'\n",
    "    and cpu.is_customized = true\n",
    "\tgroup by all \n",
    "),\n",
    "live_prices as (\n",
    "select region,cohort_id,product_id,pu_id as packing_unit_id,buc as basic_unit_count,NEW_PRICE as price\n",
    "from materialized_views.DBDP_PRICES\n",
    "where created_at = current_date\n",
    "and DATE_PART('hour',CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMEstamp())) BETWEEN SPLIT_PART(time_slot, '-', 1)::int AND (SPLIT_PART(time_slot, '-', 1)::int)+1\n",
    "and cohort_id in (700,701,702,703,704,696,695,698,697,699,1123,1124,1125,1126)\n",
    "),\n",
    "prices as (\n",
    "select *\n",
    "from (\n",
    "    SELECT *, 1 AS priority FROM live_prices\n",
    "    UNION ALL\n",
    "    SELECT *, 2 AS priority FROM local_prices\n",
    ")\n",
    "QUALIFY ROW_NUMBER() OVER (PARTITION BY region,cohort_id,product_id,packing_unit_id ORDER BY priority) = 1\n",
    ")\n",
    "select warehouse_id,product_id,price \n",
    "from prices \n",
    "join whs on prices.cohort_id = whs.cohort_id\n",
    "and basic_unit_count = 1 \n",
    "and case when product_id = 1309 then packing_unit_id <> 23 else true end\n",
    "\n",
    "'''\n",
    "product_warehouse_price = snowflake_query(\"Egypt\", query)\n",
    "for col in product_warehouse_price.columns:\n",
    "    product_warehouse_price[col] = pd.to_numeric(product_warehouse_price[col], errors='ignore')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f15810cc-6e26-4f5a-93a8-3f60efb52644",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f'''\n",
    "WITH whs as (SELECT *\n",
    "             FROM   (values\n",
    "                            ('Cairo', 'El-Marg', 38,700),\n",
    "                            ('Cairo', 'Mostorod', 1,700),\n",
    "                            ('Giza', 'Barageel', 236,701),\n",
    "                            ('Delta West', 'El-Mahala', 337,703),\n",
    "                            ('Delta West', 'Tanta', 8,703),\n",
    "                            ('Delta East', 'Mansoura FC', 339,704),\n",
    "                            ('Delta East', 'Sharqya', 170,704),\n",
    "                            ('Upper Egypt', 'Assiut FC', 501,1124),\n",
    "                            ('Upper Egypt', 'Bani sweif', 401,1126),\n",
    "                            ('Upper Egypt', 'Menya Samalot', 703,1123),\n",
    "                            ('Upper Egypt', 'Sohag', 632,1125),\n",
    "                            ('Alexandria', 'Khorshed Alex', 797,702),\n",
    "\t\t\t\t\t\t\t('Giza', 'Sakkarah', 962,701)\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t)\n",
    "                    x(region, wh, warehouse_id,cohort_id)),\n",
    "full_data as (\n",
    "select products.id as product_id, region,warehouse_id\n",
    "from products , whs \n",
    "where activation = 'true'\n",
    "),\t\t\t\t\n",
    "\n",
    "MP as (\n",
    "select region,product_id,\n",
    "min(min_price) as min_price,\n",
    "min(max_price) as max_price,\n",
    "min(mod_price) as mod_price,\n",
    "min(true_min) as true_min,\n",
    "min(true_max) as true_max\n",
    "\n",
    "from (\n",
    "select mp.region,mp.product_id,mp.pu_id,\n",
    "min_price/BASIC_UNIT_COUNT as min_price,\n",
    "max_price/BASIC_UNIT_COUNT as max_price,\n",
    "mod_price/BASIC_UNIT_COUNT as mod_price,\n",
    "TRUE_MIN_PRICE/BASIC_UNIT_COUNT as true_min,\n",
    "TRUE_MAX_PRICE/BASIC_UNIT_COUNT as true_max\n",
    "from materialized_views.marketplace_prices mp \n",
    "join packing_unit_products pup on pup.product_id = mp.product_id and pup.packing_unit_id = mp.pu_id\n",
    "join finance.all_cogs f on f.product_id = mp.product_id and CURRENT_TIMESTAMP between f.from_date and f.to_date\n",
    "where  least(min_price,mod_price) between wac_p*0.9 and wac_p*1.3 \n",
    ")\n",
    "group by all \n",
    "),\n",
    "region_mapping AS (\n",
    "    SELECT * \n",
    "\tFROM \n",
    "\t(\tVALUES\n",
    "        ('Delta East', 'Delta West'),\n",
    "        ('Delta West', 'Delta East'),\n",
    "        ('Alexandria', 'Cairo'),\n",
    "        ('Alexandria', 'Giza'),\n",
    "        ('Upper Egypt', 'Cairo'),\n",
    "        ('Upper Egypt', 'Giza'),\n",
    "\t\t('Cairo','Giza'),\n",
    "\t\t('Giza','Cairo'),\n",
    "\t\t('Delta West', 'Cairo'),\n",
    "\t\t('Delta East', 'Cairo'),\n",
    "\t\t('Delta West', 'Giza'),\n",
    "\t\t('Delta East', 'Giza')\n",
    "\t\t)\n",
    "    AS region_mapping(region, fallback_region)\n",
    ")\n",
    "\n",
    "\n",
    "select region,warehouse_id,product_id,\n",
    "min(final_min_price) as final_min_price,\n",
    "min(final_max_price) as final_max_price,\n",
    "min(final_mod_price) as final_mod_price,\n",
    "min(final_true_min) as final_true_min,\n",
    "min(final_true_max) as final_true_max\n",
    "\n",
    "from (\n",
    "SELECT\n",
    "distinct \n",
    "\tw.region,\n",
    "    w.warehouse_id,\n",
    "\tw.product_id,\n",
    "    COALESCE(m1.min_price, m2.min_price) AS final_min_price,\n",
    "    COALESCE(m1.max_price, m2.max_price) AS final_max_price,\n",
    "    COALESCE(m1.mod_price, m2.mod_price) AS final_mod_price,\n",
    "\tCOALESCE(m1.true_min, m2.true_min) AS final_true_min,\n",
    "\tCOALESCE(m1.true_max, m2.true_max) AS final_true_max,\n",
    "FROM full_data w\n",
    "LEFT JOIN MP m1\n",
    "    ON w.region = m1.region and w.product_id = m1.product_id\n",
    "JOIN region_mapping rm\n",
    "    ON w.region = rm.region\n",
    "LEFT JOIN MP m2\n",
    "    ON rm.fallback_region = m2.region\n",
    "   AND w.product_id = m2.product_id\n",
    ")\n",
    "where final_min_price is not null \n",
    "group by all \n",
    "'''\n",
    "marketplace = snowflake_query(\"Egypt\", query)\n",
    "for col in marketplace.columns:\n",
    "    marketplace[col] = pd.to_numeric(marketplace[col], errors='ignore')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de23bc1f-2b20-4b28-9132-a9fa6a125f93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Ben Soliman (competitor) prices...\n",
      "‚úì Retrieved competitor prices for 1570 products\n"
     ]
    }
   ],
   "source": [
    "query = f'''\n",
    "with lower as (\n",
    "select distinct product_id,sku,new_d*bs_price as ben_soliman_price,INJECTION_DATE\n",
    "from (\n",
    "select maxab_product_id as product_id,maxab_sku as sku,INJECTION_DATE,wac1,wac_p,(bs_price/bs_unit_count) as bs_price,diff,cu_price,case when p1 > 1 then child_quantity else 0 end as scheck,round(p1/2)*2 as p1,p2,case when (ROUND(p1 / scheck) * scheck) = 0 then p1 else (ROUND(p1 / scheck) * scheck) end as new_d\n",
    "from (\n",
    "select sm.*,wac1, wac_p, abs((bs_price/bs_unit_count)-(wac_p*maxab_basic_unit_count))/(wac_p*maxab_basic_unit_count) as diff,cpc.price as cu_price,pup.child_quantity , round((cu_price/(bs_price/bs_unit_count))) as p1, round(((bs_price/bs_unit_count)/cu_price)) as p2\n",
    "from materialized_views.savvy_mapping sm \n",
    "join finance.all_cogs f on f.product_id = sm.maxab_product_id and CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMESTAMP()) between f.from_Date and f.to_date\n",
    "join   PACKING_UNIT_PRODUCTS pu on pu.product_id = sm.maxab_product_id and pu.IS_BASIC_UNIT = 1 \n",
    "join cohort_product_packing_units cpc on cpc.PRODUCT_PACKING_UNIT_ID = pu.id and cohort_id = 700 \n",
    "join packing_unit_products pup on pup.product_id = sm.maxab_product_id and pup.is_basic_unit = 1  \n",
    "where bs_price is not null and INJECTION_DATE::date >= CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - 5 \n",
    "and diff > 0.3\n",
    "and p1 > 1\n",
    ")\n",
    ")\n",
    "qualify max(INJECTION_DATE)over(partition by product_id)  = INJECTION_DATE\n",
    "),\n",
    "m_bs as (\n",
    "select z.* from (\n",
    "\tselect maxab_product_id as product_id, maxab_sku as sku, avg(bs_final_price) as ben_soliman_price,INJECTION_DATE\n",
    "\tfrom (\n",
    "\t\tselect *, row_number() over(partition by maxab_product_id order by diff) as rnk_2 from (\n",
    "\t\t\tselect *, (bs_final_price-wac_p)/wac_p as diff_2 from (\n",
    "\t\t\t\tselect *, bs_price/maxab_basic_unit_count as bs_final_price from (\n",
    "\t\t\t\t\tselect *, row_number() over(partition by maxab_product_id, maxab_pu order by diff) as rnk from (\n",
    "\t\t\t\t\t\tselect * ,max(INJECTION_DATE::date) over(partition by maxab_product_id, maxab_pu) as max_date,\n",
    "\t\t\t\t\t\tfrom (\n",
    "\t\t\t\t\t\t\tselect sm.*,wac1, wac_p, abs(bs_price-(wac_p*maxab_basic_unit_count))/(wac_p*maxab_basic_unit_count) as diff \n",
    "\t\t\t\t\tfrom materialized_views.savvy_mapping sm \n",
    "\t\t\t\t\tjoin finance.all_cogs f on f.product_id = sm.maxab_product_id and CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMESTAMP()) between f.from_Date and f.to_date\n",
    "\t\t\t\t\twhere bs_price is not null and INJECTION_DATE::date >= CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - 5 \n",
    "\t\t\t\t\tand diff < 0.3\n",
    "\t\t\t\t\t)\n",
    "\t\t\t\t\tqualify max_date = INJECTION_DATE\n",
    "\t\t\t\t\t) qualify rnk = 1 \n",
    "\t\t\t\t)\n",
    "\t\t\t) where diff_2 between -0.5 and 0.5 \n",
    "\t\t) qualify rnk_2 = 1 \n",
    "\t) group by all\n",
    ") z \n",
    "join finance.all_cogs f on f.product_id = z.product_id and CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMESTAMP()) between f.from_Date and f.to_date\n",
    "where ben_soliman_price between f.wac_p*0.7 and f.wac_p*1.3\n",
    ")\n",
    "select product_id,sku,avg(ben_soliman_price) as ben_soliman_price\n",
    "from (\n",
    "select *\n",
    "from (\n",
    "select * \n",
    "from m_bs \n",
    "\n",
    "union all\n",
    "\n",
    " select *\n",
    " from lower\n",
    " )\n",
    " qualify max(INJECTION_DATE) over(partition by product_id) = INJECTION_DATE\n",
    " )\n",
    " group by all\n",
    "'''\n",
    "\n",
    "\n",
    "print(\"Fetching Ben Soliman (competitor) prices...\")\n",
    "bensoliman = snowflake_query(\"Egypt\", query)\n",
    "bensoliman.columns = bensoliman.columns.str.lower()\n",
    "\n",
    "for col in bensoliman.columns:\n",
    "    bensoliman[col] = pd.to_numeric(bensoliman[col], errors='ignore')\n",
    "\n",
    "print(f\"‚úì Retrieved competitor prices for {len(bensoliman)} products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ade96b0-3200-4a77-826a-dd49f0dd4e77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f'''\n",
    "WITH whs as (SELECT *\n",
    "             FROM   (values\n",
    "                            ('Cairo', 'El-Marg', 38,700),\n",
    "                            ('Cairo', 'Mostorod', 1,700),\n",
    "                            ('Giza', 'Barageel', 236,701),\n",
    "                            ('Delta West', 'El-Mahala', 337,703),\n",
    "                            ('Delta West', 'Tanta', 8,703),\n",
    "                            ('Delta East', 'Mansoura FC', 339,704),\n",
    "                            ('Delta East', 'Sharqya', 170,704),\n",
    "                            ('Upper Egypt', 'Assiut FC', 501,1124),\n",
    "                            ('Upper Egypt', 'Bani sweif', 401,1126),\n",
    "                            ('Upper Egypt', 'Menya Samalot', 703,1123),\n",
    "                            ('Upper Egypt', 'Sohag', 632,1125),\n",
    "                            ('Alexandria', 'Khorshed Alex', 797,702),\n",
    "\t\t\t\t\t\t\t('Giza', 'Sakkarah', 962,701)\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t)\n",
    "                    x(region, wh, warehouse_id,cohort_id))\n",
    "select product_id,x.region,warehouse_id,min(MARKET_PRICE) as min_scrapped,max(MARKET_PRICE) as max_scrapped,median(MARKET_PRICE) as median_scrapped\n",
    "from (\n",
    "select MATERIALIZED_VIEWS.CLEANED_MARKET_PRICES.*,max(date)over(partition by region,MATERIALIZED_VIEWS.CLEANED_MARKET_PRICES.product_id,competitor) as max_date\n",
    "from MATERIALIZED_VIEWS.CLEANED_MARKET_PRICES\n",
    "join finance.all_cogs f on f.product_id = MATERIALIZED_VIEWS.CLEANED_MARKET_PRICES.product_id and CURRENT_TIMESTAMP between f.from_date and f.to_date \n",
    "where date>= current_date -5\n",
    "and MARKET_PRICE between f.wac_p * 0.9 and wac_p*1.3\n",
    "qualify date = max_date \n",
    ") x \n",
    "left join whs on whs.region = x.region\n",
    "group by all \n",
    "'''\n",
    "try:\n",
    "    scrapped_prices = snowflake_query(\"Egypt\", query)\n",
    "    scrapped_prices.columns = scrapped_prices.columns.str.lower() \n",
    "    for col in scrapped_prices.columns:\n",
    "        scrapped_prices[col] = pd.to_numeric(scrapped_prices[col], errors='ignore')\n",
    "except: \n",
    "    scrapped_prices = pd.DataFrame(columns = ['product_id','region','warehouse_id','min_scrapped','max_scrapped','median_scrapped'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbba3668-3267-4142-a7c2-3666cef3226d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f'''\n",
    "select region,product_id,optimal_bm,MIN_BOUNDARY,MAX_BOUNDARY,MEDIAN_BM\n",
    "from (\n",
    "select region,product_id,target_bm,optimal_bm,MIN_BOUNDARY,MAX_BOUNDARY,MEDIAN_BM,max(created_at) over(partition by product_id,region) as max_date,created_at\n",
    "from materialized_views.PRODUCT_STATISTICS\n",
    "where created_at::date >= date_trunc('month',current_date - 60)\n",
    "qualify max_date = created_at\n",
    ")\n",
    "\n",
    "'''\n",
    " \n",
    "stats = snowflake_query(\"Egypt\", query)\n",
    "stats.columns = stats.columns.str.lower() \n",
    "for col in stats.columns:\n",
    "    stats[col] = pd.to_numeric(stats[col], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1329532-37a0-47a8-8835-813a3c9cfadf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f'''\n",
    "select warehouse_id,warehouse_name, region\n",
    "from (\n",
    "    select *, row_number() over(partition by warehouse_id order by nmv desc) as rnk\n",
    "    from (\n",
    "        SELECT case when regions.id = 2 then cities.name_en else regions.name_en end as region,\n",
    "            pso.warehouse_id,\n",
    "            w.name as warehouse_name,\n",
    "            sum(pso.total_price) as nmv\n",
    "        FROM product_sales_order pso\n",
    "        JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "        JOIN materialized_views.retailer_polygon on materialized_views.retailer_polygon.retailer_id = so.retailer_id\n",
    "        JOIN districts on districts.id = materialized_views.retailer_polygon.district_id\n",
    "        JOIN cities on cities.id = districts.city_id\n",
    "        join states on states.id = cities.state_id\n",
    "        join regions on regions.id = states.region_id\n",
    "        join warehouses w on w.id = pso.warehouse_id\n",
    "        WHERE True\n",
    "            AND so.created_at::date between current_date - 31 and CURRENT_DATE - 1\n",
    "            AND so.sales_order_status_id not in (7, 12)\n",
    "            AND so.channel IN ('telesales', 'retailer')\n",
    "            AND pso.purchased_item_count <> 0\n",
    "        GROUP BY ALL\n",
    "    )\n",
    "    qualify rnk = 1\n",
    ")\n",
    "'''\n",
    "\n",
    "warehouse_region = snowflake_query(\"Egypt\", query)\n",
    "warehouse_region.columns = warehouse_region.columns.str.lower()\n",
    "for col in warehouse_region.columns:\n",
    "    warehouse_region[col] = pd.to_numeric(warehouse_region[col], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e94e1fbe-ccb3-45b8-9f1e-dde8ba79d5cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f'''\n",
    "SELECT DISTINCT cat, brand, margin as target_bm\n",
    "FROM    performance.commercial_targets cplan\n",
    "QUALIFY CASE WHEN DATE_TRUNC('month', MAX(DATE)OVER()) = DATE_TRUNC('month', CURRENT_DATE) THEN DATE_TRUNC('month', CURRENT_DATE)\n",
    "ELSE DATE_TRUNC('month', CURRENT_DATE - INTERVAL '1 month') END = DATE_TRUNC('month', date)\n",
    "'''\n",
    "brand_cat_target = snowflake_query(\"Egypt\", query)\n",
    "for col in brand_cat_target.columns:\n",
    "    brand_cat_target[col] = pd.to_numeric(brand_cat_target[col], errors='ignore')\n",
    "\n",
    "query = f'''\n",
    "select cat,sum(target_bm *(target_nmv/cat_total)) as cat_target_margin\n",
    "from (\n",
    "select *,sum(target_nmv)over(partition by cat) as cat_total\n",
    "from (\n",
    "select cat,brand,avg(target_bm) as target_bm , sum(target_nmv) as target_nmv\n",
    "from (\n",
    "SELECT DISTINCT date,city as region,cat, brand, margin as target_bm,nmv as target_nmv\n",
    "FROM    performance.commercial_targets cplan\n",
    "QUALIFY CASE WHEN DATE_TRUNC('month', MAX(DATE)OVER()) = DATE_TRUNC('month', CURRENT_DATE) THEN DATE_TRUNC('month', CURRENT_DATE)\n",
    "ELSE DATE_TRUNC('month', CURRENT_DATE - INTERVAL '1 month') END = DATE_TRUNC('month', date)\n",
    ")\n",
    "group by all\n",
    ")\n",
    ")\n",
    "group by all \n",
    "'''\n",
    "cat_target = snowflake_query(\"Egypt\", query)\n",
    "for col in cat_target.columns:\n",
    "    cat_target[col] = pd.to_numeric(cat_target[col], errors='ignore')\n",
    "\n",
    "query = f'''\n",
    "SELECT  DIStinct  \n",
    "\t\tproducts.id as product_id,\n",
    "\t\tCONCAT(products.name_ar,' ',products.size,' ',product_units.name_ar) as sku,\n",
    "\t\tbrands.name_ar as brand, \n",
    "\t\tcategories.name_ar as cat,\n",
    "\t\tf.wac_p\n",
    "from products \n",
    "JOIN brands on products.brand_id = brands.id \n",
    "JOIN categories ON products.category_id = categories.id\n",
    "JOIN finance.all_cogs f  ON f.product_id = products.id and CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMEstamp()) between f.from_date and f.to_date \n",
    "JOIN product_units ON product_units.id = products.unit_id \n",
    "'''\n",
    "sku_info = snowflake_query(\"Egypt\", query)\n",
    "for col in sku_info.columns:\n",
    "    sku_info[col] = pd.to_numeric(sku_info[col], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8722c17b-0da0-43a6-ae4d-12b81b9760d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "select warehouse_id, district_id, district_name, product_id, nmv as last_d_nmv,\n",
    "    coalesce(sku_dis_nmv, 0) / nmv as sku_disc_cntrb,\n",
    "    coalesce(quantity_nmv, 0) / nmv as quant_disc_cntrb,\n",
    "    sku_disc_price,\n",
    "    quantity_price\n",
    "from (\n",
    "    SELECT DISTINCT\n",
    "        pso.warehouse_id,\n",
    "        districts.id as district_id,\n",
    "        districts.name_ar as district_name,\n",
    "        pso.product_id,\n",
    "        sum(pso.total_price) as nmv,\n",
    "        avg(item_price / basic_unit_count) as item_price,\n",
    "        sum(case when ITEM_DISCOUNT_value > 0 then pso.total_price end) as sku_dis_nmv,\n",
    "        sum(case when ITEM_quantity_DISCOUNT_value > 0 then pso.total_price end) as quantity_nmv,\n",
    "        avg(case when ITEM_DISCOUNT_value > 0 then (item_price / BASIC_UNIT_COUNT) - (ITEM_DISCOUNT_value / BASIC_UNIT_COUNT) end) as sku_disc_price,\n",
    "        avg(case when ITEM_quantity_DISCOUNT_value > 0 then (item_price / BASIC_UNIT_COUNT) - (ITEM_quantity_DISCOUNT_value / BASIC_UNIT_COUNT) end) as quantity_price\n",
    "    FROM product_sales_order pso\n",
    "    JOIN sales_orders so ON so.id = pso.sales_order_id \n",
    "        and so.retailer_id not in (select taggable_id from dynamic_taggables where dynamic_tag_id = 3038)\n",
    "    JOIN materialized_views.retailer_polygon on materialized_views.retailer_polygon.retailer_id = so.retailer_id\n",
    "    JOIN districts on districts.id = materialized_views.retailer_polygon.district_id\n",
    "    WHERE so.created_at::date = current_date - 1\n",
    "        AND so.sales_order_status_id not in (7, 12)\n",
    "        AND so.channel IN ('telesales', 'retailer')\n",
    "        AND pso.purchased_item_count <> 0\n",
    "    GROUP BY ALL\n",
    ")\n",
    "order by nmv desc\n",
    "'''\n",
    "\n",
    "last_day = snowflake_query(\"Egypt\", query)\n",
    "last_day.columns = last_day.columns.str.lower()\n",
    "for col in last_day.columns:\n",
    "    last_day[col] = pd.to_numeric(last_day[col], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d4da0bd-7018-4315-a7a1-770795bf1992",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "\n",
    "with main_data  as (\n",
    "SELECT  DISTINCT\n",
    "\t\tpso.warehouse_id,\n",
    "\t\tpso.product_id,\n",
    "\t\tCONCAT(products.name_ar,' ',products.size,' ',product_units.name_ar) as sku,\n",
    "\t\tbrands.name_ar as brand, \n",
    "\t\tcategories.name_ar as cat,\n",
    "\t\t\n",
    "        sum(pso.total_price) as nmv,\n",
    "       sum(COALESCE(f.wac_p,0) * pso.purchased_item_count * pso.basic_unit_count) as cogs_p,\n",
    "\t   ((nmv-cogs_p)/nmv) as bm_p,\n",
    "\n",
    "\n",
    "FROM product_sales_order pso\n",
    "JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "--join COHORT_PRICING_CHANGES cpc on cpc.id = pso.COHORT_PRICING_CHANGE_id\n",
    "JOIN products on products.id=pso.product_id\n",
    "JOIN brands on products.brand_id = brands.id \n",
    "JOIN categories ON products.category_id = categories.id\n",
    "JOIN finance.all_cogs f  ON f.product_id = pso.product_id\n",
    "                        AND f.from_date::date <= so.created_at ::date\n",
    "                        AND f.to_date::date > so.created_at ::date\n",
    "JOIN product_units ON product_units.id = products.unit_id \n",
    "JOIN materialized_views.retailer_polygon on materialized_views.retailer_polygon.retailer_id=so.retailer_id\n",
    "JOIN districts on districts.id=materialized_views.retailer_polygon.district_id\n",
    "JOIN cities on cities.id=districts.city_id\n",
    "join states on states.id=cities.state_id\n",
    "join regions on regions.id=states.region_id             \n",
    "\n",
    "WHERE   True\n",
    "    AND so.created_at ::date between  current_date - 5 and current_date -1 \n",
    "    AND so.sales_order_status_id not in (7,12)\n",
    "    AND so.channel IN ('telesales','retailer')\n",
    "    AND pso.purchased_item_count <> 0\n",
    "\n",
    "GROUP BY ALL\n",
    "),\n",
    "cp as (\n",
    "select cat,brand,sum(nmv) as target_nmv ,avg(margin) as target_margin\n",
    "from performance.commercial_targets \n",
    "where date  between '2025-10-01' and current_date - 1\n",
    "group by all \n",
    "),\n",
    "stocks as (\t\t\t\t\t\n",
    "select warehouse_id,warehouse,product_id,sum(stocks) as stocks\n",
    "from (\n",
    "\t\tSELECT DISTINCT product_warehouse.warehouse_id,w.name as warehouse,\n",
    "                product_warehouse.product_id,\n",
    "                (product_warehouse.available_stock)::integer as stocks\n",
    "\n",
    "        from  product_warehouse \n",
    "        JOIN products on product_warehouse.product_id = products.id\n",
    "        JOIN product_units ON products.unit_id = product_units.id\n",
    "\t\tjoin warehouses w on w.id = product_warehouse.warehouse_id\n",
    "\n",
    "        where   product_warehouse.warehouse_id not in (6,9,10)\n",
    "            AND product_warehouse.is_basic_unit = 1\n",
    "\t\t\tand product_warehouse.available_stock > 0 \n",
    "\n",
    ")\n",
    "group by all\n",
    "),\n",
    "prs AS (\n",
    "SELECT DISTINCT product_purchased_receipts.purchased_receipt_id,\n",
    "                purchased_receipts.purchased_order_id,\n",
    "                DATE_PART('Day', purchased_receipts.date::date) AS DAY,\n",
    "                DATE_PART('month', purchased_receipts.date::date) AS MONTH,\n",
    "                DATE_Part('year', purchased_receipts.date::date) AS YEAR,\n",
    "                products.id AS product_id,\n",
    "                CONCAT(products.name_ar, ' ', products.size, ' ', product_units.name_ar) AS sku,\n",
    "                brands.name_ar AS Brand,\n",
    "                categories.name_ar as category,\n",
    "                products.description,\n",
    "                purchased_receipts.warehouse_id AS warehouse_id,\n",
    "                warehouses.name as warehouse,\n",
    "                packing_units.name_ar AS packing_unit,\n",
    "                purchased_receipts.discount AS Total_discount,\n",
    "                purchased_receipts.return_orders_discount,\n",
    "                purchased_receipts.discount_type_id,\n",
    "                suppliers.id AS supplier_id,\n",
    "                suppliers.name AS supplier_name,\n",
    "                purchased_receipt_statuses.name_ar AS PR_status,\n",
    "                product_purchased_receipts.basic_unit_count,\n",
    "                product_purchased_receipts.purchased_item_count AS purchase_count,\n",
    "                product_purchased_receipts.purchased_item_count*product_purchased_receipts.basic_unit_count AS purchase_min_count,\n",
    "                product_purchased_receipts.item_price,\n",
    "                product_purchased_receipts.final_price/product_purchased_receipts.purchased_item_count AS final_item_price,\n",
    "                product_purchased_receipts.total_price AS purchase_price,\n",
    "                CASE WHEN product_purchased_receipts.vat = 'true' THEN product_purchased_receipts.total_price * 0.14\n",
    "                     ELSE CASE WHEN product_purchased_receipts.vat = 'false' THEN product_purchased_receipts.total_price * 0\n",
    "                               END\n",
    "                END AS vat,\n",
    "                CASE WHEN purchased_receipts.discount_type_id = 2 THEN (product_purchased_receipts.discount/100) * product_purchased_receipts.total_price\n",
    "                     ELSE product_purchased_receipts.discount\n",
    "                END AS SKU_discount,\n",
    "                purchased_receipts.total_price AS pr_value,\n",
    "                CASE\n",
    "                    WHEN product_purchased_receipts.t_tax_id = 1 THEN product_purchased_receipts.total_price * 0.05\n",
    "                    ELSE CASE\n",
    "                             WHEN product_purchased_receipts.t_tax_id = 2 THEN product_purchased_receipts.total_price * 0.08\n",
    "                             ELSE CASE\n",
    "                                      WHEN product_purchased_receipts.t_tax_id = 3 THEN product_purchased_receipts.total_price * 0.1\n",
    "                                      ELSE 0\n",
    "                                  END\n",
    "                         END\n",
    "                END AS table_tax,\n",
    "                product_purchased_receipts.final_price AS Final_Price,\n",
    "                product_purchased_receipts.product_type_id,\n",
    "                purchased_receipts.debt_note_value as credit_note,\n",
    "                purchased_receipts.tips,\n",
    "                purchased_receipts.delivery_fees,\n",
    "                case when purchased_receipts.is_actual = 'true' then 'Real' \n",
    "                     else 'Virtual' \n",
    "                     end as is_actual\n",
    "                     \n",
    "FROM product_purchased_receipts\n",
    "LEFT JOIN products ON products.id = product_purchased_receipts.product_id\n",
    "LEFT JOIN packing_unit_products ON packing_unit_products.product_id = products.id\n",
    "LEFT JOIN purchased_receipts ON purchased_receipts.id = product_purchased_receipts.purchased_receipt_id\n",
    "LEFT JOIN purchased_receipt_statuses ON purchased_receipt_statuses.id = purchased_receipts.purchased_receipt_status_id\n",
    "LEFT JOIN packing_units ON packing_units.id = product_purchased_receipts.packing_unit_id\n",
    "LEFT JOIN product_units ON products.unit_id = product_units.id\n",
    "LEFT JOIN suppliers ON suppliers.id = purchased_receipts.supplier_id\n",
    "LEFT JOIN brands ON brands.id = products.brand_id\n",
    "left join categories on categories.id = products.category_id\n",
    "left join warehouses on warehouses.id = purchased_receipts.warehouse_id\n",
    "WHERE product_purchased_receipts.purchased_item_count <> 0\n",
    "      AND purchased_receipts.purchased_receipt_status_id IN (4,5,7)\n",
    "      AND purchased_receipts.date::date >= current_date - 4\n",
    "    AND purchased_receipts.is_actual = 'true'\n",
    "     \n",
    "     \n",
    "    ),\n",
    "prs_data as (\n",
    "select warehouse_id , product_id,sum(final_price) as total_prs \n",
    "from prs \n",
    "group by all\n",
    ")\n",
    "\n",
    "select warehouse_id,product_id,1 as zero_rr\n",
    "from (\n",
    "select s.*,\n",
    "CONCAT(products.name_ar,' ',products.size,' ',product_units.name_ar) as sku,\n",
    "brands.name_ar as brand, \n",
    "categories.name_ar as cat,\n",
    "coalesce(md.nmv,0) as sales,wac1,\n",
    "wac1*stocks as stock_value,\n",
    "coalesce(total_prs,0) as prs_data\n",
    "from stocks s\n",
    "left join main_data md on md.product_id =s.product_id and md.warehouse_id = s.warehouse_id\n",
    "JOIN finance.all_cogs f  ON f.product_id = s.product_id\n",
    "                        AND f.from_date::date <= current_date \n",
    "                        AND f.to_date::date > current_date\n",
    "JOIN products on products.id=s.product_id\n",
    "JOIN brands on products.brand_id = brands.id \n",
    "JOIN categories ON products.category_id = categories.id\n",
    "JOIN product_units ON product_units.id = products.unit_id\n",
    "left join prs_data on prs_data.product_id =s.product_id and prs_data.warehouse_id = s.warehouse_id \n",
    "where stocks > 0 and sales = 0 \n",
    "and prs_data < 0.7*stock_value\n",
    "order by wac1* stocks desc \n",
    ")\n",
    "'''\n",
    "\n",
    "zerorr = snowflake_query(\"Egypt\", query)\n",
    "zerorr.columns = zerorr.columns.str.lower()\n",
    "for col in zerorr.columns:\n",
    "    zerorr[col] = pd.to_numeric(zerorr[col], errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6389fd8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Product Selection & Analysis\n",
    "\n",
    "This section identifies products that are underperforming and eligible for Happy Hour discounts.\n",
    "\n",
    "### 3.1 Calculate Growth Metrics\n",
    "Compute product-level and warehouse-level growth by comparing current vs historical performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e9d8f4d-8ec6-4fb3-bd10-8f52c50993be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "product_data = product_data.merge(product_warehouse_price,on=['product_id','warehouse_id'])\n",
    "product_data = product_data.merge(uth_cntrb[['warehouse_id','district_id','district_name','uth_cntrb']],on=['warehouse_id','district_id','district_name'])\n",
    "product_data['product_UTH_growth'] =(product_data['current_uth'] -product_data['prev_uth'])/product_data['prev_uth']\n",
    "product_data['product_LH_growth'] =(product_data['current_last_hour'] -product_data['prev_last_hour'])/product_data['prev_last_hour']\n",
    "product_data[['product_UTH_growth','product_LH_growth']] =product_data[['product_UTH_growth','product_LH_growth']].fillna(0) \n",
    "product_data = product_data.replace([np.inf, -np.inf], 1)\n",
    "product_data['product_closing_growth'] = (product_data['product_UTH_growth']*product_data['uth_cntrb'])+(product_data['product_LH_growth']*(1-product_data['uth_cntrb']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f55856e-b84b-463e-94ca-cbaf9639a942",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warehouse_data = product_data.groupby(['warehouse_id', 'district_id', 'district_name'])[['prev_all_day', 'prev_uth', 'prev_last_hour', 'current_all_day', 'current_uth', 'current_last_hour']].sum().reset_index()\n",
    "warehouse_data['UTH_growth'] = (warehouse_data['current_uth'] - warehouse_data['prev_uth']) / warehouse_data['prev_uth']\n",
    "warehouse_data['LH_growth'] = (warehouse_data['current_last_hour'] - warehouse_data['prev_last_hour']) / warehouse_data['prev_last_hour']\n",
    "warehouse_data = warehouse_data.merge(uth_cntrb, on=['warehouse_id', 'district_id','district_name'])\n",
    "warehouse_data['Closing_growth'] = (warehouse_data['UTH_growth'] * warehouse_data['uth_cntrb']) + (warehouse_data['LH_growth'] * (1 - warehouse_data['uth_cntrb']))\n",
    "dropping_whs = warehouse_data[warehouse_data['Closing_growth'] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fb16c40-bee2-4050-b08d-da0069012b33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "growing_products = product_data.merge(warehouse_data[['warehouse_id', 'district_id', 'district_name', 'UTH_growth', 'LH_growth', 'Closing_growth']], on=['warehouse_id', 'district_id'])\n",
    "# needs edit\n",
    "growing_products = growing_products[growing_products['product_closing_growth'] >= np.maximum(growing_products['Closing_growth'], 0.1)]\n",
    "growing_products['max_closing'] = growing_products.groupby('product_id')['product_closing_growth'].transform('sum')\n",
    "growing_products = growing_products[growing_products['max_closing'] == growing_products['product_closing_growth']]\n",
    "growing_products = growing_products.groupby(['product_id'])['price'].mean().reset_index()\n",
    "growing_products.columns = ['product_id', 'maxab_good_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b870a73b-e9ca-46c0-a3f8-02d41cd513c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_products = product_data.merge(sku_info,on=['product_id'])\n",
    "selected_products = selected_products[selected_products['brand'].isin(b_list)]\n",
    "selected_products = selected_products.merge(warehouse_data[['warehouse_id','district_id', 'district_name','UTH_growth','LH_growth','Closing_growth']],on=['warehouse_id','district_id', 'district_name'])\n",
    "selected_products=selected_products.drop(columns=['cat','brand','sku','wac_p'])\n",
    "selected_products = selected_products[selected_products['product_closing_growth'] <selected_products['Closing_growth']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1f44bd1-b895-46ef-9d04-498a51319f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_products_wh = product_data.merge(sku_info,on=['product_id']) \n",
    "selected_products_wh = selected_products_wh.merge(sku_to_add_df[['product_id','warehouse_id']],on=['product_id','warehouse_id'])\n",
    "selected_products_wh = selected_products_wh.merge(warehouse_data[['warehouse_id','district_id', 'district_name','UTH_growth','LH_growth','Closing_growth']],on=['warehouse_id','district_id', 'district_name'])\n",
    "selected_products_wh=selected_products_wh.drop(columns=['cat','brand','sku','wac_p'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd450dbd",
   "metadata": {},
   "source": [
    "### 3.2 Identify Underperforming Products\n",
    "Select products with negative growth or belonging to push brands that are underperforming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b97e09f-7b97-4e02-9dff-089e8b8c0ef7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dropping_products = product_data.merge(dropping_whs[['warehouse_id','district_id', 'district_name','UTH_growth','LH_growth','Closing_growth']],on=['warehouse_id','district_id', 'district_name'])\n",
    "dropping_products = dropping_products[dropping_products['product_closing_growth'] < 0]\n",
    "dropping_products = pd.concat([dropping_products,selected_products])\n",
    "dropping_products = pd.concat([dropping_products,selected_products_wh])\n",
    "dropping_products=dropping_products.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ffc8ac4-093a-4445-9fb8-f20e80bddde4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dropping_products = dropping_products.merge(sku_info,on=['product_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8612be22-6e29-4c14-8034-13b798c07d9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delta_wh = [8,170,337,339]\n",
    "rem_brands = ['ÿ™ÿßŸàÿ™ÿßŸà','ŸÑÿßÿ±ÿ¥','ÿßŸÑŸÉÿ®Ÿàÿ≥','ŸÉŸÖÿßÿ±ÿß']\n",
    "dropping_products = dropping_products[~((dropping_products['brand'].isin(rem_brands)) & (dropping_products['warehouse_id'].isin(delta_wh)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61f2f629-8e42-49a5-a6b7-c0fd215cbc96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dropping_products = dropping_products.sort_values(by='prev_all_day',ascending = False)\n",
    "dropping_products = dropping_products.merge(growing_products,on='product_id',how='left')\n",
    "dropping_products = dropping_products.merge(marketplace,on=['product_id','warehouse_id'],how='left')\n",
    "dropping_products = dropping_products.merge(bensoliman[['product_id','ben_soliman_price']],on=['product_id'],how='left')\n",
    "dropping_products = dropping_products.drop(columns = 'region')\n",
    "dropping_products = dropping_products.merge(scrapped_prices,on=['product_id','warehouse_id'],how='left')\n",
    "dropping_products = dropping_products.drop(columns = 'region')\n",
    "dropping_products = dropping_products.merge(zerorr,on=['product_id','warehouse_id'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea0f1062-ebf9-4006-b727-671c546ab895",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dropping_products = dropping_products.merge(warehouse_region,on=['warehouse_id'])\n",
    "dropping_products = dropping_products.merge(stats,on=['product_id','region'],how='left')\n",
    "dropping_products = dropping_products.merge(brand_cat_target,on=['brand','cat'],how='left')\n",
    "dropping_products = dropping_products.merge(cat_target,on=['cat'],how='left')\n",
    "dropping_products['Target_margin'] = dropping_products['target_bm'].fillna(dropping_products['cat_target_margin'])\n",
    "dropping_products = dropping_products[[ 'warehouse_id','district_id','district_name','product_id','sku','brand','cat', 'prev_all_day', 'prev_uth',\n",
    "       'prev_last_hour', 'current_all_day', 'current_uth', 'current_last_hour','product_UTH_growth', 'product_LH_growth',\n",
    "       'product_closing_growth','doh','wac_p','price','maxab_good_price', 'final_min_price', 'final_max_price',\n",
    "       'final_mod_price', 'final_true_min', 'final_true_max',\n",
    "       'ben_soliman_price','optimal_bm', 'min_boundary',\n",
    "       'max_boundary', 'median_bm','Target_margin','min_scrapped','max_scrapped','median_scrapped','zero_rr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25731e4a-3f03-4e9e-9ab7-13d142c8f658",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dropping_products = dropping_products.merge(last_day,on=['product_id','warehouse_id','district_id','district_name'],how='left')\n",
    "dropping_products[['last_d_nmv','sku_disc_cntrb','quant_disc_cntrb','sku_disc_price','quantity_price']] = dropping_products[['last_d_nmv','sku_disc_cntrb','quant_disc_cntrb','sku_disc_price','quantity_price']].fillna(0)\n",
    "dropping_products=dropping_products.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4a687b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Price Optimization Engine\n",
    "\n",
    "This section calculates optimal discount prices using multiple pricing signals and business rules.\n",
    "\n",
    "### 4.1 Price Selection Algorithm\n",
    "\n",
    "The `select_price_optimized` function evaluates prices from multiple sources:\n",
    "- **Marketplace prices** (min, max, mod, true_min, true_max)\n",
    "- **Competitor prices** (Ben Soliman, scraped data)\n",
    "- **Internal benchmarks** (Maxab good prices from growing products)\n",
    "- **Margin targets** (optimal, min_boundary, max_boundary, median)\n",
    "\n",
    "**Decision Logic:**\n",
    "1. For zero running rate or overstock (DOH > 45): Aggressive pricing to clear stock\n",
    "2. For normal products: Select from \"Listed\" prices meeting margin criteria\n",
    "3. Fallback: Calculate weighted average from acceptable prices (\"induced\" pricing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39e7ab85-01f8-447a-b214-28dca3e83b3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def select_price_optimized(remaining_prices, price, wac, Target_margin, min_boundary, zero_rr, doh):\n",
    "    \"\"\"\n",
    "    Optimized price selection function using numpy for faster computation.\n",
    "    Returns (target_price, source)\n",
    "    \"\"\"\n",
    "    target_price = 0.0\n",
    "    source = ''\n",
    "    current_margin = (price - wac) / price if price != 0 else 0\n",
    "    \n",
    "    # Convert to numpy array for vectorized operations\n",
    "    stocks_pricing_list = np.array(remaining_prices + [wac / (1 - (Target_margin * 0.65))])\n",
    "    stocks_pricing_list = np.sort(stocks_pricing_list)\n",
    "    \n",
    "    is_zero_rr = not np.isnan(zero_rr)\n",
    "    is_overstock = doh > 45\n",
    "    \n",
    "    if is_zero_rr or is_overstock:\n",
    "        source = 'Zero_rr' if is_zero_rr else 'OS'\n",
    "        \n",
    "        # Vectorized: find first price where new_price >= wac and diff <= -0.05\n",
    "        diffs = (stocks_pricing_list - price) / price\n",
    "        valid_mask = (stocks_pricing_list >= wac*0.9) & (diffs >= -0.05)\n",
    "        valid_prices = stocks_pricing_list[valid_mask]\n",
    "        \n",
    "        if len(valid_prices) > 0:\n",
    "            target_price = valid_prices[0]\n",
    "        elif current_margin > Target_margin and current_margin - Target_margin > 0.0025:\n",
    "            target_price = wac / (1 - Target_margin)\n",
    "        elif current_margin > min_boundary and current_margin - min_boundary > 0.0025:\n",
    "            target_price = wac / (1 - min_boundary)\n",
    "        elif current_margin > Target_margin / 2 and current_margin - Target_margin / 2 > 0.0025:\n",
    "            target_price = wac / (1 - (Target_margin / 2))\n",
    "    else:\n",
    "        remaining_arr = np.array(remaining_prices)\n",
    "        if len(remaining_arr) > 0:\n",
    "            # Vectorized margin calculations\n",
    "            new_margins = np.where(remaining_arr != 0, (remaining_arr - wac) / remaining_arr, 0)\n",
    "            diffs = (remaining_arr - price) / price if price != 0 else np.zeros_like(remaining_arr)\n",
    "            \n",
    "            # Find valid prices (reverse order - largest first that meets criteria)\n",
    "            valid_mask = (remaining_arr >= wac*0.9) & (diffs >= -0.05)&(diffs <= -0.0025)\n",
    "            \n",
    "            valid_indices = np.where(valid_mask)[0]\n",
    "            if len(valid_indices) > 0:\n",
    "                # Get the last valid index (was iterating in reverse)\n",
    "                target_price = remaining_arr[valid_indices[-1]]\n",
    "                source = 'Listed'\n",
    "            else:\n",
    "                # Find acceptable prices (positive margin)\n",
    "                acceptable_mask = remaining_arr >= wac*0.9\n",
    "                acceptable = remaining_arr[acceptable_mask]\n",
    "                \n",
    "                if len(acceptable) > 1:\n",
    "                    # Vectorized distance-weighted average\n",
    "                    price_diffs = np.abs(price - acceptable)\n",
    "                    # Avoid division by zero\n",
    "                    price_diffs = np.where(price_diffs == 0, 1e-10, price_diffs)\n",
    "                    distances = 1 / price_diffs\n",
    "                    weights = distances / np.sum(distances)\n",
    "                    final_value = np.sum(weights * acceptable)\n",
    "                    target_price = max(final_value, wac / (1 - (0.3 * Target_margin)))\n",
    "                    source = 'induced_1'\n",
    "                elif len(acceptable) == 1:\n",
    "                    final_value = (0.3 * acceptable[0]) + (0.7 * price)\n",
    "                    target_price = max(final_value, wac / (1 - (0.3 * Target_margin)))\n",
    "                    source = 'induced_2'\n",
    "    \n",
    "    return target_price, source\n",
    "\n",
    "\n",
    "def process_row(row):\n",
    "    \"\"\"\n",
    "    Process a single row to determine selected price and source.\n",
    "    Designed for use with DataFrame.apply()\n",
    "    \"\"\"\n",
    "    wac = row['wac_p']\n",
    "    price = row['price']\n",
    "    doh = row['doh']\n",
    "    Target_margin = row['Target_margin']\n",
    "    min_boundary = row['min_boundary'] if not pd.isna(row['min_boundary']) else 0\n",
    "    zero_rr = row['zero_rr']\n",
    "    \n",
    "    # Safely compute prices, handling edge cases\n",
    "    def safe_price(margin):\n",
    "        if pd.isna(margin) or margin == 1:\n",
    "            return np.nan\n",
    "        return wac / (1 - margin)\n",
    "    \n",
    "    # Build prices list\n",
    "    prices_list = [\n",
    "        row['maxab_good_price'], row['final_min_price'], row['final_max_price'],\n",
    "        row['final_mod_price'], row['final_true_min'], row['final_true_max'],\n",
    "        row['ben_soliman_price'],\n",
    "        safe_price(row['optimal_bm']),\n",
    "        safe_price(row['min_boundary']),\n",
    "        safe_price(row['max_boundary']),\n",
    "        safe_price(row['median_bm']),\n",
    "        safe_price(Target_margin),\n",
    "        row['min_scrapped'], row['max_scrapped'], row['median_scrapped']\n",
    "    ]\n",
    "    \n",
    "    # Clean prices - remove 0, nan, and duplicates\n",
    "    cleaned_prices = list({x for x in prices_list if x != 0 and not pd.isna(x) and np.isfinite(x)})\n",
    "    \n",
    "    qd_cntrb = row['quant_disc_cntrb']\n",
    "    sd_cntrb = row['sku_disc_cntrb']\n",
    "    qd_price = row['quantity_price']\n",
    "    sd_price = row['sku_disc_price']\n",
    "    ld_nmv = row['last_d_nmv']\n",
    "    prev_nmv = row['prev_all_day']\n",
    "    \n",
    "    qd_discount = ((qd_price - price) / price) * -1 if price != 0 else 0\n",
    "    sku_discount = ((sd_price - price) / price) * -1 if price != 0 else 0\n",
    "    \n",
    "    # Check previous discount conditions\n",
    "    if ld_nmv > (prev_nmv * 1.15) and (\n",
    "        ((qd_cntrb > 0) and (qd_cntrb > sd_cntrb) and (qd_discount < Target_margin * 0.25)) or \n",
    "        ((sd_cntrb > 0) and (qd_cntrb < sd_cntrb) and (sku_discount < Target_margin * 0.25))\n",
    "    ):\n",
    "        if qd_cntrb > sd_cntrb and qd_cntrb > 0 and qd_price > 0 and qd_price > wac:\n",
    "            return pd.Series({'selected_price': qd_price, 'source': 'Prev_disc'})\n",
    "        elif qd_cntrb < sd_cntrb and sd_cntrb > 0 and sd_price > 0 and sd_price > wac:\n",
    "            return pd.Series({'selected_price': sd_price, 'source': 'Prev_disc'})\n",
    "        elif sd_price > wac or qd_price > wac:\n",
    "            return pd.Series({'selected_price': max(qd_price, sd_price), 'source': 'Prev_disc'})\n",
    "    \n",
    "    # Determine remaining prices based on discount conditions\n",
    "    if qd_cntrb > sd_cntrb and qd_price > 0 and qd_discount <= Target_margin * 0.35:\n",
    "        remaining_prices = [x for x in cleaned_prices if x < qd_price and x < price]\n",
    "    elif qd_cntrb < sd_cntrb and sd_price > 0 and sku_discount <= Target_margin * 0.35:\n",
    "        remaining_prices = [x for x in cleaned_prices if x < sd_price and x < price]\n",
    "    else:\n",
    "        remaining_prices = [x for x in cleaned_prices if x < price]\n",
    "    \n",
    "    remaining_prices.sort()\n",
    "    \n",
    "    selected_price, source = select_price_optimized(\n",
    "        remaining_prices, price, wac, Target_margin, min_boundary, zero_rr, doh\n",
    "    )\n",
    "    \n",
    "    return pd.Series({'selected_price': selected_price, 'source': source})    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d667ee",
   "metadata": {},
   "source": [
    "### 4.2 Execute Price Optimization\n",
    "Apply the pricing algorithm to all selected products.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa9eeee1-ea7f-429b-8156-8a68a4924ebf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 166,807 products...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prices: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 166807/166807 [01:02<00:00, 2686.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Processed 166,807 products\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# OPTIMIZED: Using apply() instead of iterrows() + concat\n",
    "# This is ~10-50x faster than the previous implementation\n",
    "\n",
    "print(f\"Processing {len(dropping_products):,} products...\")\n",
    "\n",
    "# Enable progress bar for apply\n",
    "tqdm.pandas(desc=\"Processing prices\")\n",
    "\n",
    "# Apply the optimized row processing function\n",
    "result_cols = dropping_products.progress_apply(process_row, axis=1)\n",
    "\n",
    "# Combine original data with results\n",
    "product_final_df = dropping_products.copy()\n",
    "product_final_df['selected_price'] = result_cols['selected_price']\n",
    "product_final_df['source'] = result_cols['source']\n",
    "\n",
    "print(f\"‚úì Processed {len(product_final_df):,} products\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d8794803-3b00-4e59-ba20-2ace1ca777f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "471"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_final_df.district_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb669dd",
   "metadata": {},
   "source": [
    "### 4.3 Calculate Discounts & Filter Results\n",
    "Convert selected prices to discount percentages and apply final filters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d1621a7d-28e5-459c-bf2f-ed32ea7cd170",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "product_final_df['discount'] = abs((product_final_df['selected_price']-product_final_df['price'])/product_final_df['price'])\n",
    "product_final_df = product_final_df[(product_final_df['discount'] > 0.0025)&(product_final_df['selected_price']>0)]\n",
    "product_final_df['discount'] = product_final_df['discount']*100000\n",
    "product_final_df['discount'] = ((product_final_df['discount']//10)+1)/10000\n",
    "product_final_df['discount'] = np.minimum(product_final_df['discount'],0.05)\n",
    "product_final_df['discount']=product_final_df['discount']*100\n",
    "product_final_df['discount'] = product_final_df['discount'].apply(lambda x: f\"{x:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5fde937c-bcf2-416e-ad86-afa34c01ebb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "product_final_df = product_final_df[~product_final_df['cat'].isin(['ŸÉÿ±Ÿàÿ™ ÿ¥ÿ≠ŸÜ'])]\n",
    "product_final_df = product_final_df[~product_final_df['brand'].isin(['ŸÅŸäŸàÿ±Ÿä','ÿßŸÑÿπÿ±Ÿàÿ≥ÿ©'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dc10e321-37e5-42da-9a82-67df9e825627",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìä HAPPY HOUR DISCOUNT ANALYSIS REPORT\n",
      "================================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "1Ô∏è‚É£  OVERVIEW STATISTICS\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üì¶ Total SKU-Warehouse-District combinations: 151,190\n",
      "üì¶ Unique Products: 2,457\n",
      "üè≠ Unique Warehouses: 12\n",
      "üìç Unique Districts: 471\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "2Ô∏è‚É£  DISCOUNT ANALYSIS\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìâ Average Discount: 1.47%\n",
      "üìâ Median Discount: 0.95%\n",
      "üìâ Min Discount: 0.26%\n",
      "üìâ Max Discount: 5.00%\n",
      "üìâ Std Deviation: 1.30%\n",
      "\n",
      "üìä Discount Distribution:\n",
      "     0-1%: 78,995 ( 52.2%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "     1-2%: 34,769 ( 23.0%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "     2-3%: 15,921 ( 10.5%) ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "     3-4%:  9,109 (  6.0%) ‚ñà‚ñà‚ñà\n",
      "     4-5%: 12,396 (  8.2%) ‚ñà‚ñà‚ñà‚ñà\n",
      "      >5%:      0 (  0.0%) \n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "3Ô∏è‚É£  MARGIN ANALYSIS\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìà Average Current Margin: 7.88%\n",
      "üìà Average Margin After Discount: 6.43%\n",
      "üìâ Average Margin Change: -1.45%\n",
      "\n",
      "üìä Margin After Discount Distribution:\n",
      "           <-5%:    135 (  0.1%) \n",
      "      -5 to -4%:    253 (  0.2%) \n",
      "      -4 to -3%:    402 (  0.3%) \n",
      "      -3 to -2%:    325 (  0.2%) \n",
      "      -2 to -1%:    701 (  0.5%) \n",
      "       -1 to 0%:    822 (  0.5%) \n",
      "           0-1%:  1,573 (  1.0%) \n",
      "           1-2%:  3,546 (  2.3%) ‚ñà\n",
      "           2-3%: 11,711 (  7.7%) ‚ñà‚ñà‚ñà\n",
      "           3-4%: 19,369 ( 12.8%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "           4-5%: 19,483 ( 12.9%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "           5-6%: 20,347 ( 13.5%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "           6-7%: 15,213 ( 10.1%) ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "           7-8%: 15,217 ( 10.1%) ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "           8-9%: 13,921 (  9.2%) ‚ñà‚ñà‚ñà‚ñà\n",
      "          9-10%:  8,145 (  5.4%) ‚ñà‚ñà\n",
      "         10-11%:  5,041 (  3.3%) ‚ñà\n",
      "         11-12%:  3,908 (  2.6%) ‚ñà\n",
      "         12-13%:  2,939 (  1.9%) \n",
      "         13-14%:  1,846 (  1.2%) \n",
      "         14-15%:  1,605 (  1.1%) \n",
      "           >15%:  4,688 (  3.1%) ‚ñà\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "4Ô∏è‚É£  ‚ö†Ô∏è  NEGATIVE MARGIN ANALYSIS (CRITICAL)\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üî¥ SKUs with Negative Margin After Discount: 2,637 (1.74%)\n",
      "üî¥ Average Negative Margin: -2.17%\n",
      "üî¥ Worst Negative Margin: -8.55%\n",
      "\n",
      "üìä Negative Margin by Category:\n",
      "                      ŸÖÿ±ŸÇÿ© ŸàÿÆŸÑÿ∑ÿßÿ™: 541.0 SKUs (avg margin: -2.61%)\n",
      "                             ÿ≤ŸäŸàÿ™: 275.0 SKUs (avg margin: -1.32%)\n",
      "                           ŸÖŸÜÿ∏ŸÅÿßÿ™: 250.0 SKUs (avg margin: -0.88%)\n",
      "                   ÿ®ÿ≥ŸÉŸàŸäÿ™ Ÿà ŸÖÿπŸÖŸàŸÑ: 243.0 SKUs (avg margin: -3.70%)\n",
      "                     ÿ≠ŸÑÿßŸàÿ© ÿ∑ÿ≠ŸäŸÜŸäÿ©: 193.0 SKUs (avg margin: -2.09%)\n",
      "                           Ÿàÿ±ŸÇŸäÿßÿ™: 119.0 SKUs (avg margin: -0.89%)\n",
      "                              ÿ¥ÿßŸä: 110.0 SKUs (avg margin: -3.94%)\n",
      "                            ÿ∑ÿ≠ŸäŸÜÿ©: 109.0 SKUs (avg margin: -4.50%)\n",
      "                              ÿπÿ≥ŸÑ: 106.0 SKUs (avg margin: -1.89%)\n",
      "                            ÿ£ŸÑÿ®ÿßŸÜ:  96.0 SKUs (avg margin: -1.27%)\n",
      "\n",
      "üìä Negative Margin by Brand:\n",
      "                             ŸÉŸÜŸàÿ±: 541.0 SKUs (avg margin: -2.61%)\n",
      "                          ÿßŸÑÿ®ŸàÿßÿØŸä: 215.0 SKUs (avg margin: -3.21%)\n",
      "                           ÿ≠ŸÑŸàÿßŸÜŸä: 195.0 SKUs (avg margin: -2.07%)\n",
      "                            ÿßŸàŸÉÿ≥Ÿä: 180.0 SKUs (avg margin: -0.15%)\n",
      "                           ÿ®ÿ≥ŸÉÿßÿ™Ÿà: 168.0 SKUs (avg margin: -4.34%)\n",
      "                            ÿßŸÑÿ∂ÿ≠Ÿâ: 139.0 SKUs (avg margin: -2.00%)\n",
      "                             ÿ≤ŸäŸÜŸá: 119.0 SKUs (avg margin: -0.89%)\n",
      "                             ŸÇŸÑŸäÿ©: 111.0 SKUs (avg margin: -1.17%)\n",
      "                           ŸÑŸäÿ®ÿ™ŸàŸÜ: 100.0 SKUs (avg margin: -4.23%)\n",
      "                    ÿßŸÑŸÖÿ±ÿßÿπŸä ÿßŸÑÿ®ÿßŸÜ:  88.0 SKUs (avg margin: -1.35%)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "5Ô∏è‚É£  ANALYSIS BY CATEGORY\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "Category                                SKUs   Avg Disc    Curr Mrgn     New Mrgn\n",
      "--------------------------------------------------------------------------------\n",
      "ÿ≠ÿßÿ¨Ÿá ÿ≥ÿßŸÇÿπŸá                          14,170.0      1.60%        9.10%        7.61%\n",
      "ÿπÿµÿßŸäÿ±                               12,362.0      1.06%        7.50%        6.51%\n",
      "ŸÖŸÜÿ∏ŸÅÿßÿ™                              11,872.0      1.86%        9.02%        7.27%\n",
      "ÿ®ÿ≥ŸÉŸàŸäÿ™ Ÿà ŸÖÿπŸÖŸàŸÑ                       7,992.0      1.58%        9.78%        8.18%\n",
      "ÿ≠ŸÅÿßÿ∂ÿßÿ™ ÿ£ÿ∑ŸÅÿßŸÑ                         6,362.0      1.32%        7.92%        6.62%\n",
      "ÿ¥ŸàŸÉŸàŸÑÿßÿ™ÿ©                             5,703.0      1.51%        9.28%        7.54%\n",
      "ÿ≠ŸÑŸàŸäÿßÿ™ Ÿà ŸÑÿ®ÿßŸÜ                        5,191.0      1.60%        7.77%        6.22%\n",
      "ŸÇŸáŸàÿ©                                 5,166.0      1.82%        9.19%        7.30%\n",
      "ÿ£ŸÑÿ®ÿßŸÜ                                4,967.0      0.77%        4.27%        3.55%\n",
      "ÿ¨ÿ®ŸÜ                                  4,357.0      1.16%        5.29%        4.16%\n",
      "ŸÖŸÉÿ±ŸàŸÜÿ©                               4,323.0      1.02%        6.16%        5.20%\n",
      "ÿ≤ŸäŸàÿ™                                 4,106.0      1.19%        5.40%        4.30%\n",
      "ÿ¥Ÿäÿ®ÿ≥Ÿä                                4,048.0      1.48%       14.23%       12.94%\n",
      "Ÿàÿ±ŸÇŸäÿßÿ™                               3,935.0      1.36%        8.16%        6.83%\n",
      "ŸÖÿ±ŸÇÿ© ŸàÿÆŸÑÿ∑ÿßÿ™                          3,816.0      2.23%        6.02%        3.71%\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "6Ô∏è‚É£  ANALYSIS BY WAREHOUSE\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "Warehouse ID       SKUs   Avg Discount    Curr Margin     New Margin\n",
      "----------------------------------------------------------------------\n",
      "           1   24,721.0          1.36%          8.09%          6.78%\n",
      "           8   21,641.0          1.39%          7.82%          6.46%\n",
      "         339   16,867.0          1.47%          7.82%          6.37%\n",
      "         170   14,140.0          1.58%          7.91%          6.38%\n",
      "         962   14,112.0          1.44%          7.88%          6.53%\n",
      "         337   14,017.0          1.47%          7.69%          6.26%\n",
      "         236   11,386.0          1.40%          7.93%          6.56%\n",
      "         703    9,946.0          1.52%          7.71%          6.12%\n",
      "         501    7,627.0          1.58%          8.34%          6.68%\n",
      "         797    6,851.0          1.66%          7.47%          5.83%\n",
      "         401    5,875.0          1.67%          7.80%          6.14%\n",
      "         632    4,007.0          1.64%          7.76%          6.14%\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "7Ô∏è‚É£  PRICING SOURCE ANALYSIS\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "Source                     SKUs   Avg Discount     New Margin\n",
      "------------------------------------------------------------\n",
      "Listed                121,636.0          1.11%          6.80%\n",
      "OS                     12,871.0          3.09%          4.74%\n",
      "Zero_rr                12,820.0          2.68%          4.60%\n",
      "induced_1               2,466.0          4.49%          7.01%\n",
      "Prev_disc                 999.0          1.11%          7.02%\n",
      "induced_2                 398.0          2.03%          1.76%\n",
      "\n",
      "================================================================================\n",
      "üìã EXECUTIVE SUMMARY\n",
      "================================================================================\n",
      "\n",
      "\n",
      "                    Metric   Value\n",
      "    Total SKU Combinations 151,190\n",
      "           Unique Products   2,457\n",
      "         Unique Warehouses      12\n",
      "          Unique Districts     471\n",
      "      Average Discount (%)    1.47\n",
      "       Median Discount (%)    0.95\n",
      "Average Current Margin (%)    7.88\n",
      "    Average New Margin (%)    6.43\n",
      "         Margin Impact (%)   -1.45\n",
      " SKUs with Negative Margin   2,637\n",
      "  Negative Margin Rate (%)    1.74\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "üíæ Saving analysis to Excel...\n",
      "‚úÖ Analysis saved to 'Main_V3_HH.xlsx' with multiple sheets!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DISCOUNT & MARGIN ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "# Create analysis dataframe\n",
    "analysis_df = product_final_df.copy()\n",
    "\n",
    "# Convert discount from string to numeric (it's stored as \"2.50\" format)\n",
    "analysis_df['discount_pct'] = pd.to_numeric(analysis_df['discount'], errors='coerce')\n",
    "\n",
    "# Calculate margins\n",
    "analysis_df['current_margin'] = (analysis_df['price'] - analysis_df['wac_p']) / analysis_df['price']\n",
    "analysis_df['new_margin'] = (analysis_df['selected_price'] - analysis_df['wac_p']) / analysis_df['selected_price']\n",
    "analysis_df['margin_change'] = analysis_df['new_margin'] - analysis_df['current_margin']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üìä HAPPY HOUR DISCOUNT ANALYSIS REPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =============================================================================\n",
    "# 1. OVERVIEW STATISTICS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"‚îÄ\" * 80)\n",
    "print(\"1Ô∏è‚É£  OVERVIEW STATISTICS\")\n",
    "print(\"‚îÄ\" * 80)\n",
    "\n",
    "total_skus = len(analysis_df)\n",
    "unique_products = analysis_df['product_id'].nunique()\n",
    "unique_warehouses = analysis_df['warehouse_id'].nunique()\n",
    "unique_districts = analysis_df['district_id'].nunique()\n",
    "\n",
    "print(f\"\\nüì¶ Total SKU-Warehouse-District combinations: {total_skus:,}\")\n",
    "print(f\"üì¶ Unique Products: {unique_products:,}\")\n",
    "print(f\"üè≠ Unique Warehouses: {unique_warehouses:,}\")\n",
    "print(f\"üìç Unique Districts: {unique_districts:,}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. DISCOUNT ANALYSIS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"‚îÄ\" * 80)\n",
    "print(\"2Ô∏è‚É£  DISCOUNT ANALYSIS\")\n",
    "print(\"‚îÄ\" * 80)\n",
    "\n",
    "avg_discount = analysis_df['discount_pct'].mean()\n",
    "median_discount = analysis_df['discount_pct'].median()\n",
    "min_discount = analysis_df['discount_pct'].min()\n",
    "max_discount = analysis_df['discount_pct'].max()\n",
    "std_discount = analysis_df['discount_pct'].std()\n",
    "\n",
    "print(f\"\\nüìâ Average Discount: {avg_discount:.2f}%\")\n",
    "print(f\"üìâ Median Discount: {median_discount:.2f}%\")\n",
    "print(f\"üìâ Min Discount: {min_discount:.2f}%\")\n",
    "print(f\"üìâ Max Discount: {max_discount:.2f}%\")\n",
    "print(f\"üìâ Std Deviation: {std_discount:.2f}%\")\n",
    "\n",
    "# Discount distribution buckets\n",
    "print(\"\\nüìä Discount Distribution:\")\n",
    "discount_bins = [0, 1, 2, 3, 4, 5, 100]\n",
    "discount_labels = ['0-1%', '1-2%', '2-3%', '3-4%', '4-5%', '>5%']\n",
    "analysis_df['discount_bucket'] = pd.cut(analysis_df['discount_pct'], bins=discount_bins, labels=discount_labels, right=True)\n",
    "discount_dist = analysis_df['discount_bucket'].value_counts().sort_index()\n",
    "for bucket, count in discount_dist.items():\n",
    "    pct = (count / total_skus) * 100\n",
    "    bar = \"‚ñà\" * int(pct / 2)\n",
    "    print(f\"   {bucket:>6}: {count:>6,} ({pct:>5.1f}%) {bar}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 3. MARGIN ANALYSIS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"‚îÄ\" * 80)\n",
    "print(\"3Ô∏è‚É£  MARGIN ANALYSIS\")\n",
    "print(\"‚îÄ\" * 80)\n",
    "\n",
    "avg_current_margin = analysis_df['current_margin'].mean() * 100\n",
    "avg_new_margin = analysis_df['new_margin'].mean() * 100\n",
    "avg_margin_change = analysis_df['margin_change'].mean() * 100\n",
    "\n",
    "print(f\"\\nüìà Average Current Margin: {avg_current_margin:.2f}%\")\n",
    "print(f\"üìà Average Margin After Discount: {avg_new_margin:.2f}%\")\n",
    "print(f\"üìâ Average Margin Change: {avg_margin_change:.2f}%\")\n",
    "\n",
    "# Margin distribution after discount (1% increments)\n",
    "print(\"\\nüìä Margin After Discount Distribution:\")\n",
    "margin_bins = [-100, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 100]\n",
    "margin_labels = ['<-5%', '-5 to -4%', '-4 to -3%', '-3 to -2%', '-2 to -1%', '-1 to 0%', \n",
    "                 '0-1%', '1-2%', '2-3%', '3-4%', '4-5%', '5-6%', '6-7%', '7-8%', '8-9%', \n",
    "                 '9-10%', '10-11%', '11-12%', '12-13%', '13-14%', '14-15%', '>15%']\n",
    "analysis_df['margin_bucket'] = pd.cut(analysis_df['new_margin'] * 100, bins=margin_bins, labels=margin_labels, right=True)\n",
    "margin_dist = analysis_df['margin_bucket'].value_counts().sort_index()\n",
    "for bucket, count in margin_dist.items():\n",
    "    if count > 0:  # Only show buckets with data\n",
    "        pct = (count / total_skus) * 100\n",
    "        bar = \"‚ñà\" * int(pct / 2)\n",
    "        print(f\"   {bucket:>12}: {count:>6,} ({pct:>5.1f}%) {bar}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 4. NEGATIVE MARGIN ANALYSIS (CRITICAL)\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"‚îÄ\" * 80)\n",
    "print(\"4Ô∏è‚É£  ‚ö†Ô∏è  NEGATIVE MARGIN ANALYSIS (CRITICAL)\")\n",
    "print(\"‚îÄ\" * 80)\n",
    "\n",
    "negative_margin_df = analysis_df[analysis_df['new_margin'] < 0]\n",
    "negative_margin_count = len(negative_margin_df)\n",
    "negative_margin_pct = (negative_margin_count / total_skus) * 100\n",
    "\n",
    "print(f\"\\nüî¥ SKUs with Negative Margin After Discount: {negative_margin_count:,} ({negative_margin_pct:.2f}%)\")\n",
    "\n",
    "if negative_margin_count > 0:\n",
    "    avg_negative_margin = negative_margin_df['new_margin'].mean() * 100\n",
    "    min_negative_margin = negative_margin_df['new_margin'].min() * 100\n",
    "    print(f\"üî¥ Average Negative Margin: {avg_negative_margin:.2f}%\")\n",
    "    print(f\"üî¥ Worst Negative Margin: {min_negative_margin:.2f}%\")\n",
    "    \n",
    "    # Top categories with negative margins\n",
    "    print(\"\\nüìä Negative Margin by Category:\")\n",
    "    neg_by_cat = negative_margin_df.groupby('cat').agg({\n",
    "        'product_id': 'count',\n",
    "        'new_margin': 'mean'\n",
    "    }).rename(columns={'product_id': 'count', 'new_margin': 'avg_margin'})\n",
    "    neg_by_cat['avg_margin'] = neg_by_cat['avg_margin'] * 100\n",
    "    neg_by_cat = neg_by_cat.sort_values('count', ascending=False).head(10)\n",
    "    for cat, row in neg_by_cat.iterrows():\n",
    "        print(f\"   {cat[:30]:>30}: {row['count']:>5} SKUs (avg margin: {row['avg_margin']:.2f}%)\")\n",
    "    \n",
    "    # Top brands with negative margins\n",
    "    print(\"\\nüìä Negative Margin by Brand:\")\n",
    "    neg_by_brand = negative_margin_df.groupby('brand').agg({\n",
    "        'product_id': 'count',\n",
    "        'new_margin': 'mean'\n",
    "    }).rename(columns={'product_id': 'count', 'new_margin': 'avg_margin'})\n",
    "    neg_by_brand['avg_margin'] = neg_by_brand['avg_margin'] * 100\n",
    "    neg_by_brand = neg_by_brand.sort_values('count', ascending=False).head(10)\n",
    "    for brand, row in neg_by_brand.iterrows():\n",
    "        print(f\"   {brand[:30]:>30}: {row['count']:>5} SKUs (avg margin: {row['avg_margin']:.2f}%)\")\n",
    "else:\n",
    "    print(\"‚úÖ No SKUs with negative margin after discount!\")\n",
    "\n",
    "# =============================================================================\n",
    "# 5. ANALYSIS BY CATEGORY\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"‚îÄ\" * 80)\n",
    "print(\"5Ô∏è‚É£  ANALYSIS BY CATEGORY\")\n",
    "print(\"‚îÄ\" * 80)\n",
    "\n",
    "cat_analysis = analysis_df.groupby('cat').agg({\n",
    "    'product_id': 'count',\n",
    "    'discount_pct': 'mean',\n",
    "    'current_margin': 'mean',\n",
    "    'new_margin': 'mean'\n",
    "}).rename(columns={'product_id': 'sku_count'})\n",
    "cat_analysis['current_margin'] = cat_analysis['current_margin'] * 100\n",
    "cat_analysis['new_margin'] = cat_analysis['new_margin'] * 100\n",
    "cat_analysis = cat_analysis.sort_values('sku_count', ascending=False)\n",
    "\n",
    "print(f\"\\n{'Category':<35} {'SKUs':>8} {'Avg Disc':>10} {'Curr Mrgn':>12} {'New Mrgn':>12}\")\n",
    "print(\"-\" * 80)\n",
    "for cat, row in cat_analysis.head(15).iterrows():\n",
    "    print(f\"{cat[:34]:<35} {row['sku_count']:>8,} {row['discount_pct']:>9.2f}% {row['current_margin']:>11.2f}% {row['new_margin']:>11.2f}%\")\n",
    "\n",
    "# =============================================================================\n",
    "# 6. ANALYSIS BY WAREHOUSE\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"‚îÄ\" * 80)\n",
    "print(\"6Ô∏è‚É£  ANALYSIS BY WAREHOUSE\")\n",
    "print(\"‚îÄ\" * 80)\n",
    "\n",
    "wh_analysis = analysis_df.groupby('warehouse_id').agg({\n",
    "    'product_id': 'count',\n",
    "    'discount_pct': 'mean',\n",
    "    'current_margin': 'mean',\n",
    "    'new_margin': 'mean'\n",
    "}).rename(columns={'product_id': 'sku_count'})\n",
    "wh_analysis['current_margin'] = wh_analysis['current_margin'] * 100\n",
    "wh_analysis['new_margin'] = wh_analysis['new_margin'] * 100\n",
    "wh_analysis = wh_analysis.sort_values('sku_count', ascending=False)\n",
    "\n",
    "print(f\"\\n{'Warehouse ID':>12} {'SKUs':>10} {'Avg Discount':>14} {'Curr Margin':>14} {'New Margin':>14}\")\n",
    "print(\"-\" * 70)\n",
    "for wh_id, row in wh_analysis.iterrows():\n",
    "    print(f\"{wh_id:>12} {row['sku_count']:>10,} {row['discount_pct']:>13.2f}% {row['current_margin']:>13.2f}% {row['new_margin']:>13.2f}%\")\n",
    "\n",
    "# =============================================================================\n",
    "# 7. PRICING SOURCE ANALYSIS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"‚îÄ\" * 80)\n",
    "print(\"7Ô∏è‚É£  PRICING SOURCE ANALYSIS\")\n",
    "print(\"‚îÄ\" * 80)\n",
    "\n",
    "source_analysis = analysis_df.groupby('source').agg({\n",
    "    'product_id': 'count',\n",
    "    'discount_pct': 'mean',\n",
    "    'new_margin': 'mean'\n",
    "}).rename(columns={'product_id': 'sku_count'})\n",
    "source_analysis['new_margin'] = source_analysis['new_margin'] * 100\n",
    "source_analysis = source_analysis.sort_values('sku_count', ascending=False)\n",
    "\n",
    "print(f\"\\n{'Source':<20} {'SKUs':>10} {'Avg Discount':>14} {'New Margin':>14}\")\n",
    "print(\"-\" * 60)\n",
    "for source, row in source_analysis.iterrows():\n",
    "    print(f\"{source:<20} {row['sku_count']:>10,} {row['discount_pct']:>13.2f}% {row['new_margin']:>13.2f}%\")\n",
    "\n",
    "# =============================================================================\n",
    "# 8. SUMMARY TABLE\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìã EXECUTIVE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary_data = {\n",
    "    'Metric': [\n",
    "        'Total SKU Combinations',\n",
    "        'Unique Products',\n",
    "        'Unique Warehouses',\n",
    "        'Unique Districts',\n",
    "        'Average Discount (%)',\n",
    "        'Median Discount (%)',\n",
    "        'Average Current Margin (%)',\n",
    "        'Average New Margin (%)',\n",
    "        'Margin Impact (%)',\n",
    "        'SKUs with Negative Margin',\n",
    "        'Negative Margin Rate (%)'\n",
    "    ],\n",
    "    'Value': [\n",
    "        f\"{total_skus:,}\",\n",
    "        f\"{unique_products:,}\",\n",
    "        f\"{unique_warehouses:,}\",\n",
    "        f\"{unique_districts:,}\",\n",
    "        f\"{avg_discount:.2f}\",\n",
    "        f\"{median_discount:.2f}\",\n",
    "        f\"{avg_current_margin:.2f}\",\n",
    "        f\"{avg_new_margin:.2f}\",\n",
    "        f\"{avg_margin_change:.2f}\",\n",
    "        f\"{negative_margin_count:,}\",\n",
    "        f\"{negative_margin_pct:.2f}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Save analysis to Excel\n",
    "print(\"\\n\" + \"‚îÄ\" * 80)\n",
    "print(\"üíæ Saving analysis to Excel...\")\n",
    "with pd.ExcelWriter('Main_V3_HH.xlsx', engine='openpyxl') as writer:\n",
    "    # Main data\n",
    "    product_final_df.to_excel(writer, sheet_name='Discount_Data', index=False)\n",
    "    \n",
    "    # Summary\n",
    "    summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
    "    \n",
    "    # Category analysis\n",
    "    cat_analysis.reset_index().to_excel(writer, sheet_name='By_Category', index=False)\n",
    "    \n",
    "    # Warehouse analysis\n",
    "    wh_analysis.reset_index().to_excel(writer, sheet_name='By_Warehouse', index=False)\n",
    "    \n",
    "    # Source analysis\n",
    "    source_analysis.reset_index().to_excel(writer, sheet_name='By_Source', index=False)\n",
    "    \n",
    "    # Negative margin SKUs\n",
    "    if negative_margin_count > 0:\n",
    "        negative_margin_df[['product_id', 'sku', 'brand', 'cat', 'warehouse_id', 'district_id', \n",
    "                           'price', 'selected_price', 'wac_p', 'discount_pct', 'current_margin', \n",
    "                           'new_margin']].to_excel(writer, sheet_name='Negative_Margin_SKUs', index=False)\n",
    "\n",
    "print(\"‚úÖ Analysis saved to 'Main_V3_HH.xlsx' with multiple sheets!\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c02aedc7-0bf2-43ea-a23e-71c580ded71e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Created 151,190 product-warehouse-district combinations\n"
     ]
    }
   ],
   "source": [
    "# Include district_id in the tuple for more precise retailer targeting\n",
    "product_final_df['tuple'] = product_final_df[[\"product_id\", 'warehouse_id', 'district_id']].apply(tuple, axis=1)\n",
    "selected_skus_tuple = str(list(product_final_df['tuple']))[1:-1]\n",
    "product_final_df = product_final_df.drop(columns='tuple')\n",
    "print(f\"‚úì Created {len(product_final_df):,} product-warehouse-district combinations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffcb5e8-eb24-42df-bb3b-00944d7a0267",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## 5. Retailer Targeting\n",
    "\n",
    "This section identifies the most relevant retailers for each discounted product based on their purchase history and behavior.\n",
    "\n",
    "### 5.1 Retailer Selection Criteria\n",
    "\n",
    "Retailers are selected based on four behavioral signals:\n",
    "\n",
    "| Signal | Description | Query |\n",
    "|--------|-------------|-------|\n",
    "| **Churned/Dropped** | Previously bought product but stopped (>60% drop) | `churned_dropped` |\n",
    "| **Category Buyer** | Buys category but not this specific product | `cat_not_product` |\n",
    "| **Out of Cycle** | Past purchase cycle exceeded expected timing | `out_of_cycle` |\n",
    "| **Viewed, No Order** | Browsed brand/category but didn't purchase | `view_no_orders` |\n",
    "\n",
    "### 5.2 Churned/Dropped Retailers\n",
    "Find retailers who used to buy the product but have significantly reduced purchases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "83b73dfe-46b7-4727-bf8a-44bac6c38ed8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Churned/dropped retailers: 13,589\n"
     ]
    }
   ],
   "source": [
    "query = f'''\n",
    "with selected_prods as (\n",
    "select * \n",
    "from(\n",
    "VALUES\n",
    "{selected_skus_tuple}\n",
    ")x(product_id, warehouse_id, district_id)\n",
    "),\n",
    "sales_before as (\n",
    "select retailer_id, product_id, warehouse_id, district_id, avg(nmv) as avg_nmv_before\n",
    "from (\n",
    "SELECT DISTINCT\n",
    "    so.id as order_id,\n",
    "    sp.district_id,\n",
    "    sp.warehouse_id as warehouse_id,\n",
    "    pso.product_id as product_id,\n",
    "    so.retailer_id as retailer_id,\n",
    "    sum(pso.total_price) as nmv \n",
    "\n",
    "FROM product_sales_order pso\n",
    "JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "JOIN materialized_views.retailer_polygon on materialized_views.retailer_polygon.retailer_id = so.retailer_id\n",
    "JOIN districts on districts.id = materialized_views.retailer_polygon.district_id\n",
    "JOIN selected_prods sp on sp.product_id = pso.product_id \n",
    "    AND sp.warehouse_id = pso.warehouse_id \n",
    "    AND sp.district_id = districts.id\n",
    "\n",
    "WHERE True\n",
    "    AND so.created_at::date between current_date - 120 and current_date - 31\n",
    "    AND so.sales_order_status_id not in (7, 12)\n",
    "    AND so.channel IN ('telesales', 'retailer')\n",
    "    AND pso.purchased_item_count <> 0\n",
    "\n",
    "GROUP BY ALL\n",
    ")\n",
    "group by all \n",
    "),\n",
    "sales_after as (\n",
    "select retailer_id, product_id, warehouse_id, district_id, avg(nmv) as avg_nmv_after, max(order_date) as last_order\n",
    "from (\n",
    "SELECT DISTINCT\n",
    "    so.id as order_id,\n",
    "    so.created_at::date as order_date,\n",
    "    sales_order_status_id, \n",
    "    sp.district_id,\n",
    "    sp.warehouse_id as warehouse_id,\n",
    "    pso.product_id as product_id,\n",
    "    so.retailer_id as retailer_id,\n",
    "    sum(pso.total_price) as nmv \n",
    "\n",
    "FROM product_sales_order pso\n",
    "JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "JOIN materialized_views.retailer_polygon on materialized_views.retailer_polygon.retailer_id = so.retailer_id\n",
    "JOIN districts on districts.id = materialized_views.retailer_polygon.district_id\n",
    "JOIN selected_prods sp on sp.product_id = pso.product_id \n",
    "    AND sp.warehouse_id = pso.warehouse_id \n",
    "    AND sp.district_id = districts.id\n",
    "\n",
    "WHERE True\n",
    "    AND so.created_at::date > current_date - 31\n",
    "    AND so.sales_order_status_id not in (7, 12)\n",
    "    AND so.channel IN ('telesales', 'retailer')\n",
    "    AND pso.purchased_item_count <> 0\n",
    "\n",
    "GROUP BY ALL\n",
    ")\n",
    "group by all \n",
    "),\n",
    "made_order as (\n",
    "select distinct so.retailer_id\n",
    "\n",
    "FROM sales_orders so \n",
    "JOIN materialized_views.retailer_polygon on materialized_views.retailer_polygon.retailer_id = so.retailer_id\n",
    "JOIN districts on districts.id = materialized_views.retailer_polygon.district_id\n",
    "JOIN selected_prods sp on sp.district_id = districts.id\n",
    "\n",
    "WHERE True\n",
    "    AND so.created_at::date >= current_date - 60\n",
    "    AND so.sales_order_status_id not in (7, 12)\n",
    "    AND so.channel IN ('telesales', 'retailer')\n",
    "\n",
    "GROUP BY ALL\n",
    ")\n",
    "\n",
    "select distinct retailer_id, product_id, warehouse_id, district_id\n",
    "from (\n",
    "select sb.*, coalesce(avg_nmv_after, 0) as nmv_after, (nmv_after - avg_nmv_before) / avg_nmv_before as growth\n",
    "from sales_before sb \n",
    "left join sales_after sa on sb.retailer_id = sa.retailer_id and sb.product_id = sa.product_id and sb.district_id = sa.district_id\n",
    "left join made_order mo on mo.retailer_id = sa.retailer_id \n",
    "where growth < -0.3\n",
    "and (current_date - last_order >= 5 or last_order is null)\n",
    "and mo.retailer_id is not null \n",
    ")\n",
    "'''\n",
    "churned_dropped = snowflake_query(\"Egypt\", query)\n",
    "churned_dropped.columns = churned_dropped.columns.str.lower()\n",
    "for col in churned_dropped.columns:\n",
    "    churned_dropped[col] = pd.to_numeric(churned_dropped[col], errors='ignore')  \n",
    "print(f\"‚úì Churned/dropped retailers: {churned_dropped.retailer_id.nunique():,}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89182c7e",
   "metadata": {},
   "source": [
    "### 5.3 Category Buyers (Not This Product)\n",
    "Find retailers who buy from the same category but haven't purchased this specific product.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b3099392-144d-42d0-8b00-e6c5fbccb5a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Category buyers (not product): 54,430\n"
     ]
    }
   ],
   "source": [
    "query = f'''\n",
    "with selected_prods as (\n",
    "select * \n",
    "from(\n",
    "VALUES\n",
    "{selected_skus_tuple}\n",
    ")x(product_id, warehouse_id, district_id)\n",
    "),\n",
    "selected_prods_with_cat as (\n",
    "select distinct sp.warehouse_id, sp.product_id, sp.district_id, c.name_ar as cat, b.name_ar as brand\n",
    "from selected_prods sp\n",
    "join products p on p.id = sp.product_id\n",
    "join brands b on b.id = p.brand_id \n",
    "join categories c on c.id = p.category_id \n",
    "),\n",
    "selected_dis_cat_brand as (\n",
    "select distinct warehouse_id, district_id, cat\n",
    "from selected_prods_with_cat\n",
    "),\n",
    "\n",
    "buy_cat as (\n",
    "SELECT DISTINCT\n",
    "    sd.district_id,\n",
    "    sd.warehouse_id as warehouse_id,\n",
    "    so.retailer_id as retailer_id,\n",
    "    c.name_ar as cat,\n",
    "    b.name_ar as brand,\n",
    "    pso.product_id\n",
    "\n",
    "FROM product_sales_order pso\n",
    "JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "JOIN materialized_views.retailer_polygon on materialized_views.retailer_polygon.retailer_id = so.retailer_id\n",
    "JOIN districts on districts.id = materialized_views.retailer_polygon.district_id\n",
    "JOIN products p on p.id = pso.product_id\n",
    "JOIN brands b on b.id = p.brand_id \n",
    "JOIN categories c on c.id = p.category_id \n",
    "JOIN selected_dis_cat_brand sd on sd.cat = c.name_ar and sd.district_id = districts.id\n",
    "\n",
    "WHERE True\n",
    "    AND so.created_at::date >= current_date - 60\n",
    "    AND so.sales_order_status_id not in (7, 12)\n",
    "    AND so.channel IN ('telesales', 'retailer')\n",
    "    AND pso.purchased_item_count <> 0\n",
    "),\n",
    "chosen_products as (\n",
    "select sp.*, c.name_ar as cat, b.name_ar as brand\n",
    "from selected_prods sp \n",
    "join products p on p.id = sp.product_id\n",
    "join brands b on b.id = p.brand_id \n",
    "join categories c on c.id = p.category_id \n",
    ")\n",
    "select distinct retailer_id, selected_product_id as product_id, warehouse_id, selected_district_id as district_id\n",
    "from (\n",
    "select warehouse_id, district_id, retailer_id, cat, brand, selected_product_id, selected_district_id, max(flag) as flag\n",
    "from (\n",
    "select bc.*, cp.product_id as selected_product_id, cp.district_id as selected_district_id,\n",
    "    case when cp.product_id = bc.product_id then 1 else 0 end as flag \n",
    "from buy_cat bc \n",
    "left join chosen_products cp on cp.warehouse_id = bc.warehouse_id and cp.cat = bc.cat and cp.district_id = bc.district_id\n",
    ")\n",
    "group by all \n",
    ")\n",
    "where flag = 0 \n",
    "'''\n",
    "cat_not_product = snowflake_query(\"Egypt\", query)\n",
    "cat_not_product.columns = cat_not_product.columns.str.lower()\n",
    "for col in cat_not_product.columns:\n",
    "    cat_not_product[col] = pd.to_numeric(cat_not_product[col], errors='ignore') \n",
    "print(f\"‚úì Category buyers (not product): {cat_not_product.retailer_id.nunique():,}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccf6b8b",
   "metadata": {},
   "source": [
    "### 5.4 Out of Cycle Retailers\n",
    "Find retailers whose regular purchase cycle for this product has expired.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f94759ec-f354-4d3c-a5da-a2114f8d2eb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Out of cycle retailers: 5,949\n"
     ]
    }
   ],
   "source": [
    "query = f'''\n",
    "with selected_prods as (\n",
    "select * \n",
    "from(\n",
    "VALUES\n",
    "{selected_skus_tuple}\n",
    ")x(product_id, warehouse_id, district_id)\n",
    ")\n",
    "select retailer_id, product_id, warehouse_id, district_id\n",
    "from (\n",
    "select *, last_o_date + floor(avg_cycle + (2.5 * std))::int as next_order\n",
    "from(\n",
    "select retailer_id, product_id, warehouse_id, district_id, max(last_o_date) as last_o_date, \n",
    "    sum(order_days * (w / all_w)) as avg_cycle, stddev(order_days) as std\n",
    "from (\n",
    "select *,\n",
    "    max(order_num) over(partition by retailer_id, product_id, district_id) as max_orders,\n",
    "    lag(o_date) over(partition by product_id, retailer_id, district_id order by o_date) as prev_order,\n",
    "    o_date - prev_order as order_days,\n",
    "    case when current_date - o_date = 0 then 1 else 1 / (CURRENT_DATE - o_date) end as w,\n",
    "    sum(w) over(partition by product_id, retailer_id, district_id) as all_w\n",
    "from (\n",
    "SELECT DISTINCT\n",
    "    so.id as order_id,\n",
    "    so.created_at::date as o_date,\n",
    "    sp.district_id,\n",
    "    sp.warehouse_id as warehouse_id,\n",
    "    pso.product_id as product_id,\n",
    "    so.retailer_id as retailer_id,\n",
    "    sum(pso.total_price) as nmv,\n",
    "    row_number() over(partition by so.retailer_id, pso.product_id, sp.district_id order by o_date desc) as order_num,\n",
    "    max(o_date) over(partition by so.retailer_id, pso.product_id, sp.district_id) as last_o_date\n",
    "\n",
    "FROM product_sales_order pso\n",
    "JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "JOIN materialized_views.retailer_polygon on materialized_views.retailer_polygon.retailer_id = so.retailer_id\n",
    "JOIN districts on districts.id = materialized_views.retailer_polygon.district_id\n",
    "JOIN selected_prods sp on sp.product_id = pso.product_id \n",
    "    AND sp.warehouse_id = pso.warehouse_id \n",
    "    AND sp.district_id = districts.id\n",
    "\n",
    "WHERE so.created_at::date >= date_trunc('month', current_date - interval '1 year')\n",
    "    AND so.sales_order_status_id not in (7, 12)\n",
    "    AND so.channel IN ('telesales', 'retailer')\n",
    "    AND pso.purchased_item_count <> 0\n",
    "GROUP BY 1, 2, 3, 4, 5, 6\n",
    ")\n",
    "where last_o_date >= current_date - 60\n",
    "qualify max_orders >= 4\n",
    ")\n",
    "where prev_order is not null \n",
    "group by all\n",
    ")\n",
    "where CURRENT_DATE >= next_order\n",
    ")\n",
    "'''\n",
    "out_of_cycle = snowflake_query(\"Egypt\", query)\n",
    "out_of_cycle.columns = out_of_cycle.columns.str.lower()\n",
    "for col in out_of_cycle.columns:\n",
    "    out_of_cycle[col] = pd.to_numeric(out_of_cycle[col], errors='ignore')  \n",
    "print(f\"‚úì Out of cycle retailers: {out_of_cycle.retailer_id.nunique():,}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c30bf5a",
   "metadata": {},
   "source": [
    "### 5.5 Viewed But Didn't Order\n",
    "Find retailers who viewed the brand/category in the app but didn't complete a purchase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d86d1f3a-57ae-465a-b871-5cde09728450",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì View but no orders retailers: 31,990\n"
     ]
    }
   ],
   "source": [
    "query = f'''\n",
    "with selected_prods as (\n",
    "select * \n",
    "from(\n",
    "VALUES\n",
    "{selected_skus_tuple}\n",
    ")x(product_id, warehouse_id, district_id)\n",
    "),\n",
    "selected_prods_with_brand_cat as (\n",
    "select distinct sp.warehouse_id, sp.district_id, c.id as cat_id, b.id as brand_id, sp.product_id\n",
    "from selected_prods sp\n",
    "join products p on p.id = sp.product_id \n",
    "join brands b on b.id = p.brand_id \n",
    "join categories c on c.id = p.category_id\n",
    "),\n",
    "brand_open as (\n",
    "select        \n",
    "    event_date,\n",
    "    event_timestamp,\n",
    "    vb.retailer_id,\n",
    "    vb.brand_id,\n",
    "    vb.brand_name,\n",
    "    vb.category_id,\n",
    "    c.name_ar as cat_name\n",
    "\n",
    "FROM maxab_events.view_brand vb\n",
    "join categories c on c.id = vb.category_id\n",
    "WHERE event_timestamp::date between CURRENT_DATE - 10 and CURRENT_DATE - 2\n",
    "    AND country LIKE '%Egypt%'\n",
    "    AND user_id LIKE '%EG_retailers_%'\n",
    "    and brand_id <> 'null'\n",
    "),\n",
    "add_to_cart as (\n",
    "SELECT \n",
    "    event_date,\n",
    "    event_timestamp,\n",
    "    uc.retailer_id,\n",
    "    productsid AS product_id,\n",
    "    b.id as brand_id\n",
    "FROM maxab_events.update_cart uc\n",
    "join products p on p.id = uc.productsid \n",
    "join brands b on b.id = p.brand_id \n",
    "WHERE event_timestamp::date between CURRENT_DATE - 10 and CURRENT_DATE - 2\n",
    "    AND country LIKE '%Egypt%'\n",
    "    AND update_type = 'add'\n",
    "    AND user_id LIKE '%EG_retailers_%'\n",
    "    AND productsid REGEXP '^[0-9]+$'\n",
    "),\n",
    "in_stock_retailers as(\n",
    "select distinct retailer_id \n",
    "from sales_orders \n",
    "where sales_order_status_id = 6 \n",
    "and channel in ('retailer', 'telesales')\n",
    "and created_at::date >= date_trunc('month', current_date - interval '6 months')\n",
    "),\n",
    "sales_data as (\n",
    "select so.retailer_id, b.name_ar as brand, c.name_ar as cat, max(so.created_at::date) as o_date\n",
    "from sales_orders so\n",
    "join PRODUCT_SALES_ORDER pso on pso.sales_order_id = so.id \n",
    "join products p on p.id = pso.product_id\n",
    "join brands b on b.id = p.brand_id \n",
    "join categories c on c.id = p.category_id\n",
    "where so.created_at::date >= CURRENT_DATE - 10  \n",
    "and sales_order_status_id not in (7, 12)\n",
    "group by all\n",
    "),\n",
    "cat_brand as (\n",
    "select distinct c.id as cat, b.id as brand \n",
    "from sales_orders so\n",
    "join PRODUCT_SALES_ORDER pso on pso.sales_order_id = so.id \n",
    "join products p on p.id = pso.product_id\n",
    "join brands b on b.id = p.brand_id \n",
    "join categories c on c.id = p.category_id\n",
    "where so.created_at::date >= CURRENT_DATE - 120 \n",
    "and sales_order_status_id not in (7, 12)\n",
    "),\n",
    "main_cte as (\n",
    "select * \n",
    "from (\n",
    "select x.*, case when sd.retailer_id is not null then 1 else 0 end as ordered \n",
    "from (\n",
    "select *, max(event_date) over(partition by retailer_id, brand_id, category_id) as last_event\n",
    "from (\n",
    "select event_date, retailer_id, brand_id, brand_name, category_id,\n",
    "    cat_name, sum(count_n) as total_count\n",
    "from (\n",
    "select bo.*, count(distinct atc.product_id) as count_n\n",
    "from brand_open bo \n",
    "join cat_brand cb on bo.category_id = cb.cat and bo.brand_id = cb.brand\n",
    "join in_stock_retailers isr on isr.retailer_id = bo.retailer_id \n",
    "left join add_to_cart atc on bo.retailer_id = atc.retailer_id and bo.brand_id = atc.brand_id and atc.event_timestamp >= bo.event_timestamp\n",
    "group by all \n",
    ")\n",
    "group by all \n",
    ")\n",
    "qualify event_date = last_event\n",
    ")x \n",
    "left join sales_data sd on sd.retailer_id = x.retailer_id and x.cat_name = sd.cat and x.brand_name = sd.brand and x.event_date <= sd.o_date\n",
    ")\n",
    "where ordered = 0 and total_count = 0 \n",
    ")\n",
    "select distinct m.retailer_id, sp.product_id, sp.warehouse_id, sp.district_id\n",
    "from main_cte m \n",
    "JOIN materialized_views.retailer_polygon on materialized_views.retailer_polygon.retailer_id = m.retailer_id\n",
    "JOIN districts on districts.id = materialized_views.retailer_polygon.district_id\n",
    "JOIN selected_prods_with_brand_cat sp on sp.district_id = districts.id and sp.brand_id = m.brand_id and sp.cat_id = m.category_id\n",
    "'''\n",
    "view_no_orders = snowflake_query(\"Egypt\", query)\n",
    "view_no_orders.columns = view_no_orders.columns.str.lower()\n",
    "for col in view_no_orders.columns:\n",
    "    view_no_orders[col] = pd.to_numeric(view_no_orders[col], errors='ignore')  \n",
    "print(f\"‚úì View but no orders retailers: {view_no_orders.retailer_id.nunique():,}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81c5864",
   "metadata": {},
   "source": [
    "### 5.6 Retailer Exclusions\n",
    "Exclude retailers who are inactive, have recent failed orders, or are wholesale accounts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fb235fb7-ae1a-4ef5-9051-343bad49fa91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Excluded retailers: 125,126\n"
     ]
    }
   ],
   "source": [
    "query = f'''\n",
    "select retailer_id\n",
    "from (\n",
    "SELECT  DISTINCT\n",
    "retailer_id,\n",
    "sales_order_status_id,\n",
    "created_at::date as o_date ,\n",
    "max(o_date)over(partition by retailer_id) as last_order\n",
    "from sales_orders so \n",
    "WHERE  so.created_at ::date >= current_date - 120\n",
    "AND so.sales_order_status_id not in (7,12)\n",
    "AND so.channel IN ('telesales','retailer')\n",
    "qualify o_date = last_order\n",
    ")\n",
    "where sales_order_status_id not in (6,9,12)\n",
    "union all \n",
    "select id as retailer_id \n",
    "from retailers \n",
    "where activation = 'false'\n",
    "union all \n",
    "select distinct dta.TAGGABLE_ID as retailer_id\n",
    "from DYNAMIC_TAGS dt \n",
    "join dynamic_taggables dta on dt.id = dta.dynamic_tag_id \n",
    "where name like '%whole_sale%'\n",
    "and dt.id > 3000\n",
    "union all \n",
    "select distinct f.value::int as retailer_id \n",
    "from SKU_DISCOUNTS sd,\n",
    "LATERAL FLATTEN(\n",
    "    input => SPLIT(\n",
    "        REPLACE(REPLACE(REPLACE(sd.retailer_ids, '{{', ''), '}}', ''), '\"', ''),\n",
    "        ','\n",
    "    )\n",
    ") f\n",
    "where active = 'true'\n",
    "and CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMEstamp()) between start_at and end_at\n",
    "\n",
    "'''\n",
    "exec_rets = snowflake_query(\"Egypt\", query)\n",
    "exec_rets.columns = exec_rets.columns.str.lower()\n",
    "for col in exec_rets.columns:\n",
    "    exec_rets[col] = pd.to_numeric(exec_rets[col], errors='ignore') \n",
    "exec_rets = exec_rets.retailer_id.unique() \n",
    "print(f\"‚úì Excluded retailers: {len(exec_rets):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce446f2",
   "metadata": {},
   "source": [
    "### 5.7 Active Quantity Discounts\n",
    "Check for existing quantity discounts to avoid conflicts with SKU discounts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6d0332ed-b0be-49f7-bd28-029df48d7e87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Found 7,469,627 active quantity discounts\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    query = f'''\n",
    "    SELECT DISTINCT\n",
    "        qdv.product_id,\n",
    "        qd.dynamic_tag_id AS tag_id\n",
    "    FROM quantity_discounts qd\n",
    "    JOIN quantity_discount_values qdv \n",
    "        ON qd.id = qdv.quantity_discount_id\n",
    "    WHERE ((CURRENT_TIMESTAMP AT TIME ZONE 'Africa/Cairo'\n",
    "          BETWEEN qd.start_at AND qd.end_at) or ((qd.start_at::date = current_date) and (CURRENT_TIMESTAMP AT TIME ZONE 'Africa/Cairo' < qd.start_at)))\n",
    "    AND qd.active = TRUE\n",
    "    '''\n",
    "    quantity_data =  setup_environment_2.dwh_pg_query(query, columns = ['product_id','tag_id'])\n",
    "    quantity_data.columns = quantity_data.columns.str.lower()\n",
    "    for col in quantity_data.columns:\n",
    "        quantity_data[col] = pd.to_numeric(quantity_data[col], errors='ignore')     \n",
    "\n",
    "    qd_data = quantity_data.copy()[['tag_id']].drop_duplicates()\n",
    "    qd_data['tuple'] = \"(\"+qd_data['tag_id'].astype(str)+\")\"\n",
    "    qd_data = qd_data['tuple'].unique()\n",
    "    qd_list = ''\n",
    "    for c in qd_data:\n",
    "        qd_list = qd_list+c+\",\"\n",
    "    qd_list = qd_list[:-1]\n",
    "\n",
    "    query = f'''\n",
    "    with tags as (\n",
    "    select *\n",
    "    from(\n",
    "    values\n",
    "    {qd_list}\n",
    "    )x(dynamic_tag_id)\n",
    "\n",
    "    )\n",
    "\n",
    "    select tags.dynamic_tag_id as tag_id,taggable_id as retailer_id\n",
    "    from dynamic_taggables dt  \n",
    "    join tags on tags.dynamic_tag_id = dt.dynamic_tag_id\n",
    "    '''\n",
    "    qd_rets = snowflake_query(\"Egypt\", query)\n",
    "    for col in qd_rets.columns:\n",
    "        qd_rets[col] = pd.to_numeric(qd_rets[col], errors='ignore')  \n",
    "\n",
    "    quantity_data = quantity_data.merge(qd_rets, on='tag_id')\n",
    "    quantity_data['have_quantity'] = 1\n",
    "    print(f\"‚úì Found {len(quantity_data):,} active quantity discounts\")\n",
    "except:\n",
    "    quantity_data = pd.DataFrame(columns=['product_id', 'tag_id', 'retailer_id', 'have_quantity'])\n",
    "    print(\"‚ö† No active quantity discounts found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "267d24de-8a68-45ae-a9c6-f8d7a9c25385",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded 34,810 packing unit mappings\n"
     ]
    }
   ],
   "source": [
    "# Fetch packing unit mappings for discount output formatting\n",
    "query = '''\n",
    "SELECT DISTINCT product_id, packing_unit_id \n",
    "FROM packing_unit_products\n",
    "WHERE product_id <> 1309 OR (product_id = 1309 AND packing_unit_id <> 23)\n",
    "'''\n",
    "pus = snowflake_query(\"Egypt\", query)\n",
    "for col in pus.columns:\n",
    "    pus[col] = pd.to_numeric(pus[col], errors='ignore')\n",
    "print(f\"‚úì Loaded {len(pus):,} packing unit mappings\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7881b02e-8f82-45aa-8a92-c25a1fbdbde6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded warehouse history for 114,708 retailers\n"
     ]
    }
   ],
   "source": [
    "query ='''\n",
    "select retailer_id,warehouse_id,1 as last_wh \n",
    "from (\n",
    "SELECT  DISTINCT\n",
    "\t\tso.retailer_id,\n",
    "\t\tpso.warehouse_id,\n",
    "\t\tso.created_at::date as o_date,\n",
    "\t\tmax(so.created_at::date) over(partition by so.retailer_id) as max_date\n",
    "\n",
    "FROM product_sales_order pso\n",
    "JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "JOIN products on products.id=pso.product_id\n",
    "JOIN brands on products.brand_id = brands.id \n",
    "JOIN categories ON products.category_id = categories.id\n",
    "JOIN finance.all_cogs f  ON f.product_id = pso.product_id\n",
    "                        AND f.from_date::date <= so.created_at ::date\n",
    "                        AND f.to_date::date > so.created_at ::date\n",
    "JOIN product_units ON product_units.id = products.unit_id  \n",
    "\n",
    "\n",
    "WHERE  so.created_at::date >= current_date - 365\n",
    "    AND so.sales_order_status_id not in (7,12)\n",
    "    AND so.channel IN ('telesales','retailer')\n",
    "    AND pso.purchased_item_count <> 0\n",
    "\tand pso.warehouse_id in (1,8,170,236,337,339,401,501,632,703,797,962)\n",
    "\n",
    "GROUP BY 1,2,3\n",
    "qualify o_date = max_date\n",
    ")\n",
    "'''\n",
    "ret_wh = snowflake_query(\"Egypt\", query)\n",
    "for col in ret_wh.columns:\n",
    "    ret_wh[col] = pd.to_numeric(ret_wh[col], errors='ignore')\n",
    "print(f\"‚úì Loaded warehouse history for {ret_wh.retailer_id.nunique():,} retailers\")       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e524473",
   "metadata": {},
   "source": [
    "### 5.8 Combine & Filter Retailers\n",
    "Combine all retailer segments and apply final filters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3eaa5f5a-d837-4199-b51e-5104a50c66e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Total unique retailers: 53,873\n"
     ]
    }
   ],
   "source": [
    "# Combine all retailer sources - now including district_id\n",
    "all_retailers = pd.concat([cat_not_product, churned_dropped]).drop_duplicates().reset_index(drop=True)\n",
    "all_retailers = pd.concat([all_retailers, out_of_cycle]).drop_duplicates().reset_index(drop=True)\n",
    "all_retailers = pd.concat([all_retailers, view_no_orders]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Merge with last warehouse info\n",
    "all_retailers = all_retailers.merge(ret_wh, on=['retailer_id', 'warehouse_id'], how='left')\n",
    "all_retailers = all_retailers.fillna(0)\n",
    "\n",
    "# Rank and filter\n",
    "all_retailers['rank'] = all_retailers.groupby(['retailer_id'])['last_wh'].rank(method='dense', ascending=False).astype(int)\n",
    "all_retailers = all_retailers[all_retailers['rank'] == 1]\n",
    "all_retailers = all_retailers[~(all_retailers['retailer_id'].isin(exec_rets))]\n",
    "\n",
    "print(f\"‚úì Total unique retailers: {all_retailers.retailer_id.nunique():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "763733c2-12c8-477b-9388-1ab5790674c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Final retailer-product combinations: 6,507,552\n"
     ]
    }
   ],
   "source": [
    "# Select required columns including district_id\n",
    "product_final_df = product_final_df[['product_id', 'warehouse_id', 'district_id', 'discount']]\n",
    "\n",
    "# Merge with retailers - now matching on district_id as well for precision\n",
    "final_df = product_final_df.merge(\n",
    "    all_retailers[['warehouse_id', 'product_id', 'district_id', 'retailer_id']], \n",
    "    on=['warehouse_id', 'product_id', 'district_id']\n",
    ")\n",
    "\n",
    "# Filter out retailers with active quantity discounts\n",
    "final_df = final_df.merge(quantity_data, on=['retailer_id', 'product_id'], how='left')\n",
    "final_df = final_df[final_df['have_quantity'].isna()]\n",
    "\n",
    "print(f\"‚úì Final retailer-product combinations: {len(final_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fb7e1063-41b1-442a-8a58-adc91cee1790",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53723"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.retailer_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ca241b-2efe-4ef0-90e3-110bf56e4efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_df = final_df.groupby(['product_id', 'warehouse_id', 'district_id', 'retailer_id'])['discount'].min().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f708071a-a397-4b0a-a763-fcef6fd69174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>warehouse_id</th>\n",
       "      <th>district_id</th>\n",
       "      <th>discount</th>\n",
       "      <th>retailer_id</th>\n",
       "      <th>tag_id</th>\n",
       "      <th>have_quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2879090</th>\n",
       "      <td>11643</td>\n",
       "      <td>797</td>\n",
       "      <td>669</td>\n",
       "      <td>0.26</td>\n",
       "      <td>244804</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5852594</th>\n",
       "      <td>10939</td>\n",
       "      <td>1</td>\n",
       "      <td>587</td>\n",
       "      <td>0.26</td>\n",
       "      <td>509221</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5852595</th>\n",
       "      <td>10939</td>\n",
       "      <td>1</td>\n",
       "      <td>587</td>\n",
       "      <td>0.26</td>\n",
       "      <td>286374</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5852596</th>\n",
       "      <td>10939</td>\n",
       "      <td>1</td>\n",
       "      <td>587</td>\n",
       "      <td>0.26</td>\n",
       "      <td>769990</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5852597</th>\n",
       "      <td>10939</td>\n",
       "      <td>1</td>\n",
       "      <td>587</td>\n",
       "      <td>0.26</td>\n",
       "      <td>99843</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3833358</th>\n",
       "      <td>10466</td>\n",
       "      <td>962</td>\n",
       "      <td>580</td>\n",
       "      <td>5.00</td>\n",
       "      <td>165276</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3833359</th>\n",
       "      <td>10466</td>\n",
       "      <td>962</td>\n",
       "      <td>580</td>\n",
       "      <td>5.00</td>\n",
       "      <td>698574</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3833360</th>\n",
       "      <td>10466</td>\n",
       "      <td>962</td>\n",
       "      <td>580</td>\n",
       "      <td>5.00</td>\n",
       "      <td>587615</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3833353</th>\n",
       "      <td>10466</td>\n",
       "      <td>962</td>\n",
       "      <td>580</td>\n",
       "      <td>5.00</td>\n",
       "      <td>104371</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3977378</th>\n",
       "      <td>13028</td>\n",
       "      <td>632</td>\n",
       "      <td>1257</td>\n",
       "      <td>5.00</td>\n",
       "      <td>243967</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6475230 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         product_id  warehouse_id  district_id discount  retailer_id  tag_id  \\\n",
       "2879090       11643           797          669     0.26       244804     NaN   \n",
       "5852594       10939             1          587     0.26       509221     NaN   \n",
       "5852595       10939             1          587     0.26       286374     NaN   \n",
       "5852596       10939             1          587     0.26       769990     NaN   \n",
       "5852597       10939             1          587     0.26        99843     NaN   \n",
       "...             ...           ...          ...      ...          ...     ...   \n",
       "3833358       10466           962          580     5.00       165276     NaN   \n",
       "3833359       10466           962          580     5.00       698574     NaN   \n",
       "3833360       10466           962          580     5.00       587615     NaN   \n",
       "3833353       10466           962          580     5.00       104371     NaN   \n",
       "3977378       13028           632         1257     5.00       243967     NaN   \n",
       "\n",
       "         have_quantity  \n",
       "2879090            NaN  \n",
       "5852594            NaN  \n",
       "5852595            NaN  \n",
       "5852596            NaN  \n",
       "5852597            NaN  \n",
       "...                ...  \n",
       "3833358            NaN  \n",
       "3833359            NaN  \n",
       "3833360            NaN  \n",
       "3833353            NaN  \n",
       "3977378            NaN  \n",
       "\n",
       "[6475230 rows x 7 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = final_df.sort_values('discount').drop_duplicates(\n",
    "    subset=['product_id', 'warehouse_id', 'district_id', 'retailer_id']\n",
    "    \n",
    ")\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c78943",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Output Generation\n",
    "\n",
    "This section prepares the discount data for upload to the pricing system.\n",
    "\n",
    "### 6.1 Prepare Discount Data\n",
    "Format the discount information for each retailer-product combination.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1c5a0150-0259-41e8-ae02-e718471e17b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_df = final_df.merge(pus,on='product_id')\n",
    "final_df= final_df.drop_duplicates()\n",
    "final_df['HH_data'] = '['+(final_df['product_id']).astype(str)+','+(final_df['packing_unit_id']).astype(str)+','+(final_df['discount']).astype(str)+']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5a1a9c82-66c1-4815-8a83-fef5855b2784",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/01/2026 14:32 22/01/2026 12:59\n"
     ]
    }
   ],
   "source": [
    "slots = ['0-12','13-17','18-23']\n",
    "local_tz = pytz.timezone('Africa/Cairo')\n",
    "current_hour = datetime.now(local_tz).hour\n",
    "chosen_slot = [np.nan,np.nan]\n",
    "\n",
    "for slot in slots:\n",
    "    parts = slot.split(\"-\")\n",
    "    if(current_hour >= int(parts[0]) and current_hour < int(parts[1])):\n",
    "        chosen_slot[0] = int(parts[0]) \n",
    "        chosen_slot[1] = int(parts[1]) \n",
    "        break\n",
    "    else:\n",
    "        chosen_slot[0] = 0\n",
    "        chosen_slot[1] = 0 \n",
    "        \n",
    "today = datetime.now(local_tz)\n",
    "start_hour = np.maximum(current_hour,chosen_slot[0])\n",
    "if(start_hour==current_hour):\n",
    "    if ((datetime.now(local_tz).minute) +10) <60:\n",
    "        start_mins =  ((datetime.now(local_tz).minute) +10)\n",
    "    else:\n",
    "        start_mins =  ((datetime.now(local_tz).minute) +10)-60\n",
    "else:\n",
    "    start_mins = 30 \n",
    "if ((datetime.now(local_tz).minute) +10) > 60:\n",
    "    start_hour =start_hour+1\n",
    "    \n",
    "start_date = (today.replace(hour=start_hour, minute=start_mins, second=0, microsecond=0)+ timedelta(minutes=0)).strftime('%d/%m/%Y %H:%M')\n",
    "end_date = ((today+ timedelta(days=1)).replace(hour=12, minute=59, second=0, microsecond=0)).strftime('%d/%m/%Y %H:%M')\n",
    "print(start_date,end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "11720849-a741-4bb3-b6b1-377981796a7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_df  = final_df.groupby('retailer_id')['HH_data'].apply(list).reset_index()\n",
    "output_df['Discounts']= output_df['HH_data'].astype(str).str.replace(\"'\",'').str.replace(' ','')\n",
    "output_df = output_df.groupby('Discounts')['retailer_id'].agg(list).reset_index()\n",
    "output_df['Arabic Offer Name']= 'ÿÆÿµŸàŸÖÿßÿ™ ÿ≠ÿµÿ±Ÿäÿ©'\n",
    "output_df['Start Date/Time'] = start_date\n",
    "output_df['End Date/Time'] = end_date\n",
    "output_df = output_df[['retailer_id','Start Date/Time','End Date/Time','Discounts','Arabic Offer Name']]\n",
    "output_df['French Offer Name']=np.nan\n",
    "output_df['English Offer Name']=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c800dca1-7dc2-480f-b84c-99f8a011fdf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for i,row in output_df.iterrows():\n",
    "    \n",
    "    start_date = row['Start Date/Time']\n",
    "    end_date = row['End Date/Time']\n",
    "    retailers = row['retailer_id']\n",
    "    discount = row['Discounts']\n",
    "    name = row['Arabic Offer Name'] \n",
    "    name_f = row['French Offer Name'] \n",
    "    name_e = row['English Offer Name'] \n",
    "    \n",
    "    length = len(retailers)\n",
    "    if(length>100):\n",
    "        iters = length//100\n",
    "        remaining = length%100\n",
    "        for j in range(0,iters+1):\n",
    "            if(j<=iters):\n",
    "                start = (j*100)\n",
    "                end = (j+1)*100\n",
    "                rets = retailers[start:end]\n",
    "                data.append({'Discounts':discount,'retailer_id':rets,'Start Date/Time':start_date,'End Date/Time':end_date\n",
    "                            ,'Arabic Offer Name':name,'French Offer Name':name_f,'English Offer Name':name_e})\n",
    "            else:\n",
    "                print(\"else new\")\n",
    "            \n",
    "    else:\n",
    "        data.append({'Discounts':discount,'retailer_id':retailers,'Start Date/Time':start_date,'End Date/Time':end_date\n",
    "                            ,'Arabic Offer Name':name,'French Offer Name':name_f,'English Offer Name':name_e})\n",
    "        \n",
    "dfx = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e3df721b-5e11-4f6d-92e8-c8753065a46b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfx['English Offer Name'] = 'Special Discounts'\n",
    "dfx['Swahili Offer Name'] = ''\n",
    "dfx['Rwandan Offer Name'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "215ee4e4-20f4-4a70-a7de-e7a359e64f23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_added = dfx.iloc[0, :].to_frame().T\n",
    "df_added['retailer_id'] = \"[111780,114210]\"\n",
    "dfx = pd.concat([dfx,df_added])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576386cc",
   "metadata": {},
   "source": [
    "### 6.2 Generate Excel Files\n",
    "Split the output into multiple Excel files for batch upload.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1848d7bc-675b-496f-bf88-e7514da46898",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Moved: o_happy_hour_2026-01-19_NO._23.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._47.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._7.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._28.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._46.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._26.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._0.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._14.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._12.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._6.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._30.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._22.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._19.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._41.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._39.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._4.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._5.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._38.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._44.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._27.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._29.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._43.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._42.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._17.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._3.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._15.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._35.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._33.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._34.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._16.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._37.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._21.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._13.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._1.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._49.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._8.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._20.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._18.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._48.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._25.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._31.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._9.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._32.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._24.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._36.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._45.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._40.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._11.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._10.xlsx\n",
      "‚úì Moved: o_happy_hour_2026-01-19_NO._2.xlsx\n",
      "\n",
      "Summary: 50 files moved, 0 errors\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def move_all_files(source_dir, dest_dir):\n",
    "    \"\"\"Copy files to destination and delete from source\"\"\"\n",
    "    # Create destination directory if it doesn't exist\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "    \n",
    "    source = Path(source_dir)\n",
    "    destination = Path(dest_dir)\n",
    "    \n",
    "    moved_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    for file in source.iterdir():\n",
    "        if file.is_file():\n",
    "            try:\n",
    "                # Copy file to destination\n",
    "                dest_file = destination / file.name\n",
    "                #shutil.copy2(file, dest_file)  # copy2 preserves metadata\n",
    "                \n",
    "                # Delete from source\n",
    "                file.unlink()\n",
    "                \n",
    "                print(f\"‚úì Moved: {file.name}\")\n",
    "                moved_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"‚úó Error moving {file.name}: {e}\")\n",
    "                error_count += 1\n",
    "    \n",
    "    print(f\"\\nSummary: {moved_count} files moved, {error_count} errors\")\n",
    "\n",
    "# Usage\n",
    "move_all_files('HH_Sheets', 'HH_temp_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8ab3de1c-12ca-42f6-a0fd-b11c82835f9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:13<00:00,  3.68it/s]\n"
     ]
    }
   ],
   "source": [
    "dfx.rename(columns={'retailer_id': 'Retailers List'}, inplace=True)\n",
    "dfx = dfx[['Retailers List','Start Date/Time','End Date/Time','Discounts','Arabic Offer Name','French Offer Name','English Offer Name','Swahili Offer Name','Rwandan Offer Name']]\n",
    "# 500 row per sheet \n",
    "final=dfx.reset_index().drop(columns='index')\n",
    "mino=final.index.min()\n",
    "maxo=final.index.max()\n",
    "ran = [i for i in range(mino,maxo,1000)]\n",
    "for i in tqdm(range(len(ran))):\n",
    "    if i+1 == len(ran):\n",
    "        val1 = ran[i]\n",
    "        val2 = maxo\n",
    "    else:\n",
    "        val1 = ran[i]\n",
    "        val2 = ran[i+1] - 1\n",
    "    x=final.loc[val1:val2,:]\n",
    "    x.to_excel(f'HH_Sheets/o_happy_hour_{str((datetime.now()).date())}_NO._{i}.xlsx'.format(i),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1754b81e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. API Upload\n",
    "\n",
    "This section handles the automated upload of discount files to the MaxAB pricing system.\n",
    "\n",
    "### 7.1 API Authentication & Helper Functions\n",
    "Define functions for authenticating with the MaxAB API and uploading discount files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9aec4354-8439-44c2-8be7-09da1cc156dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_secret(secret_name):\n",
    "    region_name = \"us-east-1\"\n",
    "\n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "    # In this sample we only handle the specific exceptions for the 'GetSecretValue' API.\n",
    "    # See https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html\n",
    "    # We rethrow the exception by default.\n",
    "\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(SecretId=secret_name)\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'DecryptionFailureException':\n",
    "            # Secrets Manager can't decrypt the protected secret text using the provided KMS key.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InternalServiceErrorException':\n",
    "            # An error occurred on the server side.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidParameterException':\n",
    "            # You provided an invalid value for a parameter.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidRequestException':\n",
    "            # You provided a parameter value that is not valid for the current state of the resource.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'ResourceNotFoundException':\n",
    "            # We can't find the resource that you asked for.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "    else:\n",
    "        # Decrypts secret using the associated KMS CMK.\n",
    "        # Depending on whether the secret is a string or binary, one of these fields will be populated.\n",
    "        if 'SecretString' in get_secret_value_response:\n",
    "            return get_secret_value_response['SecretString']\n",
    "        else:\n",
    "            return base64.b64decode(get_secret_value_response['SecretBinary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "11c0205e-fd0f-4fe1-bd11-58030e5f68b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pricing_api_secret = json.loads(get_secret(\"prod/pricing/api/\"))\n",
    "username = pricing_api_secret[\"egypt_username\"]\n",
    "password = pricing_api_secret[\"egypt_password\"]\n",
    "secret = pricing_api_secret[\"egypt_secret\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a5e32466-b1b1-455c-a574-7e6351d79dcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_access_token(url, client_id, client_secret):\n",
    "    \"\"\"\n",
    "    get_access_token function takes three parameters and returns a session token\n",
    "    to connect to MaxAB APIs\n",
    "\n",
    "    :param url: production MaxAB token URL\n",
    "    :param client_id: client ID\n",
    "    :param client_secret: client sercret\n",
    "    :return: session token\n",
    "    \"\"\"\n",
    "    response = requests.post(\n",
    "        url,\n",
    "        data={\"grant_type\": \"password\",\n",
    "              \"username\": username,\n",
    "              \"password\": password},\n",
    "        auth=(client_id, client_secret),\n",
    "    )\n",
    "    return response.json()[\"access_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "58084f7e-13a5-4b66-86f9-ca72e08801c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preassigned_url():\n",
    "    token = get_access_token('https://sso.maxab.info/auth/realms/maxab/protocol/openid-connect/token',\n",
    "                             'main-system-externals',\n",
    "                             secret)\n",
    "    url = \"https://api.maxab.info/commerce/api/admins/v1/bulk-upload/presigned-url?type=SKU_DISCOUNTS\"\n",
    "    payload={}\n",
    "    headers = {\n",
    "      'Authorization': 'bearer {}'.format(token)}\n",
    "\n",
    "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "83ee4313-5124-47a7-a5ef-2d16d8833c4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def upload_sku_discount(file_name,new_url):\n",
    "    url = new_url\n",
    "    headers = {'Content-Type':'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'}\n",
    "    with open(file_name, 'rb') as f:\n",
    "        response = requests.put(new_url, data=f, headers=headers)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7747642f-865f-40bf-8949-2099d434782c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validate_skus_discount(key):\n",
    "    token = get_access_token('https://sso.maxab.info/auth/realms/maxab/protocol/openid-connect/token',\n",
    "                             'main-system-externals',\n",
    "                             secret)\n",
    "    url = 'https://api.maxab.info/commerce/api/admins/v1/bulk-upload/sheets/validate'\n",
    "    headers = {\n",
    "        'Authorization': 'bearer {}'.format(token),\n",
    "        'content-type':'application/json'\n",
    "\n",
    "       }\n",
    "    payload={\"fileName\":key,\"sheetType\":\"SKU_DISCOUNTS\"}\n",
    "    response = requests.request(\"POST\", url, headers=headers, json=payload)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8a92b90c-6cdf-411a-a280-cc08336bbeb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def proceed(key):\n",
    "    token = get_access_token('https://sso.maxab.info/auth/realms/maxab/protocol/openid-connect/token',\n",
    "                             'main-system-externals',\n",
    "                             secret)\n",
    "    url = f'https://api.maxab.info/commerce/api/admins/v1/bulk-upload/sheets/proceed/{key}?uploadType=SKU_DISCOUNTS'\n",
    "    headers = {\n",
    "        'Authorization': 'bearer {}'.format(token),\n",
    "        'content-type':'application/json'\n",
    "       }\n",
    "    response = requests.request(\"POST\", url, headers=headers)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bfd128ad-a322-4240-8881-d20fd9fe65b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def listing():\n",
    "    token = get_access_token('https://sso.maxab.info/auth/realms/maxab/protocol/openid-connect/token',\n",
    "                             'main-system-externals',\n",
    "                             secret)\n",
    "    url = 'https://api.maxab.info/commerce/api/admins/v1/bulk-upload/sheets?filter=sheetType=in=(SKU_DISCOUNTS,EDIT_SKU_DISCOUNTS);status!=DELETED&limit=1'\n",
    "    headers = {\n",
    "        'Authorization': 'bearer {}'.format(token),\n",
    "        'content-type':'application/json'\n",
    "       }\n",
    "    response = requests.request(\"GET\", url, headers=headers)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "eff346a4-8a8f-4338-8273-915150d1a7bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def sku_discount_upload_func(file_name):\n",
    "#     pre_data = preassigned_url().json()\n",
    "#     key = pre_data['key']\n",
    "#     new_url = pre_data['preSignedUrl']\n",
    "#     upload_sku_discount(file_name,new_url)\n",
    "#     validation_data = validate_skus_discount(key)\n",
    "#     #print('validate: ',validation_data)\n",
    "#     proceed_data = proceed(key)\n",
    "#     # print('proceed:',proceed_data)\n",
    "#     try:\n",
    "#         if proceed_data.ok and validation_data.ok:\n",
    "#             print('Passed')\n",
    "#         else:\n",
    "#             print('Failed')\n",
    "#     except:\n",
    "#         print(\"error\")\n",
    "# files = [f for f in os.listdir('HH_Sheets') if os.path.isfile(os.path.join('HH_Sheets', f))]\n",
    "# for file in files:\n",
    "#     print(file)\n",
    "#     sku_discount_upload_func('HH_Sheets/'+file)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b8fdfafa-13eb-4d12-a3ac-c6359ce6526a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def update_delivery_fees(token, delivery_fees_data):\n",
    "\n",
    "#     url = 'https://api.maxab.info/commerce/api/admins/v1/delivery-fees'\n",
    "    \n",
    "#     headers = {\n",
    "#         'Authorization': f'Bearer {token}',\n",
    "#         'Content-Type': 'application/json'\n",
    "#     }\n",
    "    \n",
    "#     response = requests.post(url, headers=headers, json=delivery_fees_data)\n",
    "    \n",
    "#     return response\n",
    "\n",
    "\n",
    "# # Usage\n",
    "# token = get_access_token('https://sso.maxab.info/auth/realms/maxab/protocol/openid-connect/token',\n",
    "#                              'main-system-externals',\n",
    "#                              secret)\n",
    "\n",
    "# data = [\n",
    "#      {\n",
    "#     \"dynamic_tag_id\":3154 ,\n",
    "#     \"delivery_fees\": 349,\n",
    "#     \"ticket_size\": 1000000\n",
    "        \n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# response = update_delivery_fees(token, data)\n",
    "# print(f\"Status Code: {response.status_code}\")\n",
    "# print(f\"Response: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45754c52",
   "metadata": {},
   "source": [
    "### 7.2 Execute Batch Upload\n",
    "Upload all generated discount files to the pricing system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c6b4dc3d-7184-433d-ba5d-b9c377c7b766",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def delete_delivery_fees(token,data):\n",
    "\n",
    "#     url = 'https://api.maxab.info/commerce/api/admins/v1/delivery-fees'\n",
    "    \n",
    "#     headers = {\n",
    "#         'Authorization': f'Bearer {token}',\n",
    "#         'Content-Type': 'application/json'\n",
    "#     }\n",
    "    \n",
    "#     response = requests.delete(url, headers=headers, json=data)\n",
    "    \n",
    "#     return response\n",
    "# data = {\n",
    "#   \"deliveryFeesIds\": [268,269,273,272,270,271]\n",
    "# }\n",
    "# token = get_access_token('https://sso.maxab.info/auth/realms/maxab/protocol/openid-connect/token',\n",
    "#                              'main-system-externals',\n",
    "#                              secret)\n",
    "# response = delete_delivery_fees(token,data)\n",
    "# print(f\"Status Code: {response.status_code}\")\n",
    "# print(f\"Response: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d47258-7d2f-4657-b4dd-e0b7818af06a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "21097779-7ed5-42ef-9e47-84caa5750f98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "SKU DISCOUNT BATCH UPLOAD\n",
      "######################################################################\n",
      "üìÇ Directory: HH_Sheets\n",
      "‚è∞ Started: 2026-01-21 12:22:54\n",
      "üìä Found 51 file(s) to process\n",
      "\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 1/51: o_happy_hour_2026-01-21_NO._42.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._42.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-22-54-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._42.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 2/51: o_happy_hour_2026-01-21_NO._29.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._29.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-22-56-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._29.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 3/51: o_happy_hour_2026-01-21_NO._24.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._24.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-22-58-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._24.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 4/51: o_happy_hour_2026-01-21_NO._1.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._1.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-23-01-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._1.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 5/51: o_happy_hour_2026-01-21_NO._12.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._12.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-23-04-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._12.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 6/51: o_happy_hour_2026-01-21_NO._8.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._8.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-23-05-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._8.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 7/51: o_happy_hour_2026-01-21_NO._13.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._13.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-23-07-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._13.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 8/51: o_happy_hour_2026-01-21_NO._28.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._28.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-23-08-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._28.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 9/51: o_happy_hour_2026-01-21_NO._39.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._39.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-23-09-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._39.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 10/51: o_happy_hour_2026-01-21_NO._26.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._26.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-23-10-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._26.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 11/51: o_happy_hour_2026-01-21_NO._36.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._36.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-23-12-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._36.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 12/51: o_happy_hour_2026-01-21_NO._17.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._17.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-23-14-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._17.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 13/51: o_happy_hour_2026-01-21_NO._14.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._14.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-23-15-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._14.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 14/51: o_happy_hour_2026-01-21_NO._47.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._47.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-23-16-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._47.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 15/51: o_happy_hour_2026-01-21_NO._31.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._31.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-23-18-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._31.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 16/51: o_happy_hour_2026-01-21_NO._41.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._41.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-23-21-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._41.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 17/51: o_happy_hour_2026-01-21_NO._45.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._45.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-23-22-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._45.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 18/51: o_happy_hour_2026-01-21_NO._21.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._21.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-23-25-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._21.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 19/51: o_happy_hour_2026-01-21_NO._38.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._38.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-23-27-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._38.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 20/51: o_happy_hour_2026-01-21_NO._3.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._3.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-23-29-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._3.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 21/51: o_happy_hour_2026-01-21_NO._44.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._44.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-23-30-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._44.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 22/51: o_happy_hour_2026-01-21_NO._27.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._27.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-23-32-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._27.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 23/51: o_happy_hour_2026-01-21_NO._6.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._6.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-23-34-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._6.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 24/51: o_happy_hour_2026-01-21_NO._30.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._30.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-23-38-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._30.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 25/51: o_happy_hour_2026-01-21_NO._9.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._9.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-23-40-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._9.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 26/51: o_happy_hour_2026-01-21_NO._22.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._22.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-23-42-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._22.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 27/51: o_happy_hour_2026-01-21_NO._0.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._0.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-23-44-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._0.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 28/51: o_happy_hour_2026-01-21_NO._49.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._49.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-23-45-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._49.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 29/51: o_happy_hour_2026-01-21_NO._19.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._19.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-23-47-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._19.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 30/51: o_happy_hour_2026-01-21_NO._20.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._20.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-23-50-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._20.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 31/51: o_happy_hour_2026-01-21_NO._46.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._46.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-23-52-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._46.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 32/51: o_happy_hour_2026-01-21_NO._2.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._2.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-23-54-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._2.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 33/51: o_happy_hour_2026-01-21_NO._18.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._18.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-23-57-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._18.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 34/51: o_happy_hour_2026-01-21_NO._10.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._10.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-23-59-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._10.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 35/51: o_happy_hour_2026-01-21_NO._48.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._48.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-24-00-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._48.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 36/51: o_happy_hour_2026-01-21_NO._23.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._23.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-24-03-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._23.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 37/51: o_happy_hour_2026-01-21_NO._15.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._15.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-24-06-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._15.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 38/51: o_happy_hour_2026-01-21_NO._11.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._11.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-24-08-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._11.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 39/51: o_happy_hour_2026-01-21_NO._4.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._4.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-24-10-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._4.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 40/51: o_happy_hour_2026-01-21_NO._7.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._7.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-24-14-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._7.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 41/51: o_happy_hour_2026-01-21_NO._5.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._5.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-24-15-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._5.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 42/51: o_happy_hour_2026-01-21_NO._50.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._50.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-24-18-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._50.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 43/51: o_happy_hour_2026-01-21_NO._25.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._25.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-24-20-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._25.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 44/51: o_happy_hour_2026-01-21_NO._16.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._16.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-24-21-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._16.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 45/51: o_happy_hour_2026-01-21_NO._33.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._33.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-24-23-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._33.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 46/51: o_happy_hour_2026-01-21_NO._37.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._37.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-24-24-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._37.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 47/51: o_happy_hour_2026-01-21_NO._34.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._34.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-24-27-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._34.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 48/51: o_happy_hour_2026-01-21_NO._32.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._32.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-24-30-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._32.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 49/51: o_happy_hour_2026-01-21_NO._40.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._40.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-24-32-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._40.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 50/51: o_happy_hour_2026-01-21_NO._35.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._35.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-24-34-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._35.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Processing 51/51: o_happy_hour_2026-01-21_NO._43.xlsx\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "======================================================================\n",
      "üìÅ Processing file: o_happy_hour_2026-01-21_NO._43.xlsx\n",
      "======================================================================\n",
      "\n",
      "[1/4] üîó Getting pre-signed upload URL...\n",
      "      ‚úì Key: 2026-01-21-14-24-36-user-2642.xlsx\n",
      "\n",
      "[2/4] üì§ Uploading file to S3...\n",
      "      ‚úì Upload successful (Status: 200)\n",
      "\n",
      "[3/4] ‚úÖ Validating file data...\n",
      "      ‚úì Validation passed (Status: 200)\n",
      "\n",
      "[4/4] ‚öôÔ∏è  Processing discounts...\n",
      "      ‚úì Processing completed (Status: 200)\n",
      "\n",
      "======================================================================\n",
      "üéâ SUCCESS: o_happy_hour_2026-01-21_NO._43.xlsx processed successfully!\n",
      "======================================================================\n",
      "\n",
      "\n",
      "######################################################################\n",
      "BATCH PROCESSING SUMMARY\n",
      "######################################################################\n",
      "üìä Total files: 51\n",
      "‚úÖ Successful: 51\n",
      "‚ùå Failed: 0\n",
      "‚è∞ Completed: 2026-01-21 12:24:37\n",
      "######################################################################\n",
      "\n",
      "\n",
      "‚úÖ All files processed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def sku_discount_upload_func(file_name):\n",
    "    \"\"\"\n",
    "    Upload SKU discount file and process through validation pipeline\n",
    "    \n",
    "    Args:\n",
    "        file_name: Path to the Excel file to upload\n",
    "    \n",
    "    Returns:\n",
    "        dict: Summary of upload and validation status\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìÅ Processing file: {os.path.basename(file_name)}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    results = {\n",
    "        'file': file_name,\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'steps': {}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Get pre-signed URL\n",
    "        print(\"\\n[1/4] üîó Getting pre-signed upload URL...\")\n",
    "        pre_data = preassigned_url().json()\n",
    "        key = pre_data['key']\n",
    "        new_url = pre_data['preSignedUrl']\n",
    "        print(f\"      ‚úì Key: {key}\")\n",
    "        results['steps']['presigned_url'] = 'Success'\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      ‚úó Failed to get pre-signed URL: {e}\")\n",
    "        results['steps']['presigned_url'] = f'Failed: {e}'\n",
    "        return results\n",
    "    \n",
    "    try:\n",
    "        # Step 2: Upload file\n",
    "        print(\"\\n[2/4] üì§ Uploading file to S3...\")\n",
    "        upload_response = upload_sku_discount(file_name, new_url)\n",
    "        \n",
    "        if upload_response.status_code in [200, 201, 204]:\n",
    "            print(f\"      ‚úì Upload successful (Status: {upload_response.status_code})\")\n",
    "            results['steps']['upload'] = 'Success'\n",
    "        else:\n",
    "            print(f\"      ‚úó Upload failed (Status: {upload_response.status_code})\")\n",
    "            print(f\"      Error: {upload_response.text}\")\n",
    "            results['steps']['upload'] = f'Failed: {upload_response.status_code}'\n",
    "            return results\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"      ‚úó Upload error: {e}\")\n",
    "        results['steps']['upload'] = f'Failed: {e}'\n",
    "        return results\n",
    "    \n",
    "    try:\n",
    "        # Step 3: Validate file\n",
    "        print(\"\\n[3/4] ‚úÖ Validating file data...\")\n",
    "        validation_data = validate_skus_discount(key)\n",
    "        \n",
    "        if validation_data.ok:\n",
    "            print(f\"      ‚úì Validation passed (Status: {validation_data.status_code})\")\n",
    "            results['steps']['validation'] = 'Success'\n",
    "            \n",
    "            # Try to parse validation response\n",
    "            try:\n",
    "                validation_response = validation_data.json()\n",
    "                if validation_response:\n",
    "                    print(f\"      Response: {validation_response}\")\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            print(f\"      ‚úó Validation failed (Status: {validation_data.status_code})\")\n",
    "            print(f\"      Error: {validation_data.text[:200]}\")\n",
    "            results['steps']['validation'] = f'Failed: {validation_data.status_code}'\n",
    "            return results\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"      ‚úó Validation error: {e}\")\n",
    "        results['steps']['validation'] = f'Failed: {e}'\n",
    "        return results\n",
    "    \n",
    "    try:\n",
    "        # Step 4: Proceed with processing\n",
    "        print(\"\\n[4/4] ‚öôÔ∏è  Processing discounts...\")\n",
    "        proceed_data = proceed(key)\n",
    "        \n",
    "        if proceed_data.ok:\n",
    "            print(f\"      ‚úì Processing completed (Status: {proceed_data.status_code})\")\n",
    "            results['steps']['proceed'] = 'Success'\n",
    "            results['status'] = 'SUCCESS'\n",
    "            \n",
    "            # Try to parse proceed response\n",
    "            try:\n",
    "                proceed_response = proceed_data.json()\n",
    "                if proceed_response:\n",
    "                    print(f\"      Response: {proceed_response}\")\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            print(f\"      ‚úó Processing failed (Status: {proceed_data.status_code})\")\n",
    "            print(f\"      Error: {proceed_data.text[:200]}\")\n",
    "            results['steps']['proceed'] = f'Failed: {proceed_data.status_code}'\n",
    "            results['status'] = 'FAILED'\n",
    "            return results\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"      ‚úó Processing error: {e}\")\n",
    "        results['steps']['proceed'] = f'Failed: {e}'\n",
    "        results['status'] = 'FAILED'\n",
    "        return results\n",
    "    \n",
    "    # Final status\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    if results.get('status') == 'SUCCESS':\n",
    "        print(f\"üéâ SUCCESS: {os.path.basename(file_name)} processed successfully!\")\n",
    "    else:\n",
    "        print(f\"‚ùå FAILED: {os.path.basename(file_name)} processing failed\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Main execution with summary\n",
    "def process_all_discount_files(directory='HH_Sheets'):\n",
    "    \"\"\"\n",
    "    Process all discount files in the specified directory\n",
    "    \n",
    "    Args:\n",
    "        directory: Directory containing Excel files to process\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'#'*70}\")\n",
    "    print(f\"SKU DISCOUNT BATCH UPLOAD\")\n",
    "    print(f\"{'#'*70}\")\n",
    "    print(f\"üìÇ Directory: {directory}\")\n",
    "    print(f\"‚è∞ Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # Get all files\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"\\n‚ùå ERROR: Directory '{directory}' does not exist\")\n",
    "        return\n",
    "    \n",
    "    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "    excel_files = [f for f in files if f.endswith(('.xlsx', '.xls'))]\n",
    "    \n",
    "    if not excel_files:\n",
    "        print(f\"\\n‚ö†Ô∏è  WARNING: No Excel files found in '{directory}'\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üìä Found {len(excel_files)} file(s) to process\\n\")\n",
    "    \n",
    "    # Process each file\n",
    "    all_results = []\n",
    "    success_count = 0\n",
    "    failed_count = 0\n",
    "    \n",
    "    for idx, file in enumerate(excel_files, 1):\n",
    "        file_path = os.path.join(directory, file)\n",
    "        print(f\"\\n{'‚îÄ'*70}\")\n",
    "        print(f\"Processing {idx}/{len(excel_files)}: {file}\")\n",
    "        print(f\"{'‚îÄ'*70}\")\n",
    "        \n",
    "        result = sku_discount_upload_func(file_path)\n",
    "        all_results.append(result)\n",
    "        \n",
    "        if result.get('status') == 'SUCCESS':\n",
    "            success_count += 1\n",
    "        else:\n",
    "            failed_count += 1\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n\\n{'#'*70}\")\n",
    "    print(f\"BATCH PROCESSING SUMMARY\")\n",
    "    print(f\"{'#'*70}\")\n",
    "    print(f\"üìä Total files: {len(excel_files)}\")\n",
    "    print(f\"‚úÖ Successful: {success_count}\")\n",
    "    print(f\"‚ùå Failed: {failed_count}\")\n",
    "    print(f\"‚è∞ Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"{'#'*70}\\n\")\n",
    "    \n",
    "    # Detailed results table\n",
    "    if failed_count > 0:\n",
    "        print(\"\\nüìã FAILED FILES DETAILS:\")\n",
    "        print(f\"{'‚îÄ'*70}\")\n",
    "        for result in all_results:\n",
    "            if result.get('status') != 'SUCCESS':\n",
    "                print(f\"\\n‚ùå File: {os.path.basename(result['file'])}\")\n",
    "                for step, status in result['steps'].items():\n",
    "                    if 'Failed' in str(status):\n",
    "                        print(f\"   ‚îî‚îÄ {step}: {status}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ All files processed!\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "# Usage\n",
    "results = process_all_discount_files('HH_Sheets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3600a17b-b8fd-4c1a-af72-a881b57d3b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e66aa2-66cf-468b-8272-9e81a696a17b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
