{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f8db73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pricing Status Analysis Script\n",
    "# Converted from SQL query to Python for easier editing and maintenance\n",
    "\n",
    "# =============================================================================\n",
    "# STANDARD LIBRARY IMPORTS\n",
    "# =============================================================================\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "# =============================================================================\n",
    "# THIRD-PARTY IMPORTS\n",
    "# =============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import snowflake.connector\n",
    "\n",
    "# =============================================================================\n",
    "# LOCAL IMPORTS & ENVIRONMENT SETUP\n",
    "# =============================================================================\n",
    "import setup_environment_2\n",
    "import importlib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "importlib.reload(setup_environment_2)\n",
    "setup_environment_2.initialize_env()\n",
    "\n",
    "print(\"✓ Environment initialized\")\n",
    "\n",
    "# =============================================================================\n",
    "# SNOWFLAKE QUERY FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def snowflake_query(country, query, warehouse=None, columns=[], conn=None):\n",
    "    \"\"\"\n",
    "    Execute a query against Snowflake and return results as DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        country: Country identifier (e.g., \"Egypt\")\n",
    "        query: SQL query string to execute\n",
    "        warehouse: Snowflake warehouse (optional)\n",
    "        columns: Custom column names (optional)\n",
    "        conn: Existing connection (optional)\n",
    "        \n",
    "    Returns:\n",
    "        pandas DataFrame with query results\n",
    "    \"\"\"\n",
    "    con = snowflake.connector.connect(\n",
    "        user     = os.environ[\"SNOWFLAKE_USERNAME\"],\n",
    "        account  = os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "        password = os.environ[\"SNOWFLAKE_PASSWORD\"],\n",
    "        database = os.environ[\"SNOWFLAKE_DATABASE\"]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        cur = con.cursor()\n",
    "        cur.execute(\"USE WAREHOUSE COMPUTE_WH\")\n",
    "        cur.execute(query)\n",
    "        \n",
    "        column_names = [col[0] for col in cur.description]\n",
    "        results = cur.fetchall()\n",
    "        \n",
    "        if not results:\n",
    "            out = pd.DataFrame(columns=[name.lower() for name in column_names])\n",
    "        else:\n",
    "            if len(columns) == 0:\n",
    "                out = pd.DataFrame(np.array(results), columns=column_names)\n",
    "                out.columns = out.columns.str.lower()\n",
    "            else:\n",
    "                out = pd.DataFrame(np.array(results), columns=columns)\n",
    "                out.columns = out.columns.str.lower()\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Query error: {e}\")\n",
    "        raise\n",
    "        \n",
    "    finally:\n",
    "        cur.close()\n",
    "        con.close()\n",
    "\n",
    "print(\"✓ Snowflake query function loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb08940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 1: STATIC DATA - Warehouse Mapping\n",
    "# =============================================================================\n",
    "\n",
    "def get_warehouse_mapping():\n",
    "    \"\"\"Define warehouse to region/cohort mapping.\"\"\"\n",
    "    whs_data = [\n",
    "        ('Cairo', 'Mostorod', 1, 700),\n",
    "        ('Giza', 'Barageel', 236, 701),\n",
    "        ('Delta West', 'El-Mahala', 337, 703),\n",
    "        ('Delta West', 'Tanta', 8, 703),\n",
    "        ('Delta East', 'Mansoura FC', 339, 704),\n",
    "        ('Delta East', 'Sharqya', 170, 704),\n",
    "        ('Upper Egypt', 'Assiut FC', 501, 1124),\n",
    "        ('Upper Egypt', 'Bani sweif', 401, 1126),\n",
    "        ('Upper Egypt', 'Menya Samalot', 703, 1123),\n",
    "        ('Upper Egypt', 'Sohag', 632, 1125),\n",
    "        ('Alexandria', 'Khorshed Alex', 797, 702),\n",
    "        ('Giza', 'Sakkarah', 962, 701)\n",
    "    ]\n",
    "    \n",
    "    df_whs = pd.DataFrame(whs_data, columns=['region', 'wh', 'warehouse_id', 'cohort_id'])\n",
    "    return df_whs\n",
    "\n",
    "# Get warehouse mapping\n",
    "df_whs = get_warehouse_mapping()\n",
    "print(\"Warehouse Mapping:\")\n",
    "df_whs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db2ac3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 2: FETCH COGS DATA\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_current_cogs():\n",
    "    \"\"\"Fetch current cost of goods sold data.\"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT product_id, wac_p\n",
    "    FROM finance.all_cogs\n",
    "    WHERE CURRENT_TIMESTAMP BETWEEN from_date AND to_date\n",
    "    \"\"\"\n",
    "    df = snowflake_query(\"Egypt\", query)\n",
    "    df['product_id'] = pd.to_numeric(df['product_id'])\n",
    "    df['wac_p'] = pd.to_numeric(df['wac_p'])\n",
    "    return df\n",
    "\n",
    "# Run:\n",
    "df_cogs = fetch_current_cogs()\n",
    "print(f\"COGS records: {len(df_cogs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecfca4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 3: FETCH RUNNING RATES DATA\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_running_rates():\n",
    "    \"\"\"Fetch predicted running rates - latest per product/warehouse within 14 days.\"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT product_id, warehouse_id, rr\n",
    "    FROM finance.PREDICTED_RUNNING_RATES\n",
    "    QUALIFY MAX(date) OVER (PARTITION BY product_id, warehouse_id) = date\n",
    "        AND date::DATE >= CURRENT_DATE - 14\n",
    "    \"\"\"\n",
    "    df = snowflake_query(\"Egypt\", query)\n",
    "    for col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='ignore')\n",
    "    return df\n",
    "\n",
    "# Run:\n",
    "df_rr = fetch_running_rates()\n",
    "print(f\"Running rates records: {len(df_rr)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482ee20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 4: FETCH STOCKS DATA\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_stocks():\n",
    "    \"\"\"Fetch stock data with running rates and DOH calculation.\"\"\"\n",
    "    query = \"\"\"\n",
    "    WITH rr AS (\n",
    "        SELECT product_id, warehouse_id, rr\n",
    "        FROM finance.PREDICTED_RUNNING_RATES\n",
    "        QUALIFY MAX(date) OVER (PARTITION BY product_id, warehouse_id) = date\n",
    "            AND date::DATE >= CURRENT_DATE - 14\n",
    "    )\n",
    "    SELECT \n",
    "        pw.warehouse_id,\n",
    "        pw.product_id,\n",
    "        pw.available_stock::INTEGER AS stocks,\n",
    "        COALESCE(rr.rr, 0) AS rr,\n",
    "        CASE WHEN COALESCE(rr.rr, 0) = 0 THEN pw.available_stock::INTEGER \n",
    "             ELSE pw.available_stock::INTEGER / rr.rr \n",
    "        END AS doh\n",
    "    FROM product_warehouse pw\n",
    "    LEFT JOIN rr ON rr.product_id = pw.product_id AND rr.warehouse_id = pw.warehouse_id\n",
    "    WHERE pw.warehouse_id NOT IN (6, 9, 10)\n",
    "        AND pw.is_basic_unit = 1\n",
    "    \"\"\"\n",
    "    df = snowflake_query(\"Egypt\", query)\n",
    "    for col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='ignore')\n",
    "    return df\n",
    "\n",
    "# Run:\n",
    "df_stocks = fetch_stocks()\n",
    "print(f\"Stock records: {len(df_stocks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9728c536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 5: FETCH SALES DATA\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_sales():\n",
    "    \"\"\"Fetch sales data with aggregations for RR and retailer metrics.\"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        warehouse_id, \n",
    "        product_id,\n",
    "        sku,\n",
    "        brand,\n",
    "        cat,\n",
    "        PERCENTILE_CONT(0.8) WITHIN GROUP (ORDER BY CASE WHEN date < CURRENT_DATE - 3 THEN qty END) AS high_rr,\n",
    "        PERCENTILE_CONT(0.8) WITHIN GROUP (ORDER BY CASE WHEN date < CURRENT_DATE - 3 THEN num_rets END) AS high_rets,\n",
    "        COALESCE(STDDEV(CASE WHEN date < CURRENT_DATE - 3 THEN qty END), 0) AS qty_std,\n",
    "        COALESCE(STDDEV(CASE WHEN date < CURRENT_DATE - 3 THEN num_rets END), 0) AS rets_std,\n",
    "        COALESCE(SUM(CASE WHEN date = CURRENT_DATE - 1 THEN qty END), 0) AS cu_rr,\n",
    "        COALESCE(SUM(CASE WHEN date = CURRENT_DATE THEN qty END), 0) AS today_rr,\n",
    "        COALESCE(SUM(CASE WHEN date = CURRENT_DATE - 1 THEN num_rets END), 0) AS cu_rets\n",
    "    FROM (\n",
    "        SELECT\n",
    "            so.created_at::DATE AS date,\n",
    "            pso.warehouse_id,\n",
    "            pso.product_id,\n",
    "            CONCAT(p.name_ar, ' ', p.size, ' ', pu.name_ar) AS sku,\n",
    "            b.name_ar AS brand, \n",
    "            c.name_ar AS cat,\n",
    "            SUM(pso.purchased_item_count * pso.basic_unit_count) AS qty,\n",
    "            COUNT(DISTINCT so.retailer_id) AS num_rets\n",
    "        FROM product_sales_order pso\n",
    "        JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "        JOIN products p ON p.id = pso.product_id\n",
    "        JOIN brands b ON p.brand_id = b.id \n",
    "        JOIN categories c ON p.category_id = c.id\n",
    "        JOIN product_units pu ON pu.id = p.unit_id\n",
    "        WHERE so.created_at::DATE BETWEEN CURRENT_DATE - 150 AND CURRENT_DATE \n",
    "            AND so.sales_order_status_id NOT IN (7, 12)\n",
    "            AND so.channel IN ('telesales', 'retailer')\n",
    "            AND pso.purchased_item_count <> 0\n",
    "            AND DAYNAME(so.created_at::DATE) <> 'Fri'\n",
    "        GROUP BY 1, 2, 3, 4, 5, 6\n",
    "    )\n",
    "    GROUP BY 1, 2, 3, 4, 5\n",
    "    \"\"\"\n",
    "    df = snowflake_query(\"Egypt\", query)\n",
    "    # Convert numeric columns\n",
    "    numeric_cols = ['warehouse_id', 'product_id', 'high_rr', 'high_rets', 'qty_std', 'rets_std', 'cu_rr', 'today_rr', 'cu_rets']\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='ignore')\n",
    "    return df\n",
    "\n",
    "# Run:\n",
    "df_sales = fetch_sales()\n",
    "print(f\"Sales records: {len(df_sales)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9713f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 6: FETCH PRICES DATA\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_prices():\n",
    "    \"\"\"Fetch latest prices per product/cohort.\"\"\"\n",
    "    cohort_ids = [700, 701, 702, 703, 704, 696, 695, 698, 697, 699, 1123, 1124, 1125, 1126]\n",
    "    cohort_str = ', '.join(map(str, cohort_ids))\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    SELECT cohort_id, product_id, price\n",
    "    FROM (\n",
    "        SELECT \n",
    "            cpc.cohort_id,\n",
    "            pu.product_id,\n",
    "            cpc.price,\n",
    "            ROW_NUMBER() OVER (PARTITION BY pu.product_id, cpc.cohort_id ORDER BY cpc.created_at DESC) AS rn\n",
    "        FROM cohort_pricing_changes cpc \n",
    "        JOIN PACKING_UNIT_PRODUCTS pu ON pu.id = cpc.product_packing_unit_id\n",
    "        WHERE cpc.cohort_id IN ({cohort_str})\n",
    "            AND pu.is_basic_unit = 1 \n",
    "    )\n",
    "    WHERE rn = 1\n",
    "    \"\"\"\n",
    "    df = snowflake_query(\"Egypt\", query)\n",
    "    df['cohort_id'] = pd.to_numeric(df['cohort_id'])\n",
    "    df['product_id'] = pd.to_numeric(df['product_id'])\n",
    "    df['price'] = pd.to_numeric(df['price'])\n",
    "    return df\n",
    "\n",
    "# Run:\n",
    "df_prices = fetch_prices()\n",
    "print(f\"Price records: {len(df_prices)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16388890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 7: FETCH MARKETPLACE PRICES DATA\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_marketplace_prices():\n",
    "    \"\"\"Fetch marketplace price data (min, mod, max).\"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        mp.region,\n",
    "        mp.product_id,\n",
    "        AVG(mp.min_price / pup.basic_unit_count) AS min_price,\n",
    "        AVG(mp.mod_price / pup.basic_unit_count) AS mod_price,\n",
    "        AVG(mp.max_price / pup.basic_unit_count) AS max_price\n",
    "    FROM materialized_views.marketplace_prices mp\n",
    "    JOIN PACKING_UNIT_PRODUCTS pup ON pup.product_id = mp.product_id AND mp.pu_id = pup.packing_unit_id\n",
    "    GROUP BY 1, 2\n",
    "    \"\"\"\n",
    "    df = snowflake_query(\"Egypt\", query)\n",
    "    df['product_id'] = pd.to_numeric(df['product_id'])\n",
    "    df['min_price'] = pd.to_numeric(df['min_price'])\n",
    "    df['mod_price'] = pd.to_numeric(df['mod_price'])\n",
    "    df['max_price'] = pd.to_numeric(df['max_price'])\n",
    "    return df\n",
    "\n",
    "# Run:\n",
    "df_mp = fetch_marketplace_prices()\n",
    "print(f\"Marketplace price records: {len(df_mp)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bff7a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 8: FETCH BEN SOLIMAN PRICES\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_ben_soliman_prices():\n",
    "    \"\"\"Fetch Ben Soliman competitor prices with validation.\"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT z.* \n",
    "    FROM (\n",
    "        SELECT maxab_product_id AS product_id, AVG(bs_final_price) AS ben_soliman_price\n",
    "        FROM (\n",
    "            SELECT *, ROW_NUMBER() OVER(PARTITION BY maxab_product_id ORDER BY diff) AS rnk_2\n",
    "            FROM (\n",
    "                SELECT *, (bs_final_price - wac_p) / wac_p AS diff_2\n",
    "                FROM (\n",
    "                    SELECT *, bs_price / maxab_basic_unit_count AS bs_final_price\n",
    "                    FROM (\n",
    "                        SELECT *, ROW_NUMBER() OVER(PARTITION BY maxab_product_id, maxab_pu ORDER BY diff) AS rnk \n",
    "                        FROM (\n",
    "                            SELECT *, MAX(INJECTION_DATE::DATE) OVER(PARTITION BY maxab_product_id, maxab_pu) AS max_date\n",
    "                            FROM (\n",
    "                                SELECT sm.*, wac1, wac_p, \n",
    "                                    ABS(bs_price - (wac_p * maxab_basic_unit_count)) / (wac_p * maxab_basic_unit_count) AS diff \n",
    "                                FROM materialized_views.savvy_mapping sm \n",
    "                                JOIN finance.all_cogs f ON f.product_id = sm.maxab_product_id \n",
    "                                    AND CURRENT_TIMESTAMP BETWEEN f.from_date AND f.to_date\n",
    "                                WHERE bs_price IS NOT NULL \n",
    "                                    AND INJECTION_DATE::DATE >= CURRENT_TIMESTAMP::DATE - 5 \n",
    "                                    AND ABS(bs_price - (wac_p * maxab_basic_unit_count)) / (wac_p * maxab_basic_unit_count) < 0.3\n",
    "                            )\n",
    "                            QUALIFY max_date = INJECTION_DATE\n",
    "                        )\n",
    "                        QUALIFY rnk = 1 \n",
    "                    )\n",
    "                )\n",
    "                WHERE diff_2 BETWEEN -0.5 AND 0.5 \n",
    "            )\n",
    "            QUALIFY rnk_2 = 1 \n",
    "        )\n",
    "        GROUP BY ALL\n",
    "    ) z \n",
    "    JOIN finance.all_cogs f ON f.product_id = z.product_id \n",
    "        AND CURRENT_TIMESTAMP BETWEEN f.from_date AND f.to_date\n",
    "    WHERE ben_soliman_price BETWEEN f.wac_p * 0.9 AND f.wac_p * 1.3\n",
    "    \"\"\"\n",
    "    df = snowflake_query(\"Egypt\", query)\n",
    "    df['product_id'] = pd.to_numeric(df['product_id'])\n",
    "    df['ben_soliman_price'] = pd.to_numeric(df['ben_soliman_price'])\n",
    "    return df\n",
    "\n",
    "# Run:\n",
    "df_bsp = fetch_ben_soliman_prices()\n",
    "print(f\"Ben Soliman price records: {len(df_bsp)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cddba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 9: FETCH SCRAPPED/CLEANED MARKET PRICES\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_scrapped_prices():\n",
    "    \"\"\"Fetch scraped market prices with min/max/median aggregations.\"\"\"\n",
    "    query = \"\"\"\n",
    "    WITH current_cogs AS (\n",
    "        SELECT product_id, wac_p\n",
    "        FROM finance.all_cogs\n",
    "        WHERE CURRENT_TIMESTAMP BETWEEN from_date AND to_date\n",
    "    )\n",
    "    SELECT \n",
    "        product_id,\n",
    "        region,\n",
    "        MIN(market_price) AS min_scrapped,\n",
    "        MAX(market_price) AS max_scrapped,\n",
    "        MEDIAN(market_price) AS median_scrapped\n",
    "    FROM (\n",
    "        SELECT \n",
    "            cmp.product_id,\n",
    "            cmp.region,\n",
    "            cmp.market_price\n",
    "        FROM materialized_views.cleaned_market_prices cmp\n",
    "        JOIN current_cogs f ON f.product_id = cmp.product_id\n",
    "        WHERE cmp.date >= CURRENT_DATE - 5\n",
    "            AND cmp.market_price BETWEEN f.wac_p * 0.9 AND f.wac_p * 1.3\n",
    "        QUALIFY MAX(cmp.date) OVER (PARTITION BY cmp.region, cmp.product_id, cmp.competitor) = cmp.date\n",
    "    )\n",
    "    GROUP BY 1, 2\n",
    "    \"\"\"\n",
    "    df = snowflake_query(\"Egypt\", query)\n",
    "    df['product_id'] = pd.to_numeric(df['product_id'])\n",
    "    df['min_scrapped'] = pd.to_numeric(df['min_scrapped'])\n",
    "    df['max_scrapped'] = pd.to_numeric(df['max_scrapped'])\n",
    "    df['median_scrapped'] = pd.to_numeric(df['median_scrapped'])\n",
    "    return df\n",
    "\n",
    "# Run:\n",
    "df_scrapped = fetch_scrapped_prices()\n",
    "print(f\"Scrapped price records: {len(df_scrapped)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eec751c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 10: FETCH TARGETS DATA\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_cat_brand_targets():\n",
    "    \"\"\"Fetch category/brand targets from commercial plan.\"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        cat, \n",
    "        brand, \n",
    "        SUM(nmv) AS target_nmv, \n",
    "        AVG(margin) AS target_bm,\n",
    "        DATE_TRUNC('month', DATE) AS month_date\n",
    "    FROM performance.commercial_targets\n",
    "    WHERE cat IS NOT NULL AND brand IS NOT NULL \n",
    "        AND date >= DATE_TRUNC('month', CURRENT_DATE)\n",
    "    GROUP BY ALL\n",
    "    \"\"\"\n",
    "    df = snowflake_query(\"Egypt\", query)\n",
    "    df['target_nmv'] = pd.to_numeric(df['target_nmv'])\n",
    "    df['target_bm'] = pd.to_numeric(df['target_bm'])\n",
    "    return df\n",
    "\n",
    "def fetch_cat_targets(df_cat_brand_targets):\n",
    "    \"\"\"Calculate category-level targets from brand targets.\"\"\"\n",
    "    df = df_cat_brand_targets.copy()\n",
    "    df['weighted_margin'] = df['target_bm'] * df['target_nmv']\n",
    "    cat_targets = df.groupby('cat').apply(\n",
    "        lambda x: x['weighted_margin'].sum() / x['target_nmv'].sum() if x['target_nmv'].sum() > 0 else 0\n",
    "    ).reset_index()\n",
    "    cat_targets.columns = ['cat', 'cat_target_margin']\n",
    "    return cat_targets\n",
    "\n",
    "# Run:\n",
    "df_cat_brand_targets = fetch_cat_brand_targets()\n",
    "df_cat_targets = fetch_cat_targets(df_cat_brand_targets)\n",
    "print(f\"Cat/Brand target records: {len(df_cat_brand_targets)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0479501c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 11: FETCH DISCOUNTED SALES DATA\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_discounted_sales():\n",
    "    \"\"\"Fetch yesterday's discounted sales breakdown.\"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT  \n",
    "        pso.warehouse_id,\n",
    "        pso.product_id,\n",
    "        SUM(pso.total_price) AS total_nmv,\n",
    "        SUM(CASE WHEN pso.dynamic_bundle_sales_order_id IS NOT NULL THEN pso.total_price END) AS bundle_nmv,\n",
    "        SUM(CASE WHEN pso.sku_discount_id IS NOT NULL THEN pso.total_price END) AS sku_discount_nmv,\n",
    "        SUM(CASE WHEN pso.quantity_discount_id IS NOT NULL THEN pso.total_price END) AS quantity_nmv\n",
    "    FROM product_sales_order pso \n",
    "    JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "    WHERE so.created_at::DATE = CURRENT_DATE - 1 \n",
    "        AND so.sales_order_status_id NOT IN (7, 12)\n",
    "        AND so.channel IN ('telesales', 'retailer')\n",
    "        AND pso.purchased_item_count <> 0\n",
    "    GROUP BY 1, 2\n",
    "    \"\"\"\n",
    "    df = snowflake_query(\"Egypt\", query)\n",
    "    for col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='ignore')\n",
    "    return df\n",
    "\n",
    "# Run:\n",
    "df_discounted = fetch_discounted_sales()\n",
    "print(f\"Discounted sales records: {len(df_discounted)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c41ec38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 12: FETCH COMMERCIAL CONSTRAINTS (MIN PRICES)\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_commercial_constraints():\n",
    "    \"\"\"Fetch commercial minimum price constraints.\"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT product_id, region, min_price\n",
    "    FROM (\n",
    "        SELECT \n",
    "            product_id, \n",
    "            region, \n",
    "            min_price,\n",
    "            created_at,\n",
    "            MAX(created_at) OVER (PARTITION BY product_id, region) AS max_created\n",
    "        FROM finance.minimum_prices\n",
    "        WHERE is_deleted = 'false'\n",
    "            AND created_at BETWEEN \n",
    "                CASE WHEN DATE_PART('day', CURRENT_DATE) < 7 \n",
    "                     THEN DATE_TRUNC('month', CURRENT_DATE - INTERVAL '1 month') \n",
    "                     ELSE DATE_TRUNC('month', CURRENT_DATE)\n",
    "                END\n",
    "                AND DATE_TRUNC('month', CURRENT_DATE) + INTERVAL '1 month' + INTERVAL '6 days'\n",
    "    )\n",
    "    WHERE created_at = max_created\n",
    "    \"\"\"\n",
    "    df = snowflake_query(\"Egypt\", query)\n",
    "    df['product_id'] = pd.to_numeric(df['product_id'])\n",
    "    df['min_price'] = pd.to_numeric(df['min_price'])\n",
    "    return df\n",
    "\n",
    "# Run:\n",
    "df_commercial = fetch_commercial_constraints()\n",
    "print(f\"Commercial constraint records: {len(df_commercial)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30adb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 13: FETCH TARGETS DATA (COMPLEX - WAREHOUSE SKU TARGETS)\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_targets_data(df_whs):\n",
    "    \"\"\"Fetch complex targets data with warehouse-level SKU targets.\"\"\"\n",
    "    # Build warehouse IDs list for the query\n",
    "    warehouse_ids = df_whs['warehouse_id'].tolist()\n",
    "    wh_str = ', '.join(map(str, warehouse_ids))\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    WITH whs AS (\n",
    "        SELECT *\n",
    "        FROM (VALUES\n",
    "            ('Cairo', 'Mostorod', 1, 700),\n",
    "            ('Giza', 'Barageel', 236, 701),\n",
    "            ('Delta West', 'El-Mahala', 337, 703),\n",
    "            ('Delta West', 'Tanta', 8, 703),\n",
    "            ('Delta East', 'Mansoura FC', 339, 704),\n",
    "            ('Delta East', 'Sharqya', 170, 704),\n",
    "            ('Upper Egypt', 'Assiut FC', 501, 1124),\n",
    "            ('Upper Egypt', 'Bani sweif', 401, 1126),\n",
    "            ('Upper Egypt', 'Menya Samalot', 703, 1123),\n",
    "            ('Upper Egypt', 'Sohag', 632, 1125),\n",
    "            ('Alexandria', 'Khorshed Alex', 797, 702),\n",
    "            ('Giza', 'Sakkarah', 962, 701)\n",
    "        ) x(region, wh, warehouse_id, cohort_id)\n",
    "    ),\n",
    "    base_sales AS (\n",
    "        SELECT\n",
    "            CASE WHEN whs.region LIKE '%Delta%' THEN 'Delta' \n",
    "                 WHEN whs.region = 'Cairo' OR whs.region = 'Giza' THEN 'Greater Cairo' \n",
    "                 ELSE whs.region \n",
    "            END AS region,\n",
    "            pso.warehouse_id,\n",
    "            pso.product_id,\n",
    "            c.name_ar AS cat,\n",
    "            b.name_ar AS brand,\n",
    "            SUM(pso.total_price) AS nmv,\n",
    "            so.created_at::DATE AS sale_date\n",
    "        FROM product_sales_order pso\n",
    "        JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "        JOIN products p ON p.id = pso.product_id\n",
    "        JOIN categories c ON c.id = p.category_id\n",
    "        JOIN brands b ON b.id = p.brand_id\n",
    "        JOIN whs ON whs.warehouse_id = pso.warehouse_id\n",
    "        WHERE so.sales_order_status_id NOT IN (7, 12)\n",
    "            AND pso.purchased_item_count <> 0\n",
    "            AND so.channel IN ('retailer', 'telesales')\n",
    "            AND so.created_at::DATE BETWEEN DATE_TRUNC('month', CURRENT_DATE - INTERVAL '3 month') AND CURRENT_DATE - 1\n",
    "        GROUP BY 1, 2, 3, 4, 5, 7\n",
    "    ),\n",
    "    region_product_nmv AS (\n",
    "        SELECT region, product_id, cat, brand, SUM(nmv) AS region_product_nmv\n",
    "        FROM base_sales\n",
    "        GROUP BY 1, 2, 3, 4\n",
    "    ),\n",
    "    warehouse_contribution AS (\n",
    "        SELECT \n",
    "            bs.region,\n",
    "            bs.warehouse_id,\n",
    "            bs.product_id,\n",
    "            bs.cat,\n",
    "            bs.brand,\n",
    "            SUM(bs.nmv) AS warehouse_nmv,\n",
    "            SUM(bs.nmv) / NULLIF(rpn.region_product_nmv, 0) AS wh_cntrb_in_region\n",
    "        FROM base_sales bs\n",
    "        JOIN region_product_nmv rpn ON rpn.region = bs.region \n",
    "            AND rpn.product_id = bs.product_id\n",
    "        GROUP BY 1, 2, 3, 4, 5, rpn.region_product_nmv\n",
    "    ),\n",
    "    region_sku_cntrb AS (\n",
    "        SELECT region, product_id, cat, brand,\n",
    "            SUM(region_product_nmv) / SUM(SUM(region_product_nmv)) OVER (PARTITION BY region, cat, brand) AS sku_cntrb\n",
    "        FROM region_product_nmv\n",
    "        GROUP BY 1, 2, 3, 4\n",
    "    ),\n",
    "    comm_plan AS (\n",
    "        SELECT\n",
    "            CASE WHEN city = 'Alex' THEN 'Alexandria' ELSE city END AS region,\n",
    "            cat, brand,\n",
    "            SUM(nmv) AS target\n",
    "        FROM performance.commercial_targets\n",
    "        WHERE date BETWEEN DATE_TRUNC('month', CURRENT_DATE) AND CURRENT_DATE - 1\n",
    "        GROUP BY 1, 2, 3\n",
    "    ),\n",
    "    current_month_sales AS (\n",
    "        SELECT region, warehouse_id, product_id, SUM(nmv) AS nmv\n",
    "        FROM base_sales\n",
    "        WHERE sale_date >= DATE_TRUNC('month', CURRENT_DATE)\n",
    "        GROUP BY 1, 2, 3\n",
    "    )\n",
    "    SELECT \n",
    "        wc.region,\n",
    "        wc.warehouse_id,\n",
    "        wc.product_id,\n",
    "        wc.cat,\n",
    "        wc.brand,\n",
    "        cp.target * rsc.sku_cntrb AS region_sku_target,\n",
    "        cp.target * rsc.sku_cntrb * wc.wh_cntrb_in_region AS wh_sku_target,\n",
    "        COALESCE(cms.nmv, 0) AS sales,\n",
    "        cp.target * rsc.sku_cntrb * wc.wh_cntrb_in_region - COALESCE(cms.nmv, 0) AS rem_nmv\n",
    "    FROM warehouse_contribution wc\n",
    "    JOIN region_sku_cntrb rsc ON rsc.region = wc.region \n",
    "        AND rsc.product_id = wc.product_id\n",
    "    JOIN comm_plan cp ON cp.region = wc.region \n",
    "        AND cp.cat = wc.cat \n",
    "        AND cp.brand = wc.brand\n",
    "    LEFT JOIN current_month_sales cms ON cms.product_id = wc.product_id \n",
    "        AND cms.warehouse_id = wc.warehouse_id\n",
    "        AND cms.region = wc.region\n",
    "    \"\"\"\n",
    "    df = snowflake_query(\"Egypt\", query)\n",
    "    for col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='ignore')\n",
    "    return df\n",
    "\n",
    "# Run:\n",
    "df_targets = fetch_targets_data(df_whs)\n",
    "print(f\"Targets data records: {len(df_targets)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33888c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 14: BUILD PRODUCT METRICS (MERGE ALL DATA)\n",
    "# =============================================================================\n",
    "\n",
    "def build_product_metrics(df_stocks, df_sales, df_whs, df_prices, df_cogs, \n",
    "                          df_mp, df_bsp, df_scrapped, df_cat_brand_targets, \n",
    "                          df_cat_targets, df_discounted):\n",
    "    \"\"\"\n",
    "    Merge all data sources to build product metrics.\n",
    "    This replicates the 'product_metrics' CTE from the SQL query.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start with stocks and sales join\n",
    "    df = df_stocks.merge(\n",
    "        df_sales, \n",
    "        on=['product_id', 'warehouse_id'], \n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    # Join warehouse mapping\n",
    "    df = df.merge(df_whs, on='warehouse_id', how='inner')\n",
    "    \n",
    "    # Join prices (using cohort_id from warehouse mapping)\n",
    "    df = df.merge(\n",
    "        df_prices, \n",
    "        on=['product_id', 'cohort_id'], \n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    # Join COGS\n",
    "    df = df.merge(df_cogs, on='product_id', how='inner')\n",
    "    \n",
    "    # Calculate BM (basic margin)\n",
    "    df['bm'] = (df['price'] - df['wac_p']) / df['price'].replace(0, np.nan)\n",
    "    \n",
    "    # Calculate in_stock_perc\n",
    "    df['in_stock_perc'] = (df['stocks'] > 0).astype(int)\n",
    "    \n",
    "    # Join marketplace prices\n",
    "    df = df.merge(\n",
    "        df_mp.rename(columns={\n",
    "            'min_price': 'mp_min_price',\n",
    "            'mod_price': 'mp_mod_price', \n",
    "            'max_price': 'mp_max_price'\n",
    "        }), \n",
    "        on=['product_id', 'region'], \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Join Ben Soliman prices\n",
    "    df = df.merge(df_bsp[['product_id', 'ben_soliman_price']], on='product_id', how='left')\n",
    "    \n",
    "    # Join scrapped prices\n",
    "    df = df.merge(\n",
    "        df_scrapped.rename(columns={\n",
    "            'min_scrapped': 'min_scrapped',\n",
    "            'max_scrapped': 'max_scrapped',\n",
    "            'median_scrapped': 'median_scrapped'\n",
    "        }), \n",
    "        on=['product_id', 'region'], \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Join category/brand targets\n",
    "    df = df.merge(\n",
    "        df_cat_brand_targets[['cat', 'brand', 'target_bm']].drop_duplicates(),\n",
    "        on=['cat', 'brand'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Join category targets (fallback)\n",
    "    df = df.merge(df_cat_targets, on='cat', how='left')\n",
    "    \n",
    "    # Set target_margin (use cat_brand target, fall back to cat target)\n",
    "    df['target_margin'] = df['target_bm'].fillna(df['cat_target_margin'])\n",
    "    \n",
    "    # Join discounted sales\n",
    "    df = df.merge(\n",
    "        df_discounted,\n",
    "        on=['warehouse_id', 'product_id'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Filter to positive prices and high_rr\n",
    "    df = df[(df['price'] > 0) & (df['high_rr'] > 0)]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Run after fetching all data:\n",
    "df_metrics = build_product_metrics(\n",
    "    df_stocks, df_sales, df_whs, df_prices, df_cogs,\n",
    "    df_mp, df_bsp, df_scrapped, df_cat_brand_targets,\n",
    "    df_cat_targets, df_discounted\n",
    ")\n",
    "print(f\"Product metrics records: {len(df_metrics)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01f9225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 15: SCORING AND CLASSIFICATION\n",
    "# =============================================================================\n",
    "\n",
    "def add_scoring_classification(df, df_commercial):\n",
    "    \"\"\"\n",
    "    Add scoring and classification columns.\n",
    "    Replicates 'scored_classified' and 'final_scored' CTEs.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Map region for commercial constraints\n",
    "    df['region_mapped'] = df['region'].apply(\n",
    "        lambda x: 'Greater Cairo' if x in ['Cairo', 'Giza'] else x\n",
    "    )\n",
    "    \n",
    "    # Join commercial constraints\n",
    "    df = df.merge(\n",
    "        df_commercial.rename(columns={'min_price': 'commercial_min'}),\n",
    "        left_on=['product_id', 'region_mapped'],\n",
    "        right_on=['product_id', 'region'],\n",
    "        how='left',\n",
    "        suffixes=('', '_comm')\n",
    "    )\n",
    "    \n",
    "    # Calculate offers percentage\n",
    "    df['offers_perc'] = (\n",
    "        df['bundle_nmv'].fillna(0) + \n",
    "        df['sku_discount_nmv'].fillna(0) + \n",
    "        df['quantity_nmv'].fillna(0)\n",
    "    ) / df['total_nmv'].replace(0, np.nan)\n",
    "    \n",
    "    # Calculate combined market prices\n",
    "    df['combined_min_market'] = df[['mp_min_price', 'ben_soliman_price', 'min_scrapped']].min(axis=1)\n",
    "    df['combined_max_market'] = df[['mp_max_price', 'ben_soliman_price', 'max_scrapped']].max(axis=1)\n",
    "    \n",
    "    # Calculate combined median (average of available medians)\n",
    "    median_cols = ['mp_mod_price', 'ben_soliman_price', 'median_scrapped']\n",
    "    df['combined_median_market'] = df[median_cols].mean(axis=1, skipna=True)\n",
    "    \n",
    "    # Calculate mp_price_score\n",
    "    df['mp_price_score'] = (df['price'] - df['mp_min_price']) / (\n",
    "        df['mp_max_price'] - df['mp_min_price']\n",
    "    ).replace(0, np.nan)\n",
    "    \n",
    "    # Stock comment\n",
    "    def get_stock_comment(row):\n",
    "        if row['in_stock_perc'] == 0:\n",
    "            return 'OOS'\n",
    "        elif row['doh'] > 30:\n",
    "            return 'Over Stocked'\n",
    "        elif row['doh'] < 1:\n",
    "            return 'low stock'\n",
    "        else:\n",
    "            return 'Good stocks'\n",
    "    \n",
    "    df['stock_comment'] = df.apply(get_stock_comment, axis=1)\n",
    "    \n",
    "    # RR comment\n",
    "    def get_rr_comment(row):\n",
    "        cu_rr = row['cu_rr']\n",
    "        high_rr = row['high_rr']\n",
    "        std = row['qty_std']\n",
    "        \n",
    "        if cu_rr >= high_rr - 0.5 * std and cu_rr <= high_rr + 0.5 * std:\n",
    "            return 'Normal rr'\n",
    "        elif cu_rr < high_rr - 0.5 * std:\n",
    "            return 'low rr'\n",
    "        elif cu_rr >= high_rr + 0.5 * std and cu_rr <= high_rr + 1.5 * std:\n",
    "            return 'High rr'\n",
    "        elif cu_rr > high_rr + 1.5 * std:\n",
    "            return 'Very High rr'\n",
    "        else:\n",
    "            return ''\n",
    "    \n",
    "    df['rr_comment'] = df.apply(get_rr_comment, axis=1)\n",
    "    \n",
    "    # Rets comment\n",
    "    def get_rets_comment(row):\n",
    "        cu_rets = row['cu_rets']\n",
    "        high_rets = row['high_rets']\n",
    "        rets_std = row['rets_std']\n",
    "        \n",
    "        if cu_rets >= high_rets - 0.5 * rets_std and cu_rets <= high_rets + 0.5 * rets_std:\n",
    "            return 'Normal rets'\n",
    "        elif cu_rets < high_rets - 0.5 * rets_std:\n",
    "            return 'low rets'\n",
    "        elif cu_rets >= high_rets + 0.5 * rets_std and cu_rets <= high_rets + 1.5 * rets_std:\n",
    "            return 'High rets'\n",
    "        elif cu_rets > high_rets + 1.5 * rets_std:\n",
    "            return 'Very High rets'\n",
    "        else:\n",
    "            return ''\n",
    "    \n",
    "    df['rets_comment'] = df.apply(get_rets_comment, axis=1)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b9320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 16: FINAL SCORING - MARKET POSITION & PRICE COMMENTS\n",
    "# =============================================================================\n",
    "\n",
    "def add_final_scoring(df):\n",
    "    \"\"\"\n",
    "    Add final scoring columns: combined_price_score, market_position_status, price_comment.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Combined price score\n",
    "    def calc_combined_price_score(row):\n",
    "        combined_max = row['combined_max_market']\n",
    "        combined_min = row['combined_min_market']\n",
    "        price = row['price']\n",
    "        mp_score = row['mp_price_score']\n",
    "        \n",
    "        if pd.notna(combined_max) and combined_max > 0 and pd.notna(combined_min) and combined_min < 1e9:\n",
    "            if combined_max != combined_min:\n",
    "                return (price - combined_min) / (combined_max - combined_min)\n",
    "        return mp_score\n",
    "    \n",
    "    df['combined_price_score'] = df.apply(calc_combined_price_score, axis=1)\n",
    "    \n",
    "    # Market position status\n",
    "    def get_market_position(row):\n",
    "        price = row['price']\n",
    "        combined_min = row['combined_min_market']\n",
    "        combined_median = row['combined_median_market']\n",
    "        combined_max = row['combined_max_market']\n",
    "        mp_min = row['mp_min_price']\n",
    "        bsp = row['ben_soliman_price']\n",
    "        median_scr = row['median_scrapped']\n",
    "        \n",
    "        # Check if no market data\n",
    "        if (pd.isna(combined_median) and pd.isna(mp_min) and \n",
    "            pd.isna(bsp) and pd.isna(median_scr)):\n",
    "            return 'No Market Data'\n",
    "        \n",
    "        # Adjust for edge cases\n",
    "        min_val = combined_min if pd.notna(combined_min) and combined_min < 1e9 else None\n",
    "        max_val = combined_max if pd.notna(combined_max) and combined_max > 0 else None\n",
    "        \n",
    "        if min_val is not None:\n",
    "            if price < min_val * 0.95:\n",
    "                return 'Below Market'\n",
    "            elif price <= min_val * 1.05:\n",
    "                return 'At Market Min'\n",
    "        \n",
    "        if pd.notna(combined_median):\n",
    "            if price < combined_median * 0.95:\n",
    "                return 'Below Median'\n",
    "            elif price <= combined_median * 1.05:\n",
    "                return 'At Median'\n",
    "        \n",
    "        if max_val is not None:\n",
    "            if price < max_val * 0.95:\n",
    "                return 'Above Median'\n",
    "            elif price <= max_val * 1.05:\n",
    "                return 'At Market Max'\n",
    "            elif price > max_val * 1.05:\n",
    "                return 'Above Market'\n",
    "        \n",
    "        return 'At Median'\n",
    "    \n",
    "    df['market_position_status'] = df.apply(get_market_position, axis=1)\n",
    "    \n",
    "    # Price comment\n",
    "    def get_price_comment(row):\n",
    "        combined_min = row['combined_min_market']\n",
    "        combined_max = row['combined_max_market']\n",
    "        price = row['price']\n",
    "        bm = row['bm']\n",
    "        target = row['target_margin']\n",
    "        mp_score = row['mp_price_score']\n",
    "        \n",
    "        # Calculate price score\n",
    "        if pd.notna(combined_max) and pd.notna(combined_min) and combined_max != combined_min:\n",
    "            price_score = (price - combined_min) / (combined_max - combined_min)\n",
    "        else:\n",
    "            price_score = mp_score\n",
    "        \n",
    "        if pd.isna(price_score):\n",
    "            if pd.notna(bm) and pd.notna(target):\n",
    "                return 'below target' if bm < target else 'above target'\n",
    "            return 'above target'\n",
    "        \n",
    "        if price_score > 0 and bm > target:\n",
    "            return 'High price'\n",
    "        elif price_score > 0 and bm < target:\n",
    "            return 'Credit note'\n",
    "        elif price_score < 0 and bm < target:\n",
    "            return 'Low Price'\n",
    "        elif price_score < 0 and bm > target:\n",
    "            return 'room to reduce'\n",
    "        elif bm < target:\n",
    "            return 'below target'\n",
    "        else:\n",
    "            return 'above target'\n",
    "    \n",
    "    df['price_comment'] = df.apply(get_price_comment, axis=1)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931d1969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 17: ACTION CLASSIFICATION LOGIC\n",
    "# =============================================================================\n",
    "\n",
    "def determine_action(row):\n",
    "    \"\"\"\n",
    "    Determine recommended action based on stock, price, and RR status.\n",
    "    This replicates the complex CASE statement in the final SELECT.\n",
    "    \"\"\"\n",
    "    stock_comment = row['stock_comment']\n",
    "    price_comment = row['price_comment']\n",
    "    rr_comment = row['rr_comment']\n",
    "    offers_perc = row.get('offers_perc', 0) or 0\n",
    "    commercial_min = row.get('commercial_min')\n",
    "    bm = row['bm']\n",
    "    target = row['target_margin']\n",
    "    cu_rr = row['cu_rr']\n",
    "    today_rr = row['today_rr']\n",
    "    stocks = row['stocks']\n",
    "    \n",
    "    # OOS\n",
    "    if stock_comment == 'OOS':\n",
    "        return 'purchase'\n",
    "    \n",
    "    # Good stocks scenarios\n",
    "    if stock_comment == 'Good stocks':\n",
    "        if price_comment in ['Low Price', 'below target'] and rr_comment == 'low rr':\n",
    "            if offers_perc < 0.1:\n",
    "                return 'offers & Credit Note'\n",
    "            return 'Credit Note'\n",
    "        if price_comment in ['Low Price', 'below target'] and rr_comment != 'low rr':\n",
    "            return 'increase price'\n",
    "        if price_comment == 'Credit note' and rr_comment == 'low rr':\n",
    "            return 'Credit Note'\n",
    "        if price_comment in ['High price', 'room to reduce', 'above target'] and rr_comment == 'low rr':\n",
    "            if pd.isna(commercial_min):\n",
    "                return 'Reduce price'\n",
    "            return 'Remove commercial min'\n",
    "        if rr_comment == 'Normal rr':\n",
    "            return 'No action'\n",
    "        if rr_comment == 'Very High rr' and bm < target:\n",
    "            return 'Increase price'\n",
    "        if rr_comment in ['Very High rr', 'High rr'] and bm >= target:\n",
    "            return 'No action'\n",
    "        if rr_comment == 'High rr' and bm < target:\n",
    "            return 'Increase price a bit'\n",
    "        if price_comment in ['Low Price', 'below target'] and offers_perc > 0.1:\n",
    "            return 'Credit Note'\n",
    "    \n",
    "    # Low stock scenarios\n",
    "    if stock_comment == 'low stock':\n",
    "        if price_comment == 'Credit note' and rr_comment == 'low rr':\n",
    "            return 'Credit Note & Purchase'\n",
    "        if price_comment in ['below target', 'Low Price'] and rr_comment == 'low rr':\n",
    "            if offers_perc < 0.1:\n",
    "                return 'Offers & Purchase'\n",
    "            return 'Purchase'\n",
    "        if price_comment in ['High price', 'above target', 'room to reduce'] and rr_comment == 'low rr':\n",
    "            if pd.isna(commercial_min):\n",
    "                return 'Purchase & reduce price'\n",
    "            return 'Purchase & Remove commercial min'\n",
    "        if rr_comment in ['High rr', 'Normal rr']:\n",
    "            return 'Purchase'\n",
    "        if rr_comment == 'Very High rr':\n",
    "            return 'increase price'\n",
    "    \n",
    "    # Over stocked scenarios\n",
    "    if stock_comment == 'Over Stocked':\n",
    "        if price_comment in ['below target', 'Low Price', 'Credit note'] and rr_comment == 'low rr':\n",
    "            return 'Credit Note'\n",
    "        if price_comment in ['High price', 'above target', 'room to reduce'] and rr_comment == 'low rr':\n",
    "            if cu_rr > 0:\n",
    "                if pd.isna(commercial_min):\n",
    "                    return 'Reduce price'\n",
    "                return 'Remove commercial min'\n",
    "            elif today_rr == 0:\n",
    "                return 'Check activation'\n",
    "            else:\n",
    "                return 'Remove commercial min'\n",
    "        if price_comment in ['below target', 'Low Price', 'Credit note'] and rr_comment in ['Very High rr', 'High rr', 'Normal rr']:\n",
    "            if stocks / (cu_rr if cu_rr > 0 else 1) < 30:\n",
    "                return 'No Action'\n",
    "            return 'Credit Note'\n",
    "        if price_comment in ['High price', 'above target'] and rr_comment in ['Very High rr', 'High rr', 'Normal rr']:\n",
    "            if stocks / (cu_rr if cu_rr > 0 else 1) < 30:\n",
    "                return 'No Action'\n",
    "            return 'Reduce Price'\n",
    "    \n",
    "    # Additional edge cases\n",
    "    if price_comment in ['below target', 'Low Price'] and rr_comment == 'low rr':\n",
    "        if cu_rr == 0 and today_rr > 0:\n",
    "            return 'No action'\n",
    "        elif cu_rr == 0:\n",
    "            return 'Check activation'\n",
    "    \n",
    "    return None\n",
    "\n",
    "def add_actions(df):\n",
    "    \"\"\"Add action and team assignment columns.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Determine action\n",
    "    df['action'] = df.apply(determine_action, axis=1)\n",
    "    \n",
    "    # Assign to teams based on action\n",
    "    df['pricing_team'] = df['action'].apply(\n",
    "        lambda x: 1 if pd.notna(x) and ('price' in str(x).lower() or 'offers' in str(x).lower()) else None\n",
    "    )\n",
    "    df['purchase_team'] = df['action'].apply(\n",
    "        lambda x: 1 if pd.notna(x) and 'purchase' in str(x).lower() else None\n",
    "    )\n",
    "    df['commercial_team'] = df['action'].apply(\n",
    "        lambda x: 1 if pd.notna(x) and ('credit note' in str(x).lower() or \n",
    "                                         'commercial min' in str(x).lower() or \n",
    "                                         'activation' in str(x).lower()) else None\n",
    "    )\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015fd88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 18: FINALIZE OUTPUT & ADD CALCULATED COLUMNS\n",
    "# =============================================================================\n",
    "\n",
    "def finalize_output(df, df_targets):\n",
    "    \"\"\"\n",
    "    Finalize the output DataFrame with all calculated columns.\n",
    "    Add stock value, stock contribution, and join targets data.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Calculate stock value\n",
    "    df['stock_value'] = df['stocks'] * df['price']\n",
    "    \n",
    "    # Calculate stock contribution per warehouse\n",
    "    df['stock_cntrb'] = df.groupby('warehouse_id')['stock_value'].transform(\n",
    "        lambda x: x / x.sum() if x.sum() > 0 else 0\n",
    "    )\n",
    "    \n",
    "    # Join targets data\n",
    "    df = df.merge(\n",
    "        df_targets[['warehouse_id', 'product_id', 'wh_sku_target', 'rem_nmv']],\n",
    "        on=['warehouse_id', 'product_id'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Clean up combined_min_market (replace inf with None)\n",
    "    df['combined_min_market'] = df['combined_min_market'].replace([np.inf, -np.inf, 1e9], np.nan)\n",
    "    df['combined_max_market'] = df['combined_max_market'].replace([0, np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Select and order final columns\n",
    "    final_columns = [\n",
    "        'region', 'wh', 'warehouse_id', 'product_id', 'sku', 'cat', 'brand',\n",
    "        'stocks', 'doh', 'stock_comment', 'price', 'bm', 'target_margin', 'price_comment',\n",
    "        'mp_min_price', 'mp_mod_price', 'mp_max_price', 'ben_soliman_price',\n",
    "        'min_scrapped', 'median_scrapped', 'max_scrapped',\n",
    "        'combined_min_market', 'combined_median_market', 'combined_max_market',\n",
    "        'mp_price_score', 'combined_price_score', 'market_position_status',\n",
    "        'high_rr', 'cu_rr', 'today_rr', 'rr_comment',\n",
    "        'high_rets', 'cu_rets', 'rets_comment', 'offers_perc', 'commercial_min',\n",
    "        'action', 'pricing_team', 'purchase_team', 'commercial_team',\n",
    "        'stock_value', 'stock_cntrb', 'wh_sku_target', 'rem_nmv'\n",
    "    ]\n",
    "    \n",
    "    # Keep only columns that exist\n",
    "    existing_cols = [c for c in final_columns if c in df.columns]\n",
    "    df = df[existing_cols]\n",
    "    \n",
    "    # Sort by high_rr * price descending\n",
    "    df['_sort_key'] = df['high_rr'] * df['price']\n",
    "    df = df.sort_values('_sort_key', ascending=False).drop('_sort_key', axis=1)\n",
    "    \n",
    "    # Rename 'wh' to 'warehouse_name' for clarity\n",
    "    df = df.rename(columns={'wh': 'warehouse_name', 'cu_rr': 'current_rr'})\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdb29c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 19: MAIN EXECUTION - RUN THE COMPLETE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def run_pricing_status_analysis():\n",
    "    \"\"\"\n",
    "    Main function to run the complete pricing status analysis.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with all pricing status metrics and recommended actions.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"PRICING STATUS ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Get warehouse mapping\n",
    "    print(\"\\n[1/12] Loading warehouse mapping...\")\n",
    "    df_whs = get_warehouse_mapping()\n",
    "    print(f\"       ✓ {len(df_whs)} warehouses loaded\")\n",
    "    \n",
    "    # Step 2: Fetch COGS data\n",
    "    print(\"\\n[2/12] Fetching COGS data...\")\n",
    "    df_cogs = fetch_current_cogs()\n",
    "    print(f\"       ✓ {len(df_cogs)} COGS records fetched\")\n",
    "    \n",
    "    # Step 3: Fetch stocks data\n",
    "    print(\"\\n[3/12] Fetching stock data...\")\n",
    "    df_stocks = fetch_stocks()\n",
    "    print(f\"       ✓ {len(df_stocks)} stock records fetched\")\n",
    "    \n",
    "    # Step 4: Fetch sales data\n",
    "    print(\"\\n[4/12] Fetching sales data...\")\n",
    "    df_sales = fetch_sales()\n",
    "    print(f\"       ✓ {len(df_sales)} sales records fetched\")\n",
    "    \n",
    "    # Step 5: Fetch prices\n",
    "    print(\"\\n[5/12] Fetching price data...\")\n",
    "    df_prices = fetch_prices()\n",
    "    print(f\"       ✓ {len(df_prices)} price records fetched\")\n",
    "    \n",
    "    # Step 6: Fetch marketplace prices\n",
    "    print(\"\\n[6/12] Fetching marketplace prices...\")\n",
    "    df_mp = fetch_marketplace_prices()\n",
    "    print(f\"       ✓ {len(df_mp)} marketplace price records fetched\")\n",
    "    \n",
    "    # Step 7: Fetch Ben Soliman prices\n",
    "    print(\"\\n[7/12] Fetching Ben Soliman prices...\")\n",
    "    df_bsp = fetch_ben_soliman_prices()\n",
    "    print(f\"       ✓ {len(df_bsp)} Ben Soliman price records fetched\")\n",
    "    \n",
    "    # Step 8: Fetch scrapped prices\n",
    "    print(\"\\n[8/12] Fetching scrapped market prices...\")\n",
    "    df_scrapped = fetch_scrapped_prices()\n",
    "    print(f\"       ✓ {len(df_scrapped)} scrapped price records fetched\")\n",
    "    \n",
    "    # Step 9: Fetch targets\n",
    "    print(\"\\n[9/12] Fetching category/brand targets...\")\n",
    "    df_cat_brand_targets = fetch_cat_brand_targets()\n",
    "    df_cat_targets = fetch_cat_targets(df_cat_brand_targets)\n",
    "    print(f\"       ✓ {len(df_cat_brand_targets)} category/brand targets fetched\")\n",
    "    \n",
    "    # Step 10: Fetch discounted sales\n",
    "    print(\"\\n[10/12] Fetching discounted sales data...\")\n",
    "    df_discounted = fetch_discounted_sales()\n",
    "    print(f\"       ✓ {len(df_discounted)} discounted sales records fetched\")\n",
    "    \n",
    "    # Step 11: Fetch commercial constraints\n",
    "    print(\"\\n[11/12] Fetching commercial constraints...\")\n",
    "    df_commercial = fetch_commercial_constraints()\n",
    "    print(f\"       ✓ {len(df_commercial)} commercial constraint records fetched\")\n",
    "    \n",
    "    # Step 12: Fetch targets data\n",
    "    print(\"\\n[12/12] Fetching warehouse SKU targets...\")\n",
    "    df_targets = fetch_targets_data(df_whs)\n",
    "    print(f\"       ✓ {len(df_targets)} target records fetched\")\n",
    "    \n",
    "    # Process and merge data\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"PROCESSING DATA...\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Build product metrics\n",
    "    print(\"\\n[A] Building product metrics...\")\n",
    "    df_metrics = build_product_metrics(\n",
    "        df_stocks, df_sales, df_whs, df_prices, df_cogs,\n",
    "        df_mp, df_bsp, df_scrapped, df_cat_brand_targets,\n",
    "        df_cat_targets, df_discounted\n",
    "    )\n",
    "    print(f\"    ✓ {len(df_metrics)} product-warehouse combinations\")\n",
    "    \n",
    "    # Add scoring and classification\n",
    "    print(\"\\n[B] Adding scoring and classification...\")\n",
    "    df_scored = add_scoring_classification(df_metrics, df_commercial)\n",
    "    print(f\"    ✓ Scoring added\")\n",
    "    \n",
    "    # Add final scoring\n",
    "    print(\"\\n[C] Adding final scoring (market position, price comments)...\")\n",
    "    df_final_scored = add_final_scoring(df_scored)\n",
    "    print(f\"    ✓ Final scoring added\")\n",
    "    \n",
    "    # Add actions\n",
    "    print(\"\\n[D] Determining recommended actions...\")\n",
    "    df_with_actions = add_actions(df_final_scored)\n",
    "    print(f\"    ✓ Actions determined\")\n",
    "    \n",
    "    # Finalize output\n",
    "    print(\"\\n[E] Finalizing output...\")\n",
    "    df_final = finalize_output(df_with_actions, df_targets)\n",
    "    print(f\"    ✓ Final output ready with {len(df_final)} records\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ANALYSIS COMPLETE!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "# Run the full analysis:\n",
    "df_result = run_pricing_status_analysis()\n",
    "df_result.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c646d27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 20: UTILITY FUNCTIONS - EXPORT & SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "def export_results(df, filename='pricing_status_output.xlsx'):\n",
    "    \"\"\"Export results to Excel file.\"\"\"\n",
    "    df.to_excel(filename, index=False)\n",
    "    print(f\"Results exported to {filename}\")\n",
    "    return filename\n",
    "\n",
    "def get_summary_stats(df):\n",
    "    \"\"\"Generate summary statistics from the analysis results.\"\"\"\n",
    "    summary = {\n",
    "        'Total SKU-Warehouse combinations': len(df),\n",
    "        'Unique Products': df['product_id'].nunique() if 'product_id' in df.columns else 0,\n",
    "        'Unique Warehouses': df['warehouse_id'].nunique() if 'warehouse_id' in df.columns else 0,\n",
    "    }\n",
    "    \n",
    "    # Stock status breakdown\n",
    "    if 'stock_comment' in df.columns:\n",
    "        stock_status = df['stock_comment'].value_counts().to_dict()\n",
    "        summary['Stock Status'] = stock_status\n",
    "    \n",
    "    # Action breakdown\n",
    "    if 'action' in df.columns:\n",
    "        action_counts = df['action'].value_counts().to_dict()\n",
    "        summary['Actions'] = action_counts\n",
    "    \n",
    "    # Team assignments\n",
    "    if 'pricing_team' in df.columns:\n",
    "        summary['Pricing Team Items'] = df['pricing_team'].notna().sum()\n",
    "    if 'purchase_team' in df.columns:\n",
    "        summary['Purchase Team Items'] = df['purchase_team'].notna().sum()\n",
    "    if 'commercial_team' in df.columns:\n",
    "        summary['Commercial Team Items'] = df['commercial_team'].notna().sum()\n",
    "    \n",
    "    # Market position breakdown\n",
    "    if 'market_position_status' in df.columns:\n",
    "        market_pos = df['market_position_status'].value_counts().to_dict()\n",
    "        summary['Market Position'] = market_pos\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def print_summary(summary):\n",
    "    \"\"\"Pretty print the summary statistics.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SUMMARY STATISTICS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for key, value in summary.items():\n",
    "        if isinstance(value, dict):\n",
    "            print(f\"\\n{key}:\")\n",
    "            for k, v in value.items():\n",
    "                print(f\"    {k}: {v}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# Usage:\n",
    "summary = get_summary_stats(df_result)\n",
    "print_summary(summary)\n",
    "export_results(df_result, 'pricing_status_output.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988918c0",
   "metadata": {},
   "source": [
    "# Pricing Status Analysis - Quick Reference\n",
    "\n",
    "## Data Flow Overview:\n",
    "\n",
    "1. **Static Data**: Warehouse mappings (region, cohort_id)\n",
    "2. **COGS**: Current cost of goods (wac_p)\n",
    "3. **Running Rates**: Predicted running rates from past 14 days\n",
    "4. **Stocks**: Available stock with DOH calculations\n",
    "5. **Sales**: 150-day sales history with percentile metrics\n",
    "6. **Prices**: Latest cohort pricing\n",
    "7. **Market Prices**: Min/Mod/Max from marketplace, Ben Soliman, and scraped data\n",
    "8. **Targets**: Category/brand margin targets\n",
    "9. **Discounts**: Bundle, SKU discount, quantity discount percentages\n",
    "10. **Commercial Constraints**: Minimum price restrictions\n",
    "\n",
    "## Key Metrics:\n",
    "\n",
    "| Metric | Description |\n",
    "|--------|-------------|\n",
    "| `doh` | Days on Hand (stocks / running_rate) |\n",
    "| `bm` | Basic Margin ((price - cost) / price) |\n",
    "| `high_rr` | 80th percentile of historical running rate |\n",
    "| `combined_price_score` | Position within market price range (0-1) |\n",
    "\n",
    "## Action Matrix:\n",
    "\n",
    "| Stock Status | Price Status | RR Status | Recommended Action |\n",
    "|--------------|--------------|-----------|-------------------|\n",
    "| OOS | - | - | Purchase |\n",
    "| Good stocks | Low/Below target | Low RR | Offers & Credit Note |\n",
    "| Good stocks | High | Low RR | Reduce price / Remove commercial min |\n",
    "| Low stock | - | Very High RR | Increase price |\n",
    "| Over Stocked | High | Low RR, cu_rr=0 | Check activation |\n",
    "\n",
    "## Configuration:\n",
    "\n",
    "To customize the analysis, modify:\n",
    "- `get_warehouse_mapping()` - Add/remove warehouses\n",
    "- `fetch_prices()` - Modify cohort_ids\n",
    "- `determine_action()` - Adjust action logic thresholds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a538dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RUN ANALYSIS - UNCOMMENT AND EXECUTE\n",
    "# =============================================================================\n",
    "\n",
    "# Run the full analysis:\n",
    "df_result = run_pricing_status_analysis()\n",
    "\n",
    "# View summary:\n",
    "summary = get_summary_stats(df_result)\n",
    "print_summary(summary)\n",
    "\n",
    "# Preview the data:\n",
    "df_result.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556310e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPORT RESULTS TO EXCEL (Optional)\n",
    "# =============================================================================\n",
    "\n",
    "# Export:\n",
    "export_results(df_result, 'pricing_status_output.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2595bbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# AGGREGATE ANALYSIS VIEW\n",
    "# =============================================================================\n",
    "\n",
    "def create_aggregate_analysis(df):\n",
    "    \"\"\"\n",
    "    Create aggregate analysis showing:\n",
    "    - Total target NMV (high_rr * price)\n",
    "    - Top dropping brands based on RR performance\n",
    "    - Market status breakdown by brand\n",
    "    - Required actions summary\n",
    "    \"\"\"\n",
    "    df_analysis = df.copy()\n",
    "    \n",
    "    # Calculate target NMV per row (high_rr * price)\n",
    "    df_analysis['target_nmv'] = df_analysis['high_rr'] * df_analysis['price']\n",
    "    \n",
    "    # Calculate RR drop percentage: (high_rr - current_rr) / high_rr\n",
    "    df_analysis['rr_drop_pct'] = (df_analysis['high_rr'] - df_analysis['current_rr']) / df_analysis['high_rr'].replace(0, np.nan)\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # 1. TOTAL TARGET NMV SUMMARY\n",
    "    # ==========================================================================\n",
    "    total_target_nmv = df_analysis['target_nmv'].sum()\n",
    "    total_current_nmv = (df_analysis['current_rr'] * df_analysis['price']).sum()\n",
    "    nmv_gap = total_target_nmv - total_current_nmv\n",
    "    nmv_gap_pct = nmv_gap / total_target_nmv * 100 if total_target_nmv > 0 else 0\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"📊 AGGREGATE ANALYSIS - PRICING STATUS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\n\" + \"─\" * 80)\n",
    "    print(\"💰 TOTAL NMV SUMMARY\")\n",
    "    print(\"─\" * 80)\n",
    "    print(f\"  Target NMV (High RR × Price):    {total_target_nmv:>15,.0f} EGP\")\n",
    "    print(f\"  Current NMV (Current RR × Price): {total_current_nmv:>15,.0f} EGP\")\n",
    "    print(f\"  NMV Gap:                          {nmv_gap:>15,.0f} EGP ({nmv_gap_pct:.1f}%)\")\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # 2. TOP DROPPING BRANDS ANALYSIS\n",
    "    # ==========================================================================\n",
    "    brand_agg = df_analysis.groupby('brand').agg({\n",
    "        'target_nmv': 'sum',\n",
    "        'high_rr': 'sum',\n",
    "        'current_rr': 'sum',\n",
    "        'price': 'mean',\n",
    "        'product_id': 'nunique',\n",
    "        'warehouse_id': 'nunique'\n",
    "    }).reset_index()\n",
    "    \n",
    "    brand_agg.columns = ['brand', 'target_nmv', 'total_high_rr', 'total_current_rr', \n",
    "                         'avg_price', 'num_products', 'num_warehouses']\n",
    "    \n",
    "    # Calculate current NMV and drop metrics\n",
    "    brand_agg['current_nmv'] = brand_agg['total_current_rr'] * brand_agg['avg_price']\n",
    "    brand_agg['nmv_drop'] = brand_agg['target_nmv'] - brand_agg['current_nmv']\n",
    "    brand_agg['rr_drop_pct'] = ((brand_agg['total_high_rr'] - brand_agg['total_current_rr']) / \n",
    "                                 brand_agg['total_high_rr'].replace(0, np.nan) * 100)\n",
    "    \n",
    "    # Sort by NMV drop (biggest drops first)\n",
    "    brand_agg_sorted = brand_agg.sort_values('nmv_drop', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + \"─\" * 80)\n",
    "    print(\"📉 TOP 15 DROPPING BRANDS (by NMV Gap)\")\n",
    "    print(\"─\" * 80)\n",
    "    \n",
    "    top_dropping = brand_agg_sorted.head(15)\n",
    "    print(f\"{'Brand':<30} {'Target NMV':>15} {'Current NMV':>15} {'NMV Drop':>15} {'RR Drop%':>10}\")\n",
    "    print(\"─\" * 85)\n",
    "    for _, row in top_dropping.iterrows():\n",
    "        print(f\"{str(row['brand'])[:30]:<30} {row['target_nmv']:>15,.0f} {row['current_nmv']:>15,.0f} \"\n",
    "              f\"{row['nmv_drop']:>15,.0f} {row['rr_drop_pct']:>9.1f}%\")\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # 3. MARKET STATUS BY DROPPING BRANDS\n",
    "    # ==========================================================================\n",
    "    # Get top 15 dropping brand names\n",
    "    top_dropping_brands = top_dropping['brand'].tolist()\n",
    "    \n",
    "    # Filter data to only include top dropping brands\n",
    "    df_top_brands = df_analysis[df_analysis['brand'].isin(top_dropping_brands)]\n",
    "    \n",
    "    # Market status breakdown for top dropping brands\n",
    "    market_status_by_brand = df_top_brands.groupby(['brand', 'market_position_status']).agg({\n",
    "        'target_nmv': 'sum',\n",
    "        'product_id': 'nunique'\n",
    "    }).reset_index()\n",
    "    \n",
    "    market_status_by_brand.columns = ['brand', 'market_position', 'target_nmv', 'num_skus']\n",
    "    \n",
    "    # Pivot for better view\n",
    "    market_pivot = market_status_by_brand.pivot_table(\n",
    "        index='brand', \n",
    "        columns='market_position', \n",
    "        values='num_skus', \n",
    "        fill_value=0\n",
    "    ).reset_index()\n",
    "    \n",
    "    print(\"\\n\" + \"─\" * 80)\n",
    "    print(\"🏪 MARKET POSITION STATUS (Top Dropping Brands - SKU Count)\")\n",
    "    print(\"─\" * 80)\n",
    "    print(market_pivot.to_string(index=False))\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # 4. REQUIRED ACTIONS BY BRAND\n",
    "    # ==========================================================================\n",
    "    actions_by_brand = df_top_brands.groupby(['brand', 'action']).agg({\n",
    "        'target_nmv': 'sum',\n",
    "        'product_id': 'nunique'\n",
    "    }).reset_index()\n",
    "    \n",
    "    actions_by_brand.columns = ['brand', 'action', 'target_nmv', 'num_skus']\n",
    "    \n",
    "    # Pivot actions\n",
    "    action_pivot = actions_by_brand.pivot_table(\n",
    "        index='brand',\n",
    "        columns='action',\n",
    "        values='num_skus',\n",
    "        fill_value=0\n",
    "    ).reset_index()\n",
    "    \n",
    "    print(\"\\n\" + \"─\" * 80)\n",
    "    print(\"⚡ REQUIRED ACTIONS (Top Dropping Brands - SKU Count)\")\n",
    "    print(\"─\" * 80)\n",
    "    print(action_pivot.to_string(index=False))\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # 5. ACTION SUMMARY FOR TOP DROPPING BRANDS\n",
    "    # ==========================================================================\n",
    "    action_summary = df_top_brands.groupby('action').agg({\n",
    "        'target_nmv': 'sum',\n",
    "        'product_id': 'nunique',\n",
    "        'brand': 'nunique'\n",
    "    }).reset_index()\n",
    "    \n",
    "    action_summary.columns = ['action', 'target_nmv_at_risk', 'num_skus', 'num_brands']\n",
    "    action_summary = action_summary.sort_values('target_nmv_at_risk', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + \"─\" * 80)\n",
    "    print(\"📋 ACTION PRIORITY SUMMARY (Top Dropping Brands)\")\n",
    "    print(\"─\" * 80)\n",
    "    print(f\"{'Action':<35} {'Target NMV at Risk':>18} {'# SKUs':>10} {'# Brands':>10}\")\n",
    "    print(\"─\" * 73)\n",
    "    for _, row in action_summary.iterrows():\n",
    "        action_name = str(row['action']) if pd.notna(row['action']) else 'No Action'\n",
    "        print(f\"{action_name[:35]:<35} {row['target_nmv_at_risk']:>18,.0f} {row['num_skus']:>10} {row['num_brands']:>10}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    \n",
    "    # Return dataframes for further analysis\n",
    "    return {\n",
    "        'total_metrics': {\n",
    "            'target_nmv': total_target_nmv,\n",
    "            'current_nmv': total_current_nmv,\n",
    "            'nmv_gap': nmv_gap,\n",
    "            'nmv_gap_pct': nmv_gap_pct\n",
    "        },\n",
    "        'brand_analysis': brand_agg_sorted,\n",
    "        'market_status_pivot': market_pivot,\n",
    "        'action_pivot': action_pivot,\n",
    "        'action_summary': action_summary\n",
    "    }\n",
    "\n",
    "# Run the aggregate analysis\n",
    "aggregate_results = create_aggregate_analysis(df_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b31069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DETAILED BRAND DRILLDOWN VIEW\n",
    "# =============================================================================\n",
    "\n",
    "def get_brand_drilldown(df, brand_name):\n",
    "    \"\"\"\n",
    "    Get detailed drilldown for a specific brand showing:\n",
    "    - All SKUs for the brand\n",
    "    - Their market status, RR status, and recommended actions\n",
    "    \"\"\"\n",
    "    df_brand = df[df['brand'] == brand_name].copy()\n",
    "    \n",
    "    if len(df_brand) == 0:\n",
    "        print(f\"No data found for brand: {brand_name}\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate target NMV\n",
    "    df_brand['target_nmv'] = df_brand['high_rr'] * df_brand['price']\n",
    "    df_brand['current_nmv'] = df_brand['current_rr'] * df_brand['price']\n",
    "    df_brand['nmv_gap'] = df_brand['target_nmv'] - df_brand['current_nmv']\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"🔍 BRAND DRILLDOWN: {brand_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Summary stats\n",
    "    print(f\"\\n📊 Summary:\")\n",
    "    print(f\"   Total SKUs: {df_brand['product_id'].nunique()}\")\n",
    "    print(f\"   Warehouses: {df_brand['warehouse_id'].nunique()}\")\n",
    "    print(f\"   Target NMV: {df_brand['target_nmv'].sum():,.0f} EGP\")\n",
    "    print(f\"   Current NMV: {df_brand['current_nmv'].sum():,.0f} EGP\")\n",
    "    print(f\"   NMV Gap: {df_brand['nmv_gap'].sum():,.0f} EGP\")\n",
    "    \n",
    "    # Show detailed SKU breakdown\n",
    "    columns_to_show = ['warehouse_name', 'sku', 'price', 'high_rr', 'current_rr', \n",
    "                       'stock_comment', 'market_position_status', 'price_comment', \n",
    "                       'rr_comment', 'action', 'nmv_gap']\n",
    "    \n",
    "    existing_cols = [c for c in columns_to_show if c in df_brand.columns]\n",
    "    \n",
    "    df_display = df_brand[existing_cols].sort_values('nmv_gap', ascending=False)\n",
    "    \n",
    "    print(f\"\\n📋 SKU Details (sorted by NMV Gap):\")\n",
    "    print(df_display.to_string(index=False))\n",
    "    \n",
    "    return df_brand\n",
    "\n",
    "# View top dropping brands DataFrame\n",
    "print(\"📈 TOP DROPPING BRANDS (Full DataFrame):\")\n",
    "aggregate_results['brand_analysis'].head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44476a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXAMPLE: DRILLDOWN INTO TOP DROPPING BRAND\n",
    "# =============================================================================\n",
    "\n",
    "# Get the top dropping brand name\n",
    "top_brand = aggregate_results['brand_analysis'].iloc[0]['brand']\n",
    "\n",
    "# Drilldown into the top dropping brand\n",
    "brand_detail = get_brand_drilldown(df_result, top_brand)\n",
    "\n",
    "# Or specify a brand manually:\n",
    "# brand_detail = get_brand_drilldown(df_result, \"Your Brand Name Here\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931c29cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPORT AGGREGATE ANALYSIS TO EXCEL\n",
    "# =============================================================================\n",
    "\n",
    "def get_pricing_action_skus(df):\n",
    "    \"\"\"\n",
    "    Filter SKUs that need pricing action:\n",
    "    - Good stocks or Over Stocked\n",
    "    - High RR or Low RR\n",
    "    - Price needs adjustment based on target margin or market prices\n",
    "    \"\"\"\n",
    "    df_filtered = df.copy()\n",
    "    \n",
    "    # Calculate additional metrics for context\n",
    "    df_filtered['target_nmv'] = df_filtered['high_rr'] * df_filtered['price']\n",
    "    df_filtered['current_nmv'] = df_filtered['current_rr'] * df_filtered['price']\n",
    "    df_filtered['nmv_gap'] = df_filtered['target_nmv'] - df_filtered['current_nmv']\n",
    "    df_filtered['margin_gap'] = df_filtered['bm'] - df_filtered['target_margin']\n",
    "    \n",
    "    # Filter 1: Stock status is Good stocks or Over Stocked\n",
    "    stock_filter = df_filtered['stock_comment'].isin(['Good stocks', 'Over Stocked'])\n",
    "    \n",
    "    # Filter 2: RR status is High rr, Very High rr, or low rr\n",
    "    rr_filter = df_filtered['rr_comment'].isin(['High rr', 'Very High rr', 'low rr'])\n",
    "    \n",
    "    # Filter 3: Price needs adjustment - actions related to pricing or offers\n",
    "    pricing_actions = [\n",
    "        'Reduce price', 'Reduce Price',\n",
    "        'Increase price', 'Increase Price',\n",
    "        'Increase price a bit',\n",
    "        'offers & Credit Note',\n",
    "        'Remove commercial min',\n",
    "        'Credit Note',\n",
    "        'Credit note'\n",
    "    ]\n",
    "    action_filter = df_filtered['action'].isin(pricing_actions)\n",
    "    \n",
    "    # Alternative: Also include SKUs where margin is off target or price is off market\n",
    "    margin_off_target = (df_filtered['bm'] < df_filtered['target_margin'] * 0.95) | \\\n",
    "                        (df_filtered['bm'] > df_filtered['target_margin'] * 1.1)\n",
    "    \n",
    "    price_off_market = df_filtered['market_position_status'].isin([\n",
    "        'Below Market', 'Above Market', 'Below Median', 'Above Median'\n",
    "    ])\n",
    "    \n",
    "    # Combine filters: (stock OK) AND (RR is high/low) AND (action needed OR margin/market off)\n",
    "    combined_filter = stock_filter & rr_filter & (action_filter | margin_off_target | price_off_market)\n",
    "    \n",
    "    df_pricing_action = df_filtered[combined_filter].copy()\n",
    "    \n",
    "    # Add recommendation priority\n",
    "    def get_priority(row):\n",
    "        if row['rr_comment'] == 'low rr' and row['stock_comment'] == 'Over Stocked':\n",
    "            return 1  # Highest priority - need to move stock\n",
    "        elif row['rr_comment'] == 'low rr' and row['stock_comment'] == 'Good stocks':\n",
    "            return 2\n",
    "        elif row['rr_comment'] in ['High rr', 'Very High rr'] and row['bm'] < row['target_margin']:\n",
    "            return 3  # Good opportunity to increase price\n",
    "        else:\n",
    "            return 4\n",
    "    \n",
    "    df_pricing_action['priority'] = df_pricing_action.apply(get_priority, axis=1)\n",
    "    \n",
    "    # Sort by priority and NMV gap\n",
    "    df_pricing_action = df_pricing_action.sort_values(\n",
    "        ['priority', 'nmv_gap'], \n",
    "        ascending=[True, False]\n",
    "    )\n",
    "    \n",
    "    # Select relevant columns for the pricing action sheet\n",
    "    columns_to_export = [\n",
    "        'priority', 'region', 'warehouse_name', 'product_id', 'sku', 'cat', 'brand',\n",
    "        'stocks', 'doh', 'stock_comment',\n",
    "        'price', 'bm', 'target_margin', 'margin_gap', 'price_comment',\n",
    "        'combined_min_market', 'combined_median_market', 'combined_max_market',\n",
    "        'market_position_status',\n",
    "        'high_rr', 'current_rr', 'rr_comment',\n",
    "        'action', 'offers_perc', 'commercial_min',\n",
    "        'target_nmv', 'current_nmv', 'nmv_gap'\n",
    "    ]\n",
    "    \n",
    "    existing_cols = [c for c in columns_to_export if c in df_pricing_action.columns]\n",
    "    \n",
    "    return df_pricing_action[existing_cols]\n",
    "\n",
    "\n",
    "def export_aggregate_analysis(df, aggregate_results, filename='pricing_aggregate_analysis.xlsx'):\n",
    "    \"\"\"\n",
    "    Export the aggregate analysis to an Excel file with multiple sheets:\n",
    "    - Summary: Overall metrics\n",
    "    - Pricing Action: SKUs needing price adjustment\n",
    "    - Brand Analysis: Top dropping brands\n",
    "    - Market Status: Market position by brand\n",
    "    - Actions: Required actions by brand\n",
    "    - Raw Data: Full detail data\n",
    "    \"\"\"\n",
    "    df_analysis = df.copy()\n",
    "    df_analysis['target_nmv'] = df_analysis['high_rr'] * df_analysis['price']\n",
    "    df_analysis['current_nmv'] = df_analysis['current_rr'] * df_analysis['price']\n",
    "    df_analysis['nmv_gap'] = df_analysis['target_nmv'] - df_analysis['current_nmv']\n",
    "    \n",
    "    # Get pricing action SKUs\n",
    "    df_pricing_action = get_pricing_action_skus(df)\n",
    "    \n",
    "    with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
    "        # Sheet 1: Summary\n",
    "        summary_df = pd.DataFrame([aggregate_results['total_metrics']])\n",
    "        summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
    "        \n",
    "        # Sheet 2: Pricing Action - SKUs needing price adjustment\n",
    "        df_pricing_action.to_excel(writer, sheet_name='Pricing Action', index=False)\n",
    "        \n",
    "        # Sheet 3: Brand Analysis (all brands)\n",
    "        aggregate_results['brand_analysis'].to_excel(writer, sheet_name='Brand Analysis', index=False)\n",
    "        \n",
    "        # Sheet 4: Market Status Pivot\n",
    "        aggregate_results['market_status_pivot'].to_excel(writer, sheet_name='Market Status', index=False)\n",
    "        \n",
    "        # Sheet 5: Action Pivot\n",
    "        aggregate_results['action_pivot'].to_excel(writer, sheet_name='Actions by Brand', index=False)\n",
    "        \n",
    "        # Sheet 6: Action Summary\n",
    "        aggregate_results['action_summary'].to_excel(writer, sheet_name='Action Summary', index=False)\n",
    "        \n",
    "        # Sheet 7: Raw Data with NMV calculations\n",
    "        df_analysis.to_excel(writer, sheet_name='Raw Data', index=False)\n",
    "    \n",
    "    print(f\"✓ Aggregate analysis exported to: {filename}\")\n",
    "    print(f\"  - Pricing Action sheet: {len(df_pricing_action)} SKUs needing price adjustment\")\n",
    "    return filename\n",
    "\n",
    "# Export:\n",
    "export_aggregate_analysis(df_result, aggregate_results, 'pricing_aggregate_analysis.xlsx')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
