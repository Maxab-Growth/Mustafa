{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "353eed4b-fed6-4ddb-930f-0d1c581aaf2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Upgrade pip\n",
    "!pip install --upgrade pip\n",
    "# Connectivity\n",
    "!pip install psycopg2-binary  # PostgreSQL adapter\n",
    "# !pip install snowflake-connector-python  # Snowflake connector\n",
    "!pip install snowflake-connector-python==3.15.0 # Snowflake connector Older Version\n",
    "!pip install snowflake-sqlalchemy  # Snowflake SQLAlchemy connector\n",
    "!pip install warnings # Warnings management\n",
    "# !pip install pyarrow # Serialization\n",
    "!pip install keyring==23.11.0 # Key management\n",
    "!pip install sqlalchemy==1.4.46 # SQLAlchemy\n",
    "!pip install requests # HTTP requests\n",
    "!pip install boto3 # AWS SDK\n",
    "# !pip install slackclient # Slack API\n",
    "!pip install oauth2client # Google Sheets API\n",
    "!pip install gspread==5.9.0 # Google Sheets API\n",
    "!pip install gspread_dataframe # Google Sheets API\n",
    "!pip install google.cloud # Google Cloud\n",
    "# Data manipulation and analysis\n",
    "!pip install polars\n",
    "!pip install pandas==2.2.1\n",
    "!pip install numpy\n",
    "# !pip install fastparquet\n",
    "!pip install openpyxl # Excel file handling\n",
    "!pip install xlsxwriter # Excel file handling\n",
    "# Linear programming\n",
    "!pip install pulp\n",
    "# Date and time handling\n",
    "!pip install --upgrade datetime\n",
    "!pip install python-time\n",
    "!pip install --upgrade pytz\n",
    "# Progress bar\n",
    "!pip install tqdm\n",
    "# Database data types\n",
    "!pip install db-dtypes\n",
    "# Geospatial data handling\n",
    "# !pip install geopandas\n",
    "# !pip install shapely\n",
    "# !pip install fiona\n",
    "# !pip install haversine\n",
    "# Plotting\n",
    "\n",
    "# Modeling\n",
    "!pip install statsmodels\n",
    "!pip install scikit-learn\n",
    "\n",
    "!pip install import-ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64355078-99bc-436e-b5a9-69fbc4f983fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/snowflake/connector/options.py:104: UserWarning: You have an incompatible version of 'pyarrow' installed (22.0.0), please install a version that adheres to: 'pyarrow<19.0.0; extra == \"pandas\"'\n",
      "  warn_incompatible_dep(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.Renviron\n",
      "/home/ec2-user/service_account_key.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "import json\n",
    "from datetime import date, timedelta\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import setup_environment_2\n",
    "import importlib\n",
    "import import_ipynb\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import pytz  \n",
    "import os\n",
    "import snowflake.connector\n",
    "import boto3\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "importlib.reload(setup_environment_2)\n",
    "setup_environment_2.initialize_env()\n",
    "import base64\n",
    "from botocore.exceptions import ClientError\n",
    "from requests import get\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import time\n",
    "import gspread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b5cdc8b-89b2-4b8a-a04e-9e70a9a878f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_snowflake(query, columns=[]):\n",
    "    import os\n",
    "    import snowflake.connector\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    con = snowflake.connector.connect(\n",
    "        user =  os.environ[\"SNOWFLAKE_USERNAME\"],\n",
    "        account= os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "        password= os.environ[\"SNOWFLAKE_PASSWORD\"],\n",
    "        database =os.environ[\"SNOWFLAKE_DATABASE\"]\n",
    "    )\n",
    "    try:\n",
    "        cur = con.cursor()\n",
    "        cur.execute(\"USE WAREHOUSE COMPUTE_WH\")\n",
    "        cur.execute(query)\n",
    "        if len(columns) == 0:\n",
    "            out = pd.DataFrame(np.array(cur.fetchall()))\n",
    "        else:\n",
    "            out = pd.DataFrame(np.array(cur.fetchall()),columns=columns)\n",
    "        return out\n",
    "    except Exception as e:\n",
    "        print(\"Error: \", e)\n",
    "    finally:\n",
    "        cur.close()\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5dbea5-e97b-40bc-ba43-9a1ab1848622",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scope = [\"https://spreadsheets.google.com/feeds\",\n",
    "         'https://www.googleapis.com/auth/spreadsheets',\n",
    "         \"https://www.googleapis.com/auth/drive.file\",\n",
    "         \"https://www.googleapis.com/auth/drive\"]\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_dict(json.loads(setup_environment_2.get_secret(\"prod/maxab-sheets\")), scope)\n",
    "client = gspread.authorize(creds)\n",
    "force_brands = client.open('QD_brands').worksheet('Include_brands')\n",
    "force_brands_df = pd.DataFrame(force_brands.get_all_records())\n",
    "if(force_brands_df.empty):\n",
    "    force_brands_df = pd.DataFrame(columns=['brand'])\n",
    "    brand_filter = \"\"\n",
    "else:\n",
    "    brand_filter = f\"OR brand IN ({','.join([repr(b) for b in list(force_brands_df.brand.unique())])})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a02939-f4ba-445d-bc88-940f233ae3a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#changed\n",
    "command_string = f'''\n",
    "with rr as (\n",
    "\n",
    "select product_id,warehouse_id,rr\n",
    "from (\n",
    "select * ,max(date)over(partition by product_id,warehouse_id) as max_date\n",
    "from finance.PREDICTED_RUNNING_RATES\n",
    "qualify date = max_date\n",
    "and date::date >= current_date - 14 \n",
    ")\n",
    "\n",
    "),\n",
    "stocks as (\n",
    "WITH whs as (SELECT *\n",
    "             FROM   (values\n",
    "                            ('Cairo', 'El-Marg', 38,700),\n",
    "                            ('Cairo', 'Mostorod', 1,700),\n",
    "                            ('Giza', 'Barageel', 236,701),\n",
    "                            ('Delta West', 'El-Mahala', 337,703),\n",
    "                            ('Delta West', 'Tanta', 8,703),\n",
    "                            ('Delta East', 'Mansoura FC', 339,704),\n",
    "                            ('Delta East', 'Sharqya', 170,704),\n",
    "                            ('Upper Egypt', 'Assiut FC', 501,1124),\n",
    "                            ('Upper Egypt', 'Bani sweif', 401,1126),\n",
    "                            ('Upper Egypt', 'Menya Samalot', 703,1123),\n",
    "                            ('Upper Egypt', 'Sohag', 632,1125),\n",
    "                            ('Alexandria', 'Khorshed Alex', 797,702),\n",
    "\t\t\t\t\t\t\t('Giza', 'Sakkarah', 962,701)\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t)\n",
    "                    x(region, wh, warehouse_id,cohort_id))\n",
    "select cohort_id,product_id,sum(stocks) as stocks ,case when sum(rr) > 0 then SUM(stocks)/sum(rr) else SUM(stocks) end  as doh\n",
    "from (\n",
    "\t\tSELECT DISTINCT whs.region,\n",
    "\t\t\t\tcohort_id,\t\n",
    "                whs.wh,\n",
    "                product_warehouse.product_id,\n",
    "                (product_warehouse.available_stock)::integer as stocks,\n",
    "\t\t\t\tcoalesce(rr.rr,0) as rr \n",
    "        from whs\n",
    "        JOIN product_warehouse ON product_warehouse.warehouse_id = whs.warehouse_id\n",
    "        JOIN products on product_warehouse.product_id = products.id\n",
    "        JOIN product_units ON products.unit_id = product_units.id\n",
    "\t\tleft join rr on rr.product_id= products.id and rr.warehouse_id = whs.warehouse_id\n",
    "\n",
    "        where   product_warehouse.warehouse_id not in (6,9,10)\n",
    "            AND product_warehouse.is_basic_unit = 1\n",
    "\t\t\tand product_warehouse.available_stock > 0 \n",
    "\n",
    ")\n",
    "group by all\n",
    "HAVING doh > 1 \n",
    "),\n",
    "base as (\n",
    "select *, row_number()over(partition by retailer_id order by priority) as rnk \n",
    "from (\n",
    "select x.*,TAGGABLE_ID as retailer_id \n",
    "from (\n",
    "select id as cohort_id,name as cohort_name,priority,dynamic_tag_id \n",
    "from cohorts \n",
    "where is_active = 'true'\n",
    "and id in (700,701,702,703,704,1123,1124,1125,1126)\n",
    ") x \n",
    "join DYNAMIC_TAGgables dt on x.dynamic_tag_id = dt.dynamic_tag_id\n",
    "where dt.taggable_id in (select taggable_id from DYNAMIC_TAGgables where dynamic_tag_id in (2807, 2808, 2809, 2810, 2811, 2812))\n",
    ")\n",
    "qualify rnk = 1 \n",
    "order by cohort_id\n",
    "),\n",
    "selected_skus as (\n",
    "select *\n",
    "from (\n",
    "select cohort_id,cohort_name,product_id,cat,brand,row_number()over(partition by cohort_id,cat order by cntrb) as num_skus\n",
    "from (\n",
    "select *,min(case when cumulative_sum > 0.4 then cumulative_sum end) over(partition by cat, cohort_id) as thres\n",
    "from (\n",
    "select *,SUM(cntrb) OVER (partition by cat, cohort_id ORDER BY cntrb desc ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum\n",
    "from (\n",
    "select *, num_order/sum(num_order)over(partition by cat,cohort_id) as cntrb\n",
    "from (\n",
    "SELECT  DISTINCT\n",
    "\t\tbase.cohort_id,\n",
    "\t\tbase.cohort_name,\n",
    "\t\tpso.product_id,\n",
    "\t\tCONCAT(products.name_ar,' ',products.size,' ',product_units.name_ar) as sku,\n",
    "\t\tbrands.name_ar as brand, \n",
    "\t\tcategories.name_ar as cat,\n",
    "\t\tcount(distinct parent_sales_order_id) as num_order ,\n",
    "        sum(pso.total_price) as nmv,\n",
    "       sum(COALESCE(f.wac_p,0) * pso.purchased_item_count * pso.basic_unit_count) as cogs,\n",
    "\t   (nmv-cogs)/nmv as bm \n",
    "\t\t\n",
    "\n",
    "FROM product_sales_order pso\n",
    "JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "join base on base.retailer_id = so.retailer_id\n",
    "JOIN products on products.id=pso.product_id\n",
    "JOIN brands on products.brand_id = brands.id\n",
    "JOIN categories ON products.category_id = categories.id and categories.name_ar not like '%سايب%'\n",
    "JOIN finance.all_cogs f  ON f.product_id = pso.product_id\n",
    "                        AND f.from_date::date <= so.created_at::date\n",
    "                        AND f.to_date::date > so.created_at::date\n",
    "JOIN product_units ON product_units.id = products.unit_id \n",
    "join stocks s on s.product_id = pso.product_id and s.cohort_id = base.cohort_id\n",
    "\n",
    "WHERE   True\n",
    "    AND so.created_at::date between date_trunc('month',current_date - interval '2 months') and CURRENT_date-1\n",
    "    AND so.sales_order_status_id not in (7,12)\n",
    "    AND so.channel IN ('telesales','retailer')\n",
    "    AND pso.purchased_item_count <> 0\n",
    "    and products.activation = 'true'\n",
    "\n",
    "GROUP BY ALL\n",
    ")\n",
    ")\n",
    ")\n",
    ")\n",
    "where cumulative_sum <= thres \n",
    "{brand_filter}\n",
    ")\n",
    "where  num_skus <= 10 \n",
    "{brand_filter}\n",
    "),\n",
    "\n",
    "main as (\n",
    "select * ,max(rets) over(partition by cohort_id) as max_rets \n",
    "from(\n",
    "select *,count(distinct retailer_id) over(partition by region,cohort_id) as rets \n",
    "from (\n",
    "SELECT  DISTINCT\n",
    "\t\tso.created_at::date as date,\n",
    "\t\tparent_sales_order_id,\n",
    "\t\tso.retailer_id,\n",
    "\t\tbase.cohort_id,\n",
    "\t\tbase.cohort_name,\n",
    "\t\tcase when regions.id = 2 then states.name_en else regions.name_en end as region,\n",
    "\t\tpso.product_id,\n",
    "\t\tCONCAT(products.name_ar,' ',products.size,' ',product_units.name_ar) as sku,\n",
    "\t\tpacking_unit_id,\n",
    "\t\tbrands.name_ar as brand, \n",
    "\t\tcategories.name_ar as cat,\n",
    "\t\tsum(pso.purchased_item_count) as qty,\n",
    "        sum(pso.total_price) as nmv,\n",
    "       sum(COALESCE(f.wac_p,0) * pso.purchased_item_count * pso.basic_unit_count) as cogs,\n",
    "\t   (nmv-cogs)/nmv as bm \n",
    "\t\t\n",
    "\n",
    "FROM product_sales_order pso\n",
    "JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "join base on base.retailer_id = so.retailer_id\n",
    "JOIN products on products.id=pso.product_id\n",
    "JOIN brands on products.brand_id = brands.id\n",
    "JOIN categories ON products.category_id = categories.id\n",
    "JOIN finance.all_cogs f  ON f.product_id = pso.product_id\n",
    "                        AND f.from_date::date <= so.created_at::date\n",
    "                        AND f.to_date::date > so.created_at::date\n",
    "JOIN product_units ON product_units.id = products.unit_id \n",
    "JOIN materialized_views.retailer_polygon on materialized_views.retailer_polygon.retailer_id=so.retailer_id\n",
    "JOIN districts on districts.id=materialized_views.retailer_polygon.district_id\n",
    "JOIN cities on cities.id=districts.city_id\n",
    "join states on states.id=cities.state_id\n",
    "join regions on regions.id=states.region_id\n",
    "join selected_skus ss on ss.product_id = pso.product_id and ss.cohort_id = base.cohort_id\n",
    "\n",
    "WHERE   True\n",
    "    AND so.created_at::date between date_trunc('month',current_date - interval '2 months') and CURRENT_date-1\n",
    "    AND so.sales_order_status_id not in (7,12)\n",
    "    AND so.channel IN ('telesales','retailer')\n",
    "    AND pso.purchased_item_count <> 0\n",
    "    and products.activation = 'true'\n",
    "\n",
    "GROUP BY ALL\n",
    ")\n",
    ")\n",
    "qualify rets = max_rets\n",
    "),\n",
    "cohort_data as (\n",
    "select region,cohort_id,cohort_name,product_id,sku,brand,cat,packing_unit_id,\n",
    "PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY qty) AS region_q1,\n",
    "MEDIAN(qty) as region_median,\n",
    "PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY qty) AS region_q3,\n",
    "PERCENTILE_CONT(0.85) WITHIN GROUP (ORDER BY qty) AS region_85,\n",
    "STDDEV_POP(qty) as std\n",
    "from main\n",
    "group by all \n",
    "),\n",
    "recent_cohort_data as (\n",
    "select cohort_id,cohort_name,product_id,sku,brand,cat,packing_unit_id,\n",
    "MEDIAN(qty) as recent_region_median,\n",
    "PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY qty) AS recent_region_q3,\n",
    "STDDEV_POP(qty) as recent_std\n",
    "from main\n",
    "where date between current_date - 8 and current_date - 1 \n",
    "group by all \n",
    "),\n",
    " freq_table AS (\n",
    "  SELECT\n",
    "  \t cohort_id,cohort_name,\n",
    "    PRODUCT_ID,sku,brand,cat,\n",
    "\tpacking_unit_id,\n",
    "    qty,\n",
    "    COUNT(distinct parent_sales_order_id) AS freq\n",
    "  FROM main\n",
    "  GROUP BY all\n",
    "),\n",
    "lag_lead AS (\n",
    "  SELECT\n",
    "   cohort_id,cohort_name,\n",
    "    PRODUCT_ID,sku,brand,cat,\n",
    "\tpacking_unit_id,\n",
    "    qty,\n",
    "    freq,\n",
    "    LAG(freq) OVER (PARTITION BY cohort_id,PRODUCT_ID,packing_unit_id ORDER BY qty) AS prev_freq,\n",
    "    LEAD(freq) OVER (PARTITION BY cohort_id,PRODUCT_ID,packing_unit_id ORDER BY qty) AS next_freq\n",
    "  FROM freq_table\n",
    "),\n",
    "most_freq as (\n",
    "select * \n",
    "from (\n",
    "select *,max(cntrb)over(partition by product_id,packing_unit_id,cohort_id) as max_cntrb\n",
    "from (\n",
    "SELECT *, freq/sum(freq) over(partition by product_id,packing_unit_id,cohort_id) as cntrb\n",
    "FROM lag_lead ll \n",
    "WHERE (freq > COALESCE(prev_freq, -1))\n",
    "  AND (freq > COALESCE(next_freq, -1))\n",
    "  )\n",
    "  )\n",
    "  where cntrb >= max_cntrb- 0.05\n",
    "  order by product_id\n",
    "),\n",
    "most_qty as (\n",
    "select cohort_id,cohort_name,product_id,sku,cat,brand,packing_unit_id,ceil(sum(freq_cntrb*qty)) as final_qty \n",
    "from (\n",
    "select *,freq/sum(freq)over(partition by  product_id,packing_unit_id,cohort_id) as freq_cntrb\n",
    "from most_freq \n",
    ")\n",
    "group by all \n",
    "),\n",
    "final_data as (\n",
    "select *,\n",
    "ceil(\n",
    "least(\n",
    "GREATEST(\n",
    "      recent_region_median + 0.75 * recent_std,\n",
    "      final_qty,\n",
    "\t  region_median+0.75*std,\n",
    "\t  region_median+2,\n",
    "\t  2\n",
    "    ),\n",
    "\tGREATEST(region_median+2,region_median*1.5)\n",
    "\t)\n",
    "\t\n",
    "\t) as tier_1,\n",
    " ceil(\n",
    " least(\n",
    " GREATEST(\n",
    "      final_qty + 1 * std,\n",
    "      region_q3 + 1 * std,\n",
    "\t  region_85 + 0.5 * std,\n",
    "      recent_region_q3 + 1 * recent_std,\n",
    "\t  tier_1*1.4\n",
    "    ),\n",
    "\ttier_1*3\n",
    "\t)\n",
    "\t) as tier_2\n",
    "from (\n",
    "select  rd.region,mq.*,region_q1,\n",
    "region_median,\n",
    "region_q3,\n",
    "region_85,\n",
    "std,\n",
    "COALESCE(recent_region_median,0) as recent_region_median,\n",
    "COALESCE(recent_region_q3,0) as recent_region_q3,\n",
    "COALESCE(recent_std,0) as recent_std\n",
    "from cohort_data rd \n",
    "join most_qty mq on rd.cohort_id =mq.cohort_id\n",
    "and rd.product_id =  mq.product_id\n",
    "and rd.packing_unit_id = mq.packing_unit_id \n",
    "left join recent_cohort_data rrd on rrd.cohort_id =mq.cohort_id\n",
    "and rrd.product_id =  mq.product_id\n",
    "and rrd.packing_unit_id = mq.packing_unit_id \n",
    ")\n",
    "),\n",
    "local_prices as (\n",
    "SELECT  case when cpu.cohort_id in (700,695) then 'Cairo'\n",
    "             when cpu.cohort_id in (701) then 'Giza'\n",
    "             when cpu.cohort_id in (704,698) then 'Delta East'\n",
    "             when cpu.cohort_id in (703,697) then 'Delta West'\n",
    "             when cpu.cohort_id in (696,1123,1124,1125,1126) then 'Upper Egypt'\n",
    "             when cpu.cohort_id in (702,699) then 'Alexandria'\n",
    "        end as region,\n",
    "\t\tcohort_id,\n",
    "        pu.product_id,\n",
    "\t\tpu.packing_unit_id as packing_unit_id,\n",
    "\t\tpu.basic_unit_count,\n",
    "        avg(cpu.price) as price\n",
    "FROM    cohort_product_packing_units cpu\n",
    "join    PACKING_UNIT_PRODUCTS pu on pu.id = cpu.product_packing_unit_id\n",
    "WHERE   cpu.cohort_id in (700,701,702,703,704,696,695,698,697,699,1123,1124,1125,1126)\n",
    "    and cpu.created_at::date<>'2023-07-31'\n",
    "    and cpu.is_customized = true\n",
    "\tgroup by all \n",
    "),\n",
    "live_prices as (\n",
    "select region,cohort_id,product_id,pu_id as packing_unit_id,buc as basic_unit_count,NEW_PRICE as price\n",
    "from materialized_views.DBDP_PRICES\n",
    "where created_at = current_date\n",
    "and DATE_PART('hour',CURRENT_TIME) BETWEEN SPLIT_PART(time_slot, '-', 1)::int AND SPLIT_PART(time_slot, '-', 2)::int\n",
    "and cohort_id in (700,701,702,703,704,696,695,698,697,699,1123,1124,1125,1126)\n",
    "),\n",
    "prices as (\n",
    "select *\n",
    "from (\n",
    "    SELECT *, 1 AS priority FROM live_prices\n",
    "    UNION ALL\n",
    "    SELECT *, 2 AS priority FROM local_prices\n",
    ")\n",
    "QUALIFY ROW_NUMBER() OVER (PARTITION BY region,cohort_id,product_id,packing_unit_id ORDER BY priority) = 1\n",
    "),\n",
    "region_prices as (\n",
    "select region,product_id,packing_unit_id,basic_unit_count,avg(price) as region_price\n",
    "from prices \n",
    "where price is not null \n",
    "group by all\n",
    "\n",
    "),\n",
    "finalized as (\n",
    "select fd.*,COALESCE(p.basic_unit_count,rp.basic_unit_count) as basic_unit_count,COALESCE(p.price,rp.region_price) as price\n",
    "from final_data fd \n",
    "left join prices p on fd.cohort_id = p.cohort_id and p.product_id = fd.product_id and p.packing_unit_id = fd.packing_unit_id\n",
    "left join region_prices rp on case when fd.region = 'Giza' then 'Cairo' else fd.region end = rp.region and rp.product_id = fd.product_id and rp.packing_unit_id = fd.packing_unit_id\n",
    "),\n",
    "cntrbs as (\n",
    "select main.product_id,main.cohort_id,main.packing_unit_id,\n",
    "count(distinct case when qty < tier_1 then retailer_id end ) as ret_below_t1,\n",
    "count(distinct case when qty >= tier_1  and qty <tier_2 then retailer_id end ) as ret_t1,\n",
    "count(distinct case when qty >= tier_2  then retailer_id end )as ret_t2 \n",
    "from main\n",
    "join finalized f on main.product_id = f.product_id and main.cohort_id = f.cohort_id and f.packing_unit_id = main.packing_unit_id \n",
    "group by all \n",
    ")\n",
    "select f.region,f.cohort_id,f.cohort_name,f.product_id,f.sku,f.cat,f.brand,f.packing_unit_id,TIER_1,tier_2,price, c.ret_below_t1,ret_t1,ret_t2,wac_p*BASIC_UNIT_COUNT as wac,f.region_median,f.RECENT_REGION_MEDIAN\n",
    "from finalized f\n",
    "join cntrbs c on c.product_id = f.product_id and c.cohort_id = f.cohort_id and f.packing_unit_id = c.packing_unit_id\n",
    "join finance.all_cogs cogs on cogs.product_id = f.product_id and CURRENT_TIMESTAMP between cogs.from_date and cogs.to_date\n",
    "where price is not null\n",
    "'''\n",
    "quantity_disc_data = query_snowflake(command_string, columns = ['REGION','COHORT_ID','COHORT_NAME','PRODUCT_ID','SKU','CAT','BRAND','PACKING_UNIT_ID','TIER_1','TIER_2','PRICE','RET_BELOW_T1','RET_T1','RET_T2','WAC_P','region_median','RECENT_REGION_MEDIAN'])\n",
    "quantity_disc_data.columns = quantity_disc_data.columns.str.lower()\n",
    "quantity_disc_data.product_id = pd.to_numeric(quantity_disc_data.product_id)\n",
    "quantity_disc_data.packing_unit_id = pd.to_numeric(quantity_disc_data.packing_unit_id)\n",
    "quantity_disc_data.tier_1 = pd.to_numeric(quantity_disc_data.tier_1)\n",
    "quantity_disc_data.tier_2 = pd.to_numeric(quantity_disc_data.tier_2)\n",
    "\n",
    "quantity_disc_data.price = pd.to_numeric(quantity_disc_data.price)\n",
    "quantity_disc_data.wac_p = pd.to_numeric(quantity_disc_data.wac_p)\n",
    "quantity_disc_data.ret_below_t1 = pd.to_numeric(quantity_disc_data.ret_below_t1)\n",
    "\n",
    "quantity_disc_data.ret_t1 = pd.to_numeric(quantity_disc_data.ret_t1)\n",
    "quantity_disc_data.ret_t2 = pd.to_numeric(quantity_disc_data.ret_t2)\n",
    "\n",
    "quantity_disc_data.region_median = pd.to_numeric(quantity_disc_data.region_median)\n",
    "quantity_disc_data.recent_region_median = pd.to_numeric(quantity_disc_data.recent_region_median)\n",
    "\n",
    "\n",
    "quantity_disc_data = quantity_disc_data[~quantity_disc_data['cat'].isin(['سكر','كروت شحن','مياه معدنيه','مقرمشات','شيبسي'])]\n",
    "quantity_disc_data = quantity_disc_data[~quantity_disc_data['brand'].isin(['ريد بل','بيرل','ترافل','جليد','جيليت لندن بريدج','رايد','سندة ارز','شاي الورردة','فل','كمارا','أجين','فيوري'])]\n",
    "quantity_disc_data['bm'] = (quantity_disc_data['price']-quantity_disc_data['wac_p']) / quantity_disc_data['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c08569-d3ab-4e5c-9a13-4c2d64bf6a1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "\n",
    "with base as (\n",
    "select *, row_number()over(partition by retailer_id order by priority) as rnk \n",
    "from (\n",
    "select x.*,TAGGABLE_ID as retailer_id \n",
    "from (\n",
    "select id as cohort_id,name as cohort_name,priority,dynamic_tag_id \n",
    "from cohorts \n",
    "where is_active = 'true'\n",
    "and id in (700,701,702,703,704,1123,1124,1125,1126)\n",
    ") x \n",
    "join DYNAMIC_TAGgables dt on x.dynamic_tag_id = dt.dynamic_tag_id\n",
    ")\n",
    "qualify rnk = 1 \n",
    "order by cohort_id\n",
    ")\n",
    "\n",
    "SELECT  DISTINCT\n",
    "\t\tbase.cohort_id,\n",
    "\t\tpso.product_id,\n",
    "        pso.packing_unit_id,\n",
    "        sum(pso.total_price) as nmv\n",
    "\n",
    "FROM product_sales_order pso\n",
    "JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "JOIN products on products.id=pso.product_id\n",
    "JOIN brands on products.brand_id = brands.id\n",
    "JOIN categories ON products.category_id = categories.id and categories.name_ar not like '%سايب%'\n",
    "JOIN product_units ON product_units.id = products.unit_id \n",
    "JOIN materialized_views.retailer_polygon on materialized_views.retailer_polygon.retailer_id=so.retailer_id\n",
    "JOIN districts on districts.id=materialized_views.retailer_polygon.district_id\n",
    "JOIN cities on cities.id=districts.city_id\n",
    "join states on states.id=cities.state_id\n",
    "join regions on regions.id=states.region_id\n",
    "join base on base.retailer_id = so.retailer_id\n",
    "\n",
    "WHERE   so.created_at ::date between date_trunc('month',current_date - interval '2 months') and current_date -1 \n",
    "    AND so.sales_order_status_id not in (7,12)\n",
    "    AND so.channel IN ('telesales','retailer')\n",
    "    AND pso.purchased_item_count <> 0\n",
    "    and products.activation ='true'\n",
    "\n",
    "GROUP BY ALL\n",
    "'''\n",
    "sales  = query_snowflake(query, columns = ['cohort_id','product_id','packing_unit_id','total_sales'])\n",
    "sales.product_id = pd.to_numeric(sales.product_id)\n",
    "sales.cohort_id = pd.to_numeric(sales.cohort_id)\n",
    "sales.packing_unit_id = pd.to_numeric(sales.packing_unit_id)\n",
    "sales.total_sales = pd.to_numeric(sales.total_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19719c0d-1927-4f52-b293-f10dd14abb85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "with base as (\n",
    "select *, row_number()over(partition by retailer_id order by priority) as rnk \n",
    "from (\n",
    "select x.*,TAGGABLE_ID as retailer_id \n",
    "from (\n",
    "select id as cohort_id,name as cohort_name,priority,dynamic_tag_id \n",
    "from cohorts \n",
    "where is_active = 'true'\n",
    "and id in (700,701,702,703,704,1123,1124,1125,1126)\n",
    ") x \n",
    "join DYNAMIC_TAGgables dt on x.dynamic_tag_id = dt.dynamic_tag_id\n",
    "where dt.taggable_id in (select taggable_id from DYNAMIC_TAGgables where dynamic_tag_id in (2807, 2808, 2809, 2810, 2811, 2812))\n",
    ")\n",
    "qualify rnk = 1 \n",
    "order by cohort_id\n",
    ")\n",
    "select region,cohort_id,product_id,packing_unit_id,\n",
    "avg(num_retailers) as daily_avg_retailers,\n",
    "STDDEV(num_retailers)  as std \n",
    "from (\n",
    "select * \n",
    "from (\n",
    "select *, \n",
    "PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY num_retailers) over(partition by product_id,cohort_id,packing_unit_id)AS q1,\n",
    "PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY num_retailers) over(partition by product_id,cohort_id,packing_unit_id)AS median,\n",
    "PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY num_retailers) over(partition by product_id,cohort_id,packing_unit_id)AS q3,\n",
    "STDDEV(num_retailers) over(partition by product_id,cohort_id,packing_unit_id) as std,\n",
    "q3-q1 as iqr\n",
    "from (\n",
    "select * , dense_rank() over(partition by date,product_id,cohort_id,packing_unit_id order by num_retailers desc ) as rnk \n",
    "from (\n",
    "select date,region,cohort_id,product_id,packing_unit_id,count(distinct retailer_id) as num_retailers\n",
    "from (\n",
    "SELECT  DISTINCT\n",
    "\t\tso.created_at::date as date,\n",
    "\t\tparent_sales_order_id,\n",
    "\t\tso.retailer_id,\n",
    "\t\tbase.cohort_id,\n",
    "\t\tbase.cohort_name,\n",
    "\t\tcase when regions.id = 2 then states.name_en else regions.name_en end as region,\n",
    "\t\tpso.product_id,\n",
    "\t\tCONCAT(products.name_ar,' ',products.size,' ',product_units.name_ar) as sku,\n",
    "\t\tpacking_unit_id,\n",
    "\t\tbrands.name_ar as brand, \n",
    "\t\tcategories.name_ar as cat,\n",
    "\t\tsum(pso.purchased_item_count) as qty,\n",
    "        sum(pso.total_price) as nmv,\n",
    "       sum(COALESCE(f.wac_p,0) * pso.purchased_item_count * pso.basic_unit_count) as cogs,\n",
    "\t   (nmv-cogs)/nmv as bm \n",
    "\t\t\n",
    "\n",
    "FROM product_sales_order pso\n",
    "JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "join base on base.retailer_id = so.retailer_id\n",
    "JOIN products on products.id=pso.product_id\n",
    "JOIN brands on products.brand_id = brands.id\n",
    "JOIN categories ON products.category_id = categories.id\n",
    "JOIN finance.all_cogs f  ON f.product_id = pso.product_id\n",
    "                        AND f.from_date::date <= so.created_at::date\n",
    "                        AND f.to_date::date > so.created_at::date\n",
    "JOIN product_units ON product_units.id = products.unit_id \n",
    "JOIN materialized_views.retailer_polygon on materialized_views.retailer_polygon.retailer_id=so.retailer_id\n",
    "JOIN districts on districts.id=materialized_views.retailer_polygon.district_id\n",
    "JOIN cities on cities.id=districts.city_id\n",
    "join states on states.id=cities.state_id\n",
    "join regions on regions.id=states.region_id\n",
    "\n",
    "WHERE   True\n",
    "    AND so.created_at::date between current_date -30 and  CURRENT_date-1\n",
    "    AND so.sales_order_status_id not in (7,12)\n",
    "    AND so.channel IN ('telesales','retailer')\n",
    "    AND pso.purchased_item_count <> 0\n",
    "GROUP BY ALL\n",
    ")\n",
    "group by all \n",
    ")\n",
    "qualify rnk = 1 \n",
    ")\n",
    ")\n",
    "WHERE \n",
    "num_retailers >= q1-(1.2*iqr)\n",
    "and num_retailers <= q3+(1.2*iqr)\n",
    "and num_retailers between median and median+std\n",
    ")\n",
    "group by all\n",
    "order by daily_avg_retailers desc\n",
    "'''\n",
    "avg_daily  = query_snowflake(query, columns = ['region','cohort_id','product_id','packing_unit_id','daily_avg_retailers','daily_std'])\n",
    "avg_daily.product_id = pd.to_numeric(avg_daily.product_id)\n",
    "avg_daily.cohort_id = pd.to_numeric(avg_daily.cohort_id)\n",
    "avg_daily.packing_unit_id = pd.to_numeric(avg_daily.packing_unit_id)\n",
    "avg_daily.daily_avg_retailers = pd.to_numeric(avg_daily.daily_avg_retailers)\n",
    "avg_daily['daily_std'] = pd.to_numeric(avg_daily['daily_std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987a6572-529d-484b-80ab-daeb80cbbe13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT DISTINCT cat, brand, margin as target_bm\n",
    "FROM    performance.commercial_targets cplan\n",
    "QUALIFY CASE WHEN DATE_TRUNC('month', MAX(DATE)OVER()) = DATE_TRUNC('month', CURRENT_DATE) THEN DATE_TRUNC('month', CURRENT_DATE)\n",
    "ELSE DATE_TRUNC('month', CURRENT_DATE - INTERVAL '1 month') END = DATE_TRUNC('month', date)\n",
    "'''\n",
    "target_margin = query_snowflake(query, columns = ['cat','brand','target_margin']) \n",
    "target_margin.target_margin=pd.to_numeric(target_margin.target_margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a90c95f-82c0-4779-9f39-bb4409dd1701",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "select cppu.cohort_id,product_id,packing_unit_id,basic_unit_count,COALESCE(cppu.MAX_PER_SALES_ORDER,cppu2.MAX_PER_SALES_ORDER) as current_cart_rule\n",
    "from COHORT_PRODUCT_PACKING_UNITS cppu \n",
    "join PACKING_UNIT_PRODUCTS pup on cppu.PRODUCT_PACKING_UNIT_ID = pup.id \n",
    "join cohorts c on c.id = cppu.cohort_id\n",
    "join COHORT_PRODUCT_PACKING_UNITS cppu2 on cppu.PRODUCT_PACKING_UNIT_ID = cppu2.PRODUCT_PACKING_UNIT_ID and cppu2.cohort_id = c.FALLBACK_COHORT_ID \n",
    "where cppu.cohort_id in (700,701,702,703,704,1123,1124,1125,1126)\n",
    "\n",
    "'''\n",
    "live_cart_rules = query_snowflake(query, columns = ['cohort_id','product_id','packing_unit_id','basic_unit_count','current_cart_rule']) \n",
    "live_cart_rules.cohort_id=pd.to_numeric(live_cart_rules.cohort_id)\n",
    "live_cart_rules.product_id=pd.to_numeric(live_cart_rules.product_id)\n",
    "live_cart_rules.packing_unit_id=pd.to_numeric(live_cart_rules.packing_unit_id)\n",
    "live_cart_rules.basic_unit_count=pd.to_numeric(live_cart_rules.basic_unit_count)\n",
    "live_cart_rules.current_cart_rule=pd.to_numeric(live_cart_rules.current_cart_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f86df56-c1e5-4200-821a-e99eab569d21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#changed\n",
    "quantity_disc_data=quantity_disc_data[~quantity_disc_data['wac_p'].isna()]\n",
    "quantity_disc_data=quantity_disc_data[quantity_disc_data['bm']>0]\n",
    "quantity_disc_data = quantity_disc_data.merge(target_margin,on=['cat','brand'],how='left')\n",
    "quantity_disc_data['min'] = quantity_disc_data['target_margin'] * 0.85 \n",
    "quantity_disc_data['min'] =quantity_disc_data['min'].fillna(0.02)\n",
    "quantity_disc_data=quantity_disc_data[((quantity_disc_data['bm'] >= quantity_disc_data['min'])&(quantity_disc_data['cat']!= 'حاجه ساقعه'))|((quantity_disc_data['bm'] > 0)&(quantity_disc_data['cat']== 'حاجه ساقعه')) ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ee47a6-83de-48e6-b5f7-30a264f4c4c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#changed\n",
    "main_df = quantity_disc_data.copy()\n",
    "main_df['t0_perc'] = main_df['ret_below_t1']/(main_df['ret_below_t1']+main_df['ret_t1']+main_df['ret_t2'])\n",
    "main_df['t1_perc'] = main_df['ret_t1']/(main_df['ret_below_t1']+main_df['ret_t1']+main_df['ret_t2'])\n",
    "main_df['t2_perc'] = main_df['ret_t2']/(main_df['ret_below_t1']+main_df['ret_t1']+main_df['ret_t2'])\n",
    "\n",
    "main_df['current_median'] =quantity_disc_data.region_median\n",
    "\n",
    "main_df['t0_to_others'] = round(0.25*main_df['ret_below_t1'])\n",
    "main_df['t0_to_t1'] = round(0.4*main_df['t0_to_others'])\n",
    "main_df['t0_to_t2'] = round(0.6*main_df['t0_to_others'])\n",
    "\n",
    "main_df['t0_new_rets'] = main_df['ret_below_t1']-main_df['t0_to_others']\n",
    "main_df['t1_new_rets'] = main_df['ret_t1']+main_df['t0_to_t1']\n",
    "main_df['t2_new_rets'] = main_df['ret_t2']+main_df['t0_to_t2']\n",
    "\n",
    "main_df['t0_new_perc'] = main_df['t0_new_rets']/(main_df['t0_new_rets']+main_df['t1_new_rets']+main_df['t2_new_rets'])\n",
    "main_df['t1_new_perc'] = main_df['t1_new_rets']/(main_df['t0_new_rets']+main_df['t1_new_rets']+main_df['t2_new_rets'])\n",
    "main_df['t2_new_perc'] = main_df['t2_new_rets']/(main_df['t0_new_rets']+main_df['t1_new_rets']+main_df['t2_new_rets'])\n",
    "\n",
    "main_df['new_median'] = (main_df['t0_new_perc']*quantity_disc_data.region_median)+(main_df['t1_new_perc']*main_df['tier_1'] )+(main_df['t2_new_perc']*main_df['tier_2'])\n",
    "\n",
    "main_df['t1_nmv'] = main_df['price']*(main_df['t0_new_rets']+main_df['t1_new_rets']+main_df['t2_new_rets'])*main_df['tier_1']*main_df['t1_new_perc']\n",
    "main_df['t2_nmv'] = main_df['price']*(main_df['t0_new_rets']+main_df['t1_new_rets']+main_df['t2_new_rets'])*main_df['tier_2']*main_df['t2_new_perc']\n",
    "\n",
    "main_df['median_diff'] = main_df['new_median']-main_df['current_median']\n",
    "main_df['OA_increase'] = main_df['median_diff']*main_df['price']*(main_df['t0_new_rets']+main_df['t1_new_rets']+main_df['t2_new_rets'])*main_df['bm']\n",
    "main_df['OA_burn'] = main_df['OA_increase']/(main_df['t1_nmv']+main_df['t2_nmv'])\n",
    "########old######\n",
    "# cond = [main_df['OA_burn']>=0.04 , main_df['OA_burn']<0.04]\n",
    "# cho = [np.minimum(np.maximum((1-((main_df['OA_burn']-0.03)/0.03))-0.33,0.15),0.4),0.4]\n",
    "# main_df['burn_take'] = np.select(cond,cho,default = 0.4)\n",
    "########new######\n",
    "main_df['burn_2'] = 0.012*(main_df['t1_nmv']+main_df['t2_nmv'])\n",
    "main_df['burn_40'] = 0.3*main_df['OA_increase']\n",
    "main_df['Burn_perc_margin'] = (0.2*main_df['bm'])*(main_df['t1_nmv']+main_df['t2_nmv'])\n",
    "main_df['Burn_use']=np.minimum(np.minimum(main_df['burn_2'],main_df['burn_40']),main_df['Burn_perc_margin'])\n",
    "##################\n",
    "main_df['t1_nmv_cntrb'] = main_df['t1_nmv']/(main_df['t1_nmv'] +main_df['t2_nmv']) \n",
    "main_df['t2_nmv_cntrb'] = main_df['t2_nmv']/(main_df['t1_nmv'] +main_df['t2_nmv']) \n",
    "\n",
    "main_df['Tiers_diff'] =  (main_df['tier_2'] - main_df['tier_1'] )/ main_df['tier_1']\n",
    "main_df['Discount_t1'] = ((main_df['Burn_use']/(1+main_df['Tiers_diff']))*main_df['t1_nmv_cntrb'])/main_df['t1_nmv'] \n",
    "main_df['Discount_t2'] = (main_df['Burn_use'] - ((main_df['Burn_use']/(1+main_df['Tiers_diff']))*main_df['t1_nmv_cntrb']))/main_df['t2_nmv'] \n",
    "\n",
    "main_df = main_df[(~main_df['Discount_t1'].isna()) & (~main_df['Discount_t2'].isna())]\n",
    "main_df = main_df[(main_df['t0_new_rets']>0) &(main_df['t1_new_rets']>0) & (main_df['t2_new_rets']>0)]\n",
    "main_df = main_df[(main_df['bm']>0)]\n",
    "main_df = main_df[(main_df['Discount_t1']>0) &(main_df['Discount_t2']>0)]\n",
    "main_df=main_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1a9f60-f8f5-43b5-b7d7-9f642ae5d9f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "main_df = main_df.merge(sales,on = ['cohort_id','product_id','packing_unit_id'])\n",
    "main_df = main_df.merge(avg_daily,on = ['region','cohort_id','product_id','packing_unit_id'])\n",
    "main_df= main_df.sort_values(['cohort_id', 'total_sales'], ascending=[True, False])\n",
    "main_df['row_number'] = main_df.groupby('cohort_id').cumcount() + 1\n",
    "main_df = main_df[main_df['row_number']<=100]\n",
    "main_df = main_df[main_df['cohort_id'].isin([700,701,702,703,704,1123])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592b9f97-a51f-4e66-ae3c-a3fbfa9e87a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_quantity_discount = pd.DataFrame(columns =['region','Discounts Group 1','Discounts Group 2','Description'])\n",
    "for reg in main_df.region.unique():\n",
    "    region_data = main_df[main_df['region']== reg]\n",
    "    for i,r in region_data.iterrows():\n",
    "        region = r['region']\n",
    "        product_id = r['product_id']\n",
    "        packing_unit_id = r['packing_unit_id']\n",
    "        q_1 = int(r['tier_1'])\n",
    "        q_2 = int(r['tier_2'])\n",
    "        d_1 = round(r['Discount_t1']*100,2)\n",
    "        d_2 = round(r['Discount_t2']*100,2)\n",
    "        a_1 = [product_id]+[packing_unit_id]+[q_1]+[d_1]\n",
    "        a_2 = [product_id]+[packing_unit_id]+[q_2]+[d_2]\n",
    "        new_row = {'region':region ,'Discounts Group 1':a_1,'Discounts Group 2':a_2,'Description':f'{reg}QD'}\n",
    "        new_row_df = pd.DataFrame([new_row]) \n",
    "        final_quantity_discount = pd.concat([final_quantity_discount, new_row_df], ignore_index=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb05267-8d58-47d3-9f8f-77c6a4d7beb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Tag_def = {\n",
    "    'region': ['Cairo', 'Giza', 'Alexandria', 'Upper Egypt', 'Delta East', 'Delta West'],\n",
    "    'Tag ID': [2807, 2808, 2809, 2810, 2811, 2812]\n",
    "}\n",
    "\n",
    "Tag_map = pd.DataFrame(Tag_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8befb4c1-945c-4c8d-b496-a5986b235ea1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cohort_def = {\n",
    "    'region': ['Cairo', 'Giza', 'Alexandria', 'Delta East', 'Delta West','Upper Egypt','Upper Egypt','Upper Egypt','Upper Egypt'],\n",
    "    'cohort_id': [700, 701, 702, 704, 703, 1123,1124,1125,1126]\n",
    "}\n",
    "cohort_mp = pd.DataFrame(cohort_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea81ab3-39b9-4448-adb7-50ab70d8eaae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "req_df = pd.merge(cohort_mp,Tag_map,on='region')\n",
    "req_df.to_excel('mapping.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f08c55a-c51b-4ac4-a2d1-f028d44e8146",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#changed\n",
    "slots = ['0-12','13-17','18-23']\n",
    "local_tz = pytz.timezone('Africa/Cairo')\n",
    "current_hour = datetime.now(local_tz).hour\n",
    "chosen_slot = [np.nan,np.nan]\n",
    "\n",
    "for slot in slots:\n",
    "    parts = slot.split(\"-\")\n",
    "    if(current_hour >= int(parts[0]) and current_hour < int(parts[1])):\n",
    "        chosen_slot[0] = int(parts[0]) \n",
    "        chosen_slot[1] = int(parts[1]) \n",
    "        break\n",
    "    else:\n",
    "        chosen_slot[0] = 0\n",
    "        chosen_slot[1] = 0 \n",
    "        \n",
    "today = datetime.now(local_tz) \n",
    "start_hour = np.maximum(current_hour,chosen_slot[0])\n",
    "if(start_hour==current_hour):\n",
    "    start_mins =  (datetime.now(local_tz).minute) +10\n",
    "else:\n",
    "    start_mins = 30 \n",
    "    \n",
    "    \n",
    "start_date = (today.replace(hour=start_hour, minute=0, second=0, microsecond=0)+ timedelta(minutes=start_mins)).strftime('%d/%m/%Y %H:%M')\n",
    "end_date = (today.replace(hour=16, minute=59, second=0, microsecond=0)).strftime('%d/%m/%Y %H:%M')\n",
    "final_quantity_discount = final_quantity_discount.merge(Tag_map,on='region')\n",
    "final_quantity_discount['Start Date/Time']= start_date\n",
    "final_quantity_discount['End Date/Time']= end_date\n",
    "main_df['start_date'] = start_date\n",
    "main_df['end_date'] = end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b505642a-51af-48e0-a964-97a4f0c649ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "main_df = main_df.merge(Tag_map,on='region')\n",
    "main_df = main_df.rename(columns={'Tag ID':'tag_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063ea24c-c40d-45c4-ba07-bb32b12119a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cart_rules_data = main_df[['region','product_id','packing_unit_id','tier_2']].copy()\n",
    "cohort_def = {\n",
    "    'region': ['Cairo', 'Giza', 'Alexandria', 'Delta East', 'Delta West','Upper Egypt','Upper Egypt','Upper Egypt','Upper Egypt'],\n",
    "    'cohort_id': [700, 701, 702, 704, 703, 1123,1124,1125,1126]\n",
    "}\n",
    "region_cohort_map = pd.DataFrame(cohort_def)\n",
    "cart_rules_data = cart_rules_data.merge(region_cohort_map,on='region')\n",
    "cart_rules_data = cart_rules_data.merge(live_cart_rules,on=['cohort_id','product_id','packing_unit_id'])\n",
    "cart_rules_data = cart_rules_data[cart_rules_data['tier_2']>cart_rules_data['current_cart_rule']]\n",
    "cart_rules_data=cart_rules_data[['cohort_id','product_id','packing_unit_id','tier_2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc9ad44-c8b7-4d34-980d-027652da8b0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_data = final_quantity_discount.groupby(['Tag ID','Description', 'Start Date/Time', 'End Date/Time'], as_index=False).agg({\n",
    "    'Discounts Group 1': list ,\n",
    "    'Discounts Group 2' : list\n",
    "})\n",
    "final_data.to_excel('QD_upload.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb654302-6107-4d00-999c-8f644420b5a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_secret(secret_name):\n",
    "    region_name = \"us-east-1\"\n",
    "\n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "    # In this sample we only handle the specific exceptions for the 'GetSecretValue' API.\n",
    "    # See https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html\n",
    "    # We rethrow the exception by default.\n",
    "\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(SecretId=secret_name)\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'DecryptionFailureException':\n",
    "            # Secrets Manager can't decrypt the protected secret text using the provided KMS key.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InternalServiceErrorException':\n",
    "            # An error occurred on the server side.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidParameterException':\n",
    "            # You provided an invalid value for a parameter.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidRequestException':\n",
    "            # You provided a parameter value that is not valid for the current state of the resource.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'ResourceNotFoundException':\n",
    "            # We can't find the resource that you asked for.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "    else:\n",
    "        # Decrypts secret using the associated KMS CMK.\n",
    "        # Depending on whether the secret is a string or binary, one of these fields will be populated.\n",
    "        if 'SecretString' in get_secret_value_response:\n",
    "            return get_secret_value_response['SecretString']\n",
    "        else:\n",
    "            return base64.b64decode(get_secret_value_response['SecretBinary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4aac8cc6-014b-4329-945a-404ff3d88298",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pricing_api_secret = json.loads(get_secret(\"prod/pricing/api/\"))\n",
    "username = pricing_api_secret[\"egypt_username\"]\n",
    "password = pricing_api_secret[\"egypt_password\"]\n",
    "secret = pricing_api_secret[\"egypt_secret\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d097975d-4b0f-470e-8b97-1f8433aa714a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_access_token(url, client_id, client_secret):\n",
    "    \"\"\"\n",
    "    get_access_token function takes three parameters and returns a session token\n",
    "    to connect to MaxAB APIs\n",
    "\n",
    "    :param url: production MaxAB token URL\n",
    "    :param client_id: client ID\n",
    "    :param client_secret: client sercret\n",
    "    :return: session token\n",
    "    \"\"\"\n",
    "    response = requests.post(\n",
    "        url,\n",
    "        data={\"grant_type\": \"password\",\n",
    "              \"username\": username,\n",
    "              \"password\": password},\n",
    "        auth=(client_id, client_secret),\n",
    "    )\n",
    "    return response.json()[\"access_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2822c781-728c-4878-9f7e-0f9e5559d98f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def post_QD(file_name):\n",
    "    token = get_access_token('https://sso.maxab.info/auth/realms/maxab/protocol/openid-connect/token',\n",
    "                             'main-system-externals',\n",
    "                             secret)\n",
    "    url = \"https://api.maxab.info/commerce/api/admins/v1/quantity-discounts\"\n",
    "    payload={}\n",
    "    files=[\n",
    "      ('file',(file_name,open(file_name,'rb'),'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'))\n",
    "    ]\n",
    "    headers = {\n",
    "      'Authorization': 'bearer {}'.format(token)}\n",
    "\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload, files=files)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4921ac32-c33d-4891-82d7-a16a8067ac21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def post_cart_rules(id_,file_name):\n",
    "    token = get_access_token('https://sso.maxab.info/auth/realms/maxab/protocol/openid-connect/token',\n",
    "                             'main-system-externals',\n",
    "                             secret)\n",
    "    url = \"https://api.maxab.info/main-system/api/admin-portal/cohorts/{}/cart-rules\".format(id_)\n",
    "    payload={}\n",
    "    files=[\n",
    "      ('sheet',(file_name,open(file_name,'rb'),'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'))\n",
    "    ]\n",
    "    headers = {\n",
    "      'Authorization': 'bearer {}'.format(token)}\n",
    "\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload, files=files)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed94b38e-0cd7-4007-b47c-569de4c0a375",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def writer_snowflake_query(query):\n",
    "    import os\n",
    "    import snowflake.connector\n",
    "\n",
    "    config = {\n",
    "        'user': os.environ[\"SNOWFLAKE_SERVICE_USERNAME\"],\n",
    "        'account': os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "        'private_key_file': '/tmp/sagemaker_service.p8',\n",
    "        'database': os.environ[\"SNOWFLAKE_DATABASE\"] ,\n",
    "        'role': os.environ[\"SNOWFLAKE_ROLE\"],\n",
    "        'schema': 'PUBLIC'\n",
    "        }\n",
    "\n",
    "    conn = snowflake.connector.connect(**config)\n",
    "\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(\"USE WAREHOUSE COMPUTE_WH\")\n",
    "        cur.execute(query)\n",
    "        return 1\n",
    "    except Exception as e:\n",
    "        print(\"Error: \", e)\n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "def pandas_dtype_to_snowflake(dtype):\n",
    "    \"\"\"Maps pandas/numpy dtype to Snowflake SQL data type.\"\"\"\n",
    "    if pd.api.types.is_integer_dtype(dtype):\n",
    "        return \"NUMBER,\"\n",
    "    elif pd.api.types.is_float_dtype(dtype):\n",
    "        return \"FLOAT,\"\n",
    "    elif pd.api.types.is_bool_dtype(dtype):\n",
    "        return \"BOOLEAN,\"\n",
    "    elif pd.api.types.is_datetime64_any_dtype(dtype):\n",
    "        return \"TIMESTAMP,\"\n",
    "    elif pd.api.types.is_object_dtype(dtype):\n",
    "        return \"TEXT,\"\n",
    "    elif pd.api.types.is_categorical_dtype(dtype):\n",
    "        return \"TEXT,\"\n",
    "    else:\n",
    "        return \"TEXT,\"  # fallback\n",
    "\n",
    "def dataframe_to_snowflake_columns(df, table_name):\n",
    "    \"\"\"Generates Snowflake-compatible column definitions from a DataFrame.\"\"\"\n",
    "    lines = [f'CREATE TABLE IF NOT EXISTS {table_name} (']\n",
    "    for col in df.columns:\n",
    "        snowflake_type = pandas_dtype_to_snowflake(df[col].dtype)\n",
    "        if col.lower() in ['group', 'section']:\n",
    "            col = f'\"{col}\"'\n",
    "        if col.lower() in ['start_date', 'end_date']:\n",
    "            lines.append(f'{col} TIMESTAMP,')\n",
    "        else:\n",
    "            lines.append(f'{col} {snowflake_type}')\n",
    "    return \"\\n\".join(lines)[:-1] + ');'\n",
    "\n",
    "# create tables if not exist\n",
    "def table_exist_test(df, table_name):\n",
    "    query_string = dataframe_to_snowflake_columns(df, table_name)\n",
    "    writer_snowflake_query(query_string)\n",
    "\n",
    "# Snowflake DB query to write into tables\n",
    "def eg_snowflake_writer(df, table, schema):\n",
    "    import os\n",
    "    import snowflake.connector\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from snowflake.connector.pandas_tools import write_pandas\n",
    "    import os\n",
    "\n",
    "    config = {\n",
    "        'user': os.environ[\"SNOWFLAKE_SERVICE_USERNAME\"],\n",
    "        'account': os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "        'private_key_file': '/tmp/sagemaker_service.p8',\n",
    "        'database': os.environ[\"SNOWFLAKE_DATABASE\"] ,\n",
    "        'role': os.environ[\"SNOWFLAKE_ROLE\"],\n",
    "        'schema': 'PUBLIC'\n",
    "    }\n",
    "    conn = snowflake.connector.connect(**config)\n",
    "    success, _, _, _ = write_pandas(conn=conn, df=df, table_name=table, schema=schema)\n",
    "    return success \n",
    "\n",
    "def DatabaseDump(df, path, erase='False'):\n",
    "    # initialize the standard command\n",
    "    command_string = f\"DELETE FROM {path} WHERE True\"\n",
    "    print_string = f\"Succesfuly Removed & Re-Added Data \\n for {path}\"\n",
    "\n",
    "    # if the flag is 'false', erase latest push (day / time_slot)\n",
    "    if erase.lower() == 'false':\n",
    "        if \"start_date\" in df.columns.str.lower():\n",
    "            date_value = main_df.start_date.values[0]\n",
    "            print(date_value)\n",
    "            command_string += f\" AND start_date = TO_TIMESTAMP('{date_value}')\"\n",
    "            print(command_string)\n",
    "            print_string += f\"\\ndate = {df['start_date'].values[0]}\"\n",
    "\n",
    "        if \"cohort_id\" in df.columns.str.lower():\n",
    "            command_string += f\" AND cohort_id IN {tuple(df.cohort_id.unique())}\"\n",
    "            print_string += f\"\\nCohort IDs IN {tuple(df.cohort_id.unique())}\"\n",
    "\n",
    "    # if the flag is 'month', erase current month\n",
    "    if erase.lower() == 'month':\n",
    "        if \"created_at\" in df.columns.str.lower():\n",
    "            command_string += f\" AND DATE_TRUNC('month', created_at) = DATE_TRUNC('month', SYSDATE())\"\n",
    "            print_string += f\"\\ndate = Current month\"\n",
    "        if \"date\" in df.columns.str.lower():\n",
    "            command_string += f\" AND DATE_TRUNC('month', date) = DATE_TRUNC('month', SYSDATE())\"\n",
    "            print_string += f\"\\ndate = Current month\"\n",
    "\n",
    "    # Remove data of the same day, time_slot, etc...\n",
    "    writer_snowflake_query(command_string)\n",
    "\n",
    "    # Push the new data to the table\n",
    "    df.columns = df.columns.str.upper()\n",
    "    eg_snowflake_writer(df, path.split('.')[1].upper(), path.split('.')[0].upper())\n",
    "    print(print_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "baa04284-5f65-4193-813a-513f4881631b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cohort_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>packing_unit_id</th>\n",
       "      <th>basic_unit_count</th>\n",
       "      <th>current_cart_rule</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13153</th>\n",
       "      <td>700</td>\n",
       "      <td>10407</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13154</th>\n",
       "      <td>700</td>\n",
       "      <td>8216</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13155</th>\n",
       "      <td>700</td>\n",
       "      <td>8216</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13156</th>\n",
       "      <td>700</td>\n",
       "      <td>2436</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13157</th>\n",
       "      <td>700</td>\n",
       "      <td>2436</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13158</th>\n",
       "      <td>700</td>\n",
       "      <td>11404</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66557</th>\n",
       "      <td>700</td>\n",
       "      <td>1339</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66558</th>\n",
       "      <td>700</td>\n",
       "      <td>1339</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68594</th>\n",
       "      <td>700</td>\n",
       "      <td>3431</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96196</th>\n",
       "      <td>700</td>\n",
       "      <td>10407</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cohort_id  product_id  packing_unit_id  basic_unit_count  \\\n",
       "13153        700       10407                1                12   \n",
       "13154        700        8216                1                24   \n",
       "13155        700        8216               10                 1   \n",
       "13156        700        2436                4                 1   \n",
       "13157        700        2436                1                12   \n",
       "13158        700       11404                2                 1   \n",
       "66557        700        1339                1                24   \n",
       "66558        700        1339                3                 1   \n",
       "68594        700        3431                1                 1   \n",
       "96196        700       10407               10                 1   \n",
       "\n",
       "       current_cart_rule  \n",
       "13153                150  \n",
       "13154                150  \n",
       "13155                200  \n",
       "13156                200  \n",
       "13157                150  \n",
       "13158                150  \n",
       "66557                150  \n",
       "66558                200  \n",
       "68594                150  \n",
       "96196                200  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cart_rules_data  = live_cart_rules[live_cart_rules['product_id'].isin([2436,8216,10407,11404,3431,1339])]\n",
    "cart_rules_data=cart_rules_data[cart_rules_data['cohort_id']==700]\n",
    "cart_rules_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28047751-59ce-435e-84d7-bf6ba3143e6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cohort_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>packing_unit_id</th>\n",
       "      <th>basic_unit_count</th>\n",
       "      <th>current_cart_rule</th>\n",
       "      <th>max</th>\n",
       "      <th>new_max</th>\n",
       "      <th>tier_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2294</th>\n",
       "      <td>1123</td>\n",
       "      <td>8853</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>319</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2295</th>\n",
       "      <td>703</td>\n",
       "      <td>6936</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>311</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2296</th>\n",
       "      <td>703</td>\n",
       "      <td>6935</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>311</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2297</th>\n",
       "      <td>1124</td>\n",
       "      <td>6936</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>322</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2298</th>\n",
       "      <td>700</td>\n",
       "      <td>6935</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>342</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2299</th>\n",
       "      <td>1123</td>\n",
       "      <td>6935</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>319</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2300</th>\n",
       "      <td>1124</td>\n",
       "      <td>6935</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>319</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11325</th>\n",
       "      <td>702</td>\n",
       "      <td>6936</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>316</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12352</th>\n",
       "      <td>700</td>\n",
       "      <td>6936</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>324</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12353</th>\n",
       "      <td>703</td>\n",
       "      <td>8853</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>328</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13031</th>\n",
       "      <td>1123</td>\n",
       "      <td>6936</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>322</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23936</th>\n",
       "      <td>702</td>\n",
       "      <td>6935</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>332</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24743</th>\n",
       "      <td>1124</td>\n",
       "      <td>8853</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>314</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24744</th>\n",
       "      <td>704</td>\n",
       "      <td>8853</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24745</th>\n",
       "      <td>704</td>\n",
       "      <td>6936</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>305</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31118</th>\n",
       "      <td>1125</td>\n",
       "      <td>8853</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>318</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35685</th>\n",
       "      <td>701</td>\n",
       "      <td>8853</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>302</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51791</th>\n",
       "      <td>700</td>\n",
       "      <td>8853</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>292</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53276</th>\n",
       "      <td>702</td>\n",
       "      <td>8853</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>329</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54185</th>\n",
       "      <td>1126</td>\n",
       "      <td>6936</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>319</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54186</th>\n",
       "      <td>1126</td>\n",
       "      <td>6935</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>319</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54187</th>\n",
       "      <td>1125</td>\n",
       "      <td>6936</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>331</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65188</th>\n",
       "      <td>1125</td>\n",
       "      <td>6935</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>318</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85527</th>\n",
       "      <td>704</td>\n",
       "      <td>6935</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>312</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85595</th>\n",
       "      <td>701</td>\n",
       "      <td>6935</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>310</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85596</th>\n",
       "      <td>701</td>\n",
       "      <td>6936</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>305</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104887</th>\n",
       "      <td>1126</td>\n",
       "      <td>8853</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cohort_id  product_id  packing_unit_id  basic_unit_count  \\\n",
       "2294         1123        8853                2                 1   \n",
       "2295          703        6936                2                 1   \n",
       "2296          703        6935                2                 1   \n",
       "2297         1124        6936                2                 1   \n",
       "2298          700        6935                2                 1   \n",
       "2299         1123        6935                2                 1   \n",
       "2300         1124        6935                2                 1   \n",
       "11325         702        6936                2                 1   \n",
       "12352         700        6936                2                 1   \n",
       "12353         703        8853                2                 1   \n",
       "13031        1123        6936                2                 1   \n",
       "23936         702        6935                2                 1   \n",
       "24743        1124        8853                2                 1   \n",
       "24744         704        8853                2                 1   \n",
       "24745         704        6936                2                 1   \n",
       "31118        1125        8853                2                 1   \n",
       "35685         701        8853                2                 1   \n",
       "51791         700        8853                2                 1   \n",
       "53276         702        8853                2                 1   \n",
       "54185        1126        6936                2                 1   \n",
       "54186        1126        6935                2                 1   \n",
       "54187        1125        6936                2                 1   \n",
       "65188        1125        6935                2                 1   \n",
       "85527         704        6935                2                 1   \n",
       "85595         701        6935                2                 1   \n",
       "85596         701        6936                2                 1   \n",
       "104887       1126        8853                2                 1   \n",
       "\n",
       "        current_cart_rule  max  new_max  tier_2  \n",
       "2294                  319    1       19    19.0  \n",
       "2295                  311    1       19    19.0  \n",
       "2296                  311    1       19    19.0  \n",
       "2297                  322    1       19    19.0  \n",
       "2298                  342    1       19    19.0  \n",
       "2299                  319    1       19    19.0  \n",
       "2300                  319    1       19    19.0  \n",
       "11325                 316    1       19    19.0  \n",
       "12352                 324    1       19    19.0  \n",
       "12353                 328    1       19    19.0  \n",
       "13031                 322    1       19    19.0  \n",
       "23936                 332    1       19    19.0  \n",
       "24743                 314    1       19    19.0  \n",
       "24744                 307    1       19    19.0  \n",
       "24745                 305    1       19    19.0  \n",
       "31118                 318    1       19    19.0  \n",
       "35685                 302    1       19    19.0  \n",
       "51791                 292    1       19    19.0  \n",
       "53276                 329    1       19    19.0  \n",
       "54185                 319    1       19    19.0  \n",
       "54186                 319    1       19    19.0  \n",
       "54187                 331    1       19    19.0  \n",
       "65188                 318    1       19    19.0  \n",
       "85527                 312    1       19    19.0  \n",
       "85595                 310    1       19    19.0  \n",
       "85596                 305    1       19    19.0  \n",
       "104887                 38    1       19    19.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cart_rules_data = live_cart_rules[live_cart_rules['product_id'].isin([6935,6936,8853])]\n",
    "cart_rules_data['max'] = cart_rules_data.groupby(['product_id'])['basic_unit_count'].transform(max)\n",
    "cart_rules_data['new_max'] = 19\n",
    "cart_rules_data['tier_2'] = np.minimum((cart_rules_data['new_max'])*(cart_rules_data['max']/cart_rules_data['basic_unit_count']),200)\n",
    "#cart_rules_data.loc[cart_rules_data['max'] != cart_rules_data['basic_unit_count'],'tier_2']   = np.ceil(cart_rules_data['tier_2']*0.6)\n",
    "cart_rules_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e31057-3c1a-4dd8-a560-f55bc79983df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "select *\n",
    "from (\n",
    "select cppu.cohort_id,product_id,b.name_ar as brand,cat.name_ar as cat,packing_unit_id,basic_unit_count,COALESCE(cppu.MAX_PER_SALES_ORDER,cppu2.MAX_PER_SALES_ORDER) as current_cart_rule\n",
    "from COHORT_PRODUCT_PACKING_UNITS cppu \n",
    "join PACKING_UNIT_PRODUCTS pup on cppu.PRODUCT_PACKING_UNIT_ID = pup.id \n",
    "join products p on p.id = pup.product_id \n",
    "join brands b on b.id = p.brand_id \n",
    "join categories cat on cat.id = p.category_id\n",
    "join cohorts c on c.id = cppu.cohort_id\n",
    "join COHORT_PRODUCT_PACKING_UNITS cppu2 on cppu.PRODUCT_PACKING_UNIT_ID = cppu2.PRODUCT_PACKING_UNIT_ID and cppu2.cohort_id = c.FALLBACK_COHORT_ID \n",
    "where cppu.cohort_id in (700,701,702,703,704,1123,1124,1125,1126)\n",
    ")\n",
    "'''\n",
    "\n",
    "to_correct = query_snowflake(query, columns = ['cohort_id','product_id','brand','cat','packing_unit_id','buc','current_cart_rule'])\n",
    "for col in to_correct.columns:\n",
    "    to_correct[col] = pd.to_numeric(to_correct[col], errors='ignore') \n",
    "to_correct=to_correct[to_correct['product_id']!=7630] \n",
    "to_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a818d7-57cd-4185-a915-b2d06908203b",
   "metadata": {},
   "outputs": [],
   "source": [
    "force_cart_rules = client.open('Wholesales_exec').worksheet('Cart rules')\n",
    "force_cart_rules = pd.DataFrame(force_cart_rules.get_all_records())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94c1666-1d6b-42f3-be71-fe6d532b83be",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_correct = to_correct.merge(force_cart_rules,on= ['brand','cat'])\n",
    "to_correct['tier_2']=np.maximum(np.ceil(to_correct['forced_cart_rules'] / to_correct['buc']),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f8bd01-9011-4ed6-93d0-734e79b9f7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ed243a-e69e-4e09-9036-ce585d19af44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cart_rules_data=to_correct.copy()\n",
    "cart_rules_data = cart_rules_data[['cohort_id','product_id','packing_unit_id','tier_2']]\n",
    "cart_rules_data=cart_rules_data[~cart_rules_data['product_id'].isin([1776,1777])]\n",
    "cart_rules_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38190845-6d5a-41f7-82f0-57fe018df4b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success_1123\n",
      "success_703\n",
      "success_1124\n",
      "success_700\n",
      "success_702\n",
      "success_704\n",
      "success_1125\n",
      "success_701\n"
     ]
    }
   ],
   "source": [
    "for cohort in cart_rules_data.cohort_id.unique():\n",
    "    req_data = cart_rules_data[cart_rules_data['cohort_id']==cohort]\n",
    "    if len(req_data) > 0 :\n",
    "        req_data = req_data[['product_id','packing_unit_id','tier_2']]\n",
    "        req_data.columns = ['Product ID','Packing Unit ID','Cart Rules']\n",
    "        req_data.to_excel(f'CartRules_{cohort}.xlsx', index=False, engine='xlsxwriter')\n",
    "        time.sleep(5)\n",
    "        x =  post_cart_rules(cohort,f'CartRules_{cohort}.xlsx')\n",
    "        if x.ok:\n",
    "            print(f\"success_{cohort}\")\n",
    "        else:\n",
    "            print(f\"ERROR_{cohort}\")\n",
    "            print(x.content)\n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c27a037-7eea-4d3f-997b-28cb2db15bcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#changed\n",
    "response = post_QD('QD_upload.xlsx')\n",
    "if response.ok:\n",
    "    main_df.start_date = pd.to_datetime(main_df.start_date, format=\"%d/%m/%Y %H:%M\").dt.strftime('%Y-%m-%d %H:%M')\n",
    "    main_df.end_date = pd.to_datetime(main_df.end_date, format=\"%d/%m/%Y %H:%M\").dt.strftime('%Y-%m-%d %H:%M')\n",
    "    main_df['cohort_id'] = pd.to_numeric(main_df['cohort_id'])\n",
    "    table_exist_test(main_df, \"materialized_views.qd_targets\")\n",
    "    main_df = main_df.drop(columns=[\"Burn_perc_margin\"])\n",
    "    DatabaseDump(main_df.reset_index(drop=True), \"materialized_views.qd_targets\")\n",
    "    \n",
    "else:\n",
    "    print(\"Failed with status:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6005be3-cc3c-4a32-a5e4-97bf15df6aee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b98a67-9cb6-4d71-91cb-94d7e96f317f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
