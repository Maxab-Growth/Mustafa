{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c83eefb",
   "metadata": {},
   "source": [
    "# Quantity Discount (QD) Pricing System\n",
    "\n",
    "This notebook calculates tiered pricing and quantities for products across warehouses.\n",
    "\n",
    "## Workflow:\n",
    "1. **Setup** - Imports, connections, and configuration\n",
    "2. **Product Selection** - Select top products per warehouse based on performance\n",
    "3. **Quantity Tiers** - Calculate tier 1 and tier 2 quantities based on order history\n",
    "4. **Market Prices** - Gather competitive pricing data\n",
    "5. **Price Tiers** - Calculate discounted prices for each tier\n",
    "6. **Wholesale Pricing** - Calculate wholesale prices for bulk orders\n",
    "7. **Export** - Save results to Excel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d33827a",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3d0f8d0-7f4a-4468-b218-49241a56edc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/snowflake/connector/options.py:104: UserWarning: You have an incompatible version of 'pyarrow' installed (22.0.0), please install a version that adheres to: 'pyarrow<19.0.0; extra == \"pandas\"'\n",
      "  warn_incompatible_dep(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.Renviron\n",
      "/home/ec2-user/service_account_key.json\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STANDARD LIBRARY IMPORTS\n",
    "# =============================================================================\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import base64\n",
    "import calendar\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "# =============================================================================\n",
    "# THIRD-PARTY IMPORTS\n",
    "# =============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import requests\n",
    "import gspread\n",
    "import boto3\n",
    "import snowflake.connector\n",
    "from tqdm import tqdm\n",
    "from requests import get\n",
    "from botocore.exceptions import ClientError\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "# =============================================================================\n",
    "# LOCAL IMPORTS & ENVIRONMENT SETUP\n",
    "# =============================================================================\n",
    "import setup_environment_2\n",
    "import importlib\n",
    "import import_ipynb\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "importlib.reload(setup_environment_2)\n",
    "setup_environment_2.initialize_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0727d967",
   "metadata": {},
   "source": [
    "### Configuration Constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba7201f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - Modify these parameters as needed\n",
    "# =============================================================================\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Cohort IDs for QD program\n",
    "# -----------------------------------------------------------------------------\n",
    "COHORT_IDS = [700, 701, 702, 703, 704, 1123, 1124, 1125, 1126]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Warehouse mappings: (region, warehouse_name, warehouse_id, cohort_id)\n",
    "# -----------------------------------------------------------------------------\n",
    "WAREHOUSE_MAPPING = [\n",
    "    ('Cairo',       'El-Marg',       38,  700),\n",
    "    ('Cairo',       'Mostorod',      1,   700),\n",
    "    ('Giza',        'Barageel',      236, 701),\n",
    "    ('Giza',        'Sakkarah',      962, 701),\n",
    "    ('Delta West',  'El-Mahala',     337, 703),\n",
    "    ('Delta West',  'Tanta',         8,   703),\n",
    "    ('Delta East',  'Mansoura FC',   339, 704),\n",
    "    ('Delta East',  'Sharqya',       170, 704),\n",
    "    ('Upper Egypt', 'Assiut FC',     501, 1124),\n",
    "    ('Upper Egypt', 'Bani sweif',    401, 1126),\n",
    "    ('Upper Egypt', 'Menya Samalot', 703, 1123),\n",
    "    ('Upper Egypt', 'Sohag',         632, 1125),\n",
    "    ('Alexandria',  'Khorshed Alex', 797, 702),\n",
    "]\n",
    "\n",
    "# Excluded warehouse IDs\n",
    "EXCLUDED_WAREHOUSES = [6, 9, 10]\n",
    "\n",
    "# Products to exclude from selection\n",
    "PRODUCTS_TO_REMOVE = [7630]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Pricing Parameters\n",
    "# -----------------------------------------------------------------------------\n",
    "MAX_DISCOUNT_PCT = 5.0    # Maximum discount allowed from current price (%)\n",
    "MIN_DISCOUNT_PCT = 0.35   # Minimum discount required from current price (%)\n",
    "MIN_RATIO        = 1.1    # Minimum discount-to-quantity ratio\n",
    "MAX_RATIO        = 3      # Maximum discount-to-quantity ratio\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Product Selection Thresholds\n",
    "# -----------------------------------------------------------------------------\n",
    "MIN_ORDERS    = 20    # Minimum orders in 4 months\n",
    "MIN_RETAILERS = 5     # Minimum unique retailers\n",
    "MIN_NMV       = 5000  # Minimum revenue (EGP)\n",
    "MIN_VELOCITY  = 0.5   # Minimum units per day\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Ranking Parameters\n",
    "# -----------------------------------------------------------------------------\n",
    "TOP_PRODUCTS_PER_WAREHOUSE   = 200  # Initial selection\n",
    "FINAL_PRODUCTS_PER_WAREHOUSE = 133  # Final output\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Delivery Fees\n",
    "# -----------------------------------------------------------------------------\n",
    "DELIVERY_FEE_CAIRO_GIZA = 25\n",
    "DELIVERY_FEE_OTHER      = 20\n",
    "\n",
    "print(\"✓ Configuration loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4092441e-bd5e-4b90-9b98-a4325a9757ec",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "995dfa24-7123-41e5-97fb-a45c125f3f15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_secret(secret_name):\n",
    "    \"\"\"\n",
    "    Retrieve secret from AWS Secrets Manager.\n",
    "    \n",
    "    Args:\n",
    "        secret_name: Name/ID of the secret to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        Secret string or decoded binary\n",
    "    \"\"\"\n",
    "    region_name = \"us-east-1\"\n",
    "    \n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(SecretId=secret_name)\n",
    "    except ClientError as e:\n",
    "        # Re-raise all AWS Secrets Manager exceptions\n",
    "        raise e\n",
    "    \n",
    "    # Return decrypted secret (string or binary)\n",
    "    if 'SecretString' in get_secret_value_response:\n",
    "        return get_secret_value_response['SecretString']\n",
    "    else:\n",
    "        return base64.b64decode(get_secret_value_response['SecretBinary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa2e910e-65d9-4a69-9922-988548748eb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ API credentials loaded\n"
     ]
    }
   ],
   "source": [
    "# Load API credentials from AWS Secrets Manager\n",
    "pricing_api_secret = json.loads(get_secret(\"prod/pricing/api/\"))\n",
    "username = pricing_api_secret[\"egypt_username\"]\n",
    "password = pricing_api_secret[\"egypt_password\"]\n",
    "secret   = pricing_api_secret[\"egypt_secret\"]\n",
    "\n",
    "print(\"✓ API credentials loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01f29f77-2e72-4064-9503-ec9be748e835",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_access_token(url, client_id, client_secret):\n",
    "    \"\"\"\n",
    "    Get OAuth access token for MaxAB APIs.\n",
    "    \n",
    "    Args:\n",
    "        url: Token endpoint URL\n",
    "        client_id: OAuth client ID\n",
    "        client_secret: OAuth client secret\n",
    "        \n",
    "    Returns:\n",
    "        Access token string\n",
    "    \"\"\"\n",
    "    response = requests.post(\n",
    "        url,\n",
    "        data={\n",
    "            \"grant_type\": \"password\",\n",
    "            \"username\": username,\n",
    "            \"password\": password\n",
    "        },\n",
    "        auth=(client_id, client_secret),\n",
    "    )\n",
    "    return response.json()[\"access_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bfb8760-ddf2-4192-beaf-cf52c6c92ff2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def post_QD(file_name):\n",
    "    \"\"\"\n",
    "    Upload Quantity Discount file to MaxAB API.\n",
    "    \n",
    "    Args:\n",
    "        file_name: Path to the Excel file to upload\n",
    "        \n",
    "    Returns:\n",
    "        API response object\n",
    "    \"\"\"\n",
    "    token = get_access_token(\n",
    "        'https://sso.maxab.info/auth/realms/maxab/protocol/openid-connect/token',\n",
    "        'main-system-externals',\n",
    "        secret\n",
    "    )\n",
    "    \n",
    "    url = \"https://api.maxab.info/commerce/api/admins/v1/quantity-discounts\"\n",
    "    \n",
    "    files = [\n",
    "        ('file', (file_name, open(file_name, 'rb'), \n",
    "                  'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'))\n",
    "    ]\n",
    "    headers = {'Authorization': f'bearer {token}'}\n",
    "    \n",
    "    response = requests.request(\"POST\", url, headers=headers, data={}, files=files)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d44c2b13-8395-42c1-b902-ec404ba1bca1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def post_cart_rules(cohort_id, file_name):\n",
    "    \"\"\"\n",
    "    Upload Cart Rules file for a specific cohort.\n",
    "    \n",
    "    Args:\n",
    "        cohort_id: ID of the cohort to update\n",
    "        file_name: Path to the Excel file to upload\n",
    "        \n",
    "    Returns:\n",
    "        API response object\n",
    "    \"\"\"\n",
    "    token = get_access_token(\n",
    "        'https://sso.maxab.info/auth/realms/maxab/protocol/openid-connect/token',\n",
    "        'main-system-externals',\n",
    "        secret\n",
    "    )\n",
    "    \n",
    "    url = f\"https://api.maxab.info/main-system/api/admin-portal/cohorts/{cohort_id}/cart-rules\"\n",
    "    \n",
    "    files = [\n",
    "        ('sheet', (file_name, open(file_name, 'rb'),\n",
    "                   'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'))\n",
    "    ]\n",
    "    headers = {'Authorization': f'bearer {token}'}\n",
    "    \n",
    "    response = requests.request(\"POST\", url, headers=headers, data={}, files=files)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2601556c",
   "metadata": {},
   "source": [
    "### Database Connection Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dda92e1-ffd3-4d47-80a7-b65fc9a64ee4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def snowflake_query(country, query, warehouse=None, columns=[], conn=None):\n",
    "    \"\"\"\n",
    "    Execute a query against Snowflake and return results as DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        country: Country identifier (e.g., \"Egypt\")\n",
    "        query: SQL query string to execute\n",
    "        warehouse: Snowflake warehouse (optional)\n",
    "        columns: Custom column names (optional)\n",
    "        conn: Existing connection (optional)\n",
    "        \n",
    "    Returns:\n",
    "        pandas DataFrame with query results\n",
    "    \"\"\"\n",
    "    con = snowflake.connector.connect(\n",
    "        user     = os.environ[\"SNOWFLAKE_USERNAME\"],\n",
    "        account  = os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "        password = os.environ[\"SNOWFLAKE_PASSWORD\"],\n",
    "        database = os.environ[\"SNOWFLAKE_DATABASE\"]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        cur = con.cursor()\n",
    "        cur.execute(\"USE WAREHOUSE COMPUTE_WH\")\n",
    "        cur.execute(query)\n",
    "        \n",
    "        column_names = [col[0] for col in cur.description]\n",
    "        results = cur.fetchall()\n",
    "        \n",
    "        if not results:\n",
    "            out = pd.DataFrame(columns=[name.lower() for name in column_names])\n",
    "        else:\n",
    "            if len(columns) == 0:\n",
    "                out = pd.DataFrame(np.array(results), columns=column_names)\n",
    "                out.columns = out.columns.str.lower()\n",
    "            else:\n",
    "                out = pd.DataFrame(np.array(results), columns=columns)\n",
    "                out.columns = out.columns.str.lower()\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Query error: {e}\")\n",
    "        raise\n",
    "        \n",
    "    finally:\n",
    "        cur.close()\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "febda4a3-f2e9-4601-b101-0dd290924eb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using timezone: America/Los_Angeles\n"
     ]
    }
   ],
   "source": [
    "# Get Snowflake timezone for consistent date/time handling\n",
    "query = \"SHOW PARAMETERS LIKE 'TIMEZONE'\"\n",
    "timezone_result = snowflake_query(\"Egypt\", query)\n",
    "zone_to_use = timezone_result['value'].values[0]\n",
    "print(f\"✓ Using timezone: {zone_to_use}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e25101",
   "metadata": {},
   "source": [
    "### Google Sheets Connection (Force Brands)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5d55d8",
   "metadata": {},
   "source": [
    "## 2. Product Selection\n",
    "\n",
    "Select top-performing products per warehouse based on:\n",
    "- Gross profit ranking (40% weight)\n",
    "- Sales velocity ranking (25% weight)\n",
    "- Order count ranking (20% weight)\n",
    "- Retailer count ranking (15% weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "225abe85-5954-48b3-97ac-dd49d3672c38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching product selection data...\n",
      "✓ Retrieved 2848 products from 15 warehouses\n"
     ]
    }
   ],
   "source": [
    "query = ''' \n",
    "WITH rr AS (\n",
    "    SELECT product_id, warehouse_id, rr\n",
    "    FROM (\n",
    "        SELECT *, \n",
    "               MAX(date) OVER (PARTITION BY product_id, warehouse_id) as max_date\n",
    "        FROM finance.PREDICTED_RUNNING_RATES\n",
    "        QUALIFY date = max_date\n",
    "            AND date::date >= CURRENT_DATE - 14 \n",
    "    )\n",
    "),\n",
    "\n",
    "stocks AS (\n",
    "    SELECT \n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        SUM(stocks) as stocks,\n",
    "        CASE \n",
    "            WHEN SUM(rr) > 0 THEN SUM(stocks) / SUM(rr) \n",
    "            ELSE SUM(stocks) \n",
    "        END as doh\n",
    "    FROM (\n",
    "        SELECT DISTINCT \n",
    "            product_warehouse.warehouse_id,\n",
    "            product_warehouse.product_id,\n",
    "            (product_warehouse.available_stock)::integer as stocks,\n",
    "            COALESCE(rr.rr, 0) as rr \n",
    "        FROM product_warehouse\n",
    "        JOIN products ON product_warehouse.product_id = products.id\n",
    "        JOIN product_units ON products.unit_id = product_units.id\n",
    "        LEFT JOIN rr ON rr.product_id = products.id \n",
    "            AND rr.warehouse_id = product_warehouse.warehouse_id\n",
    "        WHERE product_warehouse.warehouse_id NOT IN (6, 9, 10)\n",
    "            AND product_warehouse.is_basic_unit = 1\n",
    "            AND product_warehouse.available_stock > 0 \n",
    "    )\n",
    "    GROUP BY warehouse_id, product_id\n",
    "    HAVING doh >= 1\n",
    "),\n",
    "\n",
    "base AS (\n",
    "    SELECT *, ROW_NUMBER() OVER (PARTITION BY retailer_id ORDER BY priority) as rnk \n",
    "    FROM (\n",
    "        SELECT x.*, TAGGABLE_ID as retailer_id \n",
    "        FROM (\n",
    "            SELECT id as cohort_id, name as cohort_name, priority, dynamic_tag_id \n",
    "            FROM cohorts \n",
    "            WHERE is_active = 'true'\n",
    "                AND id IN (700,701,702,703,704,1123,1124,1125,1126)\n",
    "        ) x \n",
    "        JOIN DYNAMIC_TAGgables dt ON x.dynamic_tag_id = dt.dynamic_tag_id\n",
    "        WHERE dt.taggable_id not IN (\n",
    "            SELECT taggable_id FROM DYNAMIC_TAGgables \n",
    "            WHERE dynamic_tag_id IN (2807, 2808, 2809, 2810, 2811, 2812)\n",
    "        )\n",
    "    )\n",
    "    QUALIFY rnk = 1 \n",
    "    ORDER BY cohort_id\n",
    "),\n",
    "\n",
    "-- Count total retailers per warehouse for penetration calculation\n",
    "warehouse_retailer_counts AS (\n",
    "    SELECT \n",
    "        whs.warehouse_id,\n",
    "        COUNT(DISTINCT base.retailer_id) as total_warehouse_retailers\n",
    "    FROM base\n",
    "    CROSS JOIN (SELECT DISTINCT warehouse_id FROM (VALUES\n",
    "            (38), (1), (236), (962), (337), (8), (339), (170), \n",
    "            (501), (401), (703), (632), (797)\n",
    "        ) x(warehouse_id)\n",
    "    ) whs\n",
    "    GROUP BY whs.warehouse_id\n",
    "),\n",
    "\n",
    "-- Map cohorts to warehouses\n",
    "cohort_warehouse_map AS (\n",
    "    SELECT cohort_id, warehouse_id\n",
    "    FROM (VALUES\n",
    "        (700, 38),   -- Cairo -> El-Marg\n",
    "        (700, 1),    -- Cairo -> Mostorod\n",
    "        (701, 236),  -- Giza -> Barageel\n",
    "        (701, 962),  -- Giza -> Sakkarah\n",
    "        (703, 337),  -- Delta West -> El-Mahala\n",
    "        (703, 8),    -- Delta West -> Tanta\n",
    "        (704, 339),  -- Delta East -> Mansoura FC\n",
    "        (704, 170),  -- Delta East -> Sharqya\n",
    "        (1124, 501), -- Upper Egypt -> Assiut FC\n",
    "        (1126, 401), -- Upper Egypt -> Bani sweif\n",
    "        (1123, 703), -- Upper Egypt -> Menya Samalot\n",
    "        (1125, 632), -- Upper Egypt -> Sohag\n",
    "        (702, 797)   -- Alexandria -> Khorshed Alex\n",
    "    ) x(cohort_id, warehouse_id)\n",
    "),\n",
    "\n",
    "-- Get pricing information by cohort (which maps to warehouse)\n",
    "cohort_prices AS (\n",
    "    SELECT  \n",
    "        cpu.cohort_id,\n",
    "        pu.product_id,\n",
    "        pu.packing_unit_id,\n",
    "        pu.basic_unit_count,\n",
    "        AVG(cpu.price) as price\n",
    "    FROM cohort_product_packing_units cpu\n",
    "    JOIN PACKING_UNIT_PRODUCTS pu ON pu.id = cpu.product_packing_unit_id\n",
    "    WHERE cpu.cohort_id IN (700,701,702,703,704,1123,1124,1125,1126)\n",
    "        AND cpu.created_at::date <> '2023-07-31'\n",
    "        AND cpu.is_customized = true\n",
    "    GROUP BY \n",
    "        cpu.cohort_id,\n",
    "        pu.product_id,\n",
    "        pu.packing_unit_id,\n",
    "        pu.basic_unit_count\n",
    "),\n",
    "\n",
    "-- Get live prices by cohort\n",
    "live_cohort_prices AS (\n",
    "    SELECT \n",
    "        cohort_id,\n",
    "        product_id,\n",
    "        pu_id as packing_unit_id,\n",
    "        buc as basic_unit_count,\n",
    "        NEW_PRICE as price\n",
    "    FROM materialized_views.DBDP_PRICES\n",
    "    WHERE created_at = CURRENT_DATE\n",
    "        AND DATE_PART('hour', CURRENT_TIME) BETWEEN SPLIT_PART(time_slot, '-', 1)::int AND SPLIT_PART(time_slot, '-', 2)::int\n",
    "        AND cohort_id IN (700,701,702,703,704,1123,1124,1125,1126)\n",
    "),\n",
    "\n",
    "-- Combine live and historical prices (live takes priority)\n",
    "combined_cohort_prices AS (\n",
    "    SELECT *\n",
    "    FROM (\n",
    "        SELECT *, 1 AS priority FROM live_cohort_prices\n",
    "        UNION ALL\n",
    "        SELECT *, 2 AS priority FROM cohort_prices\n",
    "    )\n",
    "    QUALIFY ROW_NUMBER() OVER (PARTITION BY cohort_id, product_id, packing_unit_id ORDER BY priority) = 1\n",
    "),\n",
    "\n",
    "-- Map cohort prices to warehouse prices\n",
    "warehouse_prices AS (\n",
    "    SELECT \n",
    "        cwm.warehouse_id,\n",
    "        ccp.product_id,\n",
    "        ccp.packing_unit_id,\n",
    "        ccp.basic_unit_count,\n",
    "        ccp.price\n",
    "    FROM combined_cohort_prices ccp\n",
    "    JOIN cohort_warehouse_map cwm ON cwm.cohort_id = ccp.cohort_id\n",
    "    WHERE ccp.price IS NOT NULL\n",
    "),\n",
    "\n",
    "-- Get sales performance over last 4 months\n",
    "product_performance AS (\n",
    "    SELECT \n",
    "        w.name as warehouse,\n",
    "        w.id as warehouse_id,\n",
    "        pso.product_id,\n",
    "        pso.packing_unit_id,\n",
    "        CONCAT(products.name_ar, ' ', products.size, ' ', product_units.name_ar) as sku,\n",
    "        brands.name_ar as brand,\n",
    "        categories.name_ar as category,\n",
    "        \n",
    "        -- Core volume metrics\n",
    "        COUNT(DISTINCT so.parent_sales_order_id) as total_orders,\n",
    "        COUNT(DISTINCT so.retailer_id) as total_retailers,\n",
    "        SUM(pso.purchased_item_count) as total_packing_units_sold,\n",
    "        SUM(pso.purchased_item_count * pso.basic_unit_count) as total_basic_units_sold,\n",
    "        \n",
    "        -- Revenue and margin\n",
    "        SUM(pso.total_price) as total_nmv,\n",
    "        SUM(COALESCE(f.wac_p, 0) * pso.purchased_item_count * pso.basic_unit_count) as total_cogs,\n",
    "        (SUM(pso.total_price) - SUM(COALESCE(f.wac_p, 0) * pso.purchased_item_count * pso.basic_unit_count)) / \n",
    "            NULLIF(SUM(pso.total_price), 0) as blended_margin,\n",
    "        \n",
    "        -- Average order metrics\n",
    "        AVG(pso.purchased_item_count) as avg_packing_units_per_order,\n",
    "        \n",
    "        -- Velocity metrics (units per day)\n",
    "        SUM(pso.purchased_item_count) / 120.0 as packing_units_per_day\n",
    "        \n",
    "    FROM product_sales_order pso\n",
    "    JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "    JOIN base ON base.retailer_id = so.retailer_id\n",
    "    JOIN products ON products.id = pso.product_id\n",
    "    JOIN brands ON products.brand_id = brands.id\n",
    "    JOIN categories ON products.category_id = categories.id \n",
    "        AND categories.name_ar NOT LIKE '%سايب%'\n",
    "    JOIN finance.all_cogs f ON f.product_id = pso.product_id\n",
    "        AND f.from_date::date <= so.created_at::date\n",
    "        AND f.to_date::date > so.created_at::date\n",
    "    JOIN product_units ON product_units.id = products.unit_id\n",
    "\tjoin warehouses w on w.id = pso.warehouse_id\n",
    "    \n",
    "    WHERE TRUE\n",
    "        AND so.created_at::date BETWEEN current_date - 60 AND CURRENT_DATE - 1\n",
    "        AND so.sales_order_status_id NOT IN (7, 12)\n",
    "        AND so.channel IN ('telesales', 'retailer')\n",
    "        AND pso.purchased_item_count <> 0\n",
    "        AND products.activation = 'true'\n",
    "        AND w.id NOT IN (6, 9, 10)\n",
    "    \n",
    "    GROUP BY All\n",
    "),\n",
    "\n",
    "-- Add retailer penetration\n",
    "product_performance_with_penetration AS (\n",
    "    SELECT \n",
    "        pp.*,\n",
    "        wrc.total_warehouse_retailers,\n",
    "        (pp.total_retailers * 100.0 / NULLIF(wrc.total_warehouse_retailers, 0)) as retailer_penetration_pct\n",
    "    FROM product_performance pp\n",
    "    LEFT JOIN warehouse_retailer_counts wrc ON wrc.warehouse_id = pp.warehouse_id\n",
    "),\n",
    "\n",
    "-- Add pricing information at warehouse level\n",
    "product_performance_with_price AS (\n",
    "    SELECT \n",
    "        pp.*,\n",
    "        COALESCE(wp.price, 0) as product_price,\n",
    "        COALESCE(wp.basic_unit_count, 1) as basic_unit_count\n",
    "    FROM product_performance_with_penetration pp\n",
    "    LEFT JOIN warehouse_prices wp ON wp.warehouse_id = pp.warehouse_id\n",
    "        AND wp.product_id = pp.product_id \n",
    "        AND wp.packing_unit_id = pp.packing_unit_id\n",
    "),\n",
    "\n",
    "-- Add quality filters to focus on high-potential products\n",
    "qualified_products AS (\n",
    "    SELECT \n",
    "        pp.warehouse,\n",
    "        pp.warehouse_id,\n",
    "        pp.product_id,\n",
    "        pp.packing_unit_id,\n",
    "        pp.sku,\n",
    "        pp.brand,\n",
    "        pp.category,\n",
    "        pp.total_orders,\n",
    "        pp.total_retailers,\n",
    "        pp.total_packing_units_sold,\n",
    "        pp.total_basic_units_sold,\n",
    "        pp.total_nmv,\n",
    "        pp.blended_margin,\n",
    "        pp.avg_packing_units_per_order,\n",
    "        pp.packing_units_per_day,\n",
    "        pp.retailer_penetration_pct,\n",
    "        pp.product_price,\n",
    "        pp.basic_unit_count,\n",
    "        s.doh,\n",
    "        s.stocks,\n",
    "        \n",
    "        -- Calculate a simple volume-based score\n",
    "        (pp.total_nmv * pp.blended_margin) as gross_profit,\n",
    "        \n",
    "        -- Rank by gross profit within warehouse\n",
    "        ROW_NUMBER() OVER (PARTITION BY pp.warehouse_id ORDER BY (pp.total_nmv * pp.blended_margin) DESC) as gp_rank,\n",
    "        \n",
    "        -- Rank by velocity\n",
    "        ROW_NUMBER() OVER (PARTITION BY pp.warehouse_id ORDER BY pp.packing_units_per_day DESC) as velocity_rank,\n",
    "        \n",
    "        -- Rank by orders\n",
    "        ROW_NUMBER() OVER (PARTITION BY pp.warehouse_id ORDER BY pp.total_orders DESC) as order_rank,\n",
    "        \n",
    "        -- Rank by number of retailers\n",
    "        ROW_NUMBER() OVER (PARTITION BY pp.warehouse_id ORDER BY pp.total_retailers DESC) as retailer_rank\n",
    "        \n",
    "    FROM product_performance_with_price pp\n",
    "    JOIN stocks s ON s.product_id = pp.product_id \n",
    "        AND s.warehouse_id = pp.warehouse_id\n",
    "\n",
    "),\n",
    "\n",
    "-- Select top products using a combined scoring approach\n",
    "top_products AS (\n",
    "    SELECT \n",
    "        warehouse,\n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        sku,\n",
    "        brand,\n",
    "        category,\n",
    "        total_orders,\n",
    "        total_retailers,\n",
    "        total_packing_units_sold,\n",
    "        total_basic_units_sold,\n",
    "        ROUND(total_nmv, 2) as total_nmv,\n",
    "        ROUND(blended_margin * 100, 2) as margin_pct,\n",
    "        ROUND(avg_packing_units_per_order, 2) as avg_order_qty,\n",
    "        ROUND(packing_units_per_day, 2) as units_per_day,\n",
    "        ROUND(retailer_penetration_pct, 1) as retailer_penetration_pct,\n",
    "        ROUND(gross_profit, 2) as gross_profit,\n",
    "        ROUND(product_price, 2) as packing_unit_price,\n",
    "        basic_unit_count,\n",
    "        ROUND(product_price / NULLIF(basic_unit_count, 0), 2) as price_per_basic_unit,\n",
    "        gp_rank,\n",
    "        velocity_rank,\n",
    "        order_rank,\n",
    "        retailer_rank,\n",
    "        ROUND(doh, 2) as days_on_hand,\n",
    "        stocks as available_stock,\n",
    "        \n",
    "        -- Combined score: weighted average of ranks (lower is better)\n",
    "        (gp_rank * 0.15 + velocity_rank * 0.20 + order_rank * 0.30 + retailer_rank * 0.35) as combined_rank_score\n",
    "        \n",
    "    FROM qualified_products\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    warehouse,\n",
    "    warehouse_id,\n",
    "    product_id,\n",
    "    packing_unit_id,\n",
    "    sku,\n",
    "    brand,\n",
    "    category as cat,\n",
    "    total_orders,\n",
    "    total_retailers,\n",
    "    total_packing_units_sold,\n",
    "    total_basic_units_sold,\n",
    "    total_nmv,\n",
    "    margin_pct,\n",
    "    avg_order_qty,\n",
    "    units_per_day,\n",
    "    retailer_penetration_pct,\n",
    "    gross_profit,\n",
    "    packing_unit_price,\n",
    "    basic_unit_count,\n",
    "    price_per_basic_unit,\n",
    "    days_on_hand,\n",
    "    available_stock,\n",
    "    gp_rank as gross_profit_rank,\n",
    "    velocity_rank,\n",
    "    order_rank,\n",
    "    retailer_rank,\n",
    "    ROUND(combined_rank_score, 2) as combined_score,\n",
    "    ROW_NUMBER() OVER (PARTITION BY warehouse ORDER BY combined_rank_score) as final_rank\n",
    "FROM top_products\n",
    "WHERE combined_rank_score <= 500  -- Adjust this to get more/fewer products\n",
    "qualify final_rank<=200\n",
    "ORDER BY warehouse, combined_rank_score;\n",
    "'''\n",
    "\n",
    "# Execute query and convert numeric columns\n",
    "print(\"Fetching product selection data...\")\n",
    "selected_products = snowflake_query(\"Egypt\", query)\n",
    "\n",
    "for col in selected_products.columns:\n",
    "    selected_products[col] = pd.to_numeric(selected_products[col], errors='ignore')\n",
    "\n",
    "print(f\"✓ Retrieved {len(selected_products)} products from {selected_products['warehouse_id'].nunique()} warehouses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "489552af-531c-49c3-b495-88dce04f56ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Selected 2848 products after exclusions\n"
     ]
    }
   ],
   "source": [
    "# Remove excluded products\n",
    "selected_products = selected_products[~selected_products['product_id'].isin(PRODUCTS_TO_REMOVE)]\n",
    "print(f\"✓ Selected {len(selected_products)} products after exclusions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e424fc72",
   "metadata": {},
   "source": [
    "## 3. Quantity Tier Calculation\n",
    "\n",
    "Calculate tier 1 and tier 2 quantities based on:\n",
    "- Order history from frequent buyers (2+ orders)\n",
    "- Statistical analysis (median, Q3, P85, P90, P95)\n",
    "- IQR outlier removal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5d7495-748e-4337-92ae-aaa77e3b8398",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching quantity tier data...\n"
     ]
    }
   ],
   "source": [
    "selected_df = selected_products[['warehouse_id', 'product_id', 'packing_unit_id']].values.tolist()\n",
    "tuples_string = ','.join([f\"({int(wh_id)}, {int(prod_id)}, {int(pu_id)})\" for wh_id, prod_id, pu_id in selected_df])\n",
    "query = f'''\n",
    "WITH selected_products AS (\n",
    "    SELECT warehouse_id, product_id, packing_unit_id\n",
    "    FROM (VALUES\n",
    "      {tuples_string}\n",
    "    ) AS x(warehouse_id, product_id, packing_unit_id)\n",
    "),\n",
    "\n",
    "-- Same base filtering as product selection query\n",
    "-- Retailers in QD cohorts AND in specific dynamic tags\n",
    "base AS (\n",
    "    SELECT *, ROW_NUMBER() OVER (PARTITION BY retailer_id ORDER BY priority) as rnk \n",
    "    FROM (\n",
    "        SELECT x.*, TAGGABLE_ID as retailer_id \n",
    "        FROM (\n",
    "            SELECT id as cohort_id, name as cohort_name, priority, dynamic_tag_id \n",
    "            FROM cohorts \n",
    "            WHERE is_active = 'true'\n",
    "                AND id IN (700,701,702,703,704,1123,1124,1125,1126)\n",
    "        ) x \n",
    "        JOIN DYNAMIC_TAGgables dt ON x.dynamic_tag_id = dt.dynamic_tag_id\n",
    "        WHERE dt.taggable_id not IN (\n",
    "            SELECT taggable_id FROM DYNAMIC_TAGgables \n",
    "            WHERE dynamic_tag_id IN (2807, 2808, 2809, 2810, 2811, 2812)\n",
    "        )\n",
    "    )\n",
    "    QUALIFY rnk = 1 \n",
    "),\n",
    "\n",
    "raw_order_quantities AS (\n",
    "    SELECT \n",
    "        whs.wh as warehouse,\n",
    "        whs.warehouse_id,\n",
    "        pso.product_id,\n",
    "        pso.packing_unit_id,\n",
    "        CONCAT(products.name_ar, ' ', products.size, ' ', product_units.name_ar) as sku,\n",
    "        brands.name_ar as brand,\n",
    "        categories.name_ar as category,\n",
    "        so.parent_sales_order_id,\n",
    "        so.retailer_id,\n",
    "        so.created_at::date as order_date,\n",
    "        SUM(pso.purchased_item_count) as order_qty,\n",
    "        SUM(pso.total_price) as order_value,\n",
    "        -- ADD RECENCY WEIGHT: Recent orders get higher weight (exponential decay)\n",
    "        EXP(-0.02 * DATEDIFF('day', so.created_at::date, CURRENT_DATE)) as recency_weight\n",
    "        \n",
    "    FROM product_sales_order pso\n",
    "    JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "    -- Filter to only include retailers from base (same cohorts + tags as product selection)\n",
    "    JOIN base ON base.retailer_id = so.retailer_id\n",
    "    JOIN products ON products.id = pso.product_id\n",
    "    JOIN brands ON products.brand_id = brands.id\n",
    "    JOIN categories ON products.category_id = categories.id\n",
    "    JOIN product_units ON product_units.id = products.unit_id\n",
    "    JOIN materialized_views.retailer_polygon ON materialized_views.retailer_polygon.retailer_id = so.retailer_id\n",
    "    JOIN districts ON districts.id = materialized_views.retailer_polygon.district_id\n",
    "    JOIN cities ON cities.id = districts.city_id\n",
    "    JOIN states ON states.id = cities.state_id\n",
    "    JOIN regions ON regions.id = states.region_id\n",
    "    JOIN (SELECT * FROM (VALUES\n",
    "            ('Cairo', 'El-Marg', 38),\n",
    "            ('Cairo', 'Mostorod', 1),\n",
    "            ('Giza', 'Barageel', 236),\n",
    "            ('Giza', 'Sakkarah', 962),\n",
    "            ('Delta West', 'El-Mahala', 337),\n",
    "            ('Delta West', 'Tanta', 8),\n",
    "            ('Delta East', 'Mansoura FC', 339),\n",
    "            ('Delta East', 'Sharqya', 170),\n",
    "            ('Upper Egypt', 'Assiut FC', 501),\n",
    "            ('Upper Egypt', 'Bani sweif', 401),\n",
    "            ('Upper Egypt', 'Menya Samalot', 703),\n",
    "            ('Upper Egypt', 'Sohag', 632),\n",
    "            ('Alexandria', 'Khorshed Alex', 797)\n",
    "        ) x(region_name, wh, warehouse_id)\n",
    "    ) whs ON whs.region_name = CASE WHEN regions.id = 2 THEN states.name_en ELSE regions.name_en END\n",
    "    JOIN selected_products sp ON sp.warehouse_id = whs.warehouse_id \n",
    "        AND sp.product_id = pso.product_id\n",
    "        AND sp.packing_unit_id = pso.packing_unit_id\n",
    "    \n",
    "    WHERE TRUE\n",
    "        AND so.created_at::date BETWEEN DATE_TRUNC('month', CURRENT_DATE - INTERVAL '4 months') AND CURRENT_DATE - 1\n",
    "        AND so.sales_order_status_id NOT IN (7, 12)\n",
    "        AND so.channel IN ('telesales', 'retailer')\n",
    "        AND pso.purchased_item_count <> 0\n",
    "        AND products.activation = 'true'\n",
    "    \n",
    "    GROUP BY \n",
    "        whs.wh,\n",
    "        whs.warehouse_id,\n",
    "        pso.product_id,\n",
    "        pso.packing_unit_id,\n",
    "        products.name_ar,\n",
    "        products.size,\n",
    "        product_units.name_ar,\n",
    "        brands.name_ar,\n",
    "        categories.name_ar,\n",
    "        so.parent_sales_order_id,\n",
    "        so.retailer_id,\n",
    "        so.created_at::date\n",
    "),\n",
    "\n",
    "retailer_frequency AS (\n",
    "    SELECT \n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        retailer_id,\n",
    "        COUNT(DISTINCT parent_sales_order_id) as order_count,\n",
    "        COUNT(DISTINCT DATE_TRUNC('week', order_date)) as weeks_ordered,\n",
    "        MIN(order_date) as first_order_date,\n",
    "        MAX(order_date) as last_order_date,\n",
    "        DATEDIFF('day', MIN(order_date), MAX(order_date)) as days_span,\n",
    "        CASE \n",
    "            WHEN COUNT(DISTINCT parent_sales_order_id) > 1 \n",
    "            THEN DATEDIFF('day', MIN(order_date), MAX(order_date)) / (COUNT(DISTINCT parent_sales_order_id) - 1)\n",
    "            ELSE NULL \n",
    "        END as avg_days_between_orders\n",
    "    FROM raw_order_quantities\n",
    "    GROUP BY warehouse_id, product_id, packing_unit_id, retailer_id\n",
    "),\n",
    "\n",
    "frequent_buyers AS (\n",
    "    SELECT \n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        retailer_id,\n",
    "        order_count,\n",
    "        weeks_ordered,\n",
    "        avg_days_between_orders\n",
    "    FROM retailer_frequency\n",
    "    WHERE order_count >= 2 \n",
    "       OR weeks_ordered >= 2\n",
    "),\n",
    "\n",
    "filtered_orders AS (\n",
    "    SELECT roq.*\n",
    "    FROM raw_order_quantities roq\n",
    "    JOIN frequent_buyers fb \n",
    "        ON fb.warehouse_id = roq.warehouse_id\n",
    "        AND fb.product_id = roq.product_id\n",
    "        AND fb.packing_unit_id = roq.packing_unit_id\n",
    "        AND fb.retailer_id = roq.retailer_id\n",
    "),\n",
    "\n",
    "initial_stats AS (\n",
    "    SELECT \n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY order_qty) as q1,\n",
    "        PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY order_qty) as q3,\n",
    "        MEDIAN(order_qty) as median_qty,\n",
    "        STDDEV_POP(order_qty) as stddev_qty,\n",
    "        AVG(order_qty) as avg_qty\n",
    "    FROM filtered_orders\n",
    "    GROUP BY warehouse_id, product_id, packing_unit_id\n",
    "),\n",
    "\n",
    "cleaned_orders AS (\n",
    "    SELECT fo.*\n",
    "    FROM filtered_orders fo\n",
    "    JOIN initial_stats ist \n",
    "        ON ist.warehouse_id = fo.warehouse_id\n",
    "        AND ist.product_id = fo.product_id\n",
    "        AND ist.packing_unit_id = fo.packing_unit_id\n",
    "    WHERE TRUE\n",
    "        AND fo.order_qty >= ist.q1 - 1.5 * (ist.q3 - ist.q1)\n",
    "        AND fo.order_qty <= ist.q3 + 1.5 * (ist.q3 - ist.q1)\n",
    "        AND (ist.stddev_qty = 0 \n",
    "             OR ABS(fo.order_qty - ist.avg_qty) <= 3 * ist.stddev_qty)\n",
    "),\n",
    "\n",
    "-- MODIFIED: Recent orders stats (last 15 days)\n",
    "recent_trends AS (\n",
    "    SELECT \n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        -- Weighted average gives more importance to recent orders\n",
    "        SUM(order_qty * recency_weight) / NULLIF(SUM(recency_weight), 0) as weighted_avg_qty,\n",
    "        -- Last 15 days statistics\n",
    "        AVG(CASE WHEN order_date >= CURRENT_DATE - 15 THEN order_qty END) as last_15d_avg,\n",
    "        MEDIAN(CASE WHEN order_date >= CURRENT_DATE - 15 THEN order_qty END) as last_15d_median,\n",
    "        MAX(CASE WHEN order_date >= CURRENT_DATE - 15 THEN order_qty END) as last_15d_max,\n",
    "        COUNT(CASE WHEN order_date >= CURRENT_DATE - 15 THEN 1 END) as last_15d_orders\n",
    "    FROM cleaned_orders\n",
    "    GROUP BY warehouse_id, product_id, packing_unit_id\n",
    "),\n",
    "\n",
    "quantity_stats AS (\n",
    "    SELECT \n",
    "        warehouse,\n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        sku,\n",
    "        brand,\n",
    "        category,\n",
    "        \n",
    "        COUNT(DISTINCT parent_sales_order_id) as total_orders,\n",
    "        COUNT(DISTINCT retailer_id) as total_retailers,\n",
    "        \n",
    "        MIN(order_qty) as min_qty,\n",
    "        MAX(order_qty) as max_qty,\n",
    "        AVG(order_qty) as avg_qty,\n",
    "        MEDIAN(order_qty) as median_qty,\n",
    "        STDDEV_POP(order_qty) as stddev_qty,\n",
    "        \n",
    "        PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY order_qty) as q1_qty,\n",
    "        PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY order_qty) as q2_qty,\n",
    "        PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY order_qty) as q3_qty,\n",
    "        PERCENTILE_CONT(0.85) WITHIN GROUP (ORDER BY order_qty) as p85_qty,\n",
    "        PERCENTILE_CONT(0.90) WITHIN GROUP (ORDER BY order_qty) as p90_qty,\n",
    "        PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY order_qty) as p95_qty,\n",
    "        \n",
    "        SUM(order_value) as total_revenue,\n",
    "        AVG(order_value) as avg_order_value\n",
    "        \n",
    "    FROM cleaned_orders\n",
    "    GROUP BY \n",
    "        warehouse,\n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        sku,\n",
    "        brand,\n",
    "        category\n",
    "),\n",
    "\n",
    "frequency_table AS (\n",
    "    SELECT\n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        order_qty,\n",
    "        COUNT(DISTINCT parent_sales_order_id) AS freq\n",
    "    FROM cleaned_orders\n",
    "    GROUP BY warehouse_id, product_id, packing_unit_id, order_qty\n",
    "),\n",
    "\n",
    "lag_lead AS (\n",
    "    SELECT\n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        order_qty,\n",
    "        freq,\n",
    "        LAG(freq) OVER (PARTITION BY warehouse_id, product_id, packing_unit_id ORDER BY order_qty) AS prev_freq,\n",
    "        LEAD(freq) OVER (PARTITION BY warehouse_id, product_id, packing_unit_id ORDER BY order_qty) AS next_freq\n",
    "    FROM frequency_table\n",
    "),\n",
    "\n",
    "most_frequent_qty AS (\n",
    "    SELECT \n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        order_qty as mode_qty,\n",
    "        freq as mode_freq,\n",
    "        freq * 1.0 / SUM(freq) OVER (PARTITION BY warehouse_id, product_id, packing_unit_id) as mode_contribution\n",
    "    FROM (\n",
    "        SELECT *,\n",
    "               ROW_NUMBER() OVER (PARTITION BY warehouse_id, product_id, packing_unit_id ORDER BY freq DESC, order_qty DESC) as rn\n",
    "        FROM lag_lead\n",
    "        WHERE (freq > COALESCE(prev_freq, -1))\n",
    "          AND (freq > COALESCE(next_freq, -1))\n",
    "    )\n",
    "    WHERE rn = 1\n",
    "),\n",
    "\n",
    "frequency_metrics AS (\n",
    "    SELECT \n",
    "        fb.warehouse_id,\n",
    "        fb.product_id,\n",
    "        fb.packing_unit_id,\n",
    "        COUNT(DISTINCT fb.retailer_id) as frequent_retailer_count,\n",
    "        AVG(fb.order_count) as avg_orders_per_retailer,\n",
    "        AVG(fb.avg_days_between_orders) as avg_refill_days,\n",
    "        MEDIAN(fb.avg_days_between_orders) as median_refill_days\n",
    "    FROM frequent_buyers fb\n",
    "    GROUP BY fb.warehouse_id, fb.product_id, fb.packing_unit_id\n",
    "),\n",
    "\n",
    "tier_calculations AS (\n",
    "    SELECT \n",
    "        qs.*,\n",
    "        COALESCE(mf.mode_qty, qs.median_qty) as mode_qty,\n",
    "        COALESCE(mf.mode_freq, 0) as mode_freq,\n",
    "        COALESCE(mf.mode_contribution, 0) as mode_contribution,\n",
    "        COALESCE(fm.frequent_retailer_count, 0) as frequent_retailer_count,\n",
    "        COALESCE(fm.avg_orders_per_retailer, 0) as avg_orders_per_retailer,\n",
    "        COALESCE(fm.avg_refill_days, 0) as avg_refill_days,\n",
    "        COALESCE(fm.median_refill_days, 0) as median_refill_days,\n",
    "        \n",
    "        -- ADD: Recency metrics\n",
    "        rt.weighted_avg_qty,\n",
    "        rt.last_15d_avg,\n",
    "        rt.last_15d_median,\n",
    "        rt.last_15d_max,\n",
    "        rt.last_15d_orders,\n",
    "        \n",
    "        -- MODIFIED: Tier 1 with 15-day recency factor\n",
    "        -- Blends historical median with recent trends (70% historical, 30% recent)\n",
    "        CEIL(GREATEST(\n",
    "            (0.7 * qs.median_qty + 0.3 * COALESCE(rt.weighted_avg_qty, qs.median_qty)) + 1.0 * COALESCE(qs.stddev_qty, 1),\n",
    "            qs.q3_qty,\n",
    "            COALESCE(mf.mode_qty, qs.median_qty) + GREATEST(3, qs.median_qty * 0.3),\n",
    "            -- If recent 15 days show growth, adjust upward\n",
    "            CASE \n",
    "                WHEN rt.last_15d_orders >= 3 AND rt.last_15d_median > qs.median_qty \n",
    "                THEN rt.last_15d_median * 1.2\n",
    "                ELSE qs.median_qty * 1.4\n",
    "            END,\n",
    "            qs.median_qty + 3\n",
    "        )) as tier_1_qty,\n",
    "        \n",
    "        -- MODIFIED: Tier 2 with 15-day recency factor\n",
    "        CEIL(GREATEST(\n",
    "            qs.q3_qty + 1.5 * COALESCE(qs.stddev_qty, 1),\n",
    "            qs.p85_qty + 1.0 * COALESCE(qs.stddev_qty, 1),\n",
    "            qs.p90_qty + 0.5 * COALESCE(qs.stddev_qty, 1),\n",
    "            qs.p95_qty,\n",
    "            -- Blend historical and weighted average\n",
    "            (0.6 * qs.median_qty + 0.4 * COALESCE(rt.weighted_avg_qty, qs.median_qty)) * 2.0,\n",
    "            -- If last 15 days show higher demand, adjust tier 2 upward\n",
    "            CASE \n",
    "                WHEN rt.last_15d_orders >= 3 AND rt.last_15d_max > qs.p90_qty \n",
    "                THEN rt.last_15d_max * 1.1\n",
    "                ELSE qs.median_qty * 2.0\n",
    "            END\n",
    "        )) as tier_2_qty_base\n",
    "        \n",
    "    FROM quantity_stats qs\n",
    "    LEFT JOIN most_frequent_qty mf \n",
    "        ON mf.warehouse_id = qs.warehouse_id \n",
    "        AND mf.product_id = qs.product_id\n",
    "        AND mf.packing_unit_id = qs.packing_unit_id\n",
    "    LEFT JOIN frequency_metrics fm\n",
    "        ON fm.warehouse_id = qs.warehouse_id\n",
    "        AND fm.product_id = qs.product_id\n",
    "        AND fm.packing_unit_id = qs.packing_unit_id\n",
    "    LEFT JOIN recent_trends rt\n",
    "        ON rt.warehouse_id = qs.warehouse_id\n",
    "        AND rt.product_id = qs.product_id\n",
    "        AND rt.packing_unit_id = qs.packing_unit_id\n",
    "),\n",
    "\n",
    "tier_adjustments AS (\n",
    "    SELECT \n",
    "        warehouse,\n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        sku,\n",
    "        brand,\n",
    "        category,\n",
    "        total_orders,\n",
    "        total_retailers,\n",
    "        min_qty,\n",
    "        avg_qty,\n",
    "        median_qty,\n",
    "        stddev_qty,\n",
    "        q1_qty,\n",
    "        q3_qty,\n",
    "        p85_qty,\n",
    "        p90_qty,\n",
    "        p95_qty,\n",
    "        max_qty,\n",
    "        mode_qty,\n",
    "        mode_freq,\n",
    "        mode_contribution,\n",
    "        frequent_retailer_count,\n",
    "        avg_orders_per_retailer,\n",
    "        avg_refill_days,\n",
    "        median_refill_days,\n",
    "        total_revenue,\n",
    "        avg_order_value,\n",
    "        \n",
    "        -- ADD: Recency metrics to output\n",
    "        weighted_avg_qty,\n",
    "        last_15d_avg,\n",
    "        last_15d_median,\n",
    "        last_15d_max,\n",
    "        last_15d_orders,\n",
    "        \n",
    "        tier_1_qty,\n",
    "        LEAST(\n",
    "            CEIL(GREATEST(\n",
    "                tier_2_qty_base,\n",
    "                tier_1_qty * 1.6\n",
    "            )),\n",
    "            GREATEST(\n",
    "                tier_1_qty * 3.5,\n",
    "                tier_1_qty + 20\n",
    "            )\n",
    "        ) as tier_2_qty\n",
    "        \n",
    "    FROM tier_calculations\n",
    "),\n",
    "\n",
    "retailer_distribution AS (\n",
    "    SELECT \n",
    "        co.warehouse_id,\n",
    "        co.product_id,\n",
    "        co.packing_unit_id,\n",
    "        ta.tier_1_qty,\n",
    "        ta.tier_2_qty,\n",
    "        COUNT(DISTINCT CASE \n",
    "            WHEN co.order_qty < ta.tier_1_qty THEN co.retailer_id \n",
    "        END) as retailers_below_t1,\n",
    "        COUNT(DISTINCT CASE \n",
    "            WHEN co.order_qty >= ta.tier_1_qty AND co.order_qty < ta.tier_2_qty THEN co.retailer_id \n",
    "        END) as retailers_at_t1,\n",
    "        COUNT(DISTINCT CASE \n",
    "            WHEN co.order_qty >= ta.tier_2_qty THEN co.retailer_id \n",
    "        END) as retailers_at_t2,\n",
    "        COUNT(CASE \n",
    "            WHEN co.order_qty < ta.tier_1_qty THEN 1 \n",
    "        END) as orders_below_t1,\n",
    "        COUNT(CASE \n",
    "            WHEN co.order_qty >= ta.tier_1_qty AND co.order_qty < ta.tier_2_qty THEN 1 \n",
    "        END) as orders_at_t1,\n",
    "        COUNT(CASE \n",
    "            WHEN co.order_qty >= ta.tier_2_qty THEN 1 \n",
    "        END) as orders_at_t2\n",
    "    FROM cleaned_orders co\n",
    "    JOIN tier_adjustments ta \n",
    "        ON ta.warehouse_id = co.warehouse_id \n",
    "        AND ta.product_id = co.product_id\n",
    "        AND ta.packing_unit_id = co.packing_unit_id\n",
    "    GROUP BY \n",
    "        co.warehouse_id,\n",
    "        co.product_id,\n",
    "        co.packing_unit_id,\n",
    "        ta.tier_1_qty,\n",
    "        ta.tier_2_qty\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    ta.warehouse,\n",
    "    ta.warehouse_id,\n",
    "    ta.product_id,\n",
    "    ta.packing_unit_id,\n",
    "    ta.sku,\n",
    "    ta.brand,\n",
    "    ta.category,\n",
    "    \n",
    "    ta.frequent_retailer_count,\n",
    "    ROUND(ta.avg_orders_per_retailer, 2) as avg_orders_per_retailer,\n",
    "    ROUND(ta.avg_refill_days, 1) as avg_refill_days,\n",
    "    ROUND(ta.median_refill_days, 1) as median_refill_days,\n",
    "    \n",
    "    ta.total_orders,\n",
    "    ta.total_retailers,\n",
    "    \n",
    "    ta.min_qty,\n",
    "    ROUND(ta.avg_qty, 2) as avg_qty,\n",
    "    ta.median_qty,\n",
    "    ROUND(ta.weighted_avg_qty, 2) as weighted_avg_qty,\n",
    "    ta.q1_qty as q1_25_qty,\n",
    "    ta.q3_qty as q3_75_qty,\n",
    "    ta.p85_qty,\n",
    "    ta.p90_qty,\n",
    "    ta.p95_qty,\n",
    "    ta.max_qty,\n",
    "    ROUND(ta.stddev_qty, 2) as stddev_qty,\n",
    "    ta.mode_qty,\n",
    "    ta.mode_freq,\n",
    "    ROUND(ta.mode_contribution * 100, 1) as mode_pct,\n",
    "    \n",
    "    -- MODIFIED: 15-day trend metrics\n",
    "    ROUND(ta.last_15d_avg, 2) as last_15d_avg,\n",
    "    ta.last_15d_median,\n",
    "    ta.last_15d_max,\n",
    "    ta.last_15d_orders,\n",
    "    \n",
    "    ta.tier_1_qty,\n",
    "    ta.tier_2_qty,\n",
    "    ROUND((ta.tier_1_qty - ta.median_qty) * 100.0 / NULLIF(ta.median_qty, 0), 1) as tier_1_increase_pct,\n",
    "    ROUND((ta.tier_2_qty - ta.median_qty) * 100.0 / NULLIF(ta.median_qty, 0), 1) as tier_2_increase_pct,\n",
    "    ROUND(ta.tier_2_qty * 1.0 / NULLIF(ta.tier_1_qty, 0), 2) as tier_2_to_tier_1_ratio,\n",
    "    \n",
    "    rd.retailers_below_t1,\n",
    "    rd.retailers_at_t1,\n",
    "    rd.retailers_at_t2,\n",
    "    \n",
    "    rd.orders_below_t1,\n",
    "    rd.orders_at_t1,\n",
    "    rd.orders_at_t2,\n",
    "    \n",
    "    ROUND(100.0 * rd.retailers_below_t1 / NULLIF(ta.total_retailers, 0), 1) as pct_retailers_below_t1,\n",
    "    ROUND(100.0 * rd.retailers_at_t1 / NULLIF(ta.total_retailers, 0), 1) as pct_retailers_at_t1,\n",
    "    ROUND(100.0 * rd.retailers_at_t2 / NULLIF(ta.total_retailers, 0), 1) as pct_retailers_at_t2,\n",
    "    \n",
    "    ROUND(100.0 * rd.orders_below_t1 / NULLIF(ta.total_orders, 0), 1) as pct_orders_below_t1,\n",
    "    ROUND(100.0 * rd.orders_at_t1 / NULLIF(ta.total_orders, 0), 1) as pct_orders_at_t1,\n",
    "    ROUND(100.0 * rd.orders_at_t2 / NULLIF(ta.total_orders, 0), 1) as pct_orders_at_t2,\n",
    "    \n",
    "    ROUND(ta.total_revenue, 2) as total_revenue,\n",
    "    ROUND(ta.avg_order_value, 2) as avg_order_value\n",
    "\n",
    "FROM tier_adjustments ta\n",
    "JOIN retailer_distribution rd \n",
    "    ON rd.warehouse_id = ta.warehouse_id \n",
    "    AND rd.product_id = ta.product_id\n",
    "    AND rd.packing_unit_id = ta.packing_unit_id\n",
    "ORDER BY ta.warehouse, ta.total_orders DESC\n",
    "'''\n",
    "\n",
    "# Execute query and convert numeric columns\n",
    "print(\"Fetching quantity tier data...\")\n",
    "tiers_selection = snowflake_query(\"Egypt\", query)\n",
    "\n",
    "for col in tiers_selection.columns:\n",
    "    tiers_selection[col] = pd.to_numeric(tiers_selection[col], errors='ignore')\n",
    "\n",
    "print(f\"✓ Calculated tiers for {len(tiers_selection)} product-warehouse combinations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc05aac1",
   "metadata": {},
   "source": [
    "### SKU Information & Cost Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8045f25-1b32-437c-81e8-a04f9771908f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f'''\n",
    "SELECT DISTINCT  \n",
    "    products.id as product_id,\n",
    "    CONCAT(products.name_ar, ' ', products.size, ' ', product_units.name_ar) as sku,\n",
    "    brands.name_ar as brand, \n",
    "    categories.name_ar as cat,\n",
    "    f.wac_p\n",
    "FROM products \n",
    "JOIN brands ON products.brand_id = brands.id \n",
    "JOIN categories ON products.category_id = categories.id\n",
    "JOIN finance.all_cogs f ON f.product_id = products.id \n",
    "    AND CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMESTAMP()) \n",
    "        BETWEEN f.from_date AND f.to_date \n",
    "JOIN product_units ON product_units.id = products.unit_id \n",
    "'''\n",
    "\n",
    "print(\"Fetching SKU information and WAC data...\")\n",
    "sku_info = snowflake_query(\"Egypt\", query)\n",
    "sku_info['product_id'] = pd.to_numeric(sku_info['product_id'])\n",
    "sku_info['wac_p'] = pd.to_numeric(sku_info['wac_p'])\n",
    "\n",
    "print(f\"✓ Retrieved cost data for {len(sku_info)} SKUs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ff38bd-bbc5-4ca6-b152-c283255d067a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Market Prices\n",
    "\n",
    "Gather competitive pricing data from multiple sources:\n",
    "- **Marketplace prices** - Regional marketplace data with fallbacks\n",
    "- **Ben Soliman prices** - Competitor pricing\n",
    "- **Scraped prices** - Web-scraped competitor data\n",
    "- **Product statistics** - Historical margin boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd94d09",
   "metadata": {},
   "source": [
    "### 4.1 Marketplace Prices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873a9cfe-76fb-486c-b71c-b2c3c447b7af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f'''\n",
    "WITH whs as (SELECT *\n",
    "             FROM   (values\n",
    "                            ('Cairo', 'El-Marg', 38,700),\n",
    "                            ('Cairo', 'Mostorod', 1,700),\n",
    "                            ('Giza', 'Barageel', 236,701),\n",
    "                            ('Delta West', 'El-Mahala', 337,703),\n",
    "                            ('Delta West', 'Tanta', 8,703),\n",
    "                            ('Delta East', 'Mansoura FC', 339,704),\n",
    "                            ('Delta East', 'Sharqya', 170,704),\n",
    "                            ('Upper Egypt', 'Assiut FC', 501,1124),\n",
    "                            ('Upper Egypt', 'Bani sweif', 401,1126),\n",
    "                            ('Upper Egypt', 'Menya Samalot', 703,1123),\n",
    "                            ('Upper Egypt', 'Sohag', 632,1125),\n",
    "                            ('Alexandria', 'Khorshed Alex', 797,702),\n",
    "\t\t\t\t\t\t\t('Giza', 'Sakkarah', 962,701)\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t)\n",
    "                    x(region, wh, warehouse_id,cohort_id)),\n",
    "full_data as (\n",
    "select products.id as product_id, region,warehouse_id\n",
    "from products , whs \n",
    "where activation = 'true'\n",
    "),\t\t\t\t\n",
    "\n",
    "MP as (\n",
    "select region,product_id,\n",
    "min(min_price) as min_price,\n",
    "min(max_price) as max_price,\n",
    "min(mod_price) as mod_price,\n",
    "min(true_min) as true_min,\n",
    "min(true_max) as true_max\n",
    "\n",
    "from (\n",
    "select mp.region,mp.product_id,mp.pu_id,\n",
    "min_price/BASIC_UNIT_COUNT as min_price,\n",
    "max_price/BASIC_UNIT_COUNT as max_price,\n",
    "mod_price/BASIC_UNIT_COUNT as mod_price,\n",
    "TRUE_MIN_PRICE/BASIC_UNIT_COUNT as true_min,\n",
    "TRUE_MAX_PRICE/BASIC_UNIT_COUNT as true_max\n",
    "from materialized_views.marketplace_prices mp \n",
    "join packing_unit_products pup on pup.product_id = mp.product_id and pup.packing_unit_id = mp.pu_id\n",
    "join finance.all_cogs f on f.product_id = mp.product_id and CURRENT_TIMESTAMP between f.from_date and f.to_date\n",
    "where  least(min_price,mod_price) between wac_p*0.9 and wac_p*1.3 \n",
    ")\n",
    "group by all \n",
    "),\n",
    "region_mapping AS (\n",
    "    SELECT * \n",
    "\tFROM \n",
    "\t(\tVALUES\n",
    "        ('Delta East', 'Delta West'),\n",
    "        ('Delta West', 'Delta East'),\n",
    "        ('Alexandria', 'Cairo'),\n",
    "        ('Alexandria', 'Giza'),\n",
    "        ('Upper Egypt', 'Cairo'),\n",
    "        ('Upper Egypt', 'Giza'),\n",
    "\t\t('Cairo','Giza'),\n",
    "\t\t('Giza','Cairo'),\n",
    "\t\t('Delta West', 'Cairo'),\n",
    "\t\t('Delta East', 'Cairo'),\n",
    "\t\t('Delta West', 'Giza'),\n",
    "\t\t('Delta East', 'Giza')\n",
    "\t\t)\n",
    "    AS region_mapping(region, fallback_region)\n",
    ")\n",
    "\n",
    "\n",
    "select region,warehouse_id,product_id,\n",
    "min(final_min_price) as final_min_price,\n",
    "min(final_max_price) as final_max_price,\n",
    "min(final_mod_price) as final_mod_price,\n",
    "min(final_true_min) as final_true_min,\n",
    "min(final_true_max) as final_true_max\n",
    "\n",
    "from (\n",
    "SELECT\n",
    "distinct \n",
    "\tw.region,\n",
    "    w.warehouse_id,\n",
    "\tw.product_id,\n",
    "    COALESCE(m1.min_price, m2.min_price) AS final_min_price,\n",
    "    COALESCE(m1.max_price, m2.max_price) AS final_max_price,\n",
    "    COALESCE(m1.mod_price, m2.mod_price) AS final_mod_price,\n",
    "\tCOALESCE(m1.true_min, m2.true_min) AS final_true_min,\n",
    "\tCOALESCE(m1.true_max, m2.true_max) AS final_true_max,\n",
    "FROM full_data w\n",
    "LEFT JOIN MP m1\n",
    "    ON w.region = m1.region and w.product_id = m1.product_id\n",
    "JOIN region_mapping rm\n",
    "    ON w.region = rm.region\n",
    "LEFT JOIN MP m2\n",
    "    ON rm.fallback_region = m2.region\n",
    "   AND w.product_id = m2.product_id\n",
    ")\n",
    "where final_min_price is not null \n",
    "group by all \n",
    "'''\n",
    "\n",
    "print(\"Fetching marketplace prices...\")\n",
    "marketplace = snowflake_query(\"Egypt\", query)\n",
    "marketplace.columns = marketplace.columns.str.lower()\n",
    "\n",
    "for col in marketplace.columns:\n",
    "    marketplace[col] = pd.to_numeric(marketplace[col], errors='ignore')\n",
    "\n",
    "print(f\"✓ Retrieved marketplace prices for {len(marketplace)} products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd2f7ff",
   "metadata": {},
   "source": [
    "### 4.2 Ben Soliman (Competitor) Prices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede9cdec-a69d-4579-afcc-57df08a5f753",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f'''\n",
    "select z.* \n",
    "from (\n",
    "select maxab_product_id as product_id,avg(bs_final_price) as ben_soliman_price\n",
    "from (\n",
    "select * , row_number()over(partition by maxab_product_id order by diff) as rnk_2\n",
    "from (\n",
    "select *,(bs_final_price-wac_p)/wac_p as diff_2\n",
    "from (\n",
    "select * ,bs_price/maxab_basic_unit_count as bs_final_price\n",
    "from (\n",
    "select *,row_number()over(partition by maxab_product_id,maxab_pu order by diff) as rnk \n",
    "from (\n",
    "select sm.* ,max(INJECTION_DATE::date)over(partition by maxab_product_id,maxab_pu) as max_date,wac1,wac_p,abs(bs_price-(wac_p*maxab_basic_unit_count))/(wac_p*maxab_basic_unit_count) as diff \n",
    "from materialized_views.savvy_mapping sm \n",
    "join finance.all_cogs f on f.product_id = sm.maxab_product_id and current_timestamp between f.from_Date and f.to_date\n",
    "where bs_price is not null \n",
    "and INJECTION_DATE::date >= CURRENT_DATE- 5\n",
    "qualify INJECTION_DATE::date = max_date\n",
    ")\n",
    "qualify rnk = 1 \n",
    ")\n",
    ")\n",
    "where diff_2 between -0.5 and 0.5 \n",
    ")\n",
    "qualify rnk_2 = 1 \n",
    ")\n",
    "group by all\n",
    ")z \n",
    "join finance.all_cogs f on f.product_id = z.product_id and current_timestamp between f.from_Date and f.to_date\n",
    "\n",
    "where ben_soliman_price between f.wac_p*0.9 and f.wac_p*1.3\n",
    "'''\n",
    "\n",
    "print(\"Fetching Ben Soliman (competitor) prices...\")\n",
    "bensoliman = snowflake_query(\"Egypt\", query)\n",
    "bensoliman.columns = bensoliman.columns.str.lower()\n",
    "\n",
    "for col in bensoliman.columns:\n",
    "    bensoliman[col] = pd.to_numeric(bensoliman[col], errors='ignore')\n",
    "\n",
    "print(f\"✓ Retrieved competitor prices for {len(bensoliman)} products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd5c097",
   "metadata": {},
   "source": [
    "### 4.3 Scraped Competitor Prices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98943949-f930-4b02-aaf6-1e20144c674e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f'''\n",
    "WITH whs as (SELECT *\n",
    "             FROM   (values\n",
    "                            ('Cairo', 'El-Marg', 38,700),\n",
    "                            ('Cairo', 'Mostorod', 1,700),\n",
    "                            ('Giza', 'Barageel', 236,701),\n",
    "                            ('Delta West', 'El-Mahala', 337,703),\n",
    "                            ('Delta West', 'Tanta', 8,703),\n",
    "                            ('Delta East', 'Mansoura FC', 339,704),\n",
    "                            ('Delta East', 'Sharqya', 170,704),\n",
    "                            ('Upper Egypt', 'Assiut FC', 501,1124),\n",
    "                            ('Upper Egypt', 'Bani sweif', 401,1126),\n",
    "                            ('Upper Egypt', 'Menya Samalot', 703,1123),\n",
    "                            ('Upper Egypt', 'Sohag', 632,1125),\n",
    "                            ('Alexandria', 'Khorshed Alex', 797,702),\n",
    "\t\t\t\t\t\t\t('Giza', 'Sakkarah', 962,701)\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t)\n",
    "                    x(region, wh, warehouse_id,cohort_id))\n",
    "select product_id,x.region,warehouse_id,min(MARKET_PRICE) as min_scrapped,max(MARKET_PRICE) as max_scrapped,median(MARKET_PRICE) as median_scrapped\n",
    "from (\n",
    "select MATERIALIZED_VIEWS.CLEANED_MARKET_PRICES.*,max(date)over(partition by region,MATERIALIZED_VIEWS.CLEANED_MARKET_PRICES.product_id,competitor) as max_date\n",
    "from MATERIALIZED_VIEWS.CLEANED_MARKET_PRICES\n",
    "join finance.all_cogs f on f.product_id = MATERIALIZED_VIEWS.CLEANED_MARKET_PRICES.product_id and CURRENT_TIMESTAMP between f.from_date and f.to_date \n",
    "where date>= current_date -5\n",
    "and MARKET_PRICE between f.wac_p * 0.9 and wac_p*1.3\n",
    "qualify date = max_date \n",
    ") x \n",
    "left join whs on whs.region = x.region\n",
    "group by all \n",
    "'''\n",
    "\n",
    "print(\"Fetching scraped competitor prices...\")\n",
    "scrapped_prices = snowflake_query(\"Egypt\", query)\n",
    "scrapped_prices.columns = scrapped_prices.columns.str.lower()\n",
    "\n",
    "for col in scrapped_prices.columns:\n",
    "    scrapped_prices[col] = pd.to_numeric(scrapped_prices[col], errors='ignore')\n",
    "\n",
    "print(f\"✓ Retrieved scraped prices for {len(scrapped_prices)} products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ceaef4",
   "metadata": {},
   "source": [
    "### 4.4 Product Statistics (Margin Boundaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b418565-6266-4bac-9af4-6970ba53a3be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f'''\n",
    "SELECT \n",
    "    region,\n",
    "    product_id,\n",
    "    optimal_bm,\n",
    "    MIN_BOUNDARY,\n",
    "    MAX_BOUNDARY,\n",
    "    MEDIAN_BM\n",
    "FROM (\n",
    "    SELECT \n",
    "        region,\n",
    "        product_id,\n",
    "        target_bm,\n",
    "        optimal_bm,\n",
    "        MIN_BOUNDARY,\n",
    "        MAX_BOUNDARY,\n",
    "        MEDIAN_BM,\n",
    "        MAX(created_at) OVER (PARTITION BY product_id, region) as max_date,\n",
    "        created_at\n",
    "    FROM materialized_views.PRODUCT_STATISTICS\n",
    "    WHERE created_at::date >= DATE_TRUNC('month', CURRENT_DATE - 60)\n",
    "    QUALIFY max_date = created_at\n",
    ")\n",
    "'''\n",
    "\n",
    "print(\"Fetching product statistics (margin boundaries)...\")\n",
    "stats = snowflake_query(\"Egypt\", query)\n",
    "stats.columns = stats.columns.str.lower()\n",
    "\n",
    "for col in stats.columns:\n",
    "    stats[col] = pd.to_numeric(stats[col], errors='ignore')\n",
    "\n",
    "print(f\"✓ Retrieved margin statistics for {len(stats)} products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33648230",
   "metadata": {},
   "source": [
    "### 4.5 Warehouse-Region Mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb86c080-2a8c-4a63-8507-02ffcfe904f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f'''\n",
    "SELECT warehouse_id, region\n",
    "FROM (\n",
    "    SELECT *, ROW_NUMBER() OVER (PARTITION BY warehouse_id ORDER BY nmv DESC) as rnk \n",
    "    FROM (\n",
    "        SELECT \n",
    "            CASE WHEN regions.id = 2 THEN cities.name_en ELSE regions.name_en END as region,\n",
    "            pso.warehouse_id,\n",
    "            SUM(pso.total_price) as nmv\n",
    "        FROM product_sales_order pso\n",
    "        JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "        JOIN materialized_views.retailer_polygon ON materialized_views.retailer_polygon.retailer_id = so.retailer_id\n",
    "        JOIN districts ON districts.id = materialized_views.retailer_polygon.district_id\n",
    "        JOIN cities ON cities.id = districts.city_id\n",
    "        JOIN states ON states.id = cities.state_id\n",
    "        JOIN regions ON regions.id = states.region_id             \n",
    "        WHERE TRUE\n",
    "            AND so.created_at::date BETWEEN CURRENT_DATE - 31 AND CURRENT_DATE - 1\n",
    "            AND so.sales_order_status_id NOT IN (7, 12)\n",
    "            AND so.channel IN ('telesales', 'retailer')\n",
    "            AND pso.purchased_item_count <> 0\n",
    "        GROUP BY ALL\n",
    "    )\n",
    "    QUALIFY rnk = 1 \n",
    ")\n",
    "'''\n",
    "\n",
    "print(\"Fetching warehouse-region mapping...\")\n",
    "warehouse_region = snowflake_query(\"Egypt\", query)\n",
    "warehouse_region.columns = warehouse_region.columns.str.lower()\n",
    "\n",
    "for col in warehouse_region.columns:\n",
    "    warehouse_region[col] = pd.to_numeric(warehouse_region[col], errors='ignore')\n",
    "\n",
    "print(f\"✓ Mapped {len(warehouse_region)} warehouses to regions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d74ad7b",
   "metadata": {},
   "source": [
    "### 4.6 Target Margins (Brand/Category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e09273a-0aed-4e4a-8f82-739055c3047c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Brand-level target margins\n",
    "query = f'''\n",
    "SELECT DISTINCT cat, brand, margin as target_bm\n",
    "FROM performance.commercial_targets cplan\n",
    "QUALIFY \n",
    "    CASE \n",
    "        WHEN DATE_TRUNC('month', MAX(DATE) OVER()) = DATE_TRUNC('month', CURRENT_DATE) \n",
    "        THEN DATE_TRUNC('month', CURRENT_DATE)\n",
    "        ELSE DATE_TRUNC('month', CURRENT_DATE - INTERVAL '1 month') \n",
    "    END = DATE_TRUNC('month', date)\n",
    "'''\n",
    "\n",
    "print(\"Fetching brand target margins...\")\n",
    "brand_cat_target = snowflake_query(\"Egypt\", query)\n",
    "brand_cat_target['target_bm'] = pd.to_numeric(brand_cat_target['target_bm'])\n",
    "print(f\"✓ Retrieved targets for {len(brand_cat_target)} brand-category combinations\")\n",
    "\n",
    "# Category-level weighted target margins\n",
    "query = f'''\n",
    "SELECT cat, SUM(target_bm * (target_nmv / cat_total)) as cat_target_margin\n",
    "FROM (\n",
    "    SELECT *, SUM(target_nmv) OVER (PARTITION BY cat) as cat_total\n",
    "    FROM (\n",
    "        SELECT cat, brand, AVG(target_bm) as target_bm, SUM(target_nmv) as target_nmv\n",
    "        FROM (\n",
    "            SELECT DISTINCT \n",
    "                date, \n",
    "                city as region, \n",
    "                cat, \n",
    "                brand, \n",
    "                margin as target_bm, \n",
    "                nmv as target_nmv\n",
    "            FROM performance.commercial_targets cplan\n",
    "            QUALIFY \n",
    "                CASE \n",
    "                    WHEN DATE_TRUNC('month', MAX(DATE) OVER()) = DATE_TRUNC('month', CURRENT_DATE) \n",
    "                    THEN DATE_TRUNC('month', CURRENT_DATE)\n",
    "                    ELSE DATE_TRUNC('month', CURRENT_DATE - INTERVAL '1 month') \n",
    "                END = DATE_TRUNC('month', date)\n",
    "        )\n",
    "        GROUP BY ALL\n",
    "    )\n",
    ")\n",
    "GROUP BY ALL \n",
    "'''\n",
    "\n",
    "print(\"Fetching category target margins...\")\n",
    "cat_target = snowflake_query(\"Egypt\", query)\n",
    "cat_target['cat_target_margin'] = pd.to_numeric(cat_target['cat_target_margin'])\n",
    "print(f\"✓ Retrieved targets for {len(cat_target)} categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab0491b",
   "metadata": {},
   "source": [
    "### 4.7 Merge All Data Sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e97860-cb64-4212-857a-98e82a822abd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MERGE ALL DATA SOURCES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Merging all data sources...\")\n",
    "\n",
    "# Start with selected products + tier quantities\n",
    "final_data = selected_products.merge(\n",
    "    tiers_selection[[\n",
    "        'warehouse_id', 'product_id', 'packing_unit_id',\n",
    "        'tier_1_qty', 'tier_2_qty', 'median_qty',\n",
    "        'tier_1_increase_pct', 'tier_2_increase_pct'\n",
    "    ]],\n",
    "    on=['warehouse_id', 'product_id', 'packing_unit_id']\n",
    ")\n",
    "\n",
    "# Select relevant columns\n",
    "final_data = final_data[[\n",
    "    'warehouse_id', 'product_id', 'packing_unit_id', 'sku', 'brand', 'cat',\n",
    "    'packing_unit_price', 'basic_unit_count', \n",
    "    'tier_1_qty', 'tier_2_qty', 'median_qty',\n",
    "    'tier_1_increase_pct', 'tier_2_increase_pct', 'final_rank'\n",
    "]]\n",
    "\n",
    "# Add WAC (weighted average cost)\n",
    "final_data = final_data.merge(sku_info[['product_id', 'wac_p']], on='product_id')\n",
    "final_data['wac_p'] = (final_data['wac_p'] * final_data['basic_unit_count']).round(2)\n",
    "\n",
    "# Add marketplace prices\n",
    "final_data = final_data.merge(marketplace, on=['product_id', 'warehouse_id'], how='left')\n",
    "final_data = final_data.drop(columns='region')\n",
    "\n",
    "# Add competitor prices\n",
    "final_data = final_data.merge(bensoliman[['product_id', 'ben_soliman_price']], on=['product_id'], how='left')\n",
    "final_data = final_data.merge(scrapped_prices, on=['product_id', 'warehouse_id'], how='left')\n",
    "final_data = final_data.drop(columns='region')\n",
    "\n",
    "# Add region and margin data\n",
    "final_data = final_data.merge(warehouse_region, on=['warehouse_id'])\n",
    "final_data = final_data.merge(stats, on=['product_id', 'region'], how='left')\n",
    "final_data = final_data.merge(brand_cat_target, on=['brand', 'cat'], how='left')\n",
    "final_data = final_data.merge(cat_target, on=['cat'], how='left')\n",
    "\n",
    "# Use brand target margin, fall back to category target margin\n",
    "final_data['Target_margin'] = final_data['target_bm'].fillna(final_data['cat_target_margin'])\n",
    "\n",
    "print(f\"✓ Merged data: {len(final_data)} products with all pricing data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383d11b7-a8d2-4d08-9a09-db03195b9b5b",
   "metadata": {},
   "source": [
    "### LIVE CART Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec7109f-492b-414b-889d-66bb5fd78f4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT \n",
    "    cppu.cohort_id,\n",
    "    product_id,\n",
    "    packing_unit_id,\n",
    "    basic_unit_count,\n",
    "    COALESCE(cppu.MAX_PER_SALES_ORDER, cppu2.MAX_PER_SALES_ORDER) as current_cart_rule\n",
    "FROM COHORT_PRODUCT_PACKING_UNITS cppu \n",
    "JOIN PACKING_UNIT_PRODUCTS pup ON cppu.PRODUCT_PACKING_UNIT_ID = pup.id \n",
    "JOIN cohorts c ON c.id = cppu.cohort_id\n",
    "JOIN COHORT_PRODUCT_PACKING_UNITS cppu2 \n",
    "    ON cppu.PRODUCT_PACKING_UNIT_ID = cppu2.PRODUCT_PACKING_UNIT_ID \n",
    "    AND cppu2.cohort_id = c.FALLBACK_COHORT_ID \n",
    "WHERE cppu.cohort_id IN (700, 701, 702, 703, 704, 1123, 1124, 1125, 1126)\n",
    "'''\n",
    "\n",
    "print(\"Fetching live cart rules...\")\n",
    "live_cart_rules = snowflake_query(\"Egypt\", query) \n",
    "live_cart_rules.columns = live_cart_rules.columns.str.lower()\n",
    "\n",
    "for col in live_cart_rules.columns:\n",
    "    live_cart_rules[col] = pd.to_numeric(live_cart_rules[col], errors='ignore')\n",
    "\n",
    "print(f\"✓ Retrieved {len(live_cart_rules)} cart rules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045aceb9-ff2c-45bd-8aae-d931e43fc0ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cohort to Warehouse mapping\n",
    "mapping_coh_wh = pd.DataFrame({\n",
    "    'region':       ['Cairo', 'Cairo', 'Giza', 'Delta West', 'Delta West', 'Delta East', \n",
    "                     'Delta East', 'Upper Egypt', 'Upper Egypt', 'Upper Egypt', 'Upper Egypt', \n",
    "                     'Alexandria', 'Giza'],\n",
    "    'wh':           ['El-Marg', 'Mostorod', 'Barageel', 'El-Mahala', 'Tanta', 'Mansoura FC',\n",
    "                     'Sharqya', 'Assiut FC', 'Bani sweif', 'Menya Samalot', 'Sohag',\n",
    "                     'Khorshed Alex', 'Sakkarah'],\n",
    "    'warehouse_id': [38, 1, 236, 337, 8, 339, 170, 501, 401, 703, 632, 797, 962],\n",
    "    'cohort_id':    [700, 700, 701, 703, 703, 704, 704, 1124, 1126, 1123, 1125, 702, 701]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de132378-5bbb-4eff-a8d6-add1915ff206",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add warehouse mapping to cart rules\n",
    "live_cart_rules = live_cart_rules.merge(mapping_coh_wh, on='cohort_id')\n",
    "print(f\"✓ Cart rules mapped to {live_cart_rules['warehouse_id'].nunique()} warehouses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545252e6",
   "metadata": {},
   "source": [
    "## 5. Price Tier Calculation\n",
    "\n",
    "Calculate tier 1 and tier 2 prices with constraints:\n",
    "- **Max discount**: 5% from current price\n",
    "- **Min discount**: 0.35% from current price  \n",
    "- **Ratio bounds**: discount-to-quantity ratio between 1.3 and 3.5\n",
    "- **Price ordering**: WAC < Tier 2 < Tier 1 < Current Price\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c14fe97",
   "metadata": {},
   "source": [
    "### 5.1 Price Calculation Functions\n",
    "\n",
    "The `calculate_tier_prices` function uses multiple strategies:\n",
    "1. **Market prices strategy** - Use competitive pricing data if available\n",
    "2. **Margin range strategy** - Calculate from margin boundaries if no market data\n",
    "3. **Ratio adjustment** - Adjust tier_2 price to meet discount-to-quantity ratio bounds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4250db-739f-4602-9fcb-fd3638c7343c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_tier_prices(row, max_discount_pct=5.0, min_discount_pct=0.35, min_ratio=1.1, max_ratio=3.5):\n",
    "    \"\"\"\n",
    "    Calculate tier 1 and tier 2 prices for a single row.\n",
    "    \n",
    "    Parameters:\n",
    "    - max_discount_pct: Maximum allowed discount from current price (default: 5%)\n",
    "    - min_discount_pct: Minimum required discount from current price (default: 0.35%)\n",
    "    - min_ratio: Minimum discount-to-quantity ratio (default: 1.3)\n",
    "    - max_ratio: Maximum discount-to-quantity ratio (default: 3.5)\n",
    "    \n",
    "    Constraints:\n",
    "    - Tier prices must not go below price calculated with 0.3 * target_margin\n",
    "    - Ensure: WAC < Tier 2 < Tier 1 < Current Price\n",
    "    - Ensure: BOTH tiers must be valid or BOTH are None\n",
    "    - Ensure: discount_qty_ratio = (tier_2_discount/tier_1_discount) / (tier_2_qty/tier_1_qty) is between min_ratio and max_ratio\n",
    "    \"\"\"\n",
    "    \n",
    "    current_price = row['packing_unit_price']\n",
    "    wac = row['wac_p']\n",
    "    \n",
    "    # Get basic_unit_count for converting market prices\n",
    "    basic_unit_count = row.get('basic_unit_count', 1)\n",
    "    if pd.isna(basic_unit_count) or basic_unit_count <= 0:\n",
    "        basic_unit_count = 1\n",
    "    \n",
    "    # Validation\n",
    "    if pd.isna(current_price) or current_price <= 0:\n",
    "        return pd.Series({'tier_1_price': np.nan, 'tier_2_price': np.nan, 'price_source': 'invalid_current_price'})\n",
    "    \n",
    "    if pd.isna(wac) or wac <= 0:\n",
    "        return pd.Series({'tier_1_price': np.nan, 'tier_2_price': np.nan, 'price_source': 'invalid_wac'})\n",
    "    \n",
    "    if current_price <= wac:\n",
    "        return pd.Series({'tier_1_price': np.nan, 'tier_2_price': np.nan, 'price_source': 'current_price_below_wac'})\n",
    "    \n",
    "    # Calculate discount bounds\n",
    "    max_discount_price = current_price * (1 - max_discount_pct / 100)  # Minimum allowed price\n",
    "    min_discount_price = current_price * (1 - min_discount_pct / 100)  # Maximum allowed price\n",
    "    \n",
    "    # Calculate absolute minimum price based on target_margin\n",
    "    # Price must maintain at least 30% of target margin\n",
    "    absolute_min_price = wac  # Default to WAC if no target_margin\n",
    "    \n",
    "    if 'target_margin' in row.index and pd.notna(row['target_margin']) and 0 < row['target_margin'] < 1:\n",
    "        target_margin = row['target_margin']\n",
    "        # Minimum margin is 30% of target margin\n",
    "        min_margin = target_margin * 0.3\n",
    "        # Calculate minimum price: price = wac / (1 - min_margin)\n",
    "        absolute_min_price = wac / (1 - min_margin)\n",
    "    else:\n",
    "        # Fallback: use wac_cushion_pct\n",
    "        wac_cushion_pct = 0.25\n",
    "        absolute_min_price = wac / (1 - (wac_cushion_pct / 100))\n",
    "    \n",
    "    # Market price columns (these are per basic unit)\n",
    "    market_cols = [\n",
    "        'final_mod_price', 'median_scrapped', 'final_max_price', \n",
    "        'ben_soliman_price', 'max_scrapped', 'final_true_max',\n",
    "        'final_min_price', 'min_scrapped', 'final_true_min'\n",
    "    ]\n",
    "    \n",
    "    # Extract valid market prices (multiply by basic_unit_count, above absolute_min_price, within discount bounds)\n",
    "    valid_market_prices = []\n",
    "    for col in market_cols:\n",
    "        if col in row.index and pd.notna(row[col]) and row[col] > 0:\n",
    "            # Convert basic unit price to packing unit price\n",
    "            packing_price = row[col] * basic_unit_count\n",
    "            \n",
    "            # Must be: above absolute_min_price AND within discount bounds\n",
    "            if absolute_min_price < packing_price and max_discount_price <= packing_price <= min_discount_price:\n",
    "                valid_market_prices.append(packing_price)\n",
    "    \n",
    "    # Remove duplicates and sort descending\n",
    "    valid_market_prices = sorted(list(set(valid_market_prices)), reverse=True)\n",
    "    \n",
    "    tier_1 = None\n",
    "    tier_2 = None\n",
    "    source = ''\n",
    "    \n",
    "    min_gap_pct = 0.25\n",
    "    \n",
    "    # Strategy 1: Use market prices\n",
    "    if len(valid_market_prices) >= 3:\n",
    "        # Select from available prices\n",
    "        tier_1 = valid_market_prices[0]  # Highest price\n",
    "        \n",
    "        # Find tier 2 with minimum gap\n",
    "        for price in valid_market_prices[1:]:\n",
    "            if price < tier_1 * (1 - min_gap_pct / 100):\n",
    "                tier_2 = price\n",
    "                break\n",
    "        \n",
    "        # If no suitable tier 2 found, take second highest\n",
    "        if tier_2 is None and len(valid_market_prices) > 1:\n",
    "            tier_2 = valid_market_prices[1]\n",
    "        \n",
    "        if tier_1 is not None and tier_2 is not None:\n",
    "            source = 'market_prices'\n",
    "    \n",
    "    elif len(valid_market_prices) == 2:\n",
    "        tier_1 = valid_market_prices[0]\n",
    "        tier_2 = valid_market_prices[1]\n",
    "        source = 'market_prices'\n",
    "    \n",
    "    elif len(valid_market_prices) == 1:\n",
    "        # Only one market price - use margin range for the other\n",
    "        market_price = valid_market_prices[0]\n",
    "        \n",
    "        # Calculate which tier this should be based on its position\n",
    "        price_position = (market_price - max_discount_price) / (min_discount_price - max_discount_price)\n",
    "        \n",
    "        # If in upper half (>0.5), use as tier 1 and calculate tier 2\n",
    "        # If in lower half (<=0.5), use as tier 2 and calculate tier 1\n",
    "        if price_position > 0.5:\n",
    "            tier_1 = market_price\n",
    "            tier_2 = calculate_from_margin_range(row, wac, current_price, tier_1, tier=2, \n",
    "                                                 max_discount_price=max_discount_price,\n",
    "                                                 min_discount_price=min_discount_price,\n",
    "                                                 absolute_min_price=absolute_min_price)\n",
    "            if tier_2 is not None:\n",
    "                source = 'market_tier1_margin_tier2'\n",
    "        else:\n",
    "            tier_2 = market_price\n",
    "            tier_1 = calculate_from_margin_range(row, wac, current_price, tier_2, tier=1,\n",
    "                                                 max_discount_price=max_discount_price,\n",
    "                                                 min_discount_price=min_discount_price,\n",
    "                                                 absolute_min_price=absolute_min_price)\n",
    "            if tier_1 is not None:\n",
    "                source = 'margin_tier1_market_tier2'\n",
    "    \n",
    "    # Strategy 2: No market prices - use margin range method\n",
    "    if tier_1 is None or tier_2 is None:\n",
    "        tier_1, tier_2 = calculate_both_from_margin_range(row, wac, current_price,\n",
    "                                                          max_discount_price=max_discount_price,\n",
    "                                                          min_discount_price=min_discount_price,\n",
    "                                                          absolute_min_price=absolute_min_price)\n",
    "        if tier_1 is not None and tier_2 is not None:\n",
    "            source = 'margin_range_based'\n",
    "    \n",
    "    # CRITICAL: Final validation - BOTH must be valid or BOTH are None\n",
    "    if tier_1 is not None and tier_2 is not None:\n",
    "        # Ensure correct ordering\n",
    "        if tier_2 >= tier_1:\n",
    "            tier_1, tier_2 = max(tier_1, tier_2), min(tier_1, tier_2)\n",
    "        \n",
    "        # Apply discount bounds\n",
    "        tier_1 = max(tier_1, max_discount_price)\n",
    "        tier_1 = min(tier_1, min_discount_price)\n",
    "        tier_2 = max(tier_2, max_discount_price)\n",
    "        tier_2 = min(tier_2, min_discount_price)\n",
    "        \n",
    "        # Check if both above absolute minimum price\n",
    "        if tier_1 <= absolute_min_price or tier_2 <= absolute_min_price:\n",
    "            tier_1 = None\n",
    "            tier_2 = None\n",
    "            source = 'prices_below_minimum_margin'\n",
    "        else:\n",
    "            # Ensure minimum gap between tiers\n",
    "            if tier_2 > tier_1 * (1 - min_gap_pct / 100):\n",
    "                tier_2 = tier_1 * (1 - min_gap_pct / 100)\n",
    "                if tier_2 <= absolute_min_price:\n",
    "                    tier_1 = None\n",
    "                    tier_2 = None\n",
    "                    source = 'insufficient_gap_between_tiers'\n",
    "            \n",
    "            # Final check: both still valid?\n",
    "            if tier_1 is not None and tier_2 is not None:\n",
    "                if not (wac < tier_2 < tier_1 < current_price):\n",
    "                    tier_1 = None\n",
    "                    tier_2 = None\n",
    "                    source = 'invalid_tier_ordering'\n",
    "                elif not (max_discount_price <= tier_2 and tier_1 <= min_discount_price):\n",
    "                    tier_1 = None\n",
    "                    tier_2 = None\n",
    "                    source = 'tiers_outside_discount_bounds'\n",
    "                else:\n",
    "                    tier_1 = round(tier_1, 2)\n",
    "                    tier_2 = round(tier_2, 2)\n",
    "                    \n",
    "                    # Validate and adjust discount-to-quantity ratio\n",
    "                    tier_1_qty = row.get('tier_1_qty', None)\n",
    "                    tier_2_qty = row.get('tier_2_qty', None)\n",
    "                    \n",
    "                    if tier_1_qty is not None and tier_2_qty is not None and tier_1_qty > 0:\n",
    "                        tier_1_discount = current_price - tier_1\n",
    "                        tier_2_discount = current_price - tier_2\n",
    "                        \n",
    "                        if tier_1_discount > 0:\n",
    "                            diff_quantity = tier_2_qty / tier_1_qty\n",
    "                            diff_discount = tier_2_discount / tier_1_discount\n",
    "                            \n",
    "                            if diff_quantity > 0:\n",
    "                                discount_qty_ratio = diff_discount / diff_quantity\n",
    "                                \n",
    "                                # Adjust tier_2_price if ratio is outside bounds\n",
    "                                if discount_qty_ratio < min_ratio:\n",
    "                                    # Ratio too low - need more discount at tier 2\n",
    "                                    # tier_2 = current_price - (target_ratio * diff_quantity * tier_1_discount)\n",
    "                                    target_tier_2_discount = min_ratio * diff_quantity * tier_1_discount\n",
    "                                    adjusted_tier_2 = current_price - target_tier_2_discount\n",
    "                                    \n",
    "                                    # Ensure adjusted price is still valid (above WAC and absolute_min_price)\n",
    "                                    if adjusted_tier_2 > wac and adjusted_tier_2 > absolute_min_price and adjusted_tier_2 < tier_1:\n",
    "                                        tier_2 = round(adjusted_tier_2, 2)\n",
    "                                        source = source + '_ratio_adjusted_up'\n",
    "                                    else:\n",
    "                                        tier_1 = None\n",
    "                                        tier_2 = None\n",
    "                                        source = f'cannot_adjust_ratio_{discount_qty_ratio:.2f}_min_bound'\n",
    "                                \n",
    "                                elif discount_qty_ratio > max_ratio:\n",
    "                                    # Ratio too high - need less discount at tier 2\n",
    "                                    # tier_2 = current_price - (target_ratio * diff_quantity * tier_1_discount)\n",
    "                                    target_tier_2_discount = max_ratio * diff_quantity * tier_1_discount\n",
    "                                    adjusted_tier_2 = current_price - target_tier_2_discount\n",
    "                                    \n",
    "                                    # Ensure adjusted price is still valid (below tier_1 and above WAC)\n",
    "                                    if adjusted_tier_2 > wac and adjusted_tier_2 > absolute_min_price and adjusted_tier_2 < tier_1:\n",
    "                                        tier_2 = round(adjusted_tier_2, 2)\n",
    "                                        source = source + '_ratio_adjusted_down'\n",
    "                                    else:\n",
    "                                        tier_1 = None\n",
    "                                        tier_2 = None\n",
    "                                        source = f'cannot_adjust_ratio_{discount_qty_ratio:.2f}_max_bound'\n",
    "    \n",
    "    # FINAL CHECK: If only one tier exists, invalidate both\n",
    "    if (tier_1 is None and tier_2 is not None) or (tier_1 is not None and tier_2 is None):\n",
    "        tier_1 = None\n",
    "        tier_2 = None\n",
    "        source = 'incomplete_tier_pair'\n",
    "    \n",
    "    # If both are None and no source set, mark it\n",
    "    if tier_1 is None and tier_2 is None and source == '':\n",
    "        source = 'no_valid_prices'\n",
    "    \n",
    "    return pd.Series({\n",
    "        'tier_1_price': tier_1,\n",
    "        'tier_2_price': tier_2,\n",
    "        'price_source': source\n",
    "    })\n",
    "\n",
    "\n",
    "def calculate_both_from_margin_range(row, wac, current_price, max_discount_price, min_discount_price, absolute_min_price):\n",
    "    \"\"\"\n",
    "    Calculate both tier prices using margin range from minimum of (min_boundary, optimal_bm) to current margin.\n",
    "    Returns (tier_1_price, tier_2_price) or (None, None)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate current margin: margin = (price - wac) / price\n",
    "    current_margin = (current_price - wac) / current_price\n",
    "    \n",
    "    # Get min_boundary margin\n",
    "    min_boundary_margin = None\n",
    "    if 'min_boundary' in row.index and pd.notna(row['min_boundary']) and 0 < row['min_boundary'] < 1:\n",
    "        min_boundary_margin = row['min_boundary']\n",
    "    \n",
    "    # Get optimal_bm margin\n",
    "    optimal_margin = None\n",
    "    if 'optimal_bm' in row.index and pd.notna(row['optimal_bm']) and 0 < row['optimal_bm'] < 1:\n",
    "        optimal_margin = row['optimal_bm']\n",
    "    \n",
    "    # Determine starting margin: minimum of (min_boundary, optimal_bm)\n",
    "    start_margin = None\n",
    "    \n",
    "    if min_boundary_margin is not None and optimal_margin is not None:\n",
    "        start_margin = min(min_boundary_margin, optimal_margin)\n",
    "    elif min_boundary_margin is not None:\n",
    "        start_margin = min_boundary_margin\n",
    "    elif optimal_margin is not None:\n",
    "        start_margin = optimal_margin\n",
    "    else:\n",
    "        # Fallback: use 50% of current margin\n",
    "        start_margin = current_margin * 0.85\n",
    "    \n",
    "    # Ensure start_margin is less than current margin\n",
    "    if start_margin >= current_margin:\n",
    "        start_margin = current_margin * 0.85\n",
    "    \n",
    "    # Generate margin points in the range (10 points)\n",
    "    num_points = 10\n",
    "    margin_range = np.linspace(start_margin, current_margin, num_points)\n",
    "    \n",
    "    # Calculate prices from these margins: price = wac / (1 - margin)\n",
    "    price_candidates = []\n",
    "    for margin in margin_range:\n",
    "        if 0 < margin < 1:\n",
    "            price = wac / (1 - margin)\n",
    "            # Only keep prices within discount bounds and above absolute_min_price\n",
    "            if absolute_min_price < price and max_discount_price <= price <= min_discount_price:\n",
    "                price_candidates.append(price)\n",
    "    \n",
    "    if len(price_candidates) < 2:\n",
    "        return None, None\n",
    "    \n",
    "    # Sort prices descending\n",
    "    price_candidates = sorted(price_candidates, reverse=True)\n",
    "    \n",
    "    # Select Tier 1: closer to the top (less discount)\n",
    "    # Select Tier 2: further down (more discount)\n",
    "    tier_1_idx = int(len(price_candidates) * 0.25)  # 25% from top\n",
    "    tier_2_idx = int(len(price_candidates) * 0.65)  # 65% from top\n",
    "    \n",
    "    # Ensure valid indices\n",
    "    tier_1_idx = max(0, min(tier_1_idx, len(price_candidates) - 2))\n",
    "    tier_2_idx = max(tier_1_idx + 1, min(tier_2_idx, len(price_candidates) - 1))\n",
    "    \n",
    "    tier_1 = price_candidates[tier_1_idx]\n",
    "    tier_2 = price_candidates[tier_2_idx]\n",
    "    \n",
    "    # Ensure meaningful gap (at least 0.5%)\n",
    "    min_gap_pct = 0.25\n",
    "    if tier_2 > tier_1 * (1 - min_gap_pct / 100):\n",
    "        # Try to find better tier_2\n",
    "        for i in range(tier_2_idx + 1, len(price_candidates)):\n",
    "            if price_candidates[i] < tier_1 * (1 - min_gap_pct / 100):\n",
    "                tier_2 = price_candidates[i]\n",
    "                break\n",
    "    \n",
    "    # Final validation\n",
    "    if tier_2 >= tier_1 or tier_1 <= absolute_min_price or tier_2 <= absolute_min_price:\n",
    "        return None, None\n",
    "    \n",
    "    return tier_1, tier_2\n",
    "\n",
    "\n",
    "def calculate_from_margin_range(row, wac, current_price, other_tier_price, tier, \n",
    "                                max_discount_price, min_discount_price, absolute_min_price):\n",
    "    \"\"\"\n",
    "    Calculate single tier price using margin range.\n",
    "    Used when one tier is from market and we need to calculate the other.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate current margin\n",
    "    current_margin = (current_price - wac) / current_price\n",
    "    \n",
    "    # Get min_boundary margin\n",
    "    min_boundary_margin = None\n",
    "    if 'min_boundary' in row.index and pd.notna(row['min_boundary']) and 0 < row['min_boundary'] < 1:\n",
    "        min_boundary_margin = row['min_boundary']\n",
    "    \n",
    "    # Get optimal_bm margin\n",
    "    optimal_margin = None\n",
    "    if 'optimal_bm' in row.index and pd.notna(row['optimal_bm']) and 0 < row['optimal_bm'] < 1:\n",
    "        optimal_margin = row['optimal_bm']\n",
    "    \n",
    "    # Determine starting margin: minimum of (min_boundary, optimal_bm)\n",
    "    start_margin = None\n",
    "    \n",
    "    if min_boundary_margin is not None and optimal_margin is not None:\n",
    "        start_margin = min(min_boundary_margin, optimal_margin)\n",
    "    elif min_boundary_margin is not None:\n",
    "        start_margin = min_boundary_margin\n",
    "    elif optimal_margin is not None:\n",
    "        start_margin = optimal_margin\n",
    "    else:\n",
    "        start_margin = current_margin * 0.5\n",
    "    \n",
    "    # Ensure start_margin is less than current margin\n",
    "    if start_margin >= current_margin:\n",
    "        start_margin = current_margin * 0.7\n",
    "    \n",
    "    # Generate margin range (10 points)\n",
    "    num_points = 10\n",
    "    margin_range = np.linspace(start_margin, current_margin, num_points)\n",
    "    \n",
    "    # Calculate prices\n",
    "    price_candidates = []\n",
    "    for margin in margin_range:\n",
    "        if 0 < margin < 1:\n",
    "            price = wac / (1 - margin)\n",
    "            if absolute_min_price < price and max_discount_price <= price <= min_discount_price:\n",
    "                price_candidates.append(price)\n",
    "    \n",
    "    if len(price_candidates) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Sort prices descending\n",
    "    price_candidates = sorted(price_candidates, reverse=True)\n",
    "    \n",
    "    min_gap_pct = 0.5\n",
    "    \n",
    "    if tier == 1:\n",
    "        # Need tier 1 (higher price), we have tier 2 (lower price)\n",
    "        # Find prices above tier 2 with proper gap\n",
    "        target_candidates = [p for p in price_candidates \n",
    "                           if p > other_tier_price * (1 + min_gap_pct / 100)]\n",
    "        if target_candidates:\n",
    "            # Take from upper portion (25% position)\n",
    "            idx = int(len(target_candidates) * 0.25)\n",
    "            return target_candidates[idx]\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        # Need tier 2 (lower price), we have tier 1 (higher price)\n",
    "        # Find prices below tier 1 with proper gap\n",
    "        target_candidates = [p for p in price_candidates \n",
    "                           if p < other_tier_price * (1 - min_gap_pct / 100)]\n",
    "        if target_candidates:\n",
    "            # Take from lower portion (65% position)\n",
    "            idx = int(len(target_candidates) * 0.65)\n",
    "            idx = min(idx, len(target_candidates) - 1)\n",
    "            return target_candidates[idx]\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4895a40",
   "metadata": {},
   "source": [
    "### 5.2 Apply Price Calculations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d485d098-925d-4be2-8e44-18b33438c529",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# APPLY PRICE CALCULATIONS\n",
    "# =============================================================================\n",
    "\n",
    "# Normalize column names\n",
    "final_data.columns = final_data.columns.str.lower()\n",
    "\n",
    "print(f\"Processing {len(final_data)} SKUs...\")\n",
    "print(f\"Parameters: MAX_DISCOUNT={MAX_DISCOUNT_PCT}%, MIN_DISCOUNT={MIN_DISCOUNT_PCT}%, RATIO=[{MIN_RATIO}, {MAX_RATIO}]\")\n",
    "\n",
    "# Apply price calculation to each row\n",
    "result = final_data.apply(\n",
    "    lambda row: calculate_tier_prices(\n",
    "        row, \n",
    "        max_discount_pct=MAX_DISCOUNT_PCT,\n",
    "        min_discount_pct=MIN_DISCOUNT_PCT,\n",
    "        min_ratio=MIN_RATIO,\n",
    "        max_ratio=MAX_RATIO\n",
    "    ), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Merge results back to dataframe\n",
    "final_data = pd.concat([final_data, result], axis=1)\n",
    "\n",
    "# Summary of ratio adjustments\n",
    "ratio_adjusted_up = final_data['price_source'].str.contains('ratio_adjusted_up', na=False).sum()\n",
    "ratio_adjusted_down = final_data['price_source'].str.contains('ratio_adjusted_down', na=False).sum()\n",
    "cannot_adjust = final_data['price_source'].str.contains('cannot_adjust_ratio', na=False).sum()\n",
    "\n",
    "print(f\"\\n--- Ratio Adjustment Summary ---\")\n",
    "print(f\"  Adjusted up (was below {MIN_RATIO}):      {ratio_adjusted_up} SKUs\")\n",
    "print(f\"  Adjusted down (was above {MAX_RATIO}):    {ratio_adjusted_down} SKUs\")\n",
    "print(f\"  Could not adjust (constraints violated): {cannot_adjust} SKUs\")\n",
    "\n",
    "# Filter to only products with valid tier prices\n",
    "final_data = final_data[\n",
    "    (~final_data['tier_1_price'].isna()) & \n",
    "    (~final_data['tier_2_price'].isna())\n",
    "]\n",
    "\n",
    "print(f\"\\n✓ Final SKUs with valid tier prices: {len(final_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b0ceb8",
   "metadata": {},
   "source": [
    "## 6. Wholesale Pricing\n",
    "\n",
    "Calculate wholesale prices based on:\n",
    "- Vehicle capacity (quarter truck)\n",
    "- Rank-based margin tiers (20%, 25%, 40%, 60% of target margin)\n",
    "- Must be below tier_2_price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15518b2-699f-4c12-b551-f125b8abd4c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PREPARE DELIVERY FEE DATA\n",
    "# =============================================================================\n",
    "\n",
    "# Set delivery fees based on region\n",
    "final_data['delivery_fees'] = DELIVERY_FEE_OTHER\n",
    "final_data.loc[final_data['region'].isin(['Cairo', 'Giza']), 'delivery_fees'] = DELIVERY_FEE_CAIRO_GIZA\n",
    "\n",
    "# Prepare query data for wholesale calculation\n",
    "query_data = final_data[['warehouse_id', 'product_id', 'packing_unit_id', 'delivery_fees']].values.tolist()\n",
    "query_info = ','.join([\n",
    "    f\"({int(wh_id)}, {int(prod_id)}, {int(pu_id)}, {int(delivery_fees)})\" \n",
    "    for wh_id, prod_id, pu_id, delivery_fees in query_data\n",
    "])\n",
    "\n",
    "print(f\"✓ Prepared {len(query_data)} products for wholesale calculation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ec0273-9d56-4dc6-b4bc-2ba8093792b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f'''\n",
    "with chosen_products as (\n",
    "select *\n",
    "from (\n",
    "values \n",
    "{query_info}\n",
    ")x(warehouse_id,product_id,packing_unit_id,delivery_fees)\n",
    "\n",
    "),\n",
    "vec as (\n",
    "select  vt.id as vehicle_id,name_en as vehicle_name,vc.weight as vehicle_weight,vc.cbm as vehicle_cbm,900 as vehicle_cost\n",
    "from VEHICLE_TYPES  vt \n",
    "join  RETOOL.VEHICLE_CAPACITIES vc on vc.vehicle_id = vt.id\n",
    "where vehicle_id = 1\n",
    "),\n",
    "selected_products as (\n",
    "select x.*,\t(long*width*height)/1000000 AS cbm,weight/1000 AS weight,\n",
    "from chosen_products x\n",
    "join packing_unit_products on x.product_id = packing_unit_products.product_id and packing_unit_products.packing_unit_id = x.packing_unit_id\n",
    "),\n",
    "main_cte as (\n",
    "select warehouse_id,product_id,packing_unit_id,delivery_fees,\n",
    "ceil(least(quart_dababa_wht,quart_dababa_cbm)) as quart_dababa,\n",
    "vehicle_cost\n",
    "from (\n",
    "select * ,\n",
    "((vehicle_weight*0.9)/4)/weight as quart_dababa_wht , \n",
    "((vehicle_cbm*0.9)/4)/cbm as quart_dababa_cbm  \n",
    "from (\n",
    "select selected_products.*, vehicle_weight,vehicle_cbm,vehicle_cost\n",
    "from selected_products,vec\n",
    ")\n",
    ")\n",
    ")\n",
    "select mc.*, f.wac_p , \n",
    "(f.wac_p*quart_dababa)+(((vehicle_cost-(delivery_fees*4))*0.9)/4) as quart_cost,\n",
    "quart_cost/quart_dababa as unit_cost\n",
    "\n",
    "\n",
    "from main_cte mc \n",
    "join finance.all_cogs f on f.product_id = mc.product_id and CURRENT_TIMEstamp between from_date and to_date \n",
    "\n",
    "'''\n",
    "\n",
    "print(\"Fetching wholesale cost data (quarter truck calculations)...\")\n",
    "ws_data = snowflake_query(\"Egypt\", query)\n",
    "ws_data.columns = ws_data.columns.str.lower()\n",
    "\n",
    "for col in ws_data.columns:\n",
    "    ws_data[col] = pd.to_numeric(ws_data[col], errors='ignore')\n",
    "\n",
    "# Select and rename columns\n",
    "ws_data = ws_data[['warehouse_id', 'product_id', 'packing_unit_id', 'quart_dababa', 'unit_cost']]\n",
    "ws_data.columns = ['warehouse_id', 'product_id', 'packing_unit_id', 'WS_tier', 'WS_wac']\n",
    "\n",
    "print(f\"✓ Calculated wholesale data for {len(ws_data)} products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854c4e65-6403-480f-9380-519e106a5ae4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD FORCED BRANDS/CATEGORIES FROM GOOGLE SHEETS\n",
    "# =============================================================================\n",
    "\n",
    "scope = [\n",
    "    \"https://spreadsheets.google.com/feeds\",\n",
    "    \"https://www.googleapis.com/auth/spreadsheets\",\n",
    "    \"https://www.googleapis.com/auth/drive.file\",\n",
    "    \"https://www.googleapis.com/auth/drive\"\n",
    "]\n",
    "\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_dict(\n",
    "    json.loads(setup_environment_2.get_secret(\"prod/maxab-sheets\")), \n",
    "    scope\n",
    ")\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "# Load forced brands and categories\n",
    "force_brands = client.open('Wholesales_exec').worksheet('brands')\n",
    "force_cats = client.open('Wholesales_exec').worksheet('cats')\n",
    "force_brands_df = pd.DataFrame(force_brands.get_all_records())\n",
    "force_cats_df = pd.DataFrame(force_cats.get_all_records())\n",
    "\n",
    "# Extract unique lists\n",
    "forced_brand_list = force_brands_df.brand.unique() if not force_brands_df.empty else []\n",
    "forced_cat_list = force_cats_df.cat.unique() if not force_cats_df.empty else []\n",
    "\n",
    "print(f\"✓ Loaded {len(forced_brand_list)} forced brands, {len(forced_cat_list)} forced categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4830c5dc-d49e-47b4-93f0-2645896e8e7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT product_id, new_pp, forecasted_date\n",
    "FROM materialized_views.DBDP_PRICE_UPS\n",
    "WHERE region = 'Cairo'\n",
    "'''\n",
    "\n",
    "print(\"Fetching price-up forecasts...\")\n",
    "price_ups = snowflake_query(\"Egypt\", query)\n",
    "price_ups.columns = price_ups.columns.str.lower()\n",
    "\n",
    "for col in price_ups.columns:\n",
    "    price_ups[col] = pd.to_numeric(price_ups[col], errors='ignore')\n",
    "\n",
    "print(f\"✓ Retrieved {len(price_ups)} price-up forecasts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82f21df-6b2c-4910-82f9-c7b758a3ac22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge wholesale data and price-ups with final data\n",
    "final_data = final_data.merge(ws_data, on=['warehouse_id', 'product_id', 'packing_unit_id'], how='left')\n",
    "final_data['WS_wac'] = final_data['WS_wac'] * final_data['basic_unit_count']\n",
    "final_data = final_data.merge(price_ups, on='product_id', how='left')\n",
    "\n",
    "print(f\"✓ Added wholesale and price-up data to {len(final_data)} products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb77deb3-b68a-4c4a-acc8-012272cf7e7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wholesales_margin(x):\n",
    "    \"\"\"\n",
    "    Calculate wholesale price based on margins and product tiers.\n",
    "    \"\"\"\n",
    "    # Extract key variables\n",
    "    wac = x['WS_wac']\n",
    "    target_margin = x['target_margin']\n",
    "    tier_2_price = x['tier_2_price']\n",
    "    final_rank = x['final_rank']\n",
    "    new_pp = x['new_pp']\n",
    "    brand = x['brand']\n",
    "    category = x['cat']\n",
    "    margin = ((x['packing_unit_price'] - x['wac_p']) / x['packing_unit_price'])\n",
    "    \n",
    "    # Update target margin if new_pp exists\n",
    "    if not pd.isna(new_pp):\n",
    "        target_margin =  margin* 0.9\n",
    "    \n",
    "    # Define constants\n",
    "    MIN_MARGIN = 0.01\n",
    "    TOTAL_RANKS = 133\n",
    "    \n",
    "    # Special brand handling\n",
    "    if brand in forced_brand_list:\n",
    "        return _calculate_forced_brand_price(x, wac, target_margin)\n",
    "    \n",
    "    # Fiori brand special case\n",
    "    if brand == 'فيوري':\n",
    "        return wac / (1 - (margin * 0.9))\n",
    "    \n",
    "    # Paper products special case\n",
    "    if category == 'ورقيات':\n",
    "        margin = np.minimum(np.maximum(0.6 * target_margin, 0.015), target_margin)\n",
    "        return wac / (1 - margin)\n",
    "    \n",
    "    # Standard tier-based pricing\n",
    "    tier = _determine_tier(final_rank, TOTAL_RANKS)\n",
    "    price = _calculate_tier_price(wac, target_margin, tier)\n",
    "    \n",
    "    # Adjust if price exceeds tier 2 price\n",
    "    if price >= tier_2_price:\n",
    "        price = (wac + tier_2_price) / 2\n",
    "    \n",
    "    # Ensure minimum margin\n",
    "    return np.maximum(price, wac / (1 - MIN_MARGIN))\n",
    "\n",
    "\n",
    "def _calculate_forced_brand_price(x, wac, target_margin):\n",
    "    \"\"\"Calculate price for forced brands with special margin rules.\"\"\"\n",
    "    brand = x['brand']\n",
    "    margin = ((x['packing_unit_price'] - x['wac_p']) / x['packing_unit_price'])\n",
    "    min_target = 0.25 * target_margin\n",
    "    \n",
    "    if brand in ['كوكا كولا', 'شويبس']:\n",
    "        return np.maximum(wac / (1 - (margin * 0.65)), min_target)\n",
    "    elif brand == 'جود كير':\n",
    "        return np.maximum(wac / (1 - (margin * 0.5)), min_target)\n",
    "    else:\n",
    "        return wac / (1 - (margin * 0.8))\n",
    "\n",
    "\n",
    "def _determine_tier(rank, total_ranks):\n",
    "    \"\"\"Determine product tier based on ranking.\"\"\"\n",
    "    if rank <= 0.25 * total_ranks:\n",
    "        return 1\n",
    "    elif rank <= 0.5 * total_ranks:\n",
    "        return 2\n",
    "    elif rank <= 0.75 * total_ranks:\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "\n",
    "def _calculate_tier_price(wac, target_margin, tier):\n",
    "    \"\"\"Calculate price based on tier with appropriate margin adjustments.\"\"\"\n",
    "    tier_config = {\n",
    "        1: {'multiplier': 0.2, 'min_margin': 0.01},\n",
    "        2: {'multiplier': 0.25, 'min_margin': 0.015},\n",
    "        3: {'multiplier': 0.4, 'min_margin': 0.015},\n",
    "        4: {'multiplier': 0.6, 'min_margin': 0.015}\n",
    "    }\n",
    "    \n",
    "    config = tier_config[tier]\n",
    "    adjusted_margin = config['multiplier'] * target_margin\n",
    "    margin = np.minimum(np.maximum(adjusted_margin, config['min_margin']), target_margin)\n",
    "    \n",
    "    return wac / (1 - margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1ed61e-384a-48f0-9f2d-e70593bde88a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate wholesale prices\n",
    "print(\"Calculating wholesale prices...\")\n",
    "final_data['WS_price'] = final_data.apply(wholesales_margin, axis=1)\n",
    "\n",
    "# Validate: WS price must be below tier 2 price\n",
    "final_data['valid'] = final_data['WS_price'] < final_data['tier_2_price']\n",
    "final_data.loc[final_data['valid'] == False, 'WS_price'] = np.nan\n",
    "\n",
    "valid_ws = final_data['WS_price'].notna().sum()\n",
    "print(f\"✓ Valid wholesale prices: {valid_ws} / {len(final_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8658cd",
   "metadata": {},
   "source": [
    "### 6.2 Wholesale NEW Logic (Delivery Savings Based)\n",
    "\n",
    "New wholesale pricing based on delivery cost savings:\n",
    "- **Car cost**: 900 EGP per delivery\n",
    "- **Car capacity**: 1.8 tons max\n",
    "- **Max ticket size**: 50,000 EGP\n",
    "- **Logic**: If retailer orders multiples of average ticket size, they save deliveries\n",
    "  - 2x avg TS = 1 delivery saved → discount = delivery cost savings\n",
    "  - 3x avg TS = 2 deliveries saved → more discount\n",
    "- **Goal**: Find optimal quantity that gives retailer max savings while price stays above WAC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a2d8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# WHOLESALE NEW LOGIC - Configuration\n",
    "# =============================================================================\n",
    "WS_CAR_COST = 1100           # Cost per delivery (EGP)\n",
    "WS_CAR_CAPACITY_TONS = 1.8  # Max car capacity in tons\n",
    "WS_MAX_TICKET_SIZE = 40000  # Maximum ticket size (EGP)\n",
    "WS_MIN_MARGIN = 0.01        # Minimum margin (1%) above WAC\n",
    "\n",
    "# Query to get average ticket size per warehouse\n",
    "query = f'''\n",
    "WITH base AS (\n",
    "    SELECT *, ROW_NUMBER() OVER (PARTITION BY retailer_id ORDER BY priority) as rnk \n",
    "    FROM (\n",
    "        SELECT x.*, TAGGABLE_ID as retailer_id \n",
    "        FROM (\n",
    "            SELECT id as cohort_id, name as cohort_name, priority, dynamic_tag_id \n",
    "            FROM cohorts \n",
    "            WHERE is_active = 'true'\n",
    "                AND id IN (700,701,702,703,704,1123,1124,1125,1126)\n",
    "        ) x \n",
    "        JOIN DYNAMIC_TAGgables dt ON x.dynamic_tag_id = dt.dynamic_tag_id\n",
    "        WHERE dt.taggable_id IN (\n",
    "            SELECT taggable_id FROM DYNAMIC_TAGgables \n",
    "            WHERE dynamic_tag_id IN (2807, 2808, 2809, 2810, 2811, 2812)\n",
    "        )\n",
    "    )\n",
    "    QUALIFY rnk = 1 \n",
    "),\n",
    "\n",
    "-- Map regions to warehouses\n",
    "whs AS (\n",
    "    SELECT * FROM (VALUES\n",
    "        ('Cairo', 'El-Marg', 38),\n",
    "        ('Cairo', 'Mostorod', 1),\n",
    "        ('Giza', 'Barageel', 236),\n",
    "        ('Giza', 'Sakkarah', 962),\n",
    "        ('Delta West', 'El-Mahala', 337),\n",
    "        ('Delta West', 'Tanta', 8),\n",
    "        ('Delta East', 'Mansoura FC', 339),\n",
    "        ('Delta East', 'Sharqya', 170),\n",
    "        ('Upper Egypt', 'Assiut FC', 501),\n",
    "        ('Upper Egypt', 'Bani sweif', 401),\n",
    "        ('Upper Egypt', 'Menya Samalot', 703),\n",
    "        ('Upper Egypt', 'Sohag', 632),\n",
    "        ('Alexandria', 'Khorshed Alex', 797)\n",
    "    ) x(region_name, wh, warehouse_id)\n",
    "),\n",
    "\n",
    "-- Get ticket sizes (order values) for last 4 months\n",
    "ticket_sizes AS (\n",
    "    SELECT \n",
    "        whs.warehouse_id,\n",
    "        whs.wh as warehouse_name,\n",
    "        so.parent_sales_order_id,\n",
    "        so.retailer_id,\n",
    "        SUM(pso.total_price) as ticket_size,\n",
    "        SUM(pso.purchased_item_count * pup.weight / 1000) as order_weight_kg\n",
    "    FROM product_sales_order pso\n",
    "    JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "    JOIN base ON base.retailer_id = so.retailer_id\n",
    "    JOIN packing_unit_products pup ON pup.product_id = pso.product_id \n",
    "        AND pup.packing_unit_id = pso.packing_unit_id\n",
    "    JOIN materialized_views.retailer_polygon rp ON rp.retailer_id = so.retailer_id\n",
    "    JOIN districts ON districts.id = rp.district_id\n",
    "    JOIN cities ON cities.id = districts.city_id\n",
    "    JOIN states ON states.id = cities.state_id\n",
    "    JOIN regions ON regions.id = states.region_id\n",
    "    JOIN whs ON whs.region_name = CASE WHEN regions.id = 2 THEN states.name_en ELSE regions.name_en END\n",
    "    WHERE so.created_at::date BETWEEN DATE_TRUNC('month', CURRENT_DATE - INTERVAL '4 months') AND CURRENT_DATE - 1\n",
    "        AND so.sales_order_status_id NOT IN (7, 12)\n",
    "        AND so.channel IN ('telesales', 'retailer')\n",
    "        AND pso.purchased_item_count > 0\n",
    "    GROUP BY whs.warehouse_id, whs.wh, so.parent_sales_order_id, so.retailer_id\n",
    "),\n",
    "\n",
    "-- Calculate warehouse-level statistics\n",
    "warehouse_stats AS (\n",
    "    SELECT \n",
    "        warehouse_id,\n",
    "        warehouse_name,\n",
    "        COUNT(DISTINCT parent_sales_order_id) as total_orders,\n",
    "        COUNT(DISTINCT retailer_id) as total_retailers,\n",
    "        AVG(ticket_size) as avg_ticket_size,\n",
    "        MEDIAN(ticket_size) as median_ticket_size,\n",
    "        PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY ticket_size) as p75_ticket_size,\n",
    "        PERCENTILE_CONT(0.90) WITHIN GROUP (ORDER BY ticket_size) as p90_ticket_size,\n",
    "        MAX(ticket_size) as max_ticket_size,\n",
    "        AVG(order_weight_kg) as avg_order_weight_kg,\n",
    "        MEDIAN(order_weight_kg) as median_order_weight_kg\n",
    "    FROM ticket_sizes\n",
    "    WHERE ticket_size > 0\n",
    "    GROUP BY warehouse_id, warehouse_name\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    warehouse_id,\n",
    "    warehouse_name,\n",
    "    total_orders,\n",
    "    total_retailers,\n",
    "    ROUND(avg_ticket_size, 2) as avg_ticket_size,\n",
    "    ROUND(median_ticket_size, 2) as median_ticket_size,\n",
    "    ROUND(p75_ticket_size, 2) as p75_ticket_size,\n",
    "    ROUND(p90_ticket_size, 2) as p90_ticket_size,\n",
    "    ROUND(max_ticket_size, 2) as max_ticket_size,\n",
    "    ROUND(avg_order_weight_kg, 2) as avg_order_weight_kg,\n",
    "    ROUND(median_order_weight_kg, 2) as median_order_weight_kg,\n",
    "    -- Calculate how many orders fit in one car based on weight\n",
    "    ROUND({WS_CAR_CAPACITY_TONS * 1000} / NULLIF(avg_order_weight_kg, 0), 1) as orders_per_car_by_weight\n",
    "FROM warehouse_stats\n",
    "ORDER BY warehouse_id\n",
    "'''\n",
    "\n",
    "ws_ticket_data = snowflake_query(\"Egypt\", query)\n",
    "ws_ticket_data.columns = ws_ticket_data.columns.str.lower()\n",
    "for col in ws_ticket_data.columns:\n",
    "    ws_ticket_data[col] = pd.to_numeric(ws_ticket_data[col], errors='ignore')\n",
    "\n",
    "print(\"=== WAREHOUSE TICKET SIZE STATISTICS ===\")\n",
    "print(ws_ticket_data[['warehouse_name', 'avg_ticket_size', 'median_ticket_size', 'avg_order_weight_kg']].to_string(index=False))\n",
    "print(f\"\\nOverall average ticket size: {ws_ticket_data['avg_ticket_size'].mean():.2f} EGP\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784f273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge ticket size data with final_data (including orders_per_car_by_weight)\n",
    "final_data = final_data.merge(\n",
    "    ws_ticket_data[['warehouse_id', 'avg_ticket_size', 'median_ticket_size', 'avg_order_weight_kg', 'orders_per_car_by_weight']], \n",
    "    on='warehouse_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "def calculate_ws_new_logic(row):\n",
    "    \"\"\"\n",
    "    Calculate wholesale pricing based on delivery savings.\n",
    "    \n",
    "    Logic:\n",
    "    - Car cost = 900 EGP, but car serves multiple orders per trip\n",
    "    - Car cost per order = 900 / orders_per_car\n",
    "    - If retailer consolidates, they save N orders worth of car cost\n",
    "    - Savings = deliveries_saved * (car_cost / orders_per_car)\n",
    "    - Calculate scenarios from 2x to max_multiplier (capped by max TS)\n",
    "    \n",
    "    Returns: Dict with optimal scenario\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get values\n",
    "    current_price = row['packing_unit_price']\n",
    "    wac = row['wac_p']\n",
    "    avg_ts = row.get('avg_ticket_size', 4000)  # Default 4000 if missing\n",
    "    tier_2_price = row['tier_2_price']\n",
    "    \n",
    "    # Get orders per car (how many orders fit in one car trip based on weight)\n",
    "    orders_per_car = row.get('orders_per_car_by_weight', 10)  # Default 10 if missing\n",
    "    if pd.isna(orders_per_car) or orders_per_car <= 0:\n",
    "        orders_per_car = 10\n",
    "    \n",
    "    # Calculate car cost per order\n",
    "    car_cost_per_order = WS_CAR_COST / orders_per_car\n",
    "    \n",
    "    if pd.isna(avg_ts) or avg_ts <= 0:\n",
    "        avg_ts = 4000\n",
    "    \n",
    "    if pd.isna(current_price) or pd.isna(wac) or current_price <= 0 or wac <= 0 or pd.isna(tier_2_price):\n",
    "        return pd.Series({\n",
    "            'ws_new_multiplier': None,\n",
    "            'ws_new_order_value': None,\n",
    "            'ws_new_qty': None,\n",
    "            'ws_new_deliveries_saved': None,\n",
    "            'ws_new_car_cost_per_order': None,\n",
    "            'ws_new_total_savings': None,\n",
    "            'ws_new_discount_per_unit': None,\n",
    "            'ws_new_price': None,\n",
    "            'ws_new_margin': None,\n",
    "            'ws_new_savings_pct': None\n",
    "        })\n",
    "    \n",
    "    # Calculate max multiplier based on constraints\n",
    "    # Max by ticket size: WS_MAX_TICKET_SIZE / avg_ts\n",
    "    # No arbitrary cap - let WS_MAX_TICKET_SIZE (50K) be the only limit\n",
    "    max_multiplier = int(WS_MAX_TICKET_SIZE / avg_ts)\n",
    "    \n",
    "    best_scenario = None\n",
    "    best_savings_pct = 0\n",
    "    \n",
    "    # Test scenarios from 2x to max_multiplier\n",
    "    for multiplier in range(2, max_multiplier + 1):\n",
    "        # Order value at this multiplier\n",
    "        order_value = avg_ts * multiplier\n",
    "        \n",
    "        # Deliveries saved = multiplier - 1 (consolidating multiple orders into one)\n",
    "        deliveries_saved = multiplier - 1\n",
    "        \n",
    "        # Total savings = deliveries_saved * car_cost_per_order\n",
    "        # This is the actual cost saving from consolidating orders\n",
    "        total_savings = deliveries_saved * car_cost_per_order\n",
    "        \n",
    "        # How many units of this SKU fit in this order value?\n",
    "        qty_at_current_price = order_value / current_price\n",
    "        \n",
    "        if qty_at_current_price <= 0:\n",
    "            continue\n",
    "        \n",
    "        # Discount per unit from car cost savings\n",
    "        discount_per_unit = total_savings / qty_at_current_price\n",
    "        \n",
    "        # New price after passing car cost savings\n",
    "        new_price = tier_2_price - discount_per_unit\n",
    "        \n",
    "        # Check if price stays above WAC with minimum margin\n",
    "        min_acceptable_price = wac * (1 + WS_MIN_MARGIN)\n",
    "        \n",
    "        if new_price >= min_acceptable_price:\n",
    "            # Calculate margin at new price\n",
    "            margin = (new_price - wac) / new_price\n",
    "            \n",
    "            # Savings percentage for retailer\n",
    "            savings_pct = (discount_per_unit / current_price) * 100\n",
    "            \n",
    "            # Keep track of best scenario (highest savings while valid)\n",
    "            if savings_pct > best_savings_pct:\n",
    "                best_savings_pct = savings_pct\n",
    "                best_scenario = {\n",
    "                    'ws_new_multiplier': multiplier,\n",
    "                    'ws_new_order_value': round(order_value, 2),\n",
    "                    'ws_new_qty': round(qty_at_current_price, 0),\n",
    "                    'ws_new_deliveries_saved': deliveries_saved,\n",
    "                    'ws_new_car_cost_per_order': round(car_cost_per_order, 2),\n",
    "                    'ws_new_total_savings': round(total_savings, 2),\n",
    "                    'ws_new_discount_per_unit': round(discount_per_unit, 2),\n",
    "                    'ws_new_price': round(new_price, 2),\n",
    "                    'ws_new_margin': round(margin, 4),\n",
    "                    'ws_new_savings_pct': round(savings_pct, 2)\n",
    "                }\n",
    "    \n",
    "    if best_scenario:\n",
    "        return pd.Series(best_scenario)\n",
    "    else:\n",
    "        return pd.Series({\n",
    "            'ws_new_multiplier': None,\n",
    "            'ws_new_order_value': None,\n",
    "            'ws_new_qty': None,\n",
    "            'ws_new_deliveries_saved': None,\n",
    "            'ws_new_car_cost_per_order': None,\n",
    "            'ws_new_total_savings': None,\n",
    "            'ws_new_discount_per_unit': None,\n",
    "            'ws_new_price': None,\n",
    "            'ws_new_margin': None,\n",
    "            'ws_new_savings_pct': None\n",
    "        })\n",
    "\n",
    "# Apply the new wholesale logic\n",
    "print(\"Calculating new wholesale logic based on delivery savings...\")\n",
    "ws_new_results = final_data.apply(calculate_ws_new_logic, axis=1)\n",
    "final_data = pd.concat([final_data, ws_new_results], axis=1)\n",
    "\n",
    "# Summary statistics\n",
    "valid_ws_new = final_data['ws_new_price'].notna().sum()\n",
    "print(f\"\\n=== NEW WHOLESALE LOGIC SUMMARY ===\")\n",
    "print(f\"SKUs with valid WS new price: {valid_ws_new} / {len(final_data)}\")\n",
    "print(f\"Total car cost: {WS_CAR_COST} EGP\")\n",
    "print(f\"Average orders per car: {final_data['orders_per_car_by_weight'].mean():.1f}\")\n",
    "print(f\"Average car cost per order: {WS_CAR_COST / final_data['orders_per_car_by_weight'].mean():.2f} EGP\")\n",
    "\n",
    "if valid_ws_new > 0:\n",
    "    print(f\"\\nOrder Consolidation:\")\n",
    "    print(f\"  Average multiplier: {final_data['ws_new_multiplier'].mean():.1f}x of avg ticket size\")\n",
    "    print(f\"  Average order value needed: {final_data['ws_new_order_value'].mean():.2f} EGP\")\n",
    "    print(f\"  Average deliveries saved: {final_data['ws_new_deliveries_saved'].mean():.1f}\")\n",
    "    \n",
    "    print(f\"\\nCar Cost Savings:\")\n",
    "    print(f\"  Average car cost per order: {final_data['ws_new_car_cost_per_order'].mean():.2f} EGP\")\n",
    "    print(f\"  Average total savings: {final_data['ws_new_total_savings'].mean():.2f} EGP\")\n",
    "    print(f\"  Average discount per unit: {final_data['ws_new_discount_per_unit'].mean():.2f} EGP\")\n",
    "    \n",
    "    print(f\"\\nPricing:\")\n",
    "    print(f\"  Average WS new price margin: {final_data['ws_new_margin'].mean()*100:.2f}%\")\n",
    "    print(f\"  Average retailer savings: {final_data['ws_new_savings_pct'].mean():.2f}%\")\n",
    "    \n",
    "    # Distribution of multipliers\n",
    "    print(f\"\\nMultiplier distribution:\")\n",
    "    print(final_data['ws_new_multiplier'].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef736fbd-c796-473d-aaa1-b8e725768f4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FINAL RANKING FILTER\n",
    "# =============================================================================\n",
    "\n",
    "# Re-rank within each warehouse and filter to top products\n",
    "final_data['new_rank'] = final_data.groupby(['warehouse_id'])['final_rank'].rank(method='dense', ascending=True)\n",
    "final_data = final_data[final_data['new_rank'] <= FINAL_PRODUCTS_PER_WAREHOUSE]\n",
    "\n",
    "print(f\"✓ Filtered to top {FINAL_PRODUCTS_PER_WAREHOUSE} products per warehouse: {len(final_data)} total SKUs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc89a6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CALCULATE ADDITIONAL METRICS\n",
    "# =============================================================================\n",
    "\n",
    "# --- Stretch Percentages (how much retailers need to increase to reach each tier) ---\n",
    "# Already included from tiers_selection: tier_1_increase_pct, tier_2_increase_pct\n",
    "# These show: (tier_qty - median_qty) / median_qty * 100\n",
    "\n",
    "# Rename for clarity\n",
    "final_data['stretch_to_tier_1_pct'] = final_data['tier_1_increase_pct']\n",
    "final_data['stretch_to_tier_2_pct'] = final_data['tier_2_increase_pct']\n",
    "\n",
    "# --- Margins for each price tier ---\n",
    "# Margin = (price - wac) / price\n",
    "final_data['tier_1_margin'] = ((final_data['tier_1_price'] - final_data['wac_p']) / final_data['tier_1_price']).round(4)\n",
    "final_data['tier_2_margin'] = ((final_data['tier_2_price'] - final_data['wac_p']) / final_data['tier_2_price']).round(4)\n",
    "final_data['WS_margin'] = ((final_data['WS_price'] - final_data['wac_p']) / final_data['wac_p']).round(4)\n",
    "final_data['current_margin'] = ((final_data['packing_unit_price'] - final_data['wac_p']) / final_data['packing_unit_price']).round(4)\n",
    "\n",
    "# --- Discount calculations ---\n",
    "# Absolute discounts (price reduction from current price)\n",
    "final_data['discount_1'] = (final_data['packing_unit_price'] - final_data['tier_1_price']).round(2)\n",
    "final_data['discount_2'] = (final_data['packing_unit_price'] - final_data['tier_2_price']).round(2)\n",
    "\n",
    "# Discount percentages\n",
    "final_data['discount_1_pct'] = ((final_data['discount_1'] / final_data['packing_unit_price']) * 100).round(2)\n",
    "final_data['discount_2_pct'] = ((final_data['discount_2'] / final_data['packing_unit_price']) * 100).round(2)\n",
    "\n",
    "# --- Quantity and Discount Ratios ---\n",
    "# Quantity ratio (tier_2_qty / tier_1_qty)\n",
    "final_data['qty_ratio'] = (final_data['tier_2_qty'] / final_data['tier_1_qty']).round(2)\n",
    "\n",
    "# Discount ratio (discount_2 / discount_1)\n",
    "final_data['discount_ratio'] = (final_data['discount_2'] / final_data['discount_1']).round(2)\n",
    "\n",
    "# Elasticity ratio = discount_ratio / qty_ratio\n",
    "# This shows how much extra discount per unit of quantity increase\n",
    "final_data['elasticity_ratio'] = (final_data['discount_ratio'] / final_data['qty_ratio']).round(2)\n",
    "\n",
    "print(\"=== METRICS SUMMARY ===\")\n",
    "print(f\"\\nStretch Analysis (how much retailers need to increase orders):\")\n",
    "print(f\"  Average stretch to Tier 1: {final_data['stretch_to_tier_1_pct'].mean():.1f}%\")\n",
    "print(f\"  Average stretch to Tier 2: {final_data['stretch_to_tier_2_pct'].mean():.1f}%\")\n",
    "\n",
    "print(f\"\\nMargin Analysis:\")\n",
    "print(f\"  Current margin:  {final_data['current_margin'].mean()*100:.2f}%\")\n",
    "print(f\"  Tier 1 margin:   {final_data['tier_1_margin'].mean()*100:.2f}%\")\n",
    "print(f\"  Tier 2 margin:   {final_data['tier_2_margin'].mean()*100:.2f}%\")\n",
    "print(f\"  WS margin:       {final_data['WS_margin'].mean()*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nDiscount Analysis:\")\n",
    "print(f\"  Average Tier 1 discount: {final_data['discount_1_pct'].mean():.2f}%\")\n",
    "print(f\"  Average Tier 2 discount: {final_data['discount_2_pct'].mean():.2f}%\")\n",
    "\n",
    "print(f\"\\nElasticity Analysis (discount increase vs quantity increase):\")\n",
    "print(f\"  Average qty ratio (T2/T1): {final_data['qty_ratio'].mean():.2f}x\")\n",
    "print(f\"  Average discount ratio (D2/D1): {final_data['discount_ratio'].mean():.2f}x\")\n",
    "print(f\"  Average elasticity ratio: {final_data['elasticity_ratio'].mean():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb449fce",
   "metadata": {},
   "source": [
    "## 7. Conversion Scenarios & Simulation\n",
    "\n",
    "Before uploading, analyze expected blended prices and margins:\n",
    "1. **Hypothetical Scenarios** - 10 different conversion rate assumptions\n",
    "2. **Historical Simulation** - Actual tier conversion from previous month data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3090e037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "CONVERSION SCENARIOS ANALYSIS - NMV, Quantity & Gross Profit Impact\n",
      "========================================================================================================================\n",
      "\n",
      "Simulation based on 1,000 orders distributed across 1596 SKUs\n",
      "\n",
      "Scenario                             Base   T1   T2   WS |   NMV Δ%   Qty Δ%    GP Δ% | Blnd Margin     GP (EGP)\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Scenario 1 - Very Low Conversion      90%   7%   2%   1% |  +153.2%  +215.3%   +90.1% |       4.12%      28,102\n",
      "Scenario 2 - Low Conversion           80%  12%   5%   3% |  +431.0%  +623.5%  +247.8% |       3.59%      51,420\n",
      "Scenario 3 - Conservative             70%  15%  10%   5% |  +714.0% +1036.0%  +408.1% |       3.42%      75,125\n",
      "Scenario 4 - Moderate Low             65%  18%  12%   5% |  +732.3% +1050.6%  +421.3% |       3.44%      77,074\n",
      "Scenario 5 - Moderate                 60%  20%  13%   7% |  +991.8% +1444.1%  +565.8% |       3.35%      98,443\n",
      "Scenario 6 - Moderate High            55%  22%  15%   8% | +1132.0% +1649.3%  +645.3% |       3.32%     110,199\n",
      "Scenario 7 - Optimistic               50%  25%  17%   8% | +1150.3% +1663.9%  +658.5% |       3.33%     112,147\n",
      "Scenario 8 - High Conversion          45%  27%  18%  10% | +1409.8% +2057.4%  +803.0% |       3.28%     133,516\n",
      "Scenario 9 - Very High Conversion     40%  28%  20%  12% | +1671.9% +2453.2%  +948.9% |       3.25%     155,079\n",
      "Scenario 10 - Maximum Conversion      35%  30%  22%  13% | +1812.1% +2658.4% +1028.4% |       3.24%     166,835\n",
      "\n",
      "========================================================================================================================\n",
      "DETAILED COMPARISON\n",
      "========================================================================================================================\n",
      "\n",
      "              CURRENT STATE (100% Base Price)               \n",
      "------------------------------------------------------------\n",
      "  Total NMV:                269,487.47 EGP\n",
      "  Total Quantity:                1,544 units\n",
      "  Total COGS:               254,701.94 EGP\n",
      "  Total Gross Profit:        14,785.53 EGP\n",
      "  Gross Margin:                   5.49%\n",
      "\n",
      "             CONSERVATIVE SCENARIO (70/15/10/5)             \n",
      "------------------------------------------------------------\n",
      "  Total NMV:              2,193,668.52 EGP  (+714.0%)\n",
      "  Total Quantity:               17,535 units (+1036.0%)\n",
      "  Total Gross Profit:        75,125.17 EGP  (+408.1%)\n",
      "  Gross Margin:                   3.42%\n",
      "  GP Change:                +60,339.64 EGP\n",
      "\n",
      "              OPTIMISTIC SCENARIO (50/25/17/8)              \n",
      "------------------------------------------------------------\n",
      "  Total NMV:              3,369,281.63 EGP  (+1150.3%)\n",
      "  Total Quantity:               27,226 units (+1663.9%)\n",
      "  Total Gross Profit:       112,147.18 EGP  (+658.5%)\n",
      "  Gross Margin:                   3.33%\n",
      "  GP Change:                +97,361.65 EGP\n",
      "\n",
      "========================================================================================================================\n",
      "SCENARIO IMPACT SUMMARY\n",
      "========================================================================================================================\n",
      "\n",
      "Scenario                            |      NMV Change |      Qty Change |       GP Change |  GP Margin\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Scenario 1 - Very Low Conversion    |       +412,922 |         +3,324 |        +13,317 |      4.12%\n",
      "Scenario 2 - Low Conversion         |     +1,161,490 |         +9,623 |        +36,634 |      3.59%\n",
      "Scenario 3 - Conservative           |     +1,924,181 |        +15,991 |        +60,340 |      3.42%\n",
      "Scenario 4 - Moderate Low           |     +1,973,411 |        +16,216 |        +62,288 |      3.44%\n",
      "Scenario 5 - Moderate               |     +2,672,749 |        +22,291 |        +83,657 |      3.35%\n",
      "Scenario 6 - Moderate High          |     +3,050,564 |        +25,458 |        +95,413 |      3.32%\n",
      "Scenario 7 - Optimistic             |     +3,099,794 |        +25,682 |        +97,362 |      3.33%\n",
      "Scenario 8 - High Conversion        |     +3,799,132 |        +31,758 |       +118,731 |      3.28%\n",
      "Scenario 9 - Very High Conversion   |     +4,505,531 |        +37,867 |       +140,293 |      3.25%\n",
      "Scenario 10 - Maximum Conversion    |     +4,883,346 |        +41,034 |       +152,049 |      3.24%\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PART 1: HYPOTHETICAL CONVERSION SCENARIOS\n",
    "# =============================================================================\n",
    "# 10 scenarios with different conversion rates:\n",
    "# - Base (no tier): % of orders at packing_unit_price\n",
    "# - Tier 1: % of orders at tier_1_price\n",
    "# - Tier 2: % of orders at tier_2_price\n",
    "# - Wholesale: % of orders at ws_new_price\n",
    "\n",
    "# Define 10 scenarios: (base%, tier1%, tier2%, ws%)\n",
    "# Scenarios range from pessimistic (low conversion) to optimistic (high conversion)\n",
    "scenarios = {\n",
    "    'Scenario 1 - Very Low Conversion':   (90, 7, 2, 1),    # Most orders at base price\n",
    "    'Scenario 2 - Low Conversion':        (80, 12, 5, 3),   # Low tier uptake\n",
    "    'Scenario 3 - Conservative':          (70, 15, 10, 5),  # Conservative estimate\n",
    "    'Scenario 4 - Moderate Low':          (65, 18, 12, 5),  # Slightly better\n",
    "    'Scenario 5 - Moderate':              (60, 20, 13, 7),  # Moderate adoption\n",
    "    'Scenario 6 - Moderate High':         (55, 22, 15, 8),  # Good adoption\n",
    "    'Scenario 7 - Optimistic':            (50, 25, 17, 8),  # Optimistic uptake\n",
    "    'Scenario 8 - High Conversion':       (45, 27, 18, 10), # High tier adoption\n",
    "    'Scenario 9 - Very High Conversion':  (40, 28, 20, 12), # Very high uptake\n",
    "    'Scenario 10 - Maximum Conversion':   (35, 30, 22, 13), # Maximum realistic conversion\n",
    "}\n",
    "\n",
    "def calculate_blended_metrics_with_gp(df, base_pct, t1_pct, t2_pct, ws_pct, num_orders=1000):\n",
    "    \"\"\"\n",
    "    Calculate blended price, margin, NMV and Gross Profit for a given conversion scenario.\n",
    "    \n",
    "    Key insight: When retailers convert to tiers, they buy MORE quantity (that's the incentive).\n",
    "    - Base orders: quantity = median_qty (typical order before tier)\n",
    "    - Tier 1 orders: quantity = tier_1_qty (must reach this to get discount)\n",
    "    - Tier 2 orders: quantity = tier_2_qty\n",
    "    - WS orders: quantity = ws_new_qty\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with price and quantity columns\n",
    "        base_pct, t1_pct, t2_pct, ws_pct: % of orders in each tier\n",
    "        num_orders: Assumed number of total orders for simulation\n",
    "    \n",
    "    Returns:\n",
    "        dict with all metrics\n",
    "    \"\"\"\n",
    "    # Convert percentages to decimals\n",
    "    base_pct = base_pct / 100\n",
    "    t1_pct = t1_pct / 100\n",
    "    t2_pct = t2_pct / 100\n",
    "    ws_pct = ws_pct / 100\n",
    "    \n",
    "    df_calc = df.copy()\n",
    "    \n",
    "    # Fill missing values\n",
    "    df_calc['ws_price_filled'] = df_calc['ws_new_price'].fillna(df_calc['tier_2_price'])\n",
    "    df_calc['ws_qty_filled'] = df_calc['ws_new_qty'].fillna(df_calc['tier_2_qty'])\n",
    "    \n",
    "    # Calculate NMV and COGS for each tier (per SKU, per order)\n",
    "    # Base: orders at median_qty * packing_unit_price\n",
    "    df_calc['base_nmv_per_order'] = df_calc['median_qty'] * df_calc['packing_unit_price']\n",
    "    df_calc['base_cogs_per_order'] = df_calc['median_qty'] * df_calc['wac_p']\n",
    "    \n",
    "    # Tier 1: orders at tier_1_qty * tier_1_price\n",
    "    df_calc['t1_nmv_per_order'] = df_calc['tier_1_qty'] * df_calc['tier_1_price']\n",
    "    df_calc['t1_cogs_per_order'] = df_calc['tier_1_qty'] * df_calc['wac_p']\n",
    "    \n",
    "    # Tier 2: orders at tier_2_qty * tier_2_price\n",
    "    df_calc['t2_nmv_per_order'] = df_calc['tier_2_qty'] * df_calc['tier_2_price']\n",
    "    df_calc['t2_cogs_per_order'] = df_calc['tier_2_qty'] * df_calc['wac_p']\n",
    "    \n",
    "    # Wholesale: orders at ws_qty * ws_price\n",
    "    df_calc['ws_nmv_per_order'] = df_calc['ws_qty_filled'] * df_calc['ws_price_filled']\n",
    "    df_calc['ws_cogs_per_order'] = df_calc['ws_qty_filled'] * df_calc['wac_p']\n",
    "    \n",
    "    # Blended NMV per order (weighted by conversion rates)\n",
    "    df_calc['blended_nmv_per_order'] = (\n",
    "        base_pct * df_calc['base_nmv_per_order'] +\n",
    "        t1_pct * df_calc['t1_nmv_per_order'] +\n",
    "        t2_pct * df_calc['t2_nmv_per_order'] +\n",
    "        ws_pct * df_calc['ws_nmv_per_order']\n",
    "    )\n",
    "    \n",
    "    # Blended COGS per order\n",
    "    df_calc['blended_cogs_per_order'] = (\n",
    "        base_pct * df_calc['base_cogs_per_order'] +\n",
    "        t1_pct * df_calc['t1_cogs_per_order'] +\n",
    "        t2_pct * df_calc['t2_cogs_per_order'] +\n",
    "        ws_pct * df_calc['ws_cogs_per_order']\n",
    "    )\n",
    "    \n",
    "    # Blended quantity per order\n",
    "    df_calc['blended_qty_per_order'] = (\n",
    "        base_pct * df_calc['median_qty'] +\n",
    "        t1_pct * df_calc['tier_1_qty'] +\n",
    "        t2_pct * df_calc['tier_2_qty'] +\n",
    "        ws_pct * df_calc['ws_qty_filled']\n",
    "    )\n",
    "    \n",
    "    # Gross Profit per order\n",
    "    df_calc['blended_gp_per_order'] = df_calc['blended_nmv_per_order'] - df_calc['blended_cogs_per_order']\n",
    "    \n",
    "    # Current state (100% base)\n",
    "    df_calc['current_nmv_per_order'] = df_calc['base_nmv_per_order']\n",
    "    df_calc['current_cogs_per_order'] = df_calc['base_cogs_per_order']\n",
    "    df_calc['current_gp_per_order'] = df_calc['current_nmv_per_order'] - df_calc['current_cogs_per_order']\n",
    "    \n",
    "    # Aggregate across all SKUs (simulate num_orders distributed across SKUs)\n",
    "    orders_per_sku = num_orders / len(df_calc)\n",
    "    \n",
    "    total_current_nmv = (df_calc['current_nmv_per_order'] * orders_per_sku).sum()\n",
    "    total_current_cogs = (df_calc['current_cogs_per_order'] * orders_per_sku).sum()\n",
    "    total_current_gp = total_current_nmv - total_current_cogs\n",
    "    total_current_qty = (df_calc['median_qty'] * orders_per_sku).sum()\n",
    "    \n",
    "    total_blended_nmv = (df_calc['blended_nmv_per_order'] * orders_per_sku).sum()\n",
    "    total_blended_cogs = (df_calc['blended_cogs_per_order'] * orders_per_sku).sum()\n",
    "    total_blended_gp = total_blended_nmv - total_blended_cogs\n",
    "    total_blended_qty = (df_calc['blended_qty_per_order'] * orders_per_sku).sum()\n",
    "    \n",
    "    # Calculate changes\n",
    "    nmv_change = total_blended_nmv - total_current_nmv\n",
    "    nmv_change_pct = (nmv_change / total_current_nmv) * 100\n",
    "    \n",
    "    qty_change = total_blended_qty - total_current_qty\n",
    "    qty_change_pct = (qty_change / total_current_qty) * 100\n",
    "    \n",
    "    gp_change = total_blended_gp - total_current_gp\n",
    "    gp_change_pct = (gp_change / total_current_gp) * 100 if total_current_gp != 0 else 0\n",
    "    \n",
    "    # Blended margins\n",
    "    current_margin = total_current_gp / total_current_nmv if total_current_nmv != 0 else 0\n",
    "    blended_margin = total_blended_gp / total_blended_nmv if total_blended_nmv != 0 else 0\n",
    "    \n",
    "    # Average blended price per unit\n",
    "    avg_current_price = total_current_nmv / total_current_qty if total_current_qty != 0 else 0\n",
    "    avg_blended_price = total_blended_nmv / total_blended_qty if total_blended_qty != 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'current_nmv': total_current_nmv,\n",
    "        'blended_nmv': total_blended_nmv,\n",
    "        'nmv_change': nmv_change,\n",
    "        'nmv_change_pct': nmv_change_pct,\n",
    "        'current_qty': total_current_qty,\n",
    "        'blended_qty': total_blended_qty,\n",
    "        'qty_change': qty_change,\n",
    "        'qty_change_pct': qty_change_pct,\n",
    "        'current_gp': total_current_gp,\n",
    "        'blended_gp': total_blended_gp,\n",
    "        'gp_change': gp_change,\n",
    "        'gp_change_pct': gp_change_pct,\n",
    "        'current_margin': current_margin,\n",
    "        'blended_margin': blended_margin,\n",
    "        'avg_current_price': avg_current_price,\n",
    "        'avg_blended_price': avg_blended_price\n",
    "    }\n",
    "\n",
    "# Calculate and display results for each scenario\n",
    "print(\"=\" * 120)\n",
    "print(\"CONVERSION SCENARIOS ANALYSIS - NMV, Quantity & Gross Profit Impact\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "# Assume 1000 orders for simulation\n",
    "NUM_ORDERS = 1000\n",
    "\n",
    "print(f\"\\nSimulation based on {NUM_ORDERS:,} orders distributed across {len(final_data)} SKUs\")\n",
    "print(f\"\\n{'Scenario':<35} {'Base':>5} {'T1':>4} {'T2':>4} {'WS':>4} | {'NMV Δ%':>8} {'Qty Δ%':>8} {'GP Δ%':>8} | {'Blnd Margin':>11} {'GP (EGP)':>12}\")\n",
    "print(\"-\" * 120)\n",
    "\n",
    "scenario_results = []\n",
    "for scenario_name, (base, t1, t2, ws) in scenarios.items():\n",
    "    metrics = calculate_blended_metrics_with_gp(final_data, base, t1, t2, ws, NUM_ORDERS)\n",
    "    \n",
    "    scenario_results.append({\n",
    "        'scenario': scenario_name,\n",
    "        'base_pct': base,\n",
    "        't1_pct': t1,\n",
    "        't2_pct': t2,\n",
    "        'ws_pct': ws,\n",
    "        **metrics\n",
    "    })\n",
    "    \n",
    "    print(f\"{scenario_name:<35} {base:>4}% {t1:>3}% {t2:>3}% {ws:>3}% | \"\n",
    "          f\"{metrics['nmv_change_pct']:>+7.1f}% {metrics['qty_change_pct']:>+7.1f}% {metrics['gp_change_pct']:>+7.1f}% | \"\n",
    "          f\"{metrics['blended_margin']*100:>10.2f}% {metrics['blended_gp']:>11,.0f}\")\n",
    "\n",
    "# Create summary DataFrame\n",
    "scenarios_df = pd.DataFrame(scenario_results)\n",
    "\n",
    "# Current state baseline\n",
    "current_metrics = calculate_blended_metrics_with_gp(final_data, 100, 0, 0, 0, NUM_ORDERS)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"DETAILED COMPARISON\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "print(f\"\\n{'CURRENT STATE (100% Base Price)':^60}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"  Total NMV:           {current_metrics['current_nmv']:>15,.2f} EGP\")\n",
    "print(f\"  Total Quantity:      {current_metrics['current_qty']:>15,.0f} units\")\n",
    "print(f\"  Total COGS:          {current_metrics['current_nmv'] - current_metrics['current_gp']:>15,.2f} EGP\")\n",
    "print(f\"  Total Gross Profit:  {current_metrics['current_gp']:>15,.2f} EGP\")\n",
    "print(f\"  Gross Margin:        {current_metrics['current_margin']*100:>15.2f}%\")\n",
    "\n",
    "# Conservative scenario\n",
    "cons = scenarios_df[scenarios_df['scenario'].str.contains('Conservative')].iloc[0]\n",
    "print(f\"\\n{'CONSERVATIVE SCENARIO (70/15/10/5)':^60}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"  Total NMV:           {cons['blended_nmv']:>15,.2f} EGP  ({cons['nmv_change_pct']:>+.1f}%)\")\n",
    "print(f\"  Total Quantity:      {cons['blended_qty']:>15,.0f} units ({cons['qty_change_pct']:>+.1f}%)\")\n",
    "print(f\"  Total Gross Profit:  {cons['blended_gp']:>15,.2f} EGP  ({cons['gp_change_pct']:>+.1f}%)\")\n",
    "print(f\"  Gross Margin:        {cons['blended_margin']*100:>15.2f}%\")\n",
    "print(f\"  GP Change:           {cons['gp_change']:>+15,.2f} EGP\")\n",
    "\n",
    "# Optimistic scenario\n",
    "opt = scenarios_df[scenarios_df['scenario'].str.contains('Optimistic')].iloc[0]\n",
    "print(f\"\\n{'OPTIMISTIC SCENARIO (50/25/17/8)':^60}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"  Total NMV:           {opt['blended_nmv']:>15,.2f} EGP  ({opt['nmv_change_pct']:>+.1f}%)\")\n",
    "print(f\"  Total Quantity:      {opt['blended_qty']:>15,.0f} units ({opt['qty_change_pct']:>+.1f}%)\")\n",
    "print(f\"  Total Gross Profit:  {opt['blended_gp']:>15,.2f} EGP  ({opt['gp_change_pct']:>+.1f}%)\")\n",
    "print(f\"  Gross Margin:        {opt['blended_margin']*100:>15.2f}%\")\n",
    "print(f\"  GP Change:           {opt['gp_change']:>+15,.2f} EGP\")\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"SCENARIO IMPACT SUMMARY\")\n",
    "print(\"=\" * 120)\n",
    "print(f\"\\n{'Scenario':<35} | {'NMV Change':>15} | {'Qty Change':>15} | {'GP Change':>15} | {'GP Margin':>10}\")\n",
    "print(\"-\" * 100)\n",
    "for _, row in scenarios_df.iterrows():\n",
    "    print(f\"{row['scenario']:<35} | {row['nmv_change']:>+14,.0f} | {row['qty_change']:>+14,.0f} | {row['gp_change']:>+14,.0f} | {row['blended_margin']*100:>9.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b30b8dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching historical tier conversion data from previous month...\n",
      "✓ Retrieved conversion data for 1596 SKUs\n",
      "  Total orders analyzed: 674,520\n",
      "\n",
      "================================================================================\n",
      "HISTORICAL TIER CONVERSION (Previous Month)\n",
      "================================================================================\n",
      "\n",
      "Overall Conversion Rates (based on 674,520 orders):\n",
      "  Base (no tier):   90.09%  (607,696 orders)\n",
      "  Tier 1:            6.98%  (47,097 orders)\n",
      "  Tier 2:            2.92%  (19,681 orders)\n",
      "  Wholesale:         0.01%  (46 orders)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PART 2: HISTORICAL SIMULATION - Previous Month Tier Conversion\n",
    "# =============================================================================\n",
    "# Query actual order data from previous month to see real tier conversion rates\n",
    "# Then calculate what the blended price and margin would have been\n",
    "\n",
    "# Prepare product list for query\n",
    "selected_df = final_data[['warehouse_id', 'product_id', 'packing_unit_id', \n",
    "                           'tier_1_qty', 'tier_2_qty', 'ws_new_qty',\n",
    "                           'packing_unit_price', 'tier_1_price', 'tier_2_price', \n",
    "                           'ws_new_price', 'wac_p']].copy()\n",
    "\n",
    "# Create tuples string for SQL\n",
    "tuples_for_query = ','.join([\n",
    "    f\"({int(row['warehouse_id'])}, {int(row['product_id'])}, {int(row['packing_unit_id'])}, \"\n",
    "    f\"{int(row['tier_1_qty'])}, {int(row['tier_2_qty'])}, {int(row['ws_new_qty']) if pd.notna(row['ws_new_qty']) else 0})\"\n",
    "    for _, row in selected_df.iterrows()\n",
    "])\n",
    "\n",
    "query = f'''\n",
    "WITH selected_products AS (\n",
    "    SELECT warehouse_id, product_id, packing_unit_id, tier_1_qty, tier_2_qty, ws_qty\n",
    "    FROM (VALUES\n",
    "        {tuples_for_query}\n",
    "    ) AS x(warehouse_id, product_id, packing_unit_id, tier_1_qty, tier_2_qty, ws_qty)\n",
    "),\n",
    "\n",
    "-- Same base filtering as product selection\n",
    "base AS (\n",
    "    SELECT *, ROW_NUMBER() OVER (PARTITION BY retailer_id ORDER BY priority) as rnk \n",
    "    FROM (\n",
    "        SELECT x.*, TAGGABLE_ID as retailer_id \n",
    "        FROM (\n",
    "            SELECT id as cohort_id, name as cohort_name, priority, dynamic_tag_id \n",
    "            FROM cohorts \n",
    "            WHERE is_active = 'true'\n",
    "                AND id IN (700,701,702,703,704,1123,1124,1125,1126)\n",
    "        ) x \n",
    "        JOIN DYNAMIC_TAGgables dt ON x.dynamic_tag_id = dt.dynamic_tag_id\n",
    "        WHERE dt.taggable_id NOT IN (\n",
    "            SELECT taggable_id FROM DYNAMIC_TAGgables \n",
    "            WHERE dynamic_tag_id IN (2807, 2808, 2809, 2810, 2811, 2812)\n",
    "        )\n",
    "    )\n",
    "    QUALIFY rnk = 1 \n",
    "),\n",
    "\n",
    "-- Map regions to warehouses\n",
    "whs AS (\n",
    "    SELECT * FROM (VALUES\n",
    "        ('Cairo', 'El-Marg', 38),\n",
    "        ('Cairo', 'Mostorod', 1),\n",
    "        ('Giza', 'Barageel', 236),\n",
    "        ('Giza', 'Sakkarah', 962),\n",
    "        ('Delta West', 'El-Mahala', 337),\n",
    "        ('Delta West', 'Tanta', 8),\n",
    "        ('Delta East', 'Mansoura FC', 339),\n",
    "        ('Delta East', 'Sharqya', 170),\n",
    "        ('Upper Egypt', 'Assiut FC', 501),\n",
    "        ('Upper Egypt', 'Bani sweif', 401),\n",
    "        ('Upper Egypt', 'Menya Samalot', 703),\n",
    "        ('Upper Egypt', 'Sohag', 632),\n",
    "        ('Alexandria', 'Khorshed Alex', 797)\n",
    "    ) x(region_name, wh, warehouse_id)\n",
    "),\n",
    "\n",
    "-- Get order quantities from previous month\n",
    "previous_month_orders AS (\n",
    "    SELECT \n",
    "        whs.warehouse_id,\n",
    "        pso.product_id,\n",
    "        pso.packing_unit_id,\n",
    "        so.parent_sales_order_id,\n",
    "        so.retailer_id,\n",
    "        SUM(pso.purchased_item_count) as order_qty,\n",
    "        SUM(pso.total_price) as order_value\n",
    "    FROM product_sales_order pso\n",
    "    JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "    JOIN base ON base.retailer_id = so.retailer_id\n",
    "    JOIN materialized_views.retailer_polygon rp ON rp.retailer_id = so.retailer_id\n",
    "    JOIN districts ON districts.id = rp.district_id\n",
    "    JOIN cities ON cities.id = districts.city_id\n",
    "    JOIN states ON states.id = cities.state_id\n",
    "    JOIN regions ON regions.id = states.region_id\n",
    "    JOIN whs ON whs.region_name = CASE WHEN regions.id = 2 THEN states.name_en ELSE regions.name_en END\n",
    "    JOIN selected_products sp \n",
    "        ON sp.warehouse_id = whs.warehouse_id \n",
    "        AND sp.product_id = pso.product_id\n",
    "        AND sp.packing_unit_id = pso.packing_unit_id\n",
    "    WHERE so.created_at::date BETWEEN DATE_TRUNC('month', CURRENT_DATE - INTERVAL '1 month') \n",
    "                                   AND LAST_DAY(CURRENT_DATE - INTERVAL '1 month')\n",
    "        AND so.sales_order_status_id NOT IN (7, 12)\n",
    "        AND so.channel IN ('telesales', 'retailer')\n",
    "        AND pso.purchased_item_count > 0\n",
    "    GROUP BY whs.warehouse_id, pso.product_id, pso.packing_unit_id, \n",
    "             so.parent_sales_order_id, so.retailer_id\n",
    "),\n",
    "\n",
    "-- Classify each order into tiers based on quantity\n",
    "order_tiers AS (\n",
    "    SELECT \n",
    "        pmo.*,\n",
    "        sp.tier_1_qty,\n",
    "        sp.tier_2_qty,\n",
    "        sp.ws_qty,\n",
    "        CASE \n",
    "            WHEN pmo.order_qty >= sp.ws_qty AND sp.ws_qty > 0 THEN 'Wholesale'\n",
    "            WHEN pmo.order_qty >= sp.tier_2_qty THEN 'Tier 2'\n",
    "            WHEN pmo.order_qty >= sp.tier_1_qty THEN 'Tier 1'\n",
    "            ELSE 'Base'\n",
    "        END as tier_reached\n",
    "    FROM previous_month_orders pmo\n",
    "    JOIN selected_products sp \n",
    "        ON sp.warehouse_id = pmo.warehouse_id \n",
    "        AND sp.product_id = pmo.product_id\n",
    "        AND sp.packing_unit_id = pmo.packing_unit_id\n",
    "),\n",
    "\n",
    "-- Aggregate conversion rates per SKU\n",
    "sku_conversion AS (\n",
    "    SELECT \n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        COUNT(*) as total_orders,\n",
    "        SUM(order_value) as total_value,\n",
    "        \n",
    "        -- Order counts by tier\n",
    "        COUNT(CASE WHEN tier_reached = 'Base' THEN 1 END) as base_orders,\n",
    "        COUNT(CASE WHEN tier_reached = 'Tier 1' THEN 1 END) as tier1_orders,\n",
    "        COUNT(CASE WHEN tier_reached = 'Tier 2' THEN 1 END) as tier2_orders,\n",
    "        COUNT(CASE WHEN tier_reached = 'Wholesale' THEN 1 END) as ws_orders,\n",
    "        \n",
    "        -- Conversion percentages\n",
    "        ROUND(100.0 * COUNT(CASE WHEN tier_reached = 'Base' THEN 1 END) / NULLIF(COUNT(*), 0), 2) as base_pct,\n",
    "        ROUND(100.0 * COUNT(CASE WHEN tier_reached = 'Tier 1' THEN 1 END) / NULLIF(COUNT(*), 0), 2) as tier1_pct,\n",
    "        ROUND(100.0 * COUNT(CASE WHEN tier_reached = 'Tier 2' THEN 1 END) / NULLIF(COUNT(*), 0), 2) as tier2_pct,\n",
    "        ROUND(100.0 * COUNT(CASE WHEN tier_reached = 'Wholesale' THEN 1 END) / NULLIF(COUNT(*), 0), 2) as ws_pct\n",
    "        \n",
    "    FROM order_tiers\n",
    "    GROUP BY warehouse_id, product_id, packing_unit_id\n",
    "),\n",
    "\n",
    "-- Overall conversion rates\n",
    "overall_conversion AS (\n",
    "    SELECT \n",
    "        'Overall' as level,\n",
    "        COUNT(*) as total_orders,\n",
    "        SUM(order_value) as total_value,\n",
    "        ROUND(100.0 * COUNT(CASE WHEN tier_reached = 'Base' THEN 1 END) / NULLIF(COUNT(*), 0), 2) as base_pct,\n",
    "        ROUND(100.0 * COUNT(CASE WHEN tier_reached = 'Tier 1' THEN 1 END) / NULLIF(COUNT(*), 0), 2) as tier1_pct,\n",
    "        ROUND(100.0 * COUNT(CASE WHEN tier_reached = 'Tier 2' THEN 1 END) / NULLIF(COUNT(*), 0), 2) as tier2_pct,\n",
    "        ROUND(100.0 * COUNT(CASE WHEN tier_reached = 'Wholesale' THEN 1 END) / NULLIF(COUNT(*), 0), 2) as ws_pct\n",
    "    FROM order_tiers\n",
    ")\n",
    "\n",
    "-- Return both SKU-level and overall results\n",
    "SELECT \n",
    "    sc.warehouse_id,\n",
    "    sc.product_id,\n",
    "    sc.packing_unit_id,\n",
    "    sc.total_orders,\n",
    "    sc.total_value,\n",
    "    sc.base_orders,\n",
    "    sc.tier1_orders,\n",
    "    sc.tier2_orders,\n",
    "    sc.ws_orders,\n",
    "    sc.base_pct,\n",
    "    sc.tier1_pct,\n",
    "    sc.tier2_pct,\n",
    "    sc.ws_pct\n",
    "FROM sku_conversion sc\n",
    "ORDER BY sc.warehouse_id, sc.total_orders DESC\n",
    "'''\n",
    "\n",
    "print(\"Fetching historical tier conversion data from previous month...\")\n",
    "historical_conversion = snowflake_query(\"Egypt\", query)\n",
    "historical_conversion.columns = historical_conversion.columns.str.lower()\n",
    "\n",
    "for col in historical_conversion.columns:\n",
    "    historical_conversion[col] = pd.to_numeric(historical_conversion[col], errors='ignore')\n",
    "\n",
    "print(f\"✓ Retrieved conversion data for {len(historical_conversion)} SKUs\")\n",
    "print(f\"  Total orders analyzed: {historical_conversion['total_orders'].sum():,}\")\n",
    "\n",
    "# Calculate overall conversion rates\n",
    "total_orders = historical_conversion['total_orders'].sum()\n",
    "overall_base_pct = (historical_conversion['base_orders'].sum() / total_orders) * 100\n",
    "overall_t1_pct = (historical_conversion['tier1_orders'].sum() / total_orders) * 100\n",
    "overall_t2_pct = (historical_conversion['tier2_orders'].sum() / total_orders) * 100\n",
    "overall_ws_pct = (historical_conversion['ws_orders'].sum() / total_orders) * 100\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"HISTORICAL TIER CONVERSION (Previous Month)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nOverall Conversion Rates (based on {total_orders:,} orders):\")\n",
    "print(f\"  Base (no tier):  {overall_base_pct:>6.2f}%  ({historical_conversion['base_orders'].sum():,} orders)\")\n",
    "print(f\"  Tier 1:          {overall_t1_pct:>6.2f}%  ({historical_conversion['tier1_orders'].sum():,} orders)\")\n",
    "print(f\"  Tier 2:          {overall_t2_pct:>6.2f}%  ({historical_conversion['tier2_orders'].sum():,} orders)\")\n",
    "print(f\"  Wholesale:       {overall_ws_pct:>6.2f}%  ({historical_conversion['ws_orders'].sum():,} orders)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6b93ee2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================================================\n",
      "HISTORICAL SIMULATION - NMV, QUANTITY & GROSS PROFIT IMPACT\n",
      "========================================================================================================================\n",
      "\n",
      "Based on 674,520 historical orders across 1596 SKUs\n",
      "Conversion: Base 90.1% | T1 7.0% | T2 2.9% | WS 0.0%\n",
      "\n",
      "METRIC                               CURRENT            BLENDED             CHANGE     CHANGE %\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Total NMV (EGP)                 183,428,075       231,989,376       +48,561,302      +26.47%\n",
      "Total Quantity (units)            1,063,730         1,349,999          +286,270      +26.91%\n",
      "Total COGS (EGP)                173,251,189       219,803,776       +46,552,587      +26.87%\n",
      "Total Gross Profit (EGP)         10,176,886        12,185,601        +2,008,715      +19.74%\n",
      "Gross Margin (%)                       5.55%              5.25%             -0.30 pp\n",
      "\n",
      "========================================================================================================================\n",
      "WAREHOUSE-LEVEL BREAKDOWN\n",
      "========================================================================================================================\n",
      "\n",
      "   WH  Orders  Base%   T1%   T2%   WS% |        NMV Δ   NMV Δ% |         GP Δ    GP Δ% |  Margin\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "    1  123385  89.8%  7.1%  3.1%  0.0% |  +9,971,417   +27.9% |    +359,690   +19.2% |   4.88%\n",
      "    8   50597  91.7%  5.9%  2.4%  0.0% |  +2,725,657   +22.4% |    +108,704   +16.8% |   5.08%\n",
      "  170   46217  91.6%  6.1%  2.3%  0.0% |  +2,236,536   +20.0% |    +104,366   +15.7% |   5.74%\n",
      "  236   65803  90.7%  6.9%  2.4%  0.0% |  +4,606,121   +24.2% |    +192,542   +18.6% |   5.19%\n",
      "  337   50370  91.3%  6.3%  2.4%  0.0% |  +2,836,236   +23.4% |    +113,619   +17.5% |   5.10%\n",
      "  339   46708  91.5%  6.2%  2.3%  0.0% |  +2,254,223   +21.1% |    +106,372   +16.4% |   5.81%\n",
      "  401   53571  88.1%  8.4%  3.5%  0.0% |  +4,377,397   +30.3% |    +196,646   +23.6% |   5.48%\n",
      "  501   56902  88.1%  8.2%  3.7%  0.0% |  +5,128,874   +32.2% |    +213,314   +23.6% |   5.30%\n",
      "  632   44048  89.0%  7.7%  3.3%  0.0% |  +3,799,811   +29.4% |    +161,325   +22.3% |   5.29%\n",
      "  703   58133  88.8%  7.6%  3.6%  0.0% |  +5,245,986   +30.8% |    +223,459   +23.0% |   5.38%\n",
      "  797   17544  90.6%  5.6%  3.8%  0.0% |  +1,362,937   +31.1% |     +68,449   +25.7% |   5.83%\n",
      "  962   61242  91.1%  6.5%  2.5%  0.0% |  +4,016,107   +22.8% |    +160,229   +16.8% |   5.15%\n",
      "\n",
      "========================================================================================================================\n",
      "COMPARISON: Historical vs Hypothetical Scenarios\n",
      "========================================================================================================================\n",
      "\n",
      "Historical Conversion (90.1/7.0/2.9/0.0):\n",
      "  NMV Change:      +34.29%\n",
      "  Quantity Change: +27.84%\n",
      "  GP Change:       +25.20%\n",
      "  Blended Margin:  5.12%\n",
      "\n",
      "Closest Hypothetical Scenario: Scenario 1 - Very Low Conversion\n",
      "\n",
      "========================================================================================================================\n",
      "WHAT-IF SCENARIO: Move 30% of Base Orders to Tiers\n",
      "========================================================================================================================\n",
      "\n",
      "Conversion Rate Comparison:\n",
      "                               Historical  What-If (+30%)       Change\n",
      "----------------------------------------------------------------------\n",
      "Base (no tier)                      90.1%           70.1%       -20.0 pp\n",
      "Tier 1                               7.0%           21.1%       +14.1 pp\n",
      "Tier 2                               2.9%            8.8%        +5.9 pp\n",
      "Wholesale                            0.0%            0.0%        +0.0 pp\n",
      "TOTAL                              100.0%          100.0%\n",
      "\n",
      "METRIC                            CURRENT      HISTORICAL         WHAT-IF   vs Current  vs Historical\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Total NMV (EGP)              183,428,075    231,989,376    362,234,626 +178,806,551  +130,245,249\n",
      "Total Quantity                 1,063,730      1,349,999      1,931,624    +867,895      +581,625\n",
      "Total Gross Profit            10,176,886     12,185,601     17,293,859  +7,116,974    +5,108,258\n",
      "Gross Margin                       5.55%          5.25%          4.77%      -0.77 pp        -0.48 pp\n",
      "\n",
      "==============================================================================================================\n",
      "                                                IMPACT SUMMARY                                                \n",
      "==============================================================================================================\n",
      "\n",
      "If we shift 20% of base orders to tiers:\n",
      "\n",
      "📈 vs CURRENT STATE (100% base):\n",
      "   • NMV increases by:             +178,806,551 EGP  (+97.48%)\n",
      "   • Quantity increases by:            +867,895 units (+81.59%)\n",
      "   • Gross Profit changes by:        +7,116,974 EGP  (+69.93%)\n",
      "   • Margin changes:                      -0.77 pp\n",
      "\n",
      "📊 vs HISTORICAL CONVERSION (90/7/3/0):\n",
      "   • NMV additional:               +130,245,249 EGP  (+56.14%)\n",
      "   • Quantity additional:              +581,625 units (+43.08%)\n",
      "   • GP additional:                  +5,108,258 EGP  (+41.92%)\n",
      "   • Margin change:                       -0.48 pp\n",
      "\n",
      "========================================================================================================================\n",
      "KEY INSIGHT\n",
      "========================================================================================================================\n",
      "\n",
      "Based on historical conversion rates:\n",
      "• NMV INCREASES by 48,561,302 EGP (+26.5%)\n",
      "  → This is because retailers order MORE quantity to reach tier thresholds\n",
      "\n",
      "• Gross Profit INCREASES by 2,008,715 EGP (+19.7%)\n",
      "  → Higher volume offsets lower price per unit\n",
      "\n",
      "• Gross Margin changes from 5.55% to 5.25%\n",
      "  → Margin compression of 0.30 pp\n",
      "\n",
      "With additional 20% conversion (What-If):\n",
      "• Additional NMV opportunity: +130,245,249 EGP\n",
      "• Additional GP opportunity:  +5,108,258 EGP\n",
      "\n",
      "✓ Simulation results saved to 'QD_simulation_results.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PART 2B: CALCULATE BLENDED NMV, GROSS PROFIT FROM HISTORICAL CONVERSION\n",
    "# =============================================================================\n",
    "\n",
    "# Merge historical conversion data with pricing and quantity data\n",
    "simulation_data = historical_conversion.merge(\n",
    "    final_data[['warehouse_id', 'product_id', 'packing_unit_id',\n",
    "                'packing_unit_price', 'tier_1_price', 'tier_2_price', \n",
    "                'ws_new_price', 'wac_p', 'sku', 'brand',\n",
    "                'median_qty', 'tier_1_qty', 'tier_2_qty', 'ws_new_qty']],\n",
    "    on=['warehouse_id', 'product_id', 'packing_unit_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing values\n",
    "simulation_data['ws_price_filled'] = simulation_data['ws_new_price'].fillna(simulation_data['tier_2_price'])\n",
    "simulation_data['ws_qty_filled'] = simulation_data['ws_new_qty'].fillna(simulation_data['tier_2_qty'])\n",
    "\n",
    "# =============================================================================\n",
    "# Calculate NMV & GP per order for each tier\n",
    "# =============================================================================\n",
    "\n",
    "# Current state: All orders at base price with median quantity\n",
    "simulation_data['current_qty_per_order'] = simulation_data['median_qty']\n",
    "simulation_data['current_nmv_per_order'] = simulation_data['median_qty'] * simulation_data['packing_unit_price']\n",
    "simulation_data['current_cogs_per_order'] = simulation_data['median_qty'] * simulation_data['wac_p']\n",
    "simulation_data['current_gp_per_order'] = simulation_data['current_nmv_per_order'] - simulation_data['current_cogs_per_order']\n",
    "\n",
    "# Blended state: Orders distributed across tiers with corresponding quantities\n",
    "# Base orders: median_qty at packing_unit_price\n",
    "# T1 orders: tier_1_qty at tier_1_price\n",
    "# T2 orders: tier_2_qty at tier_2_price\n",
    "# WS orders: ws_qty at ws_price\n",
    "\n",
    "simulation_data['blended_qty_per_order'] = (\n",
    "    (simulation_data['base_pct'] / 100) * simulation_data['median_qty'] +\n",
    "    (simulation_data['tier1_pct'] / 100) * simulation_data['tier_1_qty'] +\n",
    "    (simulation_data['tier2_pct'] / 100) * simulation_data['tier_2_qty'] +\n",
    "    (simulation_data['ws_pct'] / 100) * simulation_data['ws_qty_filled']\n",
    ")\n",
    "\n",
    "simulation_data['blended_nmv_per_order'] = (\n",
    "    (simulation_data['base_pct'] / 100) * simulation_data['median_qty'] * simulation_data['packing_unit_price'] +\n",
    "    (simulation_data['tier1_pct'] / 100) * simulation_data['tier_1_qty'] * simulation_data['tier_1_price'] +\n",
    "    (simulation_data['tier2_pct'] / 100) * simulation_data['tier_2_qty'] * simulation_data['tier_2_price'] +\n",
    "    (simulation_data['ws_pct'] / 100) * simulation_data['ws_qty_filled'] * simulation_data['ws_price_filled']\n",
    ")\n",
    "\n",
    "simulation_data['blended_cogs_per_order'] = (\n",
    "    (simulation_data['base_pct'] / 100) * simulation_data['median_qty'] * simulation_data['wac_p'] +\n",
    "    (simulation_data['tier1_pct'] / 100) * simulation_data['tier_1_qty'] * simulation_data['wac_p'] +\n",
    "    (simulation_data['tier2_pct'] / 100) * simulation_data['tier_2_qty'] * simulation_data['wac_p'] +\n",
    "    (simulation_data['ws_pct'] / 100) * simulation_data['ws_qty_filled'] * simulation_data['wac_p']\n",
    ")\n",
    "\n",
    "simulation_data['blended_gp_per_order'] = simulation_data['blended_nmv_per_order'] - simulation_data['blended_cogs_per_order']\n",
    "\n",
    "# Calculate totals using actual order counts\n",
    "simulation_data['total_current_nmv'] = simulation_data['current_nmv_per_order'] * simulation_data['total_orders']\n",
    "simulation_data['total_current_cogs'] = simulation_data['current_cogs_per_order'] * simulation_data['total_orders']\n",
    "simulation_data['total_current_gp'] = simulation_data['total_current_nmv'] - simulation_data['total_current_cogs']\n",
    "simulation_data['total_current_qty'] = simulation_data['current_qty_per_order'] * simulation_data['total_orders']\n",
    "\n",
    "simulation_data['total_blended_nmv'] = simulation_data['blended_nmv_per_order'] * simulation_data['total_orders']\n",
    "simulation_data['total_blended_cogs'] = simulation_data['blended_cogs_per_order'] * simulation_data['total_orders']\n",
    "simulation_data['total_blended_gp'] = simulation_data['total_blended_nmv'] - simulation_data['total_blended_cogs']\n",
    "simulation_data['total_blended_qty'] = simulation_data['blended_qty_per_order'] * simulation_data['total_orders']\n",
    "\n",
    "# Calculate changes\n",
    "simulation_data['nmv_change'] = simulation_data['total_blended_nmv'] - simulation_data['total_current_nmv']\n",
    "simulation_data['qty_change'] = simulation_data['total_blended_qty'] - simulation_data['total_current_qty']\n",
    "simulation_data['gp_change'] = simulation_data['total_blended_gp'] - simulation_data['total_current_gp']\n",
    "\n",
    "# Margins\n",
    "simulation_data['current_margin'] = simulation_data['total_current_gp'] / simulation_data['total_current_nmv']\n",
    "simulation_data['blended_margin'] = simulation_data['total_blended_gp'] / simulation_data['total_blended_nmv']\n",
    "\n",
    "# =============================================================================\n",
    "# Summary Statistics\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"HISTORICAL SIMULATION - NMV, QUANTITY & GROSS PROFIT IMPACT\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "# Aggregate totals\n",
    "total_orders = simulation_data['total_orders'].sum()\n",
    "total_current_nmv = simulation_data['total_current_nmv'].sum()\n",
    "total_blended_nmv = simulation_data['total_blended_nmv'].sum()\n",
    "total_current_qty = simulation_data['total_current_qty'].sum()\n",
    "total_blended_qty = simulation_data['total_blended_qty'].sum()\n",
    "total_current_gp = simulation_data['total_current_gp'].sum()\n",
    "total_blended_gp = simulation_data['total_blended_gp'].sum()\n",
    "\n",
    "nmv_change = total_blended_nmv - total_current_nmv\n",
    "qty_change = total_blended_qty - total_current_qty\n",
    "gp_change = total_blended_gp - total_current_gp\n",
    "\n",
    "print(f\"\\nBased on {total_orders:,} historical orders across {len(simulation_data)} SKUs\")\n",
    "print(f\"Conversion: Base {overall_base_pct:.1f}% | T1 {overall_t1_pct:.1f}% | T2 {overall_t2_pct:.1f}% | WS {overall_ws_pct:.1f}%\")\n",
    "\n",
    "print(f\"\\n{'METRIC':<25} {'CURRENT':>18} {'BLENDED':>18} {'CHANGE':>18} {'CHANGE %':>12}\")\n",
    "print(\"-\" * 95)\n",
    "print(f\"{'Total NMV (EGP)':<25} {total_current_nmv:>17,.0f} {total_blended_nmv:>17,.0f} {nmv_change:>+17,.0f} {(nmv_change/total_current_nmv)*100:>+11.2f}%\")\n",
    "print(f\"{'Total Quantity (units)':<25} {total_current_qty:>17,.0f} {total_blended_qty:>17,.0f} {qty_change:>+17,.0f} {(qty_change/total_current_qty)*100:>+11.2f}%\")\n",
    "print(f\"{'Total COGS (EGP)':<25} {total_current_nmv-total_current_gp:>17,.0f} {total_blended_nmv-total_blended_gp:>17,.0f} {(total_blended_nmv-total_blended_gp)-(total_current_nmv-total_current_gp):>+17,.0f} {((total_blended_nmv-total_blended_gp)-(total_current_nmv-total_current_gp))/(total_current_nmv-total_current_gp)*100:>+11.2f}%\")\n",
    "print(f\"{'Total Gross Profit (EGP)':<25} {total_current_gp:>17,.0f} {total_blended_gp:>17,.0f} {gp_change:>+17,.0f} {(gp_change/total_current_gp)*100:>+11.2f}%\")\n",
    "print(f\"{'Gross Margin (%)':<25} {(total_current_gp/total_current_nmv)*100:>17.2f}% {(total_blended_gp/total_blended_nmv)*100:>17.2f}% {((total_blended_gp/total_blended_nmv)-(total_current_gp/total_current_nmv))*100:>+17.2f} pp\")\n",
    "\n",
    "# Warehouse breakdown with GP\n",
    "print(f\"\\n\" + \"=\" * 120)\n",
    "print(\"WAREHOUSE-LEVEL BREAKDOWN\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "warehouse_summary = simulation_data.groupby('warehouse_id').agg({\n",
    "    'total_orders': 'sum',\n",
    "    'base_orders': 'sum',\n",
    "    'tier1_orders': 'sum',\n",
    "    'tier2_orders': 'sum',\n",
    "    'ws_orders': 'sum',\n",
    "    'total_current_nmv': 'sum',\n",
    "    'total_blended_nmv': 'sum',\n",
    "    'total_current_gp': 'sum',\n",
    "    'total_blended_gp': 'sum',\n",
    "    'total_current_qty': 'sum',\n",
    "    'total_blended_qty': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "warehouse_summary['base_pct'] = (warehouse_summary['base_orders'] / warehouse_summary['total_orders'] * 100).round(1)\n",
    "warehouse_summary['tier1_pct'] = (warehouse_summary['tier1_orders'] / warehouse_summary['total_orders'] * 100).round(1)\n",
    "warehouse_summary['tier2_pct'] = (warehouse_summary['tier2_orders'] / warehouse_summary['total_orders'] * 100).round(1)\n",
    "warehouse_summary['ws_pct'] = (warehouse_summary['ws_orders'] / warehouse_summary['total_orders'] * 100).round(1)\n",
    "warehouse_summary['nmv_change'] = warehouse_summary['total_blended_nmv'] - warehouse_summary['total_current_nmv']\n",
    "warehouse_summary['nmv_change_pct'] = (warehouse_summary['nmv_change'] / warehouse_summary['total_current_nmv'] * 100).round(1)\n",
    "warehouse_summary['gp_change'] = warehouse_summary['total_blended_gp'] - warehouse_summary['total_current_gp']\n",
    "warehouse_summary['gp_change_pct'] = (warehouse_summary['gp_change'] / warehouse_summary['total_current_gp'] * 100).round(1)\n",
    "warehouse_summary['current_margin'] = (warehouse_summary['total_current_gp'] / warehouse_summary['total_current_nmv'] * 100).round(2)\n",
    "warehouse_summary['blended_margin'] = (warehouse_summary['total_blended_gp'] / warehouse_summary['total_blended_nmv'] * 100).round(2)\n",
    "\n",
    "print(f\"\\n{'WH':>5} {'Orders':>7} {'Base%':>6} {'T1%':>5} {'T2%':>5} {'WS%':>5} | {'NMV Δ':>12} {'NMV Δ%':>8} | {'GP Δ':>12} {'GP Δ%':>8} | {'Margin':>7}\")\n",
    "print(\"-\" * 110)\n",
    "\n",
    "for _, row in warehouse_summary.iterrows():\n",
    "    print(f\"{int(row['warehouse_id']):>5} {int(row['total_orders']):>7} \"\n",
    "          f\"{row['base_pct']:>5.1f}% {row['tier1_pct']:>4.1f}% {row['tier2_pct']:>4.1f}% {row['ws_pct']:>4.1f}% | \"\n",
    "          f\"{row['nmv_change']:>+11,.0f} {row['nmv_change_pct']:>+7.1f}% | \"\n",
    "          f\"{row['gp_change']:>+11,.0f} {row['gp_change_pct']:>+7.1f}% | \"\n",
    "          f\"{row['blended_margin']:>6.2f}%\")\n",
    "\n",
    "# Compare historical conversion to scenarios\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"COMPARISON: Historical vs Hypothetical Scenarios\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "# Calculate blended metrics using historical rates\n",
    "hist_metrics = calculate_blended_metrics_with_gp(\n",
    "    final_data, overall_base_pct, overall_t1_pct, overall_t2_pct, overall_ws_pct, NUM_ORDERS\n",
    ")\n",
    "\n",
    "print(f\"\\nHistorical Conversion ({overall_base_pct:.1f}/{overall_t1_pct:.1f}/{overall_t2_pct:.1f}/{overall_ws_pct:.1f}):\")\n",
    "print(f\"  NMV Change:      {hist_metrics['nmv_change_pct']:>+.2f}%\")\n",
    "print(f\"  Quantity Change: {hist_metrics['qty_change_pct']:>+.2f}%\")\n",
    "print(f\"  GP Change:       {hist_metrics['gp_change_pct']:>+.2f}%\")\n",
    "print(f\"  Blended Margin:  {hist_metrics['blended_margin']*100:.2f}%\")\n",
    "\n",
    "# Find closest scenario\n",
    "closest_scenario = None\n",
    "min_diff = float('inf')\n",
    "for scenario_name, (base, t1, t2, ws) in scenarios.items():\n",
    "    diff = abs(base - overall_base_pct) + abs(t1 - overall_t1_pct) + abs(t2 - overall_t2_pct) + abs(ws - overall_ws_pct)\n",
    "    if diff < min_diff:\n",
    "        min_diff = diff\n",
    "        closest_scenario = scenario_name\n",
    "\n",
    "print(f\"\\nClosest Hypothetical Scenario: {closest_scenario}\")\n",
    "\n",
    "# =============================================================================\n",
    "# WHAT-IF SCENARIO: Shift 30% from Base to Tiers\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"WHAT-IF SCENARIO: Move 30% of Base Orders to Tiers\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "# Calculate new conversion rates by shifting 30% from base to tiers\n",
    "SHIFT_PCT = 20  # Percentage points to shift from base\n",
    "\n",
    "# New base rate (reduced by 30 pp)\n",
    "new_base_pct = max(overall_base_pct - SHIFT_PCT, 0)\n",
    "\n",
    "# Distribute the shifted percentage to tiers proportionally\n",
    "# Based on existing tier ratios (excluding base)\n",
    "tier_total = overall_t1_pct + overall_t2_pct + overall_ws_pct\n",
    "\n",
    "if tier_total > 0:\n",
    "    # Distribute proportionally to existing tier distribution\n",
    "    t1_share = overall_t1_pct / tier_total\n",
    "    t2_share = overall_t2_pct / tier_total\n",
    "    ws_share = overall_ws_pct / tier_total\n",
    "    \n",
    "    new_t1_pct = overall_t1_pct + (SHIFT_PCT * t1_share)\n",
    "    new_t2_pct = overall_t2_pct + (SHIFT_PCT * t2_share)\n",
    "    new_ws_pct = overall_ws_pct + (SHIFT_PCT * ws_share)\n",
    "else:\n",
    "    # If no tier conversion exists, split evenly\n",
    "    new_t1_pct = overall_t1_pct + (SHIFT_PCT * 0.5)\n",
    "    new_t2_pct = overall_t2_pct + (SHIFT_PCT * 0.3)\n",
    "    new_ws_pct = overall_ws_pct + (SHIFT_PCT * 0.2)\n",
    "\n",
    "print(f\"\\nConversion Rate Comparison:\")\n",
    "print(f\"{'':>25} {'Historical':>15} {'What-If (+30%)':>15} {'Change':>12}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Base (no tier)':<25} {overall_base_pct:>14.1f}% {new_base_pct:>14.1f}% {new_base_pct - overall_base_pct:>+11.1f} pp\")\n",
    "print(f\"{'Tier 1':<25} {overall_t1_pct:>14.1f}% {new_t1_pct:>14.1f}% {new_t1_pct - overall_t1_pct:>+11.1f} pp\")\n",
    "print(f\"{'Tier 2':<25} {overall_t2_pct:>14.1f}% {new_t2_pct:>14.1f}% {new_t2_pct - overall_t2_pct:>+11.1f} pp\")\n",
    "print(f\"{'Wholesale':<25} {overall_ws_pct:>14.1f}% {new_ws_pct:>14.1f}% {new_ws_pct - overall_ws_pct:>+11.1f} pp\")\n",
    "print(f\"{'TOTAL':<25} {overall_base_pct + overall_t1_pct + overall_t2_pct + overall_ws_pct:>14.1f}% {new_base_pct + new_t1_pct + new_t2_pct + new_ws_pct:>14.1f}%\")\n",
    "\n",
    "# Calculate metrics for what-if scenario\n",
    "whatif_metrics = calculate_blended_metrics_with_gp(final_data, new_base_pct, new_t1_pct, new_t2_pct, new_ws_pct, NUM_ORDERS)\n",
    "\n",
    "# Also calculate using actual order count from historical data\n",
    "whatif_simulation = simulation_data.copy()\n",
    "\n",
    "# Recalculate with new conversion rates\n",
    "whatif_simulation['whatif_nmv_per_order'] = (\n",
    "    (new_base_pct / 100) * whatif_simulation['median_qty'] * whatif_simulation['packing_unit_price'] +\n",
    "    (new_t1_pct / 100) * whatif_simulation['tier_1_qty'] * whatif_simulation['tier_1_price'] +\n",
    "    (new_t2_pct / 100) * whatif_simulation['tier_2_qty'] * whatif_simulation['tier_2_price'] +\n",
    "    (new_ws_pct / 100) * whatif_simulation['ws_qty_filled'] * whatif_simulation['ws_price_filled']\n",
    ")\n",
    "\n",
    "whatif_simulation['whatif_cogs_per_order'] = (\n",
    "    (new_base_pct / 100) * whatif_simulation['median_qty'] * whatif_simulation['wac_p'] +\n",
    "    (new_t1_pct / 100) * whatif_simulation['tier_1_qty'] * whatif_simulation['wac_p'] +\n",
    "    (new_t2_pct / 100) * whatif_simulation['tier_2_qty'] * whatif_simulation['wac_p'] +\n",
    "    (new_ws_pct / 100) * whatif_simulation['ws_qty_filled'] * whatif_simulation['wac_p']\n",
    ")\n",
    "\n",
    "whatif_simulation['whatif_qty_per_order'] = (\n",
    "    (new_base_pct / 100) * whatif_simulation['median_qty'] +\n",
    "    (new_t1_pct / 100) * whatif_simulation['tier_1_qty'] +\n",
    "    (new_t2_pct / 100) * whatif_simulation['tier_2_qty'] +\n",
    "    (new_ws_pct / 100) * whatif_simulation['ws_qty_filled']\n",
    ")\n",
    "\n",
    "# Calculate totals with actual historical order counts\n",
    "whatif_simulation['total_whatif_nmv'] = whatif_simulation['whatif_nmv_per_order'] * whatif_simulation['total_orders']\n",
    "whatif_simulation['total_whatif_cogs'] = whatif_simulation['whatif_cogs_per_order'] * whatif_simulation['total_orders']\n",
    "whatif_simulation['total_whatif_gp'] = whatif_simulation['total_whatif_nmv'] - whatif_simulation['total_whatif_cogs']\n",
    "whatif_simulation['total_whatif_qty'] = whatif_simulation['whatif_qty_per_order'] * whatif_simulation['total_orders']\n",
    "\n",
    "# Aggregate totals\n",
    "total_whatif_nmv = whatif_simulation['total_whatif_nmv'].sum()\n",
    "total_whatif_qty = whatif_simulation['total_whatif_qty'].sum()\n",
    "total_whatif_gp = whatif_simulation['total_whatif_gp'].sum()\n",
    "\n",
    "# Calculate changes vs current and vs historical blended\n",
    "print(f\"\\n{'METRIC':<25} {'CURRENT':>15} {'HISTORICAL':>15} {'WHAT-IF':>15} {'vs Current':>12} {'vs Historical':>14}\")\n",
    "print(\"-\" * 110)\n",
    "print(f\"{'Total NMV (EGP)':<25} {total_current_nmv:>14,.0f} {total_blended_nmv:>14,.0f} {total_whatif_nmv:>14,.0f} {(total_whatif_nmv - total_current_nmv):>+11,.0f} {(total_whatif_nmv - total_blended_nmv):>+13,.0f}\")\n",
    "print(f\"{'Total Quantity':<25} {total_current_qty:>14,.0f} {total_blended_qty:>14,.0f} {total_whatif_qty:>14,.0f} {(total_whatif_qty - total_current_qty):>+11,.0f} {(total_whatif_qty - total_blended_qty):>+13,.0f}\")\n",
    "print(f\"{'Total Gross Profit':<25} {total_current_gp:>14,.0f} {total_blended_gp:>14,.0f} {total_whatif_gp:>14,.0f} {(total_whatif_gp - total_current_gp):>+11,.0f} {(total_whatif_gp - total_blended_gp):>+13,.0f}\")\n",
    "\n",
    "current_margin_pct = (total_current_gp / total_current_nmv) * 100\n",
    "historical_margin_pct = (total_blended_gp / total_blended_nmv) * 100\n",
    "whatif_margin_pct = (total_whatif_gp / total_whatif_nmv) * 100\n",
    "\n",
    "print(f\"{'Gross Margin':<25} {current_margin_pct:>13.2f}% {historical_margin_pct:>13.2f}% {whatif_margin_pct:>13.2f}% {(whatif_margin_pct - current_margin_pct):>+10.2f} pp {(whatif_margin_pct - historical_margin_pct):>+12.2f} pp\")\n",
    "\n",
    "# Summary comparison\n",
    "print(f\"\\n{'':=^110}\")\n",
    "print(f\"{'IMPACT SUMMARY':^110}\")\n",
    "print(f\"{'':=^110}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "If we shift {SHIFT_PCT}% of base orders to tiers:\n",
    "\n",
    "📈 vs CURRENT STATE (100% base):\n",
    "   • NMV increases by:          {(total_whatif_nmv - total_current_nmv):>+15,.0f} EGP  ({((total_whatif_nmv - total_current_nmv) / total_current_nmv) * 100:>+6.2f}%)\n",
    "   • Quantity increases by:     {(total_whatif_qty - total_current_qty):>+15,.0f} units ({((total_whatif_qty - total_current_qty) / total_current_qty) * 100:>+6.2f}%)\n",
    "   • Gross Profit changes by:   {(total_whatif_gp - total_current_gp):>+15,.0f} EGP  ({((total_whatif_gp - total_current_gp) / total_current_gp) * 100:>+6.2f}%)\n",
    "   • Margin changes:            {(whatif_margin_pct - current_margin_pct):>+15.2f} pp\n",
    "\n",
    "📊 vs HISTORICAL CONVERSION ({overall_base_pct:.0f}/{overall_t1_pct:.0f}/{overall_t2_pct:.0f}/{overall_ws_pct:.0f}):\n",
    "   • NMV additional:            {(total_whatif_nmv - total_blended_nmv):>+15,.0f} EGP  ({((total_whatif_nmv - total_blended_nmv) / total_blended_nmv) * 100:>+6.2f}%)\n",
    "   • Quantity additional:       {(total_whatif_qty - total_blended_qty):>+15,.0f} units ({((total_whatif_qty - total_blended_qty) / total_blended_qty) * 100:>+6.2f}%)\n",
    "   • GP additional:             {(total_whatif_gp - total_blended_gp):>+15,.0f} EGP  ({((total_whatif_gp - total_blended_gp) / total_blended_gp) * 100:>+6.2f}%)\n",
    "   • Margin change:             {(whatif_margin_pct - historical_margin_pct):>+15.2f} pp\n",
    "\"\"\")\n",
    "\n",
    "# Key insight\n",
    "print(\"=\" * 120)\n",
    "print(\"KEY INSIGHT\")\n",
    "print(\"=\" * 120)\n",
    "print(f\"\"\"\n",
    "Based on historical conversion rates:\n",
    "• NMV {'INCREASES' if nmv_change > 0 else 'DECREASES'} by {abs(nmv_change):,.0f} EGP ({(nmv_change/total_current_nmv)*100:+.1f}%)\n",
    "  → This is because retailers order MORE quantity to reach tier thresholds\n",
    "\n",
    "• Gross Profit {'INCREASES' if gp_change > 0 else 'DECREASES'} by {abs(gp_change):,.0f} EGP ({(gp_change/total_current_gp)*100:+.1f}%)\n",
    "  → {'Higher volume offsets lower price per unit' if gp_change > 0 else 'Lower prices reduce GP despite higher volume'}\n",
    "\n",
    "• Gross Margin changes from {(total_current_gp/total_current_nmv)*100:.2f}% to {(total_blended_gp/total_blended_nmv)*100:.2f}%\n",
    "  → {'Margin compression' if (total_blended_gp/total_blended_nmv) < (total_current_gp/total_current_nmv) else 'Margin improvement'} of {abs((total_blended_gp/total_blended_nmv)-(total_current_gp/total_current_nmv))*100:.2f} pp\n",
    "\n",
    "With additional {SHIFT_PCT}% conversion (What-If):\n",
    "• Additional NMV opportunity: {(total_whatif_nmv - total_blended_nmv):+,.0f} EGP\n",
    "• Additional GP opportunity:  {(total_whatif_gp - total_blended_gp):+,.0f} EGP\n",
    "\"\"\")\n",
    "\n",
    "# Save simulation results\n",
    "simulation_file = 'QD_simulation_results.xlsx'\n",
    "with pd.ExcelWriter(simulation_file, engine='openpyxl') as writer:\n",
    "    scenarios_df.to_excel(writer, sheet_name='Scenarios', index=False)\n",
    "    simulation_data.to_excel(writer, sheet_name='Historical_Simulation', index=False)\n",
    "    warehouse_summary.to_excel(writer, sheet_name='Warehouse_Summary', index=False)\n",
    "\n",
    "print(f\"✓ Simulation results saved to '{simulation_file}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ac640f",
   "metadata": {},
   "source": [
    "## 7. Final Ranking & Export\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ee57d5-5719-4d50-a45a-d3a3cbf79803",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CREATE UPLOAD FORMAT\n",
    "# =============================================================================\n",
    "# Format: ONE row per warehouse_id\n",
    "# - Discounts Group 1: List of [tier 1 items + wholesale items] (max 200, overflow goes to Group 2)\n",
    "# - Discounts Group 2: List of [tier 2 items + overflow from Group 1]\n",
    "# Each item format: [product_id, packing_unit_id, quantity, discount_pct]\n",
    "\n",
    "MAX_GROUP_SIZE = 200\n",
    "MAX_DISCOUNT_CAP = 6.0  # Maximum discount capped at 6%\n",
    "\n",
    "final_quantity_discount = pd.DataFrame(columns=['warehouse_id', 'Discounts Group 1', 'Discounts Group 2', 'Description'])\n",
    "\n",
    "for wh_id in final_data.warehouse_id.unique():\n",
    "    warehouse_data = final_data[final_data['warehouse_id'] == wh_id]\n",
    "    warehouse_id = int(wh_id)\n",
    "    \n",
    "    # Collect all tier 1 items\n",
    "    tier_1_items = []\n",
    "    # Collect all tier 2 items\n",
    "    tier_2_items = []\n",
    "    # Collect all wholesale items\n",
    "    ws_items = []\n",
    "    \n",
    "    for i, r in warehouse_data.iterrows():\n",
    "        product_id = int(r['product_id'])\n",
    "        packing_unit_id = int(r['packing_unit_id'])\n",
    "        current_price = r['packing_unit_price']\n",
    "        \n",
    "        # Tier 1 (cap discount at MAX_DISCOUNT_CAP)\n",
    "        q_1 = int(r['tier_1_qty'])\n",
    "        d_1 = min(round(r['discount_1_pct'], 2), MAX_DISCOUNT_CAP)\n",
    "        tier_1_items.append([product_id, packing_unit_id, q_1, d_1])\n",
    "        \n",
    "        # Tier 2 (cap discount at MAX_DISCOUNT_CAP)\n",
    "        q_2 = int(r['tier_2_qty'])\n",
    "        d_2 = min(round(r['discount_2_pct'], 2), MAX_DISCOUNT_CAP)\n",
    "        tier_2_items.append([product_id, packing_unit_id, q_2, d_2])\n",
    "        \n",
    "        # Wholesale (new logic) - cap discount at MAX_DISCOUNT_CAP\n",
    "        ws_qty = r.get('ws_new_qty', None)\n",
    "        ws_price = r.get('ws_new_price', None)\n",
    "        \n",
    "        if pd.notna(ws_qty) and pd.notna(ws_price) and ws_qty > 0 and current_price > 0:\n",
    "            q_ws = int(ws_qty)\n",
    "            d_ws = min(round(((current_price - ws_price) / current_price) * 100, 2), MAX_DISCOUNT_CAP)\n",
    "            ws_items.append([product_id, packing_unit_id, q_ws, d_ws])\n",
    "    \n",
    "    # Group 1: Tier 1 + Wholesale (max 200)\n",
    "    group_1_items = tier_1_items + ws_items\n",
    "    \n",
    "    # Group 2: Tier 2 + overflow from Group 1\n",
    "    if len(group_1_items) > MAX_GROUP_SIZE:\n",
    "        # Overflow goes to Group 2\n",
    "        overflow = group_1_items[MAX_GROUP_SIZE:]\n",
    "        group_1_items = group_1_items[:MAX_GROUP_SIZE]\n",
    "        group_2_items = tier_2_items + overflow\n",
    "    else:\n",
    "        group_2_items = tier_2_items\n",
    "    \n",
    "    new_row = {\n",
    "        'warehouse_id': warehouse_id,\n",
    "        'Discounts Group 1': group_1_items,\n",
    "        'Discounts Group 2': group_2_items,\n",
    "        'Description': f'{warehouse_id}QD'\n",
    "    }\n",
    "    final_quantity_discount = pd.concat([final_quantity_discount, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "# Summary\n",
    "print(f\"Upload format created: {len(final_quantity_discount)} warehouse rows\")\n",
    "print(f\"\\nPer warehouse breakdown:\")\n",
    "for idx, row in final_quantity_discount.iterrows():\n",
    "    wh = row['warehouse_id']\n",
    "    g1_count = len(row['Discounts Group 1'])\n",
    "    g2_count = len(row['Discounts Group 2'])\n",
    "    print(f\"  WH {wh}: Group 1 = {g1_count} items, Group 2 = {g2_count} items\")\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE FILES\n",
    "# =============================================================================\n",
    "\n",
    "# Save detailed data\n",
    "detailed_file = 'QD_detailed.xlsx'\n",
    "final_data.to_excel(detailed_file, index=False)\n",
    "print(f\"\\n=== DETAILED FILE ===\")\n",
    "print(f\"Saved {len(final_data)} SKUs to '{detailed_file}'\")\n",
    "\n",
    "# Save upload format\n",
    "upload_file = 'QD_Data.xlsx'\n",
    "final_quantity_discount.to_excel(upload_file, index=False)\n",
    "print(f\"\\n=== UPLOAD FILE ===\")\n",
    "print(f\"Saved {len(final_quantity_discount)} rows to '{upload_file}'\")\n",
    "print(f\"Columns: {list(final_quantity_discount.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72902879-a6b9-469a-aa6b-bd903e2fd8c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Warehouse to Tag ID mapping for upload\n",
    "df_warehouse_mapping = pd.DataFrame({\n",
    "    'warehouse_name': ['Assiut FC', 'Bani sweif', 'Barageel', 'El-Mahala', 'Khorshed Alex', \n",
    "                       'Mansoura FC', 'Menya Samalot', 'Mostorod', 'Sakkarah', 'Sharqya', \n",
    "                       'Sohag', 'Tanta'],\n",
    "    'warehouse_id':   [501, 401, 236, 337, 797, 339, 703, 1, 962, 170, 632, 8],\n",
    "    'tag_id':         [3301, 3302, 3303, 3304, 3305, 3306, 3307, 3308, 3309, 3310, 3311, 3312]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7d74cd-a60a-4778-b642-2d5c5723700e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge upload data with warehouse mapping\n",
    "to_upload = final_quantity_discount.merge(df_warehouse_mapping, on='warehouse_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe7d893-03eb-421e-b704-de77e375c062",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PREPARE FINAL UPLOAD FILE\n",
    "# =============================================================================\n",
    "\n",
    "# Set description and date/time fields\n",
    "to_upload['Description'] = to_upload['warehouse_name'].astype(str) + \"_QD\"\n",
    "\n",
    "start_date = datetime.now() + timedelta(minutes=10)\n",
    "start_date_str = start_date.strftime('%d/%m/%Y %H:%M')\n",
    "\n",
    "end_date = datetime.now() + timedelta(days=3)\n",
    "end_date = end_date.replace(hour=23, minute=59, second=0, microsecond=0)\n",
    "end_date_str = end_date.strftime('%d/%m/%Y %H:%M')\n",
    "\n",
    "to_upload['Start Date/Time'] = start_date_str\n",
    "to_upload['End Date/Time'] = end_date_str\n",
    "to_upload = to_upload.rename(columns={'tag_id': 'Tag ID'})\n",
    "\n",
    "# Aggregate by Tag ID\n",
    "to_upload = to_upload.groupby(\n",
    "    ['Tag ID', 'Description', 'Start Date/Time', 'End Date/Time'], \n",
    "    as_index=False\n",
    ").agg({\n",
    "    'Discounts Group 1': list,\n",
    "    'Discounts Group 2': list\n",
    "})\n",
    "\n",
    "# Save upload file\n",
    "to_upload.to_excel('QD_upload.xlsx', index=False)\n",
    "print(f\"✓ Saved upload file: QD_upload.xlsx ({len(to_upload)} warehouses)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1e89eb-6eba-4c3f-a7de-25433787171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# UPLOAD TO API\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Uploading QD file to API...\")\n",
    "response = post_QD('QD_upload.xlsx')\n",
    "\n",
    "if response.ok:\n",
    "    print(f\"✓ Upload succeeded (status: {response.status_code})\")\n",
    "else:\n",
    "    print(f\"❌ Upload failed (status: {response.status_code})\")\n",
    "    print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310698a1-a266-492a-a5cb-5e861571a55c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PREPARE CART RULES UPDATE\n",
    "# =============================================================================\n",
    "\n",
    "# Merge current cart rules with new tier data\n",
    "cart_rules_update = live_cart_rules.merge(\n",
    "    final_data[['warehouse_id', 'product_id', 'packing_unit_id', 'tier_2_qty', 'ws_new_qty']],\n",
    "    on=['warehouse_id', 'product_id', 'packing_unit_id']\n",
    ")\n",
    "cart_rules_update = cart_rules_update.fillna(0)\n",
    "\n",
    "# New cart rule = max of tier_2_qty and ws_new_qty\n",
    "cart_rules_update['tier_2'] = np.maximum(cart_rules_update['tier_2_qty'], cart_rules_update['ws_new_qty'])\n",
    "\n",
    "# Only update rules that need to increase\n",
    "cart_rules_update = cart_rules_update[cart_rules_update['tier_2'] > cart_rules_update['current_cart_rule']]\n",
    "cart_rules_update = cart_rules_update[['cohort_id', 'product_id', 'packing_unit_id', 'tier_2']]\n",
    "\n",
    "print(f\"✓ Cart rules to update: {len(cart_rules_update)} products across {cart_rules_update['cohort_id'].nunique()} cohorts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee06fc22-b921-4fad-bcb6-9a7c44e66cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# UPLOAD CART RULES BY COHORT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Uploading cart rules by cohort...\")\n",
    "\n",
    "for cohort in cart_rules_update.cohort_id.unique():\n",
    "    req_data = cart_rules_update[cart_rules_update['cohort_id'] == cohort]\n",
    "    \n",
    "    if len(req_data) > 0:\n",
    "        # Prepare data for upload\n",
    "        req_data = req_data[['product_id', 'packing_unit_id', 'tier_2']]\n",
    "        req_data.columns = ['Product ID', 'Packing Unit ID', 'Cart Rules']\n",
    "        \n",
    "        # Save and upload\n",
    "        filename = f'CartRules_{cohort}.xlsx'\n",
    "        req_data.to_excel(filename, index=False, engine='xlsxwriter')\n",
    "        \n",
    "        time.sleep(5)\n",
    "        response = post_cart_rules(cohort, filename)\n",
    "        \n",
    "        if response.ok:\n",
    "            print(f\"  ✓ Cohort {cohort}: {len(req_data)} rules uploaded\")\n",
    "        else:\n",
    "            print(f\"  ❌ Cohort {cohort}: Upload failed\")\n",
    "            print(response.content)\n",
    "            break\n",
    "\n",
    "print(\"\\n✓ Cart rules upload complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
