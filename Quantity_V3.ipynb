{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c83eefb",
   "metadata": {},
   "source": [
    "# Quantity Discount (QD) Pricing System\n",
    "\n",
    "This notebook calculates tiered pricing and quantities for products across warehouses.\n",
    "\n",
    "## Workflow:\n",
    "1. **Setup** - Imports, connections, and configuration\n",
    "2. **Product Selection** - Select top products per warehouse based on performance\n",
    "3. **Quantity Tiers** - Calculate tier 1 and tier 2 quantities based on order history\n",
    "4. **Market Prices** - Gather competitive pricing data\n",
    "5. **Price Tiers** - Calculate discounted prices for each tier\n",
    "6. **Wholesale Pricing** - Calculate wholesale prices for bulk orders\n",
    "7. **Export** - Save results to Excel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d33827a",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3d0f8d0-7f4a-4468-b218-49241a56edc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'oauth2client'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10786/966739998.py\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0moauth2client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice_account\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mServiceAccountCredentials\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msetup_environment_2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'oauth2client'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "import json\n",
    "from datetime import date, timedelta\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import setup_environment_2\n",
    "import importlib\n",
    "import import_ipynb\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import pytz  \n",
    "import os\n",
    "import snowflake.connector\n",
    "import boto3\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "importlib.reload(setup_environment_2)\n",
    "setup_environment_2.initialize_env()\n",
    "import base64\n",
    "from botocore.exceptions import ClientError\n",
    "from requests import get\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import time\n",
    "import gspread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0727d967",
   "metadata": {},
   "source": [
    "### Configuration Constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba7201f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - Modify these parameters as needed\n",
    "# =============================================================================\n",
    "\n",
    "# Cohort IDs for QD program\n",
    "COHORT_IDS = [700, 701, 702, 703, 704, 1123, 1124, 1125, 1126]\n",
    "\n",
    "# Warehouse mappings: (region, warehouse_name, warehouse_id, cohort_id)\n",
    "WAREHOUSE_MAPPING = [\n",
    "    ('Cairo', 'El-Marg', 38, 700),\n",
    "    ('Cairo', 'Mostorod', 1, 700),\n",
    "    ('Giza', 'Barageel', 236, 701),\n",
    "    ('Giza', 'Sakkarah', 962, 701),\n",
    "    ('Delta West', 'El-Mahala', 337, 703),\n",
    "    ('Delta West', 'Tanta', 8, 703),\n",
    "    ('Delta East', 'Mansoura FC', 339, 704),\n",
    "    ('Delta East', 'Sharqya', 170, 704),\n",
    "    ('Upper Egypt', 'Assiut FC', 501, 1124),\n",
    "    ('Upper Egypt', 'Bani sweif', 401, 1126),\n",
    "    ('Upper Egypt', 'Menya Samalot', 703, 1123),\n",
    "    ('Upper Egypt', 'Sohag', 632, 1125),\n",
    "    ('Alexandria', 'Khorshed Alex', 797, 702),\n",
    "]\n",
    "\n",
    "# Excluded warehouse IDs\n",
    "EXCLUDED_WAREHOUSES = [6, 9, 10]\n",
    "\n",
    "# Pricing parameters\n",
    "MAX_DISCOUNT_PCT = 5.0      # Maximum discount allowed from current price (%)\n",
    "MIN_DISCOUNT_PCT = 0.35     # Minimum discount required from current price (%)\n",
    "MIN_RATIO = 1.3             # Minimum discount-to-quantity ratio\n",
    "MAX_RATIO = 3             # Maximum discount-to-quantity ratio\n",
    "\n",
    "# Product selection thresholds\n",
    "MIN_ORDERS = 20             # Minimum orders in 4 months\n",
    "MIN_RETAILERS = 5           # Minimum unique retailers\n",
    "MIN_NMV = 5000              # Minimum revenue (EGP)\n",
    "MIN_VELOCITY = 0.5          # Minimum units per day\n",
    "\n",
    "# Ranking parameters\n",
    "TOP_PRODUCTS_PER_WAREHOUSE = 200   # Initial selection\n",
    "FINAL_PRODUCTS_PER_WAREHOUSE = 133 # Final output\n",
    "\n",
    "# Delivery fees\n",
    "DELIVERY_FEE_CAIRO_GIZA = 25\n",
    "DELIVERY_FEE_OTHER = 20\n",
    "\n",
    "print(\"Configuration loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2601556c",
   "metadata": {},
   "source": [
    "### Database Connection Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dda92e1-ffd3-4d47-80a7-b65fc9a64ee4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def snowflake_query(country, query, warehouse=None, columns=[], conn=None):\n",
    "    import snowflake.connector\n",
    "    \n",
    "    con = snowflake.connector.connect(\n",
    "        user =  os.environ[\"SNOWFLAKE_USERNAME\"],\n",
    "        account= os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "        password= os.environ[\"SNOWFLAKE_PASSWORD\"],\n",
    "        database =os.environ[\"SNOWFLAKE_DATABASE\"]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        cur = con.cursor()\n",
    "        cur.execute(\"USE WAREHOUSE COMPUTE_WH\")\n",
    "        cur.execute(query)\n",
    "        \n",
    "        column_names = [col[0] for col in cur.description]\n",
    "        \n",
    "        results = cur.fetchall()\n",
    "        \n",
    "        if not results:\n",
    "            out = pd.DataFrame(columns=[name.lower() for name in column_names])\n",
    "        else:\n",
    "            if len(columns) == 0:\n",
    "                out = pd.DataFrame(np.array(results), columns=column_names)\n",
    "                out.columns = out.columns.str.lower()\n",
    "            else:\n",
    "                out = pd.DataFrame(np.array(results), columns=columns)\n",
    "                out.columns = out.columns.str.lower()\n",
    "        \n",
    "        return out\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        cur.close()\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "febda4a3-f2e9-4601-b101-0dd290924eb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "SHOW PARAMETERS LIKE 'TIMEZONE'\n",
    "'''\n",
    "x  = snowflake_query(\"Egypt\",query)\n",
    "zone_to_use = x['value'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e25101",
   "metadata": {},
   "source": [
    "### Google Sheets Connection (Force Brands)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e0ae1ad-a98c-4565-89fd-bcb3376c8d84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "APIError",
     "evalue": "{'code': 500, 'message': 'Internal error encountered.', 'status': 'INTERNAL'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29478/3236670261.py\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mServiceAccountCredentials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_json_keyfile_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetup_environment_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_secret\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prod/maxab-sheets\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgspread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mforce_brands\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'QD_brands'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworksheet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Include_brands'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mforce_brands_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_brands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_brands_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/gspread/client.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, title, folder_id)\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mproperties\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"title\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mSpreadsheet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mSpreadsheetNotFound\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/gspread/spreadsheet.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, client, properties)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_properties\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch_sheet_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_properties\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"properties\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/gspread/spreadsheet.py\u001b[0m in \u001b[0;36mfetch_sheet_metadata\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSPREADSHEET_URL\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"get\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/gspread/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, endpoint, params, data, json, files, headers)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAPIError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_file_drive_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAPIError\u001b[0m: {'code': 500, 'message': 'Internal error encountered.', 'status': 'INTERNAL'}"
     ]
    }
   ],
   "source": [
    "# scope = [\"https://spreadsheets.google.com/feeds\",\n",
    "#          'https://www.googleapis.com/auth/spreadsheets',\n",
    "#          \"https://www.googleapis.com/auth/drive.file\",\n",
    "#          \"https://www.googleapis.com/auth/drive\"]\n",
    "# creds = ServiceAccountCredentials.from_json_keyfile_dict(json.loads(setup_environment_2.get_secret(\"prod/maxab-sheets\")), scope)\n",
    "# client = gspread.authorize(creds)\n",
    "# force_brands = client.open('QD_brands').worksheet('Include_brands')\n",
    "# force_brands_df = pd.DataFrame(force_brands.get_all_records())\n",
    "# if(force_brands_df.empty):\n",
    "#     force_brands_df = pd.DataFrame(columns=['brand'])\n",
    "#     brand_filter = \"\"\n",
    "# else:\n",
    "#     brand_filter = f\"OR brand IN ({','.join([repr(b) for b in list(force_brands_df.brand.unique())])})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5d55d8",
   "metadata": {},
   "source": [
    "## 2. Product Selection\n",
    "\n",
    "Select top-performing products per warehouse based on:\n",
    "- Gross profit ranking (40% weight)\n",
    "- Sales velocity ranking (25% weight)\n",
    "- Order count ranking (20% weight)\n",
    "- Retailer count ranking (15% weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "225abe85-5954-48b3-97ac-dd49d3672c38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = ''' \n",
    "WITH rr AS (\n",
    "    SELECT product_id, warehouse_id, rr\n",
    "    FROM (\n",
    "        SELECT *, \n",
    "               MAX(date) OVER (PARTITION BY product_id, warehouse_id) as max_date\n",
    "        FROM finance.PREDICTED_RUNNING_RATES\n",
    "        QUALIFY date = max_date\n",
    "            AND date::date >= CURRENT_DATE - 14 \n",
    "    )\n",
    "),\n",
    "\n",
    "stocks AS (\n",
    "    SELECT \n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        SUM(stocks) as stocks,\n",
    "        CASE \n",
    "            WHEN SUM(rr) > 0 THEN SUM(stocks) / SUM(rr) \n",
    "            ELSE SUM(stocks) \n",
    "        END as doh\n",
    "    FROM (\n",
    "        SELECT DISTINCT \n",
    "            product_warehouse.warehouse_id,\n",
    "            product_warehouse.product_id,\n",
    "            (product_warehouse.available_stock)::integer as stocks,\n",
    "            COALESCE(rr.rr, 0) as rr \n",
    "        FROM product_warehouse\n",
    "        JOIN products ON product_warehouse.product_id = products.id\n",
    "        JOIN product_units ON products.unit_id = product_units.id\n",
    "        LEFT JOIN rr ON rr.product_id = products.id \n",
    "            AND rr.warehouse_id = product_warehouse.warehouse_id\n",
    "        WHERE product_warehouse.warehouse_id NOT IN (6, 9, 10)\n",
    "            AND product_warehouse.is_basic_unit = 1\n",
    "            AND product_warehouse.available_stock > 0 \n",
    "    )\n",
    "    GROUP BY warehouse_id, product_id\n",
    "    HAVING doh >= 1\n",
    "),\n",
    "\n",
    "base AS (\n",
    "    SELECT *, ROW_NUMBER() OVER (PARTITION BY retailer_id ORDER BY priority) as rnk \n",
    "    FROM (\n",
    "        SELECT x.*, TAGGABLE_ID as retailer_id \n",
    "        FROM (\n",
    "            SELECT id as cohort_id, name as cohort_name, priority, dynamic_tag_id \n",
    "            FROM cohorts \n",
    "            WHERE is_active = 'true'\n",
    "                AND id IN (700,701,702,703,704,1123,1124,1125,1126)\n",
    "        ) x \n",
    "        JOIN DYNAMIC_TAGgables dt ON x.dynamic_tag_id = dt.dynamic_tag_id\n",
    "        WHERE dt.taggable_id not IN (\n",
    "            SELECT taggable_id FROM DYNAMIC_TAGgables \n",
    "            WHERE dynamic_tag_id IN (2807, 2808, 2809, 2810, 2811, 2812)\n",
    "        )\n",
    "    )\n",
    "    QUALIFY rnk = 1 \n",
    "    ORDER BY cohort_id\n",
    "),\n",
    "\n",
    "-- Count total retailers per warehouse for penetration calculation\n",
    "warehouse_retailer_counts AS (\n",
    "    SELECT \n",
    "        whs.warehouse_id,\n",
    "        COUNT(DISTINCT base.retailer_id) as total_warehouse_retailers\n",
    "    FROM base\n",
    "    CROSS JOIN (SELECT DISTINCT warehouse_id FROM (VALUES\n",
    "            (38), (1), (236), (962), (337), (8), (339), (170), \n",
    "            (501), (401), (703), (632), (797)\n",
    "        ) x(warehouse_id)\n",
    "    ) whs\n",
    "    GROUP BY whs.warehouse_id\n",
    "),\n",
    "\n",
    "-- Map cohorts to warehouses\n",
    "cohort_warehouse_map AS (\n",
    "    SELECT cohort_id, warehouse_id\n",
    "    FROM (VALUES\n",
    "        (700, 38),   -- Cairo -> El-Marg\n",
    "        (700, 1),    -- Cairo -> Mostorod\n",
    "        (701, 236),  -- Giza -> Barageel\n",
    "        (701, 962),  -- Giza -> Sakkarah\n",
    "        (703, 337),  -- Delta West -> El-Mahala\n",
    "        (703, 8),    -- Delta West -> Tanta\n",
    "        (704, 339),  -- Delta East -> Mansoura FC\n",
    "        (704, 170),  -- Delta East -> Sharqya\n",
    "        (1124, 501), -- Upper Egypt -> Assiut FC\n",
    "        (1126, 401), -- Upper Egypt -> Bani sweif\n",
    "        (1123, 703), -- Upper Egypt -> Menya Samalot\n",
    "        (1125, 632), -- Upper Egypt -> Sohag\n",
    "        (702, 797)   -- Alexandria -> Khorshed Alex\n",
    "    ) x(cohort_id, warehouse_id)\n",
    "),\n",
    "\n",
    "-- Get pricing information by cohort (which maps to warehouse)\n",
    "cohort_prices AS (\n",
    "    SELECT  \n",
    "        cpu.cohort_id,\n",
    "        pu.product_id,\n",
    "        pu.packing_unit_id,\n",
    "        pu.basic_unit_count,\n",
    "        AVG(cpu.price) as price\n",
    "    FROM cohort_product_packing_units cpu\n",
    "    JOIN PACKING_UNIT_PRODUCTS pu ON pu.id = cpu.product_packing_unit_id\n",
    "    WHERE cpu.cohort_id IN (700,701,702,703,704,1123,1124,1125,1126)\n",
    "        AND cpu.created_at::date <> '2023-07-31'\n",
    "        AND cpu.is_customized = true\n",
    "    GROUP BY \n",
    "        cpu.cohort_id,\n",
    "        pu.product_id,\n",
    "        pu.packing_unit_id,\n",
    "        pu.basic_unit_count\n",
    "),\n",
    "\n",
    "-- Get live prices by cohort\n",
    "live_cohort_prices AS (\n",
    "    SELECT \n",
    "        cohort_id,\n",
    "        product_id,\n",
    "        pu_id as packing_unit_id,\n",
    "        buc as basic_unit_count,\n",
    "        NEW_PRICE as price\n",
    "    FROM materialized_views.DBDP_PRICES\n",
    "    WHERE created_at = CURRENT_DATE\n",
    "        AND DATE_PART('hour', CURRENT_TIME) BETWEEN SPLIT_PART(time_slot, '-', 1)::int AND SPLIT_PART(time_slot, '-', 2)::int\n",
    "        AND cohort_id IN (700,701,702,703,704,1123,1124,1125,1126)\n",
    "),\n",
    "\n",
    "-- Combine live and historical prices (live takes priority)\n",
    "combined_cohort_prices AS (\n",
    "    SELECT *\n",
    "    FROM (\n",
    "        SELECT *, 1 AS priority FROM live_cohort_prices\n",
    "        UNION ALL\n",
    "        SELECT *, 2 AS priority FROM cohort_prices\n",
    "    )\n",
    "    QUALIFY ROW_NUMBER() OVER (PARTITION BY cohort_id, product_id, packing_unit_id ORDER BY priority) = 1\n",
    "),\n",
    "\n",
    "-- Map cohort prices to warehouse prices\n",
    "warehouse_prices AS (\n",
    "    SELECT \n",
    "        cwm.warehouse_id,\n",
    "        ccp.product_id,\n",
    "        ccp.packing_unit_id,\n",
    "        ccp.basic_unit_count,\n",
    "        ccp.price\n",
    "    FROM combined_cohort_prices ccp\n",
    "    JOIN cohort_warehouse_map cwm ON cwm.cohort_id = ccp.cohort_id\n",
    "    WHERE ccp.price IS NOT NULL\n",
    "),\n",
    "\n",
    "-- Get sales performance over last 4 months\n",
    "product_performance AS (\n",
    "    SELECT \n",
    "        w.name as warehouse,\n",
    "        w.id as warehouse_id,\n",
    "        pso.product_id,\n",
    "        pso.packing_unit_id,\n",
    "        CONCAT(products.name_ar, ' ', products.size, ' ', product_units.name_ar) as sku,\n",
    "        brands.name_ar as brand,\n",
    "        categories.name_ar as category,\n",
    "        \n",
    "        -- Core volume metrics\n",
    "        COUNT(DISTINCT so.parent_sales_order_id) as total_orders,\n",
    "        COUNT(DISTINCT so.retailer_id) as total_retailers,\n",
    "        SUM(pso.purchased_item_count) as total_packing_units_sold,\n",
    "        SUM(pso.purchased_item_count * pso.basic_unit_count) as total_basic_units_sold,\n",
    "        \n",
    "        -- Revenue and margin\n",
    "        SUM(pso.total_price) as total_nmv,\n",
    "        SUM(COALESCE(f.wac_p, 0) * pso.purchased_item_count * pso.basic_unit_count) as total_cogs,\n",
    "        (SUM(pso.total_price) - SUM(COALESCE(f.wac_p, 0) * pso.purchased_item_count * pso.basic_unit_count)) / \n",
    "            NULLIF(SUM(pso.total_price), 0) as blended_margin,\n",
    "        \n",
    "        -- Average order metrics\n",
    "        AVG(pso.purchased_item_count) as avg_packing_units_per_order,\n",
    "        \n",
    "        -- Velocity metrics (units per day)\n",
    "        SUM(pso.purchased_item_count) / 120.0 as packing_units_per_day\n",
    "        \n",
    "    FROM product_sales_order pso\n",
    "    JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "    JOIN base ON base.retailer_id = so.retailer_id\n",
    "    JOIN products ON products.id = pso.product_id\n",
    "    JOIN brands ON products.brand_id = brands.id\n",
    "    JOIN categories ON products.category_id = categories.id \n",
    "        AND categories.name_ar NOT LIKE '%سايب%'\n",
    "    JOIN finance.all_cogs f ON f.product_id = pso.product_id\n",
    "        AND f.from_date::date <= so.created_at::date\n",
    "        AND f.to_date::date > so.created_at::date\n",
    "    JOIN product_units ON product_units.id = products.unit_id\n",
    "\tjoin warehouses w on w.id = pso.warehouse_id\n",
    "    \n",
    "    WHERE TRUE\n",
    "        AND so.created_at::date BETWEEN current_date - 60 AND CURRENT_DATE - 1\n",
    "        AND so.sales_order_status_id NOT IN (7, 12)\n",
    "        AND so.channel IN ('telesales', 'retailer')\n",
    "        AND pso.purchased_item_count <> 0\n",
    "        AND products.activation = 'true'\n",
    "        AND w.id NOT IN (6, 9, 10)\n",
    "    \n",
    "    GROUP BY All\n",
    "),\n",
    "\n",
    "-- Add retailer penetration\n",
    "product_performance_with_penetration AS (\n",
    "    SELECT \n",
    "        pp.*,\n",
    "        wrc.total_warehouse_retailers,\n",
    "        (pp.total_retailers * 100.0 / NULLIF(wrc.total_warehouse_retailers, 0)) as retailer_penetration_pct\n",
    "    FROM product_performance pp\n",
    "    LEFT JOIN warehouse_retailer_counts wrc ON wrc.warehouse_id = pp.warehouse_id\n",
    "),\n",
    "\n",
    "-- Add pricing information at warehouse level\n",
    "product_performance_with_price AS (\n",
    "    SELECT \n",
    "        pp.*,\n",
    "        COALESCE(wp.price, 0) as product_price,\n",
    "        COALESCE(wp.basic_unit_count, 1) as basic_unit_count\n",
    "    FROM product_performance_with_penetration pp\n",
    "    LEFT JOIN warehouse_prices wp ON wp.warehouse_id = pp.warehouse_id\n",
    "        AND wp.product_id = pp.product_id \n",
    "        AND wp.packing_unit_id = pp.packing_unit_id\n",
    "),\n",
    "\n",
    "-- Add quality filters to focus on high-potential products\n",
    "qualified_products AS (\n",
    "    SELECT \n",
    "        pp.warehouse,\n",
    "        pp.warehouse_id,\n",
    "        pp.product_id,\n",
    "        pp.packing_unit_id,\n",
    "        pp.sku,\n",
    "        pp.brand,\n",
    "        pp.category,\n",
    "        pp.total_orders,\n",
    "        pp.total_retailers,\n",
    "        pp.total_packing_units_sold,\n",
    "        pp.total_basic_units_sold,\n",
    "        pp.total_nmv,\n",
    "        pp.blended_margin,\n",
    "        pp.avg_packing_units_per_order,\n",
    "        pp.packing_units_per_day,\n",
    "        pp.retailer_penetration_pct,\n",
    "        pp.product_price,\n",
    "        pp.basic_unit_count,\n",
    "        s.doh,\n",
    "        s.stocks,\n",
    "        \n",
    "        -- Calculate a simple volume-based score\n",
    "        (pp.total_nmv * pp.blended_margin) as gross_profit,\n",
    "        \n",
    "        -- Rank by gross profit within warehouse\n",
    "        ROW_NUMBER() OVER (PARTITION BY pp.warehouse_id ORDER BY (pp.total_nmv * pp.blended_margin) DESC) as gp_rank,\n",
    "        \n",
    "        -- Rank by velocity\n",
    "        ROW_NUMBER() OVER (PARTITION BY pp.warehouse_id ORDER BY pp.packing_units_per_day DESC) as velocity_rank,\n",
    "        \n",
    "        -- Rank by orders\n",
    "        ROW_NUMBER() OVER (PARTITION BY pp.warehouse_id ORDER BY pp.total_orders DESC) as order_rank,\n",
    "        \n",
    "        -- Rank by number of retailers\n",
    "        ROW_NUMBER() OVER (PARTITION BY pp.warehouse_id ORDER BY pp.total_retailers DESC) as retailer_rank\n",
    "        \n",
    "    FROM product_performance_with_price pp\n",
    "    JOIN stocks s ON s.product_id = pp.product_id \n",
    "        AND s.warehouse_id = pp.warehouse_id\n",
    "\n",
    "),\n",
    "\n",
    "-- Select top products using a combined scoring approach\n",
    "top_products AS (\n",
    "    SELECT \n",
    "        warehouse,\n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        sku,\n",
    "        brand,\n",
    "        category,\n",
    "        total_orders,\n",
    "        total_retailers,\n",
    "        total_packing_units_sold,\n",
    "        total_basic_units_sold,\n",
    "        ROUND(total_nmv, 2) as total_nmv,\n",
    "        ROUND(blended_margin * 100, 2) as margin_pct,\n",
    "        ROUND(avg_packing_units_per_order, 2) as avg_order_qty,\n",
    "        ROUND(packing_units_per_day, 2) as units_per_day,\n",
    "        ROUND(retailer_penetration_pct, 1) as retailer_penetration_pct,\n",
    "        ROUND(gross_profit, 2) as gross_profit,\n",
    "        ROUND(product_price, 2) as packing_unit_price,\n",
    "        basic_unit_count,\n",
    "        ROUND(product_price / NULLIF(basic_unit_count, 0), 2) as price_per_basic_unit,\n",
    "        gp_rank,\n",
    "        velocity_rank,\n",
    "        order_rank,\n",
    "        retailer_rank,\n",
    "        ROUND(doh, 2) as days_on_hand,\n",
    "        stocks as available_stock,\n",
    "        \n",
    "        -- Combined score: weighted average of ranks (lower is better)\n",
    "        (gp_rank * 0.15 + velocity_rank * 0.20 + order_rank * 0.30 + retailer_rank * 0.35) as combined_rank_score\n",
    "        \n",
    "    FROM qualified_products\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    warehouse,\n",
    "    warehouse_id,\n",
    "    product_id,\n",
    "    packing_unit_id,\n",
    "    sku,\n",
    "    brand,\n",
    "    category as cat,\n",
    "    total_orders,\n",
    "    total_retailers,\n",
    "    total_packing_units_sold,\n",
    "    total_basic_units_sold,\n",
    "    total_nmv,\n",
    "    margin_pct,\n",
    "    avg_order_qty,\n",
    "    units_per_day,\n",
    "    retailer_penetration_pct,\n",
    "    gross_profit,\n",
    "    packing_unit_price,\n",
    "    basic_unit_count,\n",
    "    price_per_basic_unit,\n",
    "    days_on_hand,\n",
    "    available_stock,\n",
    "    gp_rank as gross_profit_rank,\n",
    "    velocity_rank,\n",
    "    order_rank,\n",
    "    retailer_rank,\n",
    "    ROUND(combined_rank_score, 2) as combined_score,\n",
    "    ROW_NUMBER() OVER (PARTITION BY warehouse ORDER BY combined_rank_score) as final_rank\n",
    "FROM top_products\n",
    "WHERE combined_rank_score <= 500  -- Adjust this to get more/fewer products\n",
    "qualify final_rank<=200\n",
    "ORDER BY warehouse, combined_rank_score;\n",
    "'''\n",
    "selected_products = snowflake_query(\"Egypt\",query)\n",
    "for col in selected_products.columns:\n",
    "    selected_products[col] = pd.to_numeric(selected_products[col], errors='ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e424fc72",
   "metadata": {},
   "source": [
    "## 3. Quantity Tier Calculation\n",
    "\n",
    "Calculate tier 1 and tier 2 quantities based on:\n",
    "- Order history from frequent buyers (2+ orders)\n",
    "- Statistical analysis (median, Q3, P85, P90, P95)\n",
    "- IQR outlier removal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e5d7495-748e-4337-92ae-aaa77e3b8398",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_df = selected_products[['warehouse_id', 'product_id', 'packing_unit_id']].values.tolist()\n",
    "tuples_string = ','.join([f\"({int(wh_id)}, {int(prod_id)}, {int(pu_id)})\" for wh_id, prod_id, pu_id in selected_df])\n",
    "query = f'''\n",
    "WITH selected_products AS (\n",
    "    SELECT warehouse_id, product_id, packing_unit_id\n",
    "    FROM (VALUES\n",
    "      {tuples_string}\n",
    "    ) AS x(warehouse_id, product_id, packing_unit_id)\n",
    "),\n",
    "\n",
    "-- Same base filtering as product selection query\n",
    "-- Retailers in QD cohorts AND in specific dynamic tags\n",
    "base AS (\n",
    "    SELECT *, ROW_NUMBER() OVER (PARTITION BY retailer_id ORDER BY priority) as rnk \n",
    "    FROM (\n",
    "        SELECT x.*, TAGGABLE_ID as retailer_id \n",
    "        FROM (\n",
    "            SELECT id as cohort_id, name as cohort_name, priority, dynamic_tag_id \n",
    "            FROM cohorts \n",
    "            WHERE is_active = 'true'\n",
    "                AND id IN (700,701,702,703,704,1123,1124,1125,1126)\n",
    "        ) x \n",
    "        JOIN DYNAMIC_TAGgables dt ON x.dynamic_tag_id = dt.dynamic_tag_id\n",
    "        WHERE dt.taggable_id not IN (\n",
    "            SELECT taggable_id FROM DYNAMIC_TAGgables \n",
    "            WHERE dynamic_tag_id IN (2807, 2808, 2809, 2810, 2811, 2812)\n",
    "        )\n",
    "    )\n",
    "    QUALIFY rnk = 1 \n",
    "),\n",
    "\n",
    "raw_order_quantities AS (\n",
    "    SELECT \n",
    "        whs.wh as warehouse,\n",
    "        whs.warehouse_id,\n",
    "        pso.product_id,\n",
    "        pso.packing_unit_id,\n",
    "        CONCAT(products.name_ar, ' ', products.size, ' ', product_units.name_ar) as sku,\n",
    "        brands.name_ar as brand,\n",
    "        categories.name_ar as category,\n",
    "        so.parent_sales_order_id,\n",
    "        so.retailer_id,\n",
    "        so.created_at::date as order_date,\n",
    "        SUM(pso.purchased_item_count) as order_qty,\n",
    "        SUM(pso.total_price) as order_value,\n",
    "        -- ADD RECENCY WEIGHT: Recent orders get higher weight (exponential decay)\n",
    "        EXP(-0.02 * DATEDIFF('day', so.created_at::date, CURRENT_DATE)) as recency_weight\n",
    "        \n",
    "    FROM product_sales_order pso\n",
    "    JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "    -- Filter to only include retailers from base (same cohorts + tags as product selection)\n",
    "    JOIN base ON base.retailer_id = so.retailer_id\n",
    "    JOIN products ON products.id = pso.product_id\n",
    "    JOIN brands ON products.brand_id = brands.id\n",
    "    JOIN categories ON products.category_id = categories.id\n",
    "    JOIN product_units ON product_units.id = products.unit_id\n",
    "    JOIN materialized_views.retailer_polygon ON materialized_views.retailer_polygon.retailer_id = so.retailer_id\n",
    "    JOIN districts ON districts.id = materialized_views.retailer_polygon.district_id\n",
    "    JOIN cities ON cities.id = districts.city_id\n",
    "    JOIN states ON states.id = cities.state_id\n",
    "    JOIN regions ON regions.id = states.region_id\n",
    "    JOIN (SELECT * FROM (VALUES\n",
    "            ('Cairo', 'El-Marg', 38),\n",
    "            ('Cairo', 'Mostorod', 1),\n",
    "            ('Giza', 'Barageel', 236),\n",
    "            ('Giza', 'Sakkarah', 962),\n",
    "            ('Delta West', 'El-Mahala', 337),\n",
    "            ('Delta West', 'Tanta', 8),\n",
    "            ('Delta East', 'Mansoura FC', 339),\n",
    "            ('Delta East', 'Sharqya', 170),\n",
    "            ('Upper Egypt', 'Assiut FC', 501),\n",
    "            ('Upper Egypt', 'Bani sweif', 401),\n",
    "            ('Upper Egypt', 'Menya Samalot', 703),\n",
    "            ('Upper Egypt', 'Sohag', 632),\n",
    "            ('Alexandria', 'Khorshed Alex', 797)\n",
    "        ) x(region_name, wh, warehouse_id)\n",
    "    ) whs ON whs.region_name = CASE WHEN regions.id = 2 THEN states.name_en ELSE regions.name_en END\n",
    "    JOIN selected_products sp ON sp.warehouse_id = whs.warehouse_id \n",
    "        AND sp.product_id = pso.product_id\n",
    "        AND sp.packing_unit_id = pso.packing_unit_id\n",
    "    \n",
    "    WHERE TRUE\n",
    "        AND so.created_at::date BETWEEN DATE_TRUNC('month', CURRENT_DATE - INTERVAL '4 months') AND CURRENT_DATE - 1\n",
    "        AND so.sales_order_status_id NOT IN (7, 12)\n",
    "        AND so.channel IN ('telesales', 'retailer')\n",
    "        AND pso.purchased_item_count <> 0\n",
    "        AND products.activation = 'true'\n",
    "    \n",
    "    GROUP BY \n",
    "        whs.wh,\n",
    "        whs.warehouse_id,\n",
    "        pso.product_id,\n",
    "        pso.packing_unit_id,\n",
    "        products.name_ar,\n",
    "        products.size,\n",
    "        product_units.name_ar,\n",
    "        brands.name_ar,\n",
    "        categories.name_ar,\n",
    "        so.parent_sales_order_id,\n",
    "        so.retailer_id,\n",
    "        so.created_at::date\n",
    "),\n",
    "\n",
    "retailer_frequency AS (\n",
    "    SELECT \n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        retailer_id,\n",
    "        COUNT(DISTINCT parent_sales_order_id) as order_count,\n",
    "        COUNT(DISTINCT DATE_TRUNC('week', order_date)) as weeks_ordered,\n",
    "        MIN(order_date) as first_order_date,\n",
    "        MAX(order_date) as last_order_date,\n",
    "        DATEDIFF('day', MIN(order_date), MAX(order_date)) as days_span,\n",
    "        CASE \n",
    "            WHEN COUNT(DISTINCT parent_sales_order_id) > 1 \n",
    "            THEN DATEDIFF('day', MIN(order_date), MAX(order_date)) / (COUNT(DISTINCT parent_sales_order_id) - 1)\n",
    "            ELSE NULL \n",
    "        END as avg_days_between_orders\n",
    "    FROM raw_order_quantities\n",
    "    GROUP BY warehouse_id, product_id, packing_unit_id, retailer_id\n",
    "),\n",
    "\n",
    "frequent_buyers AS (\n",
    "    SELECT \n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        retailer_id,\n",
    "        order_count,\n",
    "        weeks_ordered,\n",
    "        avg_days_between_orders\n",
    "    FROM retailer_frequency\n",
    "    WHERE order_count >= 2 \n",
    "       OR weeks_ordered >= 2\n",
    "),\n",
    "\n",
    "filtered_orders AS (\n",
    "    SELECT roq.*\n",
    "    FROM raw_order_quantities roq\n",
    "    JOIN frequent_buyers fb \n",
    "        ON fb.warehouse_id = roq.warehouse_id\n",
    "        AND fb.product_id = roq.product_id\n",
    "        AND fb.packing_unit_id = roq.packing_unit_id\n",
    "        AND fb.retailer_id = roq.retailer_id\n",
    "),\n",
    "\n",
    "initial_stats AS (\n",
    "    SELECT \n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY order_qty) as q1,\n",
    "        PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY order_qty) as q3,\n",
    "        MEDIAN(order_qty) as median_qty,\n",
    "        STDDEV_POP(order_qty) as stddev_qty,\n",
    "        AVG(order_qty) as avg_qty\n",
    "    FROM filtered_orders\n",
    "    GROUP BY warehouse_id, product_id, packing_unit_id\n",
    "),\n",
    "\n",
    "cleaned_orders AS (\n",
    "    SELECT fo.*\n",
    "    FROM filtered_orders fo\n",
    "    JOIN initial_stats ist \n",
    "        ON ist.warehouse_id = fo.warehouse_id\n",
    "        AND ist.product_id = fo.product_id\n",
    "        AND ist.packing_unit_id = fo.packing_unit_id\n",
    "    WHERE TRUE\n",
    "        AND fo.order_qty >= ist.q1 - 1.5 * (ist.q3 - ist.q1)\n",
    "        AND fo.order_qty <= ist.q3 + 1.5 * (ist.q3 - ist.q1)\n",
    "        AND (ist.stddev_qty = 0 \n",
    "             OR ABS(fo.order_qty - ist.avg_qty) <= 3 * ist.stddev_qty)\n",
    "),\n",
    "\n",
    "-- MODIFIED: Recent orders stats (last 15 days)\n",
    "recent_trends AS (\n",
    "    SELECT \n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        -- Weighted average gives more importance to recent orders\n",
    "        SUM(order_qty * recency_weight) / NULLIF(SUM(recency_weight), 0) as weighted_avg_qty,\n",
    "        -- Last 15 days statistics\n",
    "        AVG(CASE WHEN order_date >= CURRENT_DATE - 15 THEN order_qty END) as last_15d_avg,\n",
    "        MEDIAN(CASE WHEN order_date >= CURRENT_DATE - 15 THEN order_qty END) as last_15d_median,\n",
    "        MAX(CASE WHEN order_date >= CURRENT_DATE - 15 THEN order_qty END) as last_15d_max,\n",
    "        COUNT(CASE WHEN order_date >= CURRENT_DATE - 15 THEN 1 END) as last_15d_orders\n",
    "    FROM cleaned_orders\n",
    "    GROUP BY warehouse_id, product_id, packing_unit_id\n",
    "),\n",
    "\n",
    "quantity_stats AS (\n",
    "    SELECT \n",
    "        warehouse,\n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        sku,\n",
    "        brand,\n",
    "        category,\n",
    "        \n",
    "        COUNT(DISTINCT parent_sales_order_id) as total_orders,\n",
    "        COUNT(DISTINCT retailer_id) as total_retailers,\n",
    "        \n",
    "        MIN(order_qty) as min_qty,\n",
    "        MAX(order_qty) as max_qty,\n",
    "        AVG(order_qty) as avg_qty,\n",
    "        MEDIAN(order_qty) as median_qty,\n",
    "        STDDEV_POP(order_qty) as stddev_qty,\n",
    "        \n",
    "        PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY order_qty) as q1_qty,\n",
    "        PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY order_qty) as q2_qty,\n",
    "        PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY order_qty) as q3_qty,\n",
    "        PERCENTILE_CONT(0.85) WITHIN GROUP (ORDER BY order_qty) as p85_qty,\n",
    "        PERCENTILE_CONT(0.90) WITHIN GROUP (ORDER BY order_qty) as p90_qty,\n",
    "        PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY order_qty) as p95_qty,\n",
    "        \n",
    "        SUM(order_value) as total_revenue,\n",
    "        AVG(order_value) as avg_order_value\n",
    "        \n",
    "    FROM cleaned_orders\n",
    "    GROUP BY \n",
    "        warehouse,\n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        sku,\n",
    "        brand,\n",
    "        category\n",
    "),\n",
    "\n",
    "frequency_table AS (\n",
    "    SELECT\n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        order_qty,\n",
    "        COUNT(DISTINCT parent_sales_order_id) AS freq\n",
    "    FROM cleaned_orders\n",
    "    GROUP BY warehouse_id, product_id, packing_unit_id, order_qty\n",
    "),\n",
    "\n",
    "lag_lead AS (\n",
    "    SELECT\n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        order_qty,\n",
    "        freq,\n",
    "        LAG(freq) OVER (PARTITION BY warehouse_id, product_id, packing_unit_id ORDER BY order_qty) AS prev_freq,\n",
    "        LEAD(freq) OVER (PARTITION BY warehouse_id, product_id, packing_unit_id ORDER BY order_qty) AS next_freq\n",
    "    FROM frequency_table\n",
    "),\n",
    "\n",
    "most_frequent_qty AS (\n",
    "    SELECT \n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        order_qty as mode_qty,\n",
    "        freq as mode_freq,\n",
    "        freq * 1.0 / SUM(freq) OVER (PARTITION BY warehouse_id, product_id, packing_unit_id) as mode_contribution\n",
    "    FROM (\n",
    "        SELECT *,\n",
    "               ROW_NUMBER() OVER (PARTITION BY warehouse_id, product_id, packing_unit_id ORDER BY freq DESC, order_qty DESC) as rn\n",
    "        FROM lag_lead\n",
    "        WHERE (freq > COALESCE(prev_freq, -1))\n",
    "          AND (freq > COALESCE(next_freq, -1))\n",
    "    )\n",
    "    WHERE rn = 1\n",
    "),\n",
    "\n",
    "frequency_metrics AS (\n",
    "    SELECT \n",
    "        fb.warehouse_id,\n",
    "        fb.product_id,\n",
    "        fb.packing_unit_id,\n",
    "        COUNT(DISTINCT fb.retailer_id) as frequent_retailer_count,\n",
    "        AVG(fb.order_count) as avg_orders_per_retailer,\n",
    "        AVG(fb.avg_days_between_orders) as avg_refill_days,\n",
    "        MEDIAN(fb.avg_days_between_orders) as median_refill_days\n",
    "    FROM frequent_buyers fb\n",
    "    GROUP BY fb.warehouse_id, fb.product_id, fb.packing_unit_id\n",
    "),\n",
    "\n",
    "tier_calculations AS (\n",
    "    SELECT \n",
    "        qs.*,\n",
    "        COALESCE(mf.mode_qty, qs.median_qty) as mode_qty,\n",
    "        COALESCE(mf.mode_freq, 0) as mode_freq,\n",
    "        COALESCE(mf.mode_contribution, 0) as mode_contribution,\n",
    "        COALESCE(fm.frequent_retailer_count, 0) as frequent_retailer_count,\n",
    "        COALESCE(fm.avg_orders_per_retailer, 0) as avg_orders_per_retailer,\n",
    "        COALESCE(fm.avg_refill_days, 0) as avg_refill_days,\n",
    "        COALESCE(fm.median_refill_days, 0) as median_refill_days,\n",
    "        \n",
    "        -- ADD: Recency metrics\n",
    "        rt.weighted_avg_qty,\n",
    "        rt.last_15d_avg,\n",
    "        rt.last_15d_median,\n",
    "        rt.last_15d_max,\n",
    "        rt.last_15d_orders,\n",
    "        \n",
    "        -- MODIFIED: Tier 1 with 15-day recency factor\n",
    "        -- Blends historical median with recent trends (70% historical, 30% recent)\n",
    "        CEIL(GREATEST(\n",
    "            (0.7 * qs.median_qty + 0.3 * COALESCE(rt.weighted_avg_qty, qs.median_qty)) + 1.0 * COALESCE(qs.stddev_qty, 1),\n",
    "            qs.q3_qty,\n",
    "            COALESCE(mf.mode_qty, qs.median_qty) + GREATEST(3, qs.median_qty * 0.3),\n",
    "            -- If recent 15 days show growth, adjust upward\n",
    "            CASE \n",
    "                WHEN rt.last_15d_orders >= 3 AND rt.last_15d_median > qs.median_qty \n",
    "                THEN rt.last_15d_median * 1.2\n",
    "                ELSE qs.median_qty * 1.4\n",
    "            END,\n",
    "            qs.median_qty + 3\n",
    "        )) as tier_1_qty,\n",
    "        \n",
    "        -- MODIFIED: Tier 2 with 15-day recency factor\n",
    "        CEIL(GREATEST(\n",
    "            qs.q3_qty + 1.5 * COALESCE(qs.stddev_qty, 1),\n",
    "            qs.p85_qty + 1.0 * COALESCE(qs.stddev_qty, 1),\n",
    "            qs.p90_qty + 0.5 * COALESCE(qs.stddev_qty, 1),\n",
    "            qs.p95_qty,\n",
    "            -- Blend historical and weighted average\n",
    "            (0.6 * qs.median_qty + 0.4 * COALESCE(rt.weighted_avg_qty, qs.median_qty)) * 2.0,\n",
    "            -- If last 15 days show higher demand, adjust tier 2 upward\n",
    "            CASE \n",
    "                WHEN rt.last_15d_orders >= 3 AND rt.last_15d_max > qs.p90_qty \n",
    "                THEN rt.last_15d_max * 1.1\n",
    "                ELSE qs.median_qty * 2.0\n",
    "            END\n",
    "        )) as tier_2_qty_base\n",
    "        \n",
    "    FROM quantity_stats qs\n",
    "    LEFT JOIN most_frequent_qty mf \n",
    "        ON mf.warehouse_id = qs.warehouse_id \n",
    "        AND mf.product_id = qs.product_id\n",
    "        AND mf.packing_unit_id = qs.packing_unit_id\n",
    "    LEFT JOIN frequency_metrics fm\n",
    "        ON fm.warehouse_id = qs.warehouse_id\n",
    "        AND fm.product_id = qs.product_id\n",
    "        AND fm.packing_unit_id = qs.packing_unit_id\n",
    "    LEFT JOIN recent_trends rt\n",
    "        ON rt.warehouse_id = qs.warehouse_id\n",
    "        AND rt.product_id = qs.product_id\n",
    "        AND rt.packing_unit_id = qs.packing_unit_id\n",
    "),\n",
    "\n",
    "tier_adjustments AS (\n",
    "    SELECT \n",
    "        warehouse,\n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        sku,\n",
    "        brand,\n",
    "        category,\n",
    "        total_orders,\n",
    "        total_retailers,\n",
    "        min_qty,\n",
    "        avg_qty,\n",
    "        median_qty,\n",
    "        stddev_qty,\n",
    "        q1_qty,\n",
    "        q3_qty,\n",
    "        p85_qty,\n",
    "        p90_qty,\n",
    "        p95_qty,\n",
    "        max_qty,\n",
    "        mode_qty,\n",
    "        mode_freq,\n",
    "        mode_contribution,\n",
    "        frequent_retailer_count,\n",
    "        avg_orders_per_retailer,\n",
    "        avg_refill_days,\n",
    "        median_refill_days,\n",
    "        total_revenue,\n",
    "        avg_order_value,\n",
    "        \n",
    "        -- ADD: Recency metrics to output\n",
    "        weighted_avg_qty,\n",
    "        last_15d_avg,\n",
    "        last_15d_median,\n",
    "        last_15d_max,\n",
    "        last_15d_orders,\n",
    "        \n",
    "        tier_1_qty,\n",
    "        LEAST(\n",
    "            CEIL(GREATEST(\n",
    "                tier_2_qty_base,\n",
    "                tier_1_qty * 1.6\n",
    "            )),\n",
    "            GREATEST(\n",
    "                tier_1_qty * 3.5,\n",
    "                tier_1_qty + 20\n",
    "            )\n",
    "        ) as tier_2_qty\n",
    "        \n",
    "    FROM tier_calculations\n",
    "),\n",
    "\n",
    "retailer_distribution AS (\n",
    "    SELECT \n",
    "        co.warehouse_id,\n",
    "        co.product_id,\n",
    "        co.packing_unit_id,\n",
    "        ta.tier_1_qty,\n",
    "        ta.tier_2_qty,\n",
    "        COUNT(DISTINCT CASE \n",
    "            WHEN co.order_qty < ta.tier_1_qty THEN co.retailer_id \n",
    "        END) as retailers_below_t1,\n",
    "        COUNT(DISTINCT CASE \n",
    "            WHEN co.order_qty >= ta.tier_1_qty AND co.order_qty < ta.tier_2_qty THEN co.retailer_id \n",
    "        END) as retailers_at_t1,\n",
    "        COUNT(DISTINCT CASE \n",
    "            WHEN co.order_qty >= ta.tier_2_qty THEN co.retailer_id \n",
    "        END) as retailers_at_t2,\n",
    "        COUNT(CASE \n",
    "            WHEN co.order_qty < ta.tier_1_qty THEN 1 \n",
    "        END) as orders_below_t1,\n",
    "        COUNT(CASE \n",
    "            WHEN co.order_qty >= ta.tier_1_qty AND co.order_qty < ta.tier_2_qty THEN 1 \n",
    "        END) as orders_at_t1,\n",
    "        COUNT(CASE \n",
    "            WHEN co.order_qty >= ta.tier_2_qty THEN 1 \n",
    "        END) as orders_at_t2\n",
    "    FROM cleaned_orders co\n",
    "    JOIN tier_adjustments ta \n",
    "        ON ta.warehouse_id = co.warehouse_id \n",
    "        AND ta.product_id = co.product_id\n",
    "        AND ta.packing_unit_id = co.packing_unit_id\n",
    "    GROUP BY \n",
    "        co.warehouse_id,\n",
    "        co.product_id,\n",
    "        co.packing_unit_id,\n",
    "        ta.tier_1_qty,\n",
    "        ta.tier_2_qty\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    ta.warehouse,\n",
    "    ta.warehouse_id,\n",
    "    ta.product_id,\n",
    "    ta.packing_unit_id,\n",
    "    ta.sku,\n",
    "    ta.brand,\n",
    "    ta.category,\n",
    "    \n",
    "    ta.frequent_retailer_count,\n",
    "    ROUND(ta.avg_orders_per_retailer, 2) as avg_orders_per_retailer,\n",
    "    ROUND(ta.avg_refill_days, 1) as avg_refill_days,\n",
    "    ROUND(ta.median_refill_days, 1) as median_refill_days,\n",
    "    \n",
    "    ta.total_orders,\n",
    "    ta.total_retailers,\n",
    "    \n",
    "    ta.min_qty,\n",
    "    ROUND(ta.avg_qty, 2) as avg_qty,\n",
    "    ta.median_qty,\n",
    "    ROUND(ta.weighted_avg_qty, 2) as weighted_avg_qty,\n",
    "    ta.q1_qty as q1_25_qty,\n",
    "    ta.q3_qty as q3_75_qty,\n",
    "    ta.p85_qty,\n",
    "    ta.p90_qty,\n",
    "    ta.p95_qty,\n",
    "    ta.max_qty,\n",
    "    ROUND(ta.stddev_qty, 2) as stddev_qty,\n",
    "    ta.mode_qty,\n",
    "    ta.mode_freq,\n",
    "    ROUND(ta.mode_contribution * 100, 1) as mode_pct,\n",
    "    \n",
    "    -- MODIFIED: 15-day trend metrics\n",
    "    ROUND(ta.last_15d_avg, 2) as last_15d_avg,\n",
    "    ta.last_15d_median,\n",
    "    ta.last_15d_max,\n",
    "    ta.last_15d_orders,\n",
    "    \n",
    "    ta.tier_1_qty,\n",
    "    ta.tier_2_qty,\n",
    "    ROUND((ta.tier_1_qty - ta.median_qty) * 100.0 / NULLIF(ta.median_qty, 0), 1) as tier_1_increase_pct,\n",
    "    ROUND((ta.tier_2_qty - ta.median_qty) * 100.0 / NULLIF(ta.median_qty, 0), 1) as tier_2_increase_pct,\n",
    "    ROUND(ta.tier_2_qty * 1.0 / NULLIF(ta.tier_1_qty, 0), 2) as tier_2_to_tier_1_ratio,\n",
    "    \n",
    "    rd.retailers_below_t1,\n",
    "    rd.retailers_at_t1,\n",
    "    rd.retailers_at_t2,\n",
    "    \n",
    "    rd.orders_below_t1,\n",
    "    rd.orders_at_t1,\n",
    "    rd.orders_at_t2,\n",
    "    \n",
    "    ROUND(100.0 * rd.retailers_below_t1 / NULLIF(ta.total_retailers, 0), 1) as pct_retailers_below_t1,\n",
    "    ROUND(100.0 * rd.retailers_at_t1 / NULLIF(ta.total_retailers, 0), 1) as pct_retailers_at_t1,\n",
    "    ROUND(100.0 * rd.retailers_at_t2 / NULLIF(ta.total_retailers, 0), 1) as pct_retailers_at_t2,\n",
    "    \n",
    "    ROUND(100.0 * rd.orders_below_t1 / NULLIF(ta.total_orders, 0), 1) as pct_orders_below_t1,\n",
    "    ROUND(100.0 * rd.orders_at_t1 / NULLIF(ta.total_orders, 0), 1) as pct_orders_at_t1,\n",
    "    ROUND(100.0 * rd.orders_at_t2 / NULLIF(ta.total_orders, 0), 1) as pct_orders_at_t2,\n",
    "    \n",
    "    ROUND(ta.total_revenue, 2) as total_revenue,\n",
    "    ROUND(ta.avg_order_value, 2) as avg_order_value\n",
    "\n",
    "FROM tier_adjustments ta\n",
    "JOIN retailer_distribution rd \n",
    "    ON rd.warehouse_id = ta.warehouse_id \n",
    "    AND rd.product_id = ta.product_id\n",
    "    AND rd.packing_unit_id = ta.packing_unit_id\n",
    "ORDER BY ta.warehouse, ta.total_orders DESC\n",
    "'''\n",
    "tiers_selection = snowflake_query(\"Egypt\",query)\n",
    "for col in tiers_selection.columns:\n",
    "    tiers_selection[col] = pd.to_numeric(tiers_selection[col], errors='ignore') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc05aac1",
   "metadata": {},
   "source": [
    "### SKU Information & Cost Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8045f25-1b32-437c-81e8-a04f9771908f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f'''\n",
    "SELECT  DIStinct  \n",
    "\t\tproducts.id as product_id,\n",
    "\t\tCONCAT(products.name_ar,' ',products.size,' ',product_units.name_ar) as sku,\n",
    "\t\tbrands.name_ar as brand, \n",
    "\t\tcategories.name_ar as cat,\n",
    "\t\tf.wac_p\n",
    "from products \n",
    "JOIN brands on products.brand_id = brands.id \n",
    "JOIN categories ON products.category_id = categories.id\n",
    "JOIN finance.all_cogs f  ON f.product_id = products.id and CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMEstamp()) between f.from_date and f.to_date \n",
    "JOIN product_units ON product_units.id = products.unit_id \n",
    "'''\n",
    "sku_info  = snowflake_query(\"Egypt\",query)\n",
    "sku_info.product_id=pd.to_numeric(sku_info.product_id)\n",
    "sku_info.wac_p=pd.to_numeric(sku_info.wac_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ff38bd-bbc5-4ca6-b152-c283255d067a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Market Prices\n",
    "\n",
    "Gather competitive pricing data from multiple sources:\n",
    "- **Marketplace prices** - Regional marketplace data with fallbacks\n",
    "- **Ben Soliman prices** - Competitor pricing\n",
    "- **Scraped prices** - Web-scraped competitor data\n",
    "- **Product statistics** - Historical margin boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd94d09",
   "metadata": {},
   "source": [
    "### 4.1 Marketplace Prices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "873a9cfe-76fb-486c-b71c-b2c3c447b7af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f'''\n",
    "WITH whs as (SELECT *\n",
    "             FROM   (values\n",
    "                            ('Cairo', 'El-Marg', 38,700),\n",
    "                            ('Cairo', 'Mostorod', 1,700),\n",
    "                            ('Giza', 'Barageel', 236,701),\n",
    "                            ('Delta West', 'El-Mahala', 337,703),\n",
    "                            ('Delta West', 'Tanta', 8,703),\n",
    "                            ('Delta East', 'Mansoura FC', 339,704),\n",
    "                            ('Delta East', 'Sharqya', 170,704),\n",
    "                            ('Upper Egypt', 'Assiut FC', 501,1124),\n",
    "                            ('Upper Egypt', 'Bani sweif', 401,1126),\n",
    "                            ('Upper Egypt', 'Menya Samalot', 703,1123),\n",
    "                            ('Upper Egypt', 'Sohag', 632,1125),\n",
    "                            ('Alexandria', 'Khorshed Alex', 797,702),\n",
    "\t\t\t\t\t\t\t('Giza', 'Sakkarah', 962,701)\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t)\n",
    "                    x(region, wh, warehouse_id,cohort_id)),\n",
    "full_data as (\n",
    "select products.id as product_id, region,warehouse_id\n",
    "from products , whs \n",
    "where activation = 'true'\n",
    "),\t\t\t\t\n",
    "\n",
    "MP as (\n",
    "select region,product_id,\n",
    "min(min_price) as min_price,\n",
    "min(max_price) as max_price,\n",
    "min(mod_price) as mod_price,\n",
    "min(true_min) as true_min,\n",
    "min(true_max) as true_max\n",
    "\n",
    "from (\n",
    "select mp.region,mp.product_id,mp.pu_id,\n",
    "min_price/BASIC_UNIT_COUNT as min_price,\n",
    "max_price/BASIC_UNIT_COUNT as max_price,\n",
    "mod_price/BASIC_UNIT_COUNT as mod_price,\n",
    "TRUE_MIN_PRICE/BASIC_UNIT_COUNT as true_min,\n",
    "TRUE_MAX_PRICE/BASIC_UNIT_COUNT as true_max\n",
    "from materialized_views.marketplace_prices mp \n",
    "join packing_unit_products pup on pup.product_id = mp.product_id and pup.packing_unit_id = mp.pu_id\n",
    "join finance.all_cogs f on f.product_id = mp.product_id and CURRENT_TIMESTAMP between f.from_date and f.to_date\n",
    "where  least(min_price,mod_price) between wac_p*0.9 and wac_p*1.3 \n",
    ")\n",
    "group by all \n",
    "),\n",
    "region_mapping AS (\n",
    "    SELECT * \n",
    "\tFROM \n",
    "\t(\tVALUES\n",
    "        ('Delta East', 'Delta West'),\n",
    "        ('Delta West', 'Delta East'),\n",
    "        ('Alexandria', 'Cairo'),\n",
    "        ('Alexandria', 'Giza'),\n",
    "        ('Upper Egypt', 'Cairo'),\n",
    "        ('Upper Egypt', 'Giza'),\n",
    "\t\t('Cairo','Giza'),\n",
    "\t\t('Giza','Cairo'),\n",
    "\t\t('Delta West', 'Cairo'),\n",
    "\t\t('Delta East', 'Cairo'),\n",
    "\t\t('Delta West', 'Giza'),\n",
    "\t\t('Delta East', 'Giza')\n",
    "\t\t)\n",
    "    AS region_mapping(region, fallback_region)\n",
    ")\n",
    "\n",
    "\n",
    "select region,warehouse_id,product_id,\n",
    "min(final_min_price) as final_min_price,\n",
    "min(final_max_price) as final_max_price,\n",
    "min(final_mod_price) as final_mod_price,\n",
    "min(final_true_min) as final_true_min,\n",
    "min(final_true_max) as final_true_max\n",
    "\n",
    "from (\n",
    "SELECT\n",
    "distinct \n",
    "\tw.region,\n",
    "    w.warehouse_id,\n",
    "\tw.product_id,\n",
    "    COALESCE(m1.min_price, m2.min_price) AS final_min_price,\n",
    "    COALESCE(m1.max_price, m2.max_price) AS final_max_price,\n",
    "    COALESCE(m1.mod_price, m2.mod_price) AS final_mod_price,\n",
    "\tCOALESCE(m1.true_min, m2.true_min) AS final_true_min,\n",
    "\tCOALESCE(m1.true_max, m2.true_max) AS final_true_max,\n",
    "FROM full_data w\n",
    "LEFT JOIN MP m1\n",
    "    ON w.region = m1.region and w.product_id = m1.product_id\n",
    "JOIN region_mapping rm\n",
    "    ON w.region = rm.region\n",
    "LEFT JOIN MP m2\n",
    "    ON rm.fallback_region = m2.region\n",
    "   AND w.product_id = m2.product_id\n",
    ")\n",
    "where final_min_price is not null \n",
    "group by all \n",
    "'''\n",
    "marketplace = snowflake_query(\"Egypt\",query)\n",
    "marketplace.columns = marketplace.columns.str.lower()\n",
    "for col in marketplace.columns:\n",
    "    marketplace[col] = pd.to_numeric(marketplace[col], errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd2f7ff",
   "metadata": {},
   "source": [
    "### 4.2 Ben Soliman (Competitor) Prices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ede9cdec-a69d-4579-afcc-57df08a5f753",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f'''\n",
    "select z.* \n",
    "from (\n",
    "select maxab_product_id as product_id,avg(bs_final_price) as ben_soliman_price\n",
    "from (\n",
    "select * , row_number()over(partition by maxab_product_id order by diff) as rnk_2\n",
    "from (\n",
    "select *,(bs_final_price-wac_p)/wac_p as diff_2\n",
    "from (\n",
    "select * ,bs_price/maxab_basic_unit_count as bs_final_price\n",
    "from (\n",
    "select *,row_number()over(partition by maxab_product_id,maxab_pu order by diff) as rnk \n",
    "from (\n",
    "select sm.* ,max(INJECTION_DATE::date)over(partition by maxab_product_id,maxab_pu) as max_date,wac1,wac_p,abs(bs_price-(wac_p*maxab_basic_unit_count))/(wac_p*maxab_basic_unit_count) as diff \n",
    "from materialized_views.savvy_mapping sm \n",
    "join finance.all_cogs f on f.product_id = sm.maxab_product_id and current_timestamp between f.from_Date and f.to_date\n",
    "where bs_price is not null \n",
    "and INJECTION_DATE::date >= CURRENT_DATE- 5\n",
    "qualify INJECTION_DATE::date = max_date\n",
    ")\n",
    "qualify rnk = 1 \n",
    ")\n",
    ")\n",
    "where diff_2 between -0.5 and 0.5 \n",
    ")\n",
    "qualify rnk_2 = 1 \n",
    ")\n",
    "group by all\n",
    ")z \n",
    "join finance.all_cogs f on f.product_id = z.product_id and current_timestamp between f.from_Date and f.to_date\n",
    "\n",
    "where ben_soliman_price between f.wac_p*0.9 and f.wac_p*1.3\n",
    "'''\n",
    "\n",
    "bensoliman =  snowflake_query(\"Egypt\",query)\n",
    "bensoliman.columns = bensoliman.columns.str.lower()\n",
    "for col in bensoliman.columns:\n",
    "    bensoliman[col] = pd.to_numeric(bensoliman[col], errors='ignore')         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd5c097",
   "metadata": {},
   "source": [
    "### 4.3 Scraped Competitor Prices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98943949-f930-4b02-aaf6-1e20144c674e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f'''\n",
    "WITH whs as (SELECT *\n",
    "             FROM   (values\n",
    "                            ('Cairo', 'El-Marg', 38,700),\n",
    "                            ('Cairo', 'Mostorod', 1,700),\n",
    "                            ('Giza', 'Barageel', 236,701),\n",
    "                            ('Delta West', 'El-Mahala', 337,703),\n",
    "                            ('Delta West', 'Tanta', 8,703),\n",
    "                            ('Delta East', 'Mansoura FC', 339,704),\n",
    "                            ('Delta East', 'Sharqya', 170,704),\n",
    "                            ('Upper Egypt', 'Assiut FC', 501,1124),\n",
    "                            ('Upper Egypt', 'Bani sweif', 401,1126),\n",
    "                            ('Upper Egypt', 'Menya Samalot', 703,1123),\n",
    "                            ('Upper Egypt', 'Sohag', 632,1125),\n",
    "                            ('Alexandria', 'Khorshed Alex', 797,702),\n",
    "\t\t\t\t\t\t\t('Giza', 'Sakkarah', 962,701)\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t)\n",
    "                    x(region, wh, warehouse_id,cohort_id))\n",
    "select product_id,x.region,warehouse_id,min(MARKET_PRICE) as min_scrapped,max(MARKET_PRICE) as max_scrapped,median(MARKET_PRICE) as median_scrapped\n",
    "from (\n",
    "select MATERIALIZED_VIEWS.CLEANED_MARKET_PRICES.*,max(date)over(partition by region,MATERIALIZED_VIEWS.CLEANED_MARKET_PRICES.product_id,competitor) as max_date\n",
    "from MATERIALIZED_VIEWS.CLEANED_MARKET_PRICES\n",
    "join finance.all_cogs f on f.product_id = MATERIALIZED_VIEWS.CLEANED_MARKET_PRICES.product_id and CURRENT_TIMESTAMP between f.from_date and f.to_date \n",
    "where date>= current_date -5\n",
    "and MARKET_PRICE between f.wac_p * 0.9 and wac_p*1.3\n",
    "qualify date = max_date \n",
    ") x \n",
    "left join whs on whs.region = x.region\n",
    "group by all \n",
    "'''\n",
    "scrapped_prices = snowflake_query(\"Egypt\",query)\n",
    "scrapped_prices.columns = scrapped_prices.columns.str.lower()\n",
    "for col in scrapped_prices.columns:\n",
    "    scrapped_prices[col] = pd.to_numeric(scrapped_prices[col], errors='ignore')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ceaef4",
   "metadata": {},
   "source": [
    "### 4.4 Product Statistics (Margin Boundaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b418565-6266-4bac-9af4-6970ba53a3be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f'''\n",
    "select region,product_id,optimal_bm,MIN_BOUNDARY,MAX_BOUNDARY,MEDIAN_BM\n",
    "from (\n",
    "select region,product_id,target_bm,optimal_bm,MIN_BOUNDARY,MAX_BOUNDARY,MEDIAN_BM,max(created_at) over(partition by product_id,region) as max_date,created_at\n",
    "from materialized_views.PRODUCT_STATISTICS\n",
    "where created_at::date >= date_trunc('month',current_date - 60)\n",
    "qualify max_date = created_at\n",
    ")\n",
    "\n",
    "'''\n",
    " \n",
    "stats = snowflake_query(\"Egypt\",query)\n",
    "stats.columns = stats.columns.str.lower()\n",
    "for col in stats.columns:\n",
    "    stats[col] = pd.to_numeric(stats[col], errors='ignore')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33648230",
   "metadata": {},
   "source": [
    "### 4.5 Warehouse-Region Mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb86c080-2a8c-4a63-8507-02ffcfe904f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f'''\n",
    "select warehouse_id,region\n",
    "from (\n",
    "select * ,row_number()over(partition by warehouse_id order by nmv desc) as rnk \n",
    "from (\n",
    "SELECT case when regions.id = 2 then cities.name_en else regions.name_en end as region,\n",
    "\t   pso.warehouse_id,\n",
    "        sum(pso.total_price) as nmv\n",
    "\n",
    "\n",
    "\n",
    "FROM product_sales_order pso\n",
    "JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "JOIN materialized_views.retailer_polygon on materialized_views.retailer_polygon.retailer_id=so.retailer_id\n",
    "JOIN districts on districts.id=materialized_views.retailer_polygon.district_id\n",
    "JOIN cities on cities.id=districts.city_id\n",
    "join states on states.id=cities.state_id\n",
    "join regions on regions.id=states.region_id             \n",
    "\n",
    "WHERE   True\n",
    "    AND so.created_at ::date between current_date-31 and CURRENT_DATE-1\n",
    "    AND so.sales_order_status_id not in (7,12)\n",
    "    AND so.channel IN ('telesales','retailer')\n",
    "    AND pso.purchased_item_count <> 0\n",
    "\n",
    "GROUP BY ALL\n",
    ")\n",
    "qualify rnk = 1 \n",
    ")\n",
    "'''\n",
    "warehouse_region = snowflake_query(\"Egypt\",query)\n",
    "warehouse_region.columns = warehouse_region.columns.str.lower()\n",
    "for col in warehouse_region.columns:\n",
    "    warehouse_region[col] = pd.to_numeric(warehouse_region[col], errors='ignore')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d74ad7b",
   "metadata": {},
   "source": [
    "### 4.6 Target Margins (Brand/Category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e09273a-0aed-4e4a-8f82-739055c3047c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f'''\n",
    "SELECT DISTINCT cat, brand, margin as target_bm\n",
    "FROM    performance.commercial_targets cplan\n",
    "QUALIFY CASE WHEN DATE_TRUNC('month', MAX(DATE)OVER()) = DATE_TRUNC('month', CURRENT_DATE) THEN DATE_TRUNC('month', CURRENT_DATE)\n",
    "ELSE DATE_TRUNC('month', CURRENT_DATE - INTERVAL '1 month') END = DATE_TRUNC('month', date)\n",
    "'''\n",
    "brand_cat_target  = snowflake_query(\"Egypt\",query)\n",
    "brand_cat_target.target_bm=pd.to_numeric(brand_cat_target.target_bm)\n",
    "\n",
    "query = f'''\n",
    "select cat,sum(target_bm *(target_nmv/cat_total)) as cat_target_margin\n",
    "from (\n",
    "select *,sum(target_nmv)over(partition by cat) as cat_total\n",
    "from (\n",
    "select cat,brand,avg(target_bm) as target_bm , sum(target_nmv) as target_nmv\n",
    "from (\n",
    "SELECT DISTINCT date,city as region,cat, brand, margin as target_bm,nmv as target_nmv\n",
    "FROM    performance.commercial_targets cplan\n",
    "QUALIFY CASE WHEN DATE_TRUNC('month', MAX(DATE)OVER()) = DATE_TRUNC('month', CURRENT_DATE) THEN DATE_TRUNC('month', CURRENT_DATE)\n",
    "ELSE DATE_TRUNC('month', CURRENT_DATE - INTERVAL '1 month') END = DATE_TRUNC('month', date)\n",
    ")\n",
    "group by all\n",
    ")\n",
    ")\n",
    "group by all \n",
    "'''\n",
    "cat_target  =snowflake_query(\"Egypt\",query)\n",
    "cat_target.cat_target_margin=pd.to_numeric(cat_target.cat_target_margin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab0491b",
   "metadata": {},
   "source": [
    "### 4.7 Merge All Data Sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4e97860-cb64-4212-857a-98e82a822abd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_data = selected_products.merge(\n",
    "tiers_selection[['warehouse_id','product_id','packing_unit_id','tier_1_qty','tier_2_qty','median_qty','tier_1_increase_pct','tier_2_increase_pct']],\n",
    "on= ['warehouse_id','product_id','packing_unit_id']\n",
    ")\n",
    "final_data=final_data[['warehouse_id','product_id','packing_unit_id','sku','brand', 'cat','packing_unit_price','basic_unit_count','tier_1_qty','tier_2_qty','median_qty','tier_1_increase_pct','tier_2_increase_pct','final_rank']]\n",
    "final_data=final_data.merge(sku_info[['product_id','wac_p']],on='product_id')\n",
    "final_data['wac_p'] = (final_data['wac_p']*final_data['basic_unit_count']).round(2)\n",
    "\n",
    "final_data = final_data.merge(marketplace,on=['product_id','warehouse_id'],how='left')\n",
    "final_data = final_data.drop(columns = 'region')\n",
    "final_data = final_data.merge(bensoliman[['product_id','ben_soliman_price']],on=['product_id'],how='left')\n",
    "final_data = final_data.merge(scrapped_prices,on=['product_id','warehouse_id'],how='left')\n",
    "final_data = final_data.drop(columns = 'region')\n",
    "\n",
    "final_data = final_data.merge(warehouse_region,on=['warehouse_id'])\n",
    "final_data = final_data.merge(stats,on=['product_id','region'],how='left')\n",
    "final_data = final_data.merge(brand_cat_target,on=['brand','cat'],how='left')\n",
    "final_data = final_data.merge(cat_target,on=['cat'],how='left')\n",
    "final_data['Target_margin'] = final_data['target_bm'].fillna(final_data['cat_target_margin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545252e6",
   "metadata": {},
   "source": [
    "## 5. Price Tier Calculation\n",
    "\n",
    "Calculate tier 1 and tier 2 prices with constraints:\n",
    "- **Max discount**: 5% from current price\n",
    "- **Min discount**: 0.35% from current price  \n",
    "- **Ratio bounds**: discount-to-quantity ratio between 1.3 and 3.5\n",
    "- **Price ordering**: WAC < Tier 2 < Tier 1 < Current Price\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c14fe97",
   "metadata": {},
   "source": [
    "### 5.1 Price Calculation Functions\n",
    "\n",
    "The `calculate_tier_prices` function uses multiple strategies:\n",
    "1. **Market prices strategy** - Use competitive pricing data if available\n",
    "2. **Margin range strategy** - Calculate from margin boundaries if no market data\n",
    "3. **Ratio adjustment** - Adjust tier_2 price to meet discount-to-quantity ratio bounds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a4250db-739f-4602-9fcb-fd3638c7343c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_tier_prices(row, max_discount_pct=5.0, min_discount_pct=0.35, min_ratio=1.3, max_ratio=3.5):\n",
    "    \"\"\"\n",
    "    Calculate tier 1 and tier 2 prices for a single row.\n",
    "    \n",
    "    Parameters:\n",
    "    - max_discount_pct: Maximum allowed discount from current price (default: 5%)\n",
    "    - min_discount_pct: Minimum required discount from current price (default: 0.35%)\n",
    "    - min_ratio: Minimum discount-to-quantity ratio (default: 1.3)\n",
    "    - max_ratio: Maximum discount-to-quantity ratio (default: 3.5)\n",
    "    \n",
    "    Constraints:\n",
    "    - Tier prices must not go below price calculated with 0.3 * target_margin\n",
    "    - Ensure: WAC < Tier 2 < Tier 1 < Current Price\n",
    "    - Ensure: BOTH tiers must be valid or BOTH are None\n",
    "    - Ensure: discount_qty_ratio = (tier_2_discount/tier_1_discount) / (tier_2_qty/tier_1_qty) is between min_ratio and max_ratio\n",
    "    \"\"\"\n",
    "    \n",
    "    current_price = row['packing_unit_price']\n",
    "    wac = row['wac_p']\n",
    "    \n",
    "    # Get basic_unit_count for converting market prices\n",
    "    basic_unit_count = row.get('basic_unit_count', 1)\n",
    "    if pd.isna(basic_unit_count) or basic_unit_count <= 0:\n",
    "        basic_unit_count = 1\n",
    "    \n",
    "    # Validation\n",
    "    if pd.isna(current_price) or current_price <= 0:\n",
    "        return pd.Series({'tier_1_price': np.nan, 'tier_2_price': np.nan, 'price_source': 'invalid_current_price'})\n",
    "    \n",
    "    if pd.isna(wac) or wac <= 0:\n",
    "        return pd.Series({'tier_1_price': np.nan, 'tier_2_price': np.nan, 'price_source': 'invalid_wac'})\n",
    "    \n",
    "    if current_price <= wac:\n",
    "        return pd.Series({'tier_1_price': np.nan, 'tier_2_price': np.nan, 'price_source': 'current_price_below_wac'})\n",
    "    \n",
    "    # Calculate discount bounds\n",
    "    max_discount_price = current_price * (1 - max_discount_pct / 100)  # Minimum allowed price\n",
    "    min_discount_price = current_price * (1 - min_discount_pct / 100)  # Maximum allowed price\n",
    "    \n",
    "    # Calculate absolute minimum price based on target_margin\n",
    "    # Price must maintain at least 30% of target margin\n",
    "    absolute_min_price = wac  # Default to WAC if no target_margin\n",
    "    \n",
    "    if 'target_margin' in row.index and pd.notna(row['target_margin']) and 0 < row['target_margin'] < 1:\n",
    "        target_margin = row['target_margin']\n",
    "        # Minimum margin is 30% of target margin\n",
    "        min_margin = target_margin * 0.3\n",
    "        # Calculate minimum price: price = wac / (1 - min_margin)\n",
    "        absolute_min_price = wac / (1 - min_margin)\n",
    "    else:\n",
    "        # Fallback: use wac_cushion_pct\n",
    "        wac_cushion_pct = 0.25\n",
    "        absolute_min_price = wac / (1 - (wac_cushion_pct / 100))\n",
    "    \n",
    "    # Market price columns (these are per basic unit)\n",
    "    market_cols = [\n",
    "        'final_mod_price', 'median_scrapped', 'final_max_price', \n",
    "        'ben_soliman_price', 'max_scrapped', 'final_true_max',\n",
    "        'final_min_price', 'min_scrapped', 'final_true_min'\n",
    "    ]\n",
    "    \n",
    "    # Extract valid market prices (multiply by basic_unit_count, above absolute_min_price, within discount bounds)\n",
    "    valid_market_prices = []\n",
    "    for col in market_cols:\n",
    "        if col in row.index and pd.notna(row[col]) and row[col] > 0:\n",
    "            # Convert basic unit price to packing unit price\n",
    "            packing_price = row[col] * basic_unit_count\n",
    "            \n",
    "            # Must be: above absolute_min_price AND within discount bounds\n",
    "            if absolute_min_price < packing_price and max_discount_price <= packing_price <= min_discount_price:\n",
    "                valid_market_prices.append(packing_price)\n",
    "    \n",
    "    # Remove duplicates and sort descending\n",
    "    valid_market_prices = sorted(list(set(valid_market_prices)), reverse=True)\n",
    "    \n",
    "    tier_1 = None\n",
    "    tier_2 = None\n",
    "    source = ''\n",
    "    \n",
    "    min_gap_pct = 0.25\n",
    "    \n",
    "    # Strategy 1: Use market prices\n",
    "    if len(valid_market_prices) >= 3:\n",
    "        # Select from available prices\n",
    "        tier_1 = valid_market_prices[0]  # Highest price\n",
    "        \n",
    "        # Find tier 2 with minimum gap\n",
    "        for price in valid_market_prices[1:]:\n",
    "            if price < tier_1 * (1 - min_gap_pct / 100):\n",
    "                tier_2 = price\n",
    "                break\n",
    "        \n",
    "        # If no suitable tier 2 found, take second highest\n",
    "        if tier_2 is None and len(valid_market_prices) > 1:\n",
    "            tier_2 = valid_market_prices[1]\n",
    "        \n",
    "        if tier_1 is not None and tier_2 is not None:\n",
    "            source = 'market_prices'\n",
    "    \n",
    "    elif len(valid_market_prices) == 2:\n",
    "        tier_1 = valid_market_prices[0]\n",
    "        tier_2 = valid_market_prices[1]\n",
    "        source = 'market_prices'\n",
    "    \n",
    "    elif len(valid_market_prices) == 1:\n",
    "        # Only one market price - use margin range for the other\n",
    "        market_price = valid_market_prices[0]\n",
    "        \n",
    "        # Calculate which tier this should be based on its position\n",
    "        price_position = (market_price - max_discount_price) / (min_discount_price - max_discount_price)\n",
    "        \n",
    "        # If in upper half (>0.5), use as tier 1 and calculate tier 2\n",
    "        # If in lower half (<=0.5), use as tier 2 and calculate tier 1\n",
    "        if price_position > 0.5:\n",
    "            tier_1 = market_price\n",
    "            tier_2 = calculate_from_margin_range(row, wac, current_price, tier_1, tier=2, \n",
    "                                                 max_discount_price=max_discount_price,\n",
    "                                                 min_discount_price=min_discount_price,\n",
    "                                                 absolute_min_price=absolute_min_price)\n",
    "            if tier_2 is not None:\n",
    "                source = 'market_tier1_margin_tier2'\n",
    "        else:\n",
    "            tier_2 = market_price\n",
    "            tier_1 = calculate_from_margin_range(row, wac, current_price, tier_2, tier=1,\n",
    "                                                 max_discount_price=max_discount_price,\n",
    "                                                 min_discount_price=min_discount_price,\n",
    "                                                 absolute_min_price=absolute_min_price)\n",
    "            if tier_1 is not None:\n",
    "                source = 'margin_tier1_market_tier2'\n",
    "    \n",
    "    # Strategy 2: No market prices - use margin range method\n",
    "    if tier_1 is None or tier_2 is None:\n",
    "        tier_1, tier_2 = calculate_both_from_margin_range(row, wac, current_price,\n",
    "                                                          max_discount_price=max_discount_price,\n",
    "                                                          min_discount_price=min_discount_price,\n",
    "                                                          absolute_min_price=absolute_min_price)\n",
    "        if tier_1 is not None and tier_2 is not None:\n",
    "            source = 'margin_range_based'\n",
    "    \n",
    "    # CRITICAL: Final validation - BOTH must be valid or BOTH are None\n",
    "    if tier_1 is not None and tier_2 is not None:\n",
    "        # Ensure correct ordering\n",
    "        if tier_2 >= tier_1:\n",
    "            tier_1, tier_2 = max(tier_1, tier_2), min(tier_1, tier_2)\n",
    "        \n",
    "        # Apply discount bounds\n",
    "        tier_1 = max(tier_1, max_discount_price)\n",
    "        tier_1 = min(tier_1, min_discount_price)\n",
    "        tier_2 = max(tier_2, max_discount_price)\n",
    "        tier_2 = min(tier_2, min_discount_price)\n",
    "        \n",
    "        # Check if both above absolute minimum price\n",
    "        if tier_1 <= absolute_min_price or tier_2 <= absolute_min_price:\n",
    "            tier_1 = None\n",
    "            tier_2 = None\n",
    "            source = 'prices_below_minimum_margin'\n",
    "        else:\n",
    "            # Ensure minimum gap between tiers\n",
    "            if tier_2 > tier_1 * (1 - min_gap_pct / 100):\n",
    "                tier_2 = tier_1 * (1 - min_gap_pct / 100)\n",
    "                if tier_2 <= absolute_min_price:\n",
    "                    tier_1 = None\n",
    "                    tier_2 = None\n",
    "                    source = 'insufficient_gap_between_tiers'\n",
    "            \n",
    "            # Final check: both still valid?\n",
    "            if tier_1 is not None and tier_2 is not None:\n",
    "                if not (wac < tier_2 < tier_1 < current_price):\n",
    "                    tier_1 = None\n",
    "                    tier_2 = None\n",
    "                    source = 'invalid_tier_ordering'\n",
    "                elif not (max_discount_price <= tier_2 and tier_1 <= min_discount_price):\n",
    "                    tier_1 = None\n",
    "                    tier_2 = None\n",
    "                    source = 'tiers_outside_discount_bounds'\n",
    "                else:\n",
    "                    tier_1 = round(tier_1, 2)\n",
    "                    tier_2 = round(tier_2, 2)\n",
    "                    \n",
    "                    # Validate and adjust discount-to-quantity ratio\n",
    "                    tier_1_qty = row.get('tier_1_qty', None)\n",
    "                    tier_2_qty = row.get('tier_2_qty', None)\n",
    "                    \n",
    "                    if tier_1_qty is not None and tier_2_qty is not None and tier_1_qty > 0:\n",
    "                        tier_1_discount = current_price - tier_1\n",
    "                        tier_2_discount = current_price - tier_2\n",
    "                        \n",
    "                        if tier_1_discount > 0:\n",
    "                            diff_quantity = tier_2_qty / tier_1_qty\n",
    "                            diff_discount = tier_2_discount / tier_1_discount\n",
    "                            \n",
    "                            if diff_quantity > 0:\n",
    "                                discount_qty_ratio = diff_discount / diff_quantity\n",
    "                                \n",
    "                                # Adjust tier_2_price if ratio is outside bounds\n",
    "                                if discount_qty_ratio < min_ratio:\n",
    "                                    # Ratio too low - need more discount at tier 2\n",
    "                                    # tier_2 = current_price - (target_ratio * diff_quantity * tier_1_discount)\n",
    "                                    target_tier_2_discount = min_ratio * diff_quantity * tier_1_discount\n",
    "                                    adjusted_tier_2 = current_price - target_tier_2_discount\n",
    "                                    \n",
    "                                    # Ensure adjusted price is still valid (above WAC and absolute_min_price)\n",
    "                                    if adjusted_tier_2 > wac and adjusted_tier_2 > absolute_min_price and adjusted_tier_2 < tier_1:\n",
    "                                        tier_2 = round(adjusted_tier_2, 2)\n",
    "                                        source = source + '_ratio_adjusted_up'\n",
    "                                    else:\n",
    "                                        tier_1 = None\n",
    "                                        tier_2 = None\n",
    "                                        source = f'cannot_adjust_ratio_{discount_qty_ratio:.2f}_min_bound'\n",
    "                                \n",
    "                                elif discount_qty_ratio > max_ratio:\n",
    "                                    # Ratio too high - need less discount at tier 2\n",
    "                                    # tier_2 = current_price - (target_ratio * diff_quantity * tier_1_discount)\n",
    "                                    target_tier_2_discount = max_ratio * diff_quantity * tier_1_discount\n",
    "                                    adjusted_tier_2 = current_price - target_tier_2_discount\n",
    "                                    \n",
    "                                    # Ensure adjusted price is still valid (below tier_1 and above WAC)\n",
    "                                    if adjusted_tier_2 > wac and adjusted_tier_2 > absolute_min_price and adjusted_tier_2 < tier_1:\n",
    "                                        tier_2 = round(adjusted_tier_2, 2)\n",
    "                                        source = source + '_ratio_adjusted_down'\n",
    "                                    else:\n",
    "                                        tier_1 = None\n",
    "                                        tier_2 = None\n",
    "                                        source = f'cannot_adjust_ratio_{discount_qty_ratio:.2f}_max_bound'\n",
    "    \n",
    "    # FINAL CHECK: If only one tier exists, invalidate both\n",
    "    if (tier_1 is None and tier_2 is not None) or (tier_1 is not None and tier_2 is None):\n",
    "        tier_1 = None\n",
    "        tier_2 = None\n",
    "        source = 'incomplete_tier_pair'\n",
    "    \n",
    "    # If both are None and no source set, mark it\n",
    "    if tier_1 is None and tier_2 is None and source == '':\n",
    "        source = 'no_valid_prices'\n",
    "    \n",
    "    return pd.Series({\n",
    "        'tier_1_price': tier_1,\n",
    "        'tier_2_price': tier_2,\n",
    "        'price_source': source\n",
    "    })\n",
    "\n",
    "\n",
    "def calculate_both_from_margin_range(row, wac, current_price, max_discount_price, min_discount_price, absolute_min_price):\n",
    "    \"\"\"\n",
    "    Calculate both tier prices using margin range from minimum of (min_boundary, optimal_bm) to current margin.\n",
    "    Returns (tier_1_price, tier_2_price) or (None, None)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate current margin: margin = (price - wac) / price\n",
    "    current_margin = (current_price - wac) / current_price\n",
    "    \n",
    "    # Get min_boundary margin\n",
    "    min_boundary_margin = None\n",
    "    if 'min_boundary' in row.index and pd.notna(row['min_boundary']) and 0 < row['min_boundary'] < 1:\n",
    "        min_boundary_margin = row['min_boundary']\n",
    "    \n",
    "    # Get optimal_bm margin\n",
    "    optimal_margin = None\n",
    "    if 'optimal_bm' in row.index and pd.notna(row['optimal_bm']) and 0 < row['optimal_bm'] < 1:\n",
    "        optimal_margin = row['optimal_bm']\n",
    "    \n",
    "    # Determine starting margin: minimum of (min_boundary, optimal_bm)\n",
    "    start_margin = None\n",
    "    \n",
    "    if min_boundary_margin is not None and optimal_margin is not None:\n",
    "        start_margin = min(min_boundary_margin, optimal_margin)\n",
    "    elif min_boundary_margin is not None:\n",
    "        start_margin = min_boundary_margin\n",
    "    elif optimal_margin is not None:\n",
    "        start_margin = optimal_margin\n",
    "    else:\n",
    "        # Fallback: use 50% of current margin\n",
    "        start_margin = current_margin * 0.85\n",
    "    \n",
    "    # Ensure start_margin is less than current margin\n",
    "    if start_margin >= current_margin:\n",
    "        start_margin = current_margin * 0.85\n",
    "    \n",
    "    # Generate margin points in the range (10 points)\n",
    "    num_points = 10\n",
    "    margin_range = np.linspace(start_margin, current_margin, num_points)\n",
    "    \n",
    "    # Calculate prices from these margins: price = wac / (1 - margin)\n",
    "    price_candidates = []\n",
    "    for margin in margin_range:\n",
    "        if 0 < margin < 1:\n",
    "            price = wac / (1 - margin)\n",
    "            # Only keep prices within discount bounds and above absolute_min_price\n",
    "            if absolute_min_price < price and max_discount_price <= price <= min_discount_price:\n",
    "                price_candidates.append(price)\n",
    "    \n",
    "    if len(price_candidates) < 2:\n",
    "        return None, None\n",
    "    \n",
    "    # Sort prices descending\n",
    "    price_candidates = sorted(price_candidates, reverse=True)\n",
    "    \n",
    "    # Select Tier 1: closer to the top (less discount)\n",
    "    # Select Tier 2: further down (more discount)\n",
    "    tier_1_idx = int(len(price_candidates) * 0.25)  # 25% from top\n",
    "    tier_2_idx = int(len(price_candidates) * 0.65)  # 65% from top\n",
    "    \n",
    "    # Ensure valid indices\n",
    "    tier_1_idx = max(0, min(tier_1_idx, len(price_candidates) - 2))\n",
    "    tier_2_idx = max(tier_1_idx + 1, min(tier_2_idx, len(price_candidates) - 1))\n",
    "    \n",
    "    tier_1 = price_candidates[tier_1_idx]\n",
    "    tier_2 = price_candidates[tier_2_idx]\n",
    "    \n",
    "    # Ensure meaningful gap (at least 0.5%)\n",
    "    min_gap_pct = 0.25\n",
    "    if tier_2 > tier_1 * (1 - min_gap_pct / 100):\n",
    "        # Try to find better tier_2\n",
    "        for i in range(tier_2_idx + 1, len(price_candidates)):\n",
    "            if price_candidates[i] < tier_1 * (1 - min_gap_pct / 100):\n",
    "                tier_2 = price_candidates[i]\n",
    "                break\n",
    "    \n",
    "    # Final validation\n",
    "    if tier_2 >= tier_1 or tier_1 <= absolute_min_price or tier_2 <= absolute_min_price:\n",
    "        return None, None\n",
    "    \n",
    "    return tier_1, tier_2\n",
    "\n",
    "\n",
    "def calculate_from_margin_range(row, wac, current_price, other_tier_price, tier, \n",
    "                                max_discount_price, min_discount_price, absolute_min_price):\n",
    "    \"\"\"\n",
    "    Calculate single tier price using margin range.\n",
    "    Used when one tier is from market and we need to calculate the other.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate current margin\n",
    "    current_margin = (current_price - wac) / current_price\n",
    "    \n",
    "    # Get min_boundary margin\n",
    "    min_boundary_margin = None\n",
    "    if 'min_boundary' in row.index and pd.notna(row['min_boundary']) and 0 < row['min_boundary'] < 1:\n",
    "        min_boundary_margin = row['min_boundary']\n",
    "    \n",
    "    # Get optimal_bm margin\n",
    "    optimal_margin = None\n",
    "    if 'optimal_bm' in row.index and pd.notna(row['optimal_bm']) and 0 < row['optimal_bm'] < 1:\n",
    "        optimal_margin = row['optimal_bm']\n",
    "    \n",
    "    # Determine starting margin: minimum of (min_boundary, optimal_bm)\n",
    "    start_margin = None\n",
    "    \n",
    "    if min_boundary_margin is not None and optimal_margin is not None:\n",
    "        start_margin = min(min_boundary_margin, optimal_margin)\n",
    "    elif min_boundary_margin is not None:\n",
    "        start_margin = min_boundary_margin\n",
    "    elif optimal_margin is not None:\n",
    "        start_margin = optimal_margin\n",
    "    else:\n",
    "        start_margin = current_margin * 0.5\n",
    "    \n",
    "    # Ensure start_margin is less than current margin\n",
    "    if start_margin >= current_margin:\n",
    "        start_margin = current_margin * 0.7\n",
    "    \n",
    "    # Generate margin range (10 points)\n",
    "    num_points = 10\n",
    "    margin_range = np.linspace(start_margin, current_margin, num_points)\n",
    "    \n",
    "    # Calculate prices\n",
    "    price_candidates = []\n",
    "    for margin in margin_range:\n",
    "        if 0 < margin < 1:\n",
    "            price = wac / (1 - margin)\n",
    "            if absolute_min_price < price and max_discount_price <= price <= min_discount_price:\n",
    "                price_candidates.append(price)\n",
    "    \n",
    "    if len(price_candidates) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Sort prices descending\n",
    "    price_candidates = sorted(price_candidates, reverse=True)\n",
    "    \n",
    "    min_gap_pct = 0.5\n",
    "    \n",
    "    if tier == 1:\n",
    "        # Need tier 1 (higher price), we have tier 2 (lower price)\n",
    "        # Find prices above tier 2 with proper gap\n",
    "        target_candidates = [p for p in price_candidates \n",
    "                           if p > other_tier_price * (1 + min_gap_pct / 100)]\n",
    "        if target_candidates:\n",
    "            # Take from upper portion (25% position)\n",
    "            idx = int(len(target_candidates) * 0.25)\n",
    "            return target_candidates[idx]\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        # Need tier 2 (lower price), we have tier 1 (higher price)\n",
    "        # Find prices below tier 1 with proper gap\n",
    "        target_candidates = [p for p in price_candidates \n",
    "                           if p < other_tier_price * (1 - min_gap_pct / 100)]\n",
    "        if target_candidates:\n",
    "            # Take from lower portion (65% position)\n",
    "            idx = int(len(target_candidates) * 0.65)\n",
    "            idx = min(idx, len(target_candidates) - 1)\n",
    "            return target_candidates[idx]\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4895a40",
   "metadata": {},
   "source": [
    "### 5.2 Apply Price Calculations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d485d098-925d-4be2-8e44-18b33438c529",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2400 SKUs...\n",
      "Ratio adjusted up (was below 1.3): 1577 SKUs\n",
      "Ratio adjusted down (was above 3.5): 29 SKUs\n",
      "Could not adjust (constraints violated): 83 SKUs\n",
      "Final SKUs with valid tier prices: 1974\n"
     ]
    }
   ],
   "source": [
    "df = final_data.copy()\n",
    "final_data.columns = final_data.columns.str.lower()\n",
    "\n",
    "print(f\"Processing {len(final_data)} SKUs...\")\n",
    "\n",
    "# QUESTIONS FOR USER - Set these parameters:\n",
    "MAX_DISCOUNT_PCT = 5.0      # Maximum discount allowed\n",
    "MIN_DISCOUNT_PCT = 0.35     # Minimum discount required\n",
    "MIN_RATIO = 1.3             # Minimum discount-to-quantity ratio\n",
    "MAX_RATIO = 3.5             # Maximum discount-to-quantity ratio\n",
    "\n",
    "# Apply function to each row with discount bounds and ratio constraints\n",
    "result = final_data.apply(lambda row: calculate_tier_prices(row, \n",
    "                                                    max_discount_pct=MAX_DISCOUNT_PCT,\n",
    "                                                    min_discount_pct=MIN_DISCOUNT_PCT,\n",
    "                                                    min_ratio=MIN_RATIO,\n",
    "                                                    max_ratio=MAX_RATIO), \n",
    "                 axis=1)\n",
    "\n",
    "# Merge results back to dataframe\n",
    "final_data = pd.concat([final_data, result], axis=1)\n",
    "\n",
    "# Show how many were adjusted vs couldn't be adjusted\n",
    "ratio_adjusted_up = final_data['price_source'].str.contains('ratio_adjusted_up', na=False).sum()\n",
    "ratio_adjusted_down = final_data['price_source'].str.contains('ratio_adjusted_down', na=False).sum()\n",
    "cannot_adjust = final_data['price_source'].str.contains('cannot_adjust_ratio', na=False).sum()\n",
    "print(f\"Ratio adjusted up (was below {MIN_RATIO}): {ratio_adjusted_up} SKUs\")\n",
    "print(f\"Ratio adjusted down (was above {MAX_RATIO}): {ratio_adjusted_down} SKUs\")\n",
    "print(f\"Could not adjust (constraints violated): {cannot_adjust} SKUs\")\n",
    "\n",
    "final_data = final_data[(~final_data['tier_1_price'].isna())&(~final_data['tier_2_price'].isna())]\n",
    "print(f\"Final SKUs with valid tier prices: {len(final_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b0ceb8",
   "metadata": {},
   "source": [
    "## 6. Wholesale Pricing\n",
    "\n",
    "Calculate wholesale prices based on:\n",
    "- Vehicle capacity (quarter truck)\n",
    "- Rank-based margin tiers (20%, 25%, 40%, 60% of target margin)\n",
    "- Must be below tier_2_price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f15518b2-699f-4c12-b551-f125b8abd4c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_data['delivery_fees'] =  DELIVERY_FEE_OTHER\n",
    "final_data.loc[final_data['region'].isin(['Cairo','Giza']),'delivery_fees'] = DELIVERY_FEE_CAIRO_GIZA\n",
    "query_data = final_data[['warehouse_id', 'product_id', 'packing_unit_id','delivery_fees']].values.tolist()\n",
    "query_info = ','.join([f\"({int(wh_id)}, {int(prod_id)}, {int(pu_id)}, {int(delivery_fees)})\" for wh_id, prod_id, pu_id,delivery_fees in query_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55ec0273-9d56-4dc6-b4bc-2ba8093792b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f'''\n",
    "with chosen_products as (\n",
    "select *\n",
    "from (\n",
    "values \n",
    "{query_info}\n",
    ")x(warehouse_id,product_id,packing_unit_id,delivery_fees)\n",
    "\n",
    "),\n",
    "vec as (\n",
    "select  vt.id as vehicle_id,name_en as vehicle_name,vc.weight as vehicle_weight,vc.cbm as vehicle_cbm,900 as vehicle_cost\n",
    "from VEHICLE_TYPES  vt \n",
    "join  RETOOL.VEHICLE_CAPACITIES vc on vc.vehicle_id = vt.id\n",
    "where vehicle_id = 1\n",
    "),\n",
    "selected_products as (\n",
    "select x.*,\t(long*width*height)/1000000 AS cbm,weight/1000 AS weight,\n",
    "from chosen_products x\n",
    "join packing_unit_products on x.product_id = packing_unit_products.product_id and packing_unit_products.packing_unit_id = x.packing_unit_id\n",
    "),\n",
    "main_cte as (\n",
    "select warehouse_id,product_id,packing_unit_id,delivery_fees,\n",
    "ceil(least(quart_dababa_wht,quart_dababa_cbm)) as quart_dababa,\n",
    "vehicle_cost\n",
    "from (\n",
    "select * ,\n",
    "((vehicle_weight*0.9)/4)/weight as quart_dababa_wht , \n",
    "((vehicle_cbm*0.9)/4)/cbm as quart_dababa_cbm  \n",
    "from (\n",
    "select selected_products.*, vehicle_weight,vehicle_cbm,vehicle_cost\n",
    "from selected_products,vec\n",
    ")\n",
    ")\n",
    ")\n",
    "select mc.*, f.wac_p , \n",
    "(f.wac_p*quart_dababa)+(((vehicle_cost-(delivery_fees*4))*0.9)/4) as quart_cost,\n",
    "quart_cost/quart_dababa as unit_cost\n",
    "\n",
    "\n",
    "from main_cte mc \n",
    "join finance.all_cogs f on f.product_id = mc.product_id and CURRENT_TIMEstamp between from_date and to_date \n",
    "\n",
    "'''\n",
    "ws_data  =snowflake_query(\"Egypt\",query)\n",
    "ws_data.columns = ws_data.columns.str.lower()\n",
    "for col in ws_data.columns:\n",
    "    ws_data[col] = pd.to_numeric(ws_data[col], errors='ignore') \n",
    "ws_data=ws_data[['warehouse_id', 'product_id', 'packing_unit_id','quart_dababa','unit_cost']]\n",
    "ws_data.columns = ['warehouse_id', 'product_id', 'packing_unit_id','WS_tier','WS_wac']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d82f21df-6b2c-4910-82f9-c7b758a3ac22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_data = final_data.merge(ws_data,on=['warehouse_id', 'product_id', 'packing_unit_id'],how='left')\n",
    "final_data['WS_wac'] = final_data['WS_wac']*final_data['basic_unit_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb77deb3-b68a-4c4a-acc8-012272cf7e7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wholesales_margin(x):\n",
    "    wac = x['WS_wac']\n",
    "    target_margin =x['target_margin']\n",
    "    min_margin = min(x['optimal_bm'],x['min_boundary'])\n",
    "    tier_2_price = x['tier_2_price']\n",
    "    final_rank = x['final_rank']\n",
    "    tier = 0 \n",
    "    new_price = 0\n",
    "    price = wac/(1-0.01)\n",
    "    if final_rank <= 0.25*133:\n",
    "        tier = 1 \n",
    "    elif final_rank > 0.25*133 and final_rank <= 0.5*133:  \n",
    "        tier = 2\n",
    "    elif final_rank > 0.5*133 and final_rank <= 0.75*133:  \n",
    "        tier = 3\n",
    "    else:\n",
    "        tier = 4 \n",
    "        \n",
    "    if  tier == 1 :\n",
    "            price= wac/ (1-np.minimum(np.maximum(((0.2)*x['target_margin']),0.01),x['target_margin']))\n",
    "    elif tier == 2 :  \n",
    "        price= wac / (1-np.minimum(np.maximum(((0.25)*x['target_margin']),0.015),x['target_margin']))\n",
    "    elif tier == 3 :  \n",
    "        price= wac / (1-np.minimum(np.maximum(((0.4)*x['target_margin']),0.015),x['target_margin']))    \n",
    "    else:\n",
    "        price = wac / (1-np.minimum(np.maximum(((0.6)*x['target_margin']),0.015),x['target_margin']))\n",
    "    if price >= tier_2_price:\n",
    "        new_price = (wac+tier_2_price)/2\n",
    "    return np.maximum(new_price,wac/(1-0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a1ed61e-384a-48f0-9f2d-e70593bde88a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_data['WS_price'] = final_data.apply(wholesales_margin,axis=1)\n",
    "final_data['valid'] = final_data['WS_price']<final_data['tier_2_price']\n",
    "final_data.loc[final_data['valid']==False,'WS_price']=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef736fbd-c796-473d-aaa1-b8e725768f4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_data['new_rank'] = final_data.groupby(['warehouse_id'])['final_rank'].rank(method='dense', ascending=True)\n",
    "final_data=final_data[final_data['new_rank']<=133]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6cc89a6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== METRICS SUMMARY ===\n",
      "\n",
      "Stretch Analysis (how much retailers need to increase orders):\n",
      "  Average stretch to Tier 1: 255.5%\n",
      "  Average stretch to Tier 2: 519.1%\n",
      "\n",
      "Margin Analysis:\n",
      "  Current margin:  5.74%\n",
      "  Tier 1 margin:   5.03%\n",
      "  Tier 2 margin:   3.98%\n",
      "  WS margin:       2.31%\n",
      "\n",
      "Discount Analysis:\n",
      "  Average Tier 1 discount: 0.75%\n",
      "  Average Tier 2 discount: 1.83%\n",
      "\n",
      "Elasticity Analysis (discount increase vs quantity increase):\n",
      "  Average qty ratio (T2/T1): 1.74x\n",
      "  Average discount ratio (D2/D1): 2.48x\n",
      "  Average elasticity ratio: 1.43\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CALCULATE ADDITIONAL METRICS\n",
    "# =============================================================================\n",
    "\n",
    "# --- Stretch Percentages (how much retailers need to increase to reach each tier) ---\n",
    "# Already included from tiers_selection: tier_1_increase_pct, tier_2_increase_pct\n",
    "# These show: (tier_qty - median_qty) / median_qty * 100\n",
    "\n",
    "# Rename for clarity\n",
    "final_data['stretch_to_tier_1_pct'] = final_data['tier_1_increase_pct']\n",
    "final_data['stretch_to_tier_2_pct'] = final_data['tier_2_increase_pct']\n",
    "\n",
    "# --- Margins for each price tier ---\n",
    "# Margin = (price - wac) / price\n",
    "final_data['tier_1_margin'] = ((final_data['tier_1_price'] - final_data['wac_p']) / final_data['tier_1_price']).round(4)\n",
    "final_data['tier_2_margin'] = ((final_data['tier_2_price'] - final_data['wac_p']) / final_data['tier_2_price']).round(4)\n",
    "final_data['WS_margin'] = ((final_data['WS_price'] - final_data['wac_p']) / final_data['wac_p']).round(4)\n",
    "final_data['current_margin'] = ((final_data['packing_unit_price'] - final_data['wac_p']) / final_data['packing_unit_price']).round(4)\n",
    "\n",
    "# --- Discount calculations ---\n",
    "# Absolute discounts (price reduction from current price)\n",
    "final_data['discount_1'] = (final_data['packing_unit_price'] - final_data['tier_1_price']).round(2)\n",
    "final_data['discount_2'] = (final_data['packing_unit_price'] - final_data['tier_2_price']).round(2)\n",
    "\n",
    "# Discount percentages\n",
    "final_data['discount_1_pct'] = ((final_data['discount_1'] / final_data['packing_unit_price']) * 100).round(2)\n",
    "final_data['discount_2_pct'] = ((final_data['discount_2'] / final_data['packing_unit_price']) * 100).round(2)\n",
    "\n",
    "# --- Quantity and Discount Ratios ---\n",
    "# Quantity ratio (tier_2_qty / tier_1_qty)\n",
    "final_data['qty_ratio'] = (final_data['tier_2_qty'] / final_data['tier_1_qty']).round(2)\n",
    "\n",
    "# Discount ratio (discount_2 / discount_1)\n",
    "final_data['discount_ratio'] = (final_data['discount_2'] / final_data['discount_1']).round(2)\n",
    "\n",
    "# Elasticity ratio = discount_ratio / qty_ratio\n",
    "# This shows how much extra discount per unit of quantity increase\n",
    "final_data['elasticity_ratio'] = (final_data['discount_ratio'] / final_data['qty_ratio']).round(2)\n",
    "\n",
    "print(\"=== METRICS SUMMARY ===\")\n",
    "print(f\"\\nStretch Analysis (how much retailers need to increase orders):\")\n",
    "print(f\"  Average stretch to Tier 1: {final_data['stretch_to_tier_1_pct'].mean():.1f}%\")\n",
    "print(f\"  Average stretch to Tier 2: {final_data['stretch_to_tier_2_pct'].mean():.1f}%\")\n",
    "\n",
    "print(f\"\\nMargin Analysis:\")\n",
    "print(f\"  Current margin:  {final_data['current_margin'].mean()*100:.2f}%\")\n",
    "print(f\"  Tier 1 margin:   {final_data['tier_1_margin'].mean()*100:.2f}%\")\n",
    "print(f\"  Tier 2 margin:   {final_data['tier_2_margin'].mean()*100:.2f}%\")\n",
    "print(f\"  WS margin:       {final_data['WS_margin'].mean()*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nDiscount Analysis:\")\n",
    "print(f\"  Average Tier 1 discount: {final_data['discount_1_pct'].mean():.2f}%\")\n",
    "print(f\"  Average Tier 2 discount: {final_data['discount_2_pct'].mean():.2f}%\")\n",
    "\n",
    "print(f\"\\nElasticity Analysis (discount increase vs quantity increase):\")\n",
    "print(f\"  Average qty ratio (T2/T1): {final_data['qty_ratio'].mean():.2f}x\")\n",
    "print(f\"  Average discount ratio (D2/D1): {final_data['discount_ratio'].mean():.2f}x\")\n",
    "print(f\"  Average elasticity ratio: {final_data['elasticity_ratio'].mean():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ac640f",
   "metadata": {},
   "source": [
    "## 7. Final Ranking & Export\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65ee57d5-5719-4d50-a45a-d3a3cbf79803",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXPORT COMPLETE ===\n",
      "Saved 1596 SKUs to 'QD_upload.xlsx'\n",
      "\n",
      "New columns added:\n",
      "  - stretch_to_tier_1_pct: % increase from median qty to tier 1\n",
      "  - stretch_to_tier_2_pct: % increase from median qty to tier 2\n",
      "  - tier_1_margin, tier_2_margin, WS_margin: margins for each price\n",
      "  - current_margin: margin at current price\n",
      "  - discount_1, discount_2: absolute discount amounts\n",
      "  - discount_1_pct, discount_2_pct: discount percentages\n",
      "  - qty_ratio: tier_2_qty / tier_1_qty\n",
      "  - discount_ratio: discount_2 / discount_1\n",
      "  - elasticity_ratio: discount_ratio / qty_ratio\n"
     ]
    }
   ],
   "source": [
    "# Save to Excel with all metrics\n",
    "output_file = 'QD_upload.xlsx'\n",
    "final_data.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"\\n=== EXPORT COMPLETE ===\")\n",
    "print(f\"Saved {len(final_data)} SKUs to '{output_file}'\")\n",
    "print(f\"\\nNew columns added:\")\n",
    "print(\"  - stretch_to_tier_1_pct: % increase from median qty to tier 1\")\n",
    "print(\"  - stretch_to_tier_2_pct: % increase from median qty to tier 2\")\n",
    "print(\"  - tier_1_margin, tier_2_margin, WS_margin: margins for each price\")\n",
    "print(\"  - current_margin: margin at current price\")\n",
    "print(\"  - discount_1, discount_2: absolute discount amounts\")\n",
    "print(\"  - discount_1_pct, discount_2_pct: discount percentages\")\n",
    "print(\"  - qty_ratio: tier_2_qty / tier_1_qty\")\n",
    "print(\"  - discount_ratio: discount_2 / discount_1\")\n",
    "print(\"  - elasticity_ratio: discount_ratio / qty_ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7d74cd-a60a-4778-b642-2d5c5723700e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
