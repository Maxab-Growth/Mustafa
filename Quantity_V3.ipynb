{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c83eefb",
   "metadata": {},
   "source": [
    "# Quantity Discount (QD) Pricing System\n",
    "\n",
    "This notebook calculates tiered pricing and quantities for products across warehouses.\n",
    "\n",
    "## Workflow:\n",
    "1. **Setup** - Imports, connections, and configuration\n",
    "2. **Product Selection** - Select top products per warehouse based on performance\n",
    "3. **Quantity Tiers** - Calculate tier 1 and tier 2 quantities based on order history\n",
    "4. **Market Prices** - Gather competitive pricing data\n",
    "5. **Price Tiers** - Calculate discounted prices for each tier\n",
    "6. **Wholesale Pricing** - Calculate wholesale prices for bulk orders\n",
    "7. **Export** - Save results to Excel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d33827a",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d0f8d0-7f4a-4468-b218-49241a56edc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/snowflake/connector/options.py:104: UserWarning: You have an incompatible version of 'pyarrow' installed (22.0.0), please install a version that adheres to: 'pyarrow<19.0.0; extra == \"pandas\"'\n",
      "  warn_incompatible_dep(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.Renviron\n",
      "/home/ec2-user/service_account_key.json\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STANDARD LIBRARY IMPORTS\n",
    "# =============================================================================\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import base64\n",
    "import calendar\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "# =============================================================================\n",
    "# THIRD-PARTY IMPORTS\n",
    "# =============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import requests\n",
    "import gspread\n",
    "import boto3\n",
    "import snowflake.connector\n",
    "from tqdm import tqdm\n",
    "from requests import get\n",
    "from botocore.exceptions import ClientError\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "# =============================================================================\n",
    "# LOCAL IMPORTS & ENVIRONMENT SETUP\n",
    "# =============================================================================\n",
    "import setup_environment_2\n",
    "import importlib\n",
    "import import_ipynb\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "importlib.reload(setup_environment_2)\n",
    "setup_environment_2.initialize_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0727d967",
   "metadata": {},
   "source": [
    "### Configuration Constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7201f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - Modify these parameters as needed\n",
    "# =============================================================================\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Cohort IDs for QD program\n",
    "# -----------------------------------------------------------------------------\n",
    "COHORT_IDS = [700, 701, 702, 703, 704, 1123, 1124, 1125, 1126]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Warehouse mappings: (region, warehouse_name, warehouse_id, cohort_id)\n",
    "# -----------------------------------------------------------------------------\n",
    "WAREHOUSE_MAPPING = [\n",
    "    ('Cairo',       'El-Marg',       38,  700),\n",
    "    ('Cairo',       'Mostorod',      1,   700),\n",
    "    ('Giza',        'Barageel',      236, 701),\n",
    "    ('Giza',        'Sakkarah',      962, 701),\n",
    "    ('Delta West',  'El-Mahala',     337, 703),\n",
    "    ('Delta West',  'Tanta',         8,   703),\n",
    "    ('Delta East',  'Mansoura FC',   339, 704),\n",
    "    ('Delta East',  'Sharqya',       170, 704),\n",
    "    ('Upper Egypt', 'Assiut FC',     501, 1124),\n",
    "    ('Upper Egypt', 'Bani sweif',    401, 1126),\n",
    "    ('Upper Egypt', 'Menya Samalot', 703, 1123),\n",
    "    ('Upper Egypt', 'Sohag',         632, 1125),\n",
    "    ('Alexandria',  'Khorshed Alex', 797, 702),\n",
    "]\n",
    "\n",
    "# Excluded warehouse IDs\n",
    "EXCLUDED_WAREHOUSES = [6, 9, 10]\n",
    "\n",
    "# Products to exclude from selection\n",
    "PRODUCTS_TO_REMOVE = [7630]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Pricing Parameters\n",
    "# -----------------------------------------------------------------------------\n",
    "MAX_DISCOUNT_PCT = 5.0    # Maximum discount allowed from current price (%)\n",
    "MIN_DISCOUNT_PCT = 0.35   # Minimum discount required from current price (%)\n",
    "MIN_RATIO        = 1.1    # Minimum discount-to-quantity ratio\n",
    "MAX_RATIO        = 3      # Maximum discount-to-quantity ratio\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Product Selection Thresholds\n",
    "# -----------------------------------------------------------------------------\n",
    "MIN_ORDERS    = 20    # Minimum orders in 4 months\n",
    "MIN_RETAILERS = 5     # Minimum unique retailers\n",
    "MIN_NMV       = 5000  # Minimum revenue (EGP)\n",
    "MIN_VELOCITY  = 0.5   # Minimum units per day\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Ranking Parameters\n",
    "# -----------------------------------------------------------------------------\n",
    "TOP_PRODUCTS_PER_WAREHOUSE   = 200  # Initial selection\n",
    "FINAL_PRODUCTS_PER_WAREHOUSE = 133  # Final output\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Delivery Fees\n",
    "# -----------------------------------------------------------------------------\n",
    "DELIVERY_FEE_CAIRO_GIZA = 25\n",
    "DELIVERY_FEE_OTHER      = 20\n",
    "\n",
    "print(\"✓ Configuration loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4092441e-bd5e-4b90-9b98-a4325a9757ec",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995dfa24-7123-41e5-97fb-a45c125f3f15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_secret(secret_name):\n",
    "    \"\"\"\n",
    "    Retrieve secret from AWS Secrets Manager.\n",
    "    \n",
    "    Args:\n",
    "        secret_name: Name/ID of the secret to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        Secret string or decoded binary\n",
    "    \"\"\"\n",
    "    region_name = \"us-east-1\"\n",
    "    \n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(SecretId=secret_name)\n",
    "    except ClientError as e:\n",
    "        # Re-raise all AWS Secrets Manager exceptions\n",
    "        raise e\n",
    "    \n",
    "    # Return decrypted secret (string or binary)\n",
    "    if 'SecretString' in get_secret_value_response:\n",
    "        return get_secret_value_response['SecretString']\n",
    "    else:\n",
    "        return base64.b64decode(get_secret_value_response['SecretBinary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2e910e-65d9-4a69-9922-988548748eb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load API credentials from AWS Secrets Manager\n",
    "pricing_api_secret = json.loads(get_secret(\"prod/pricing/api/\"))\n",
    "username = pricing_api_secret[\"egypt_username\"]\n",
    "password = pricing_api_secret[\"egypt_password\"]\n",
    "secret   = pricing_api_secret[\"egypt_secret\"]\n",
    "\n",
    "print(\"✓ API credentials loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f29f77-2e72-4064-9503-ec9be748e835",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_access_token(url, client_id, client_secret):\n",
    "    \"\"\"\n",
    "    Get OAuth access token for MaxAB APIs.\n",
    "    \n",
    "    Args:\n",
    "        url: Token endpoint URL\n",
    "        client_id: OAuth client ID\n",
    "        client_secret: OAuth client secret\n",
    "        \n",
    "    Returns:\n",
    "        Access token string\n",
    "    \"\"\"\n",
    "    response = requests.post(\n",
    "        url,\n",
    "        data={\n",
    "            \"grant_type\": \"password\",\n",
    "            \"username\": username,\n",
    "            \"password\": password\n",
    "        },\n",
    "        auth=(client_id, client_secret),\n",
    "    )\n",
    "    return response.json()[\"access_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfb8760-ddf2-4192-beaf-cf52c6c92ff2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def post_QD(file_name):\n",
    "    \"\"\"\n",
    "    Upload Quantity Discount file to MaxAB API.\n",
    "    \n",
    "    Args:\n",
    "        file_name: Path to the Excel file to upload\n",
    "        \n",
    "    Returns:\n",
    "        API response object\n",
    "    \"\"\"\n",
    "    token = get_access_token(\n",
    "        'https://sso.maxab.info/auth/realms/maxab/protocol/openid-connect/token',\n",
    "        'main-system-externals',\n",
    "        secret\n",
    "    )\n",
    "    \n",
    "    url = \"https://api.maxab.info/commerce/api/admins/v1/quantity-discounts\"\n",
    "    \n",
    "    files = [\n",
    "        ('file', (file_name, open(file_name, 'rb'), \n",
    "                  'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'))\n",
    "    ]\n",
    "    headers = {'Authorization': f'bearer {token}'}\n",
    "    \n",
    "    response = requests.request(\"POST\", url, headers=headers, data={}, files=files)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44c2b13-8395-42c1-b902-ec404ba1bca1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def post_cart_rules(cohort_id, file_name):\n",
    "    \"\"\"\n",
    "    Upload Cart Rules file for a specific cohort.\n",
    "    \n",
    "    Args:\n",
    "        cohort_id: ID of the cohort to update\n",
    "        file_name: Path to the Excel file to upload\n",
    "        \n",
    "    Returns:\n",
    "        API response object\n",
    "    \"\"\"\n",
    "    token = get_access_token(\n",
    "        'https://sso.maxab.info/auth/realms/maxab/protocol/openid-connect/token',\n",
    "        'main-system-externals',\n",
    "        secret\n",
    "    )\n",
    "    \n",
    "    url = f\"https://api.maxab.info/main-system/api/admin-portal/cohorts/{cohort_id}/cart-rules\"\n",
    "    \n",
    "    files = [\n",
    "        ('sheet', (file_name, open(file_name, 'rb'),\n",
    "                   'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'))\n",
    "    ]\n",
    "    headers = {'Authorization': f'bearer {token}'}\n",
    "    \n",
    "    response = requests.request(\"POST\", url, headers=headers, data={}, files=files)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2601556c",
   "metadata": {},
   "source": [
    "### Database Connection Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dda92e1-ffd3-4d47-80a7-b65fc9a64ee4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def snowflake_query(country, query, warehouse=None, columns=[], conn=None):\n",
    "    \"\"\"\n",
    "    Execute a query against Snowflake and return results as DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        country: Country identifier (e.g., \"Egypt\")\n",
    "        query: SQL query string to execute\n",
    "        warehouse: Snowflake warehouse (optional)\n",
    "        columns: Custom column names (optional)\n",
    "        conn: Existing connection (optional)\n",
    "        \n",
    "    Returns:\n",
    "        pandas DataFrame with query results\n",
    "    \"\"\"\n",
    "    con = snowflake.connector.connect(\n",
    "        user     = os.environ[\"SNOWFLAKE_USERNAME\"],\n",
    "        account  = os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "        password = os.environ[\"SNOWFLAKE_PASSWORD\"],\n",
    "        database = os.environ[\"SNOWFLAKE_DATABASE\"]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        cur = con.cursor()\n",
    "        cur.execute(\"USE WAREHOUSE COMPUTE_WH\")\n",
    "        cur.execute(query)\n",
    "        \n",
    "        column_names = [col[0] for col in cur.description]\n",
    "        results = cur.fetchall()\n",
    "        \n",
    "        if not results:\n",
    "            out = pd.DataFrame(columns=[name.lower() for name in column_names])\n",
    "        else:\n",
    "            if len(columns) == 0:\n",
    "                out = pd.DataFrame(np.array(results), columns=column_names)\n",
    "                out.columns = out.columns.str.lower()\n",
    "            else:\n",
    "                out = pd.DataFrame(np.array(results), columns=columns)\n",
    "                out.columns = out.columns.str.lower()\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Query error: {e}\")\n",
    "        raise\n",
    "        \n",
    "    finally:\n",
    "        cur.close()\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febda4a3-f2e9-4601-b101-0dd290924eb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get Snowflake timezone for consistent date/time handling\n",
    "query = \"SHOW PARAMETERS LIKE 'TIMEZONE'\"\n",
    "timezone_result = snowflake_query(\"Egypt\", query)\n",
    "zone_to_use = timezone_result['value'].values[0]\n",
    "print(f\"✓ Using timezone: {zone_to_use}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e25101",
   "metadata": {},
   "source": [
    "### Google Sheets Connection (Force Brands)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5d55d8",
   "metadata": {},
   "source": [
    "## 2. Product Selection\n",
    "\n",
    "Select top-performing products per warehouse based on:\n",
    "- Gross profit ranking (40% weight)\n",
    "- Sales velocity ranking (25% weight)\n",
    "- Order count ranking (20% weight)\n",
    "- Retailer count ranking (15% weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225abe85-5954-48b3-97ac-dd49d3672c38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = ''' \n",
    "WITH rr AS (\n",
    "    SELECT product_id, warehouse_id, rr\n",
    "    FROM (\n",
    "        SELECT *, \n",
    "               MAX(date) OVER (PARTITION BY product_id, warehouse_id) as max_date\n",
    "        FROM finance.PREDICTED_RUNNING_RATES\n",
    "        QUALIFY date = max_date\n",
    "            AND date::date >= CURRENT_DATE - 14 \n",
    "    )\n",
    "),\n",
    "\n",
    "stocks AS (\n",
    "    SELECT \n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        SUM(stocks) as stocks,\n",
    "        CASE \n",
    "            WHEN SUM(rr) > 0 THEN SUM(stocks) / SUM(rr) \n",
    "            ELSE SUM(stocks) \n",
    "        END as doh\n",
    "    FROM (\n",
    "        SELECT DISTINCT \n",
    "            product_warehouse.warehouse_id,\n",
    "            product_warehouse.product_id,\n",
    "            (product_warehouse.available_stock)::integer as stocks,\n",
    "            COALESCE(rr.rr, 0) as rr \n",
    "        FROM product_warehouse\n",
    "        JOIN products ON product_warehouse.product_id = products.id\n",
    "        JOIN product_units ON products.unit_id = product_units.id\n",
    "        LEFT JOIN rr ON rr.product_id = products.id \n",
    "            AND rr.warehouse_id = product_warehouse.warehouse_id\n",
    "        WHERE product_warehouse.warehouse_id NOT IN (6, 9, 10)\n",
    "            AND product_warehouse.is_basic_unit = 1\n",
    "            AND product_warehouse.available_stock > 0 \n",
    "    )\n",
    "    GROUP BY warehouse_id, product_id\n",
    "    HAVING doh >= 1\n",
    "),\n",
    "\n",
    "base AS (\n",
    "    SELECT *, ROW_NUMBER() OVER (PARTITION BY retailer_id ORDER BY priority) as rnk \n",
    "    FROM (\n",
    "        SELECT x.*, TAGGABLE_ID as retailer_id \n",
    "        FROM (\n",
    "            SELECT id as cohort_id, name as cohort_name, priority, dynamic_tag_id \n",
    "            FROM cohorts \n",
    "            WHERE is_active = 'true'\n",
    "                AND id IN (700,701,702,703,704,1123,1124,1125,1126)\n",
    "        ) x \n",
    "        JOIN DYNAMIC_TAGgables dt ON x.dynamic_tag_id = dt.dynamic_tag_id\n",
    "        WHERE dt.taggable_id not IN (\n",
    "            SELECT taggable_id FROM DYNAMIC_TAGgables \n",
    "            WHERE dynamic_tag_id IN (2807, 2808, 2809, 2810, 2811, 2812)\n",
    "        )\n",
    "    )\n",
    "    QUALIFY rnk = 1 \n",
    "    ORDER BY cohort_id\n",
    "),\n",
    "\n",
    "-- Count total retailers per warehouse for penetration calculation\n",
    "warehouse_retailer_counts AS (\n",
    "    SELECT \n",
    "        whs.warehouse_id,\n",
    "        COUNT(DISTINCT base.retailer_id) as total_warehouse_retailers\n",
    "    FROM base\n",
    "    CROSS JOIN (SELECT DISTINCT warehouse_id FROM (VALUES\n",
    "            (38), (1), (236), (962), (337), (8), (339), (170), \n",
    "            (501), (401), (703), (632), (797)\n",
    "        ) x(warehouse_id)\n",
    "    ) whs\n",
    "    GROUP BY whs.warehouse_id\n",
    "),\n",
    "\n",
    "-- Map cohorts to warehouses\n",
    "cohort_warehouse_map AS (\n",
    "    SELECT cohort_id, warehouse_id\n",
    "    FROM (VALUES\n",
    "        (700, 38),   -- Cairo -> El-Marg\n",
    "        (700, 1),    -- Cairo -> Mostorod\n",
    "        (701, 236),  -- Giza -> Barageel\n",
    "        (701, 962),  -- Giza -> Sakkarah\n",
    "        (703, 337),  -- Delta West -> El-Mahala\n",
    "        (703, 8),    -- Delta West -> Tanta\n",
    "        (704, 339),  -- Delta East -> Mansoura FC\n",
    "        (704, 170),  -- Delta East -> Sharqya\n",
    "        (1124, 501), -- Upper Egypt -> Assiut FC\n",
    "        (1126, 401), -- Upper Egypt -> Bani sweif\n",
    "        (1123, 703), -- Upper Egypt -> Menya Samalot\n",
    "        (1125, 632), -- Upper Egypt -> Sohag\n",
    "        (702, 797)   -- Alexandria -> Khorshed Alex\n",
    "    ) x(cohort_id, warehouse_id)\n",
    "),\n",
    "\n",
    "-- Get pricing information by cohort (which maps to warehouse)\n",
    "cohort_prices AS (\n",
    "    SELECT  \n",
    "        cpu.cohort_id,\n",
    "        pu.product_id,\n",
    "        pu.packing_unit_id,\n",
    "        pu.basic_unit_count,\n",
    "        AVG(cpu.price) as price\n",
    "    FROM cohort_product_packing_units cpu\n",
    "    JOIN PACKING_UNIT_PRODUCTS pu ON pu.id = cpu.product_packing_unit_id\n",
    "    WHERE cpu.cohort_id IN (700,701,702,703,704,1123,1124,1125,1126)\n",
    "        AND cpu.created_at::date <> '2023-07-31'\n",
    "        AND cpu.is_customized = true\n",
    "    GROUP BY \n",
    "        cpu.cohort_id,\n",
    "        pu.product_id,\n",
    "        pu.packing_unit_id,\n",
    "        pu.basic_unit_count\n",
    "),\n",
    "\n",
    "-- Get live prices by cohort\n",
    "live_cohort_prices AS (\n",
    "    SELECT \n",
    "        cohort_id,\n",
    "        product_id,\n",
    "        pu_id as packing_unit_id,\n",
    "        buc as basic_unit_count,\n",
    "        NEW_PRICE as price\n",
    "    FROM materialized_views.DBDP_PRICES\n",
    "    WHERE created_at = CURRENT_DATE\n",
    "        AND DATE_PART('hour', CURRENT_TIME) BETWEEN SPLIT_PART(time_slot, '-', 1)::int AND SPLIT_PART(time_slot, '-', 2)::int\n",
    "        AND cohort_id IN (700,701,702,703,704,1123,1124,1125,1126)\n",
    "),\n",
    "\n",
    "-- Combine live and historical prices (live takes priority)\n",
    "combined_cohort_prices AS (\n",
    "    SELECT *\n",
    "    FROM (\n",
    "        SELECT *, 1 AS priority FROM live_cohort_prices\n",
    "        UNION ALL\n",
    "        SELECT *, 2 AS priority FROM cohort_prices\n",
    "    )\n",
    "    QUALIFY ROW_NUMBER() OVER (PARTITION BY cohort_id, product_id, packing_unit_id ORDER BY priority) = 1\n",
    "),\n",
    "\n",
    "-- Map cohort prices to warehouse prices\n",
    "warehouse_prices AS (\n",
    "    SELECT \n",
    "        cwm.warehouse_id,\n",
    "        ccp.product_id,\n",
    "        ccp.packing_unit_id,\n",
    "        ccp.basic_unit_count,\n",
    "        ccp.price\n",
    "    FROM combined_cohort_prices ccp\n",
    "    JOIN cohort_warehouse_map cwm ON cwm.cohort_id = ccp.cohort_id\n",
    "    WHERE ccp.price IS NOT NULL\n",
    "),\n",
    "\n",
    "-- Get sales performance over last 4 months\n",
    "product_performance AS (\n",
    "    SELECT \n",
    "        w.name as warehouse,\n",
    "        w.id as warehouse_id,\n",
    "        pso.product_id,\n",
    "        pso.packing_unit_id,\n",
    "        CONCAT(products.name_ar, ' ', products.size, ' ', product_units.name_ar) as sku,\n",
    "        brands.name_ar as brand,\n",
    "        categories.name_ar as category,\n",
    "        \n",
    "        -- Core volume metrics\n",
    "        COUNT(DISTINCT so.parent_sales_order_id) as total_orders,\n",
    "        COUNT(DISTINCT so.retailer_id) as total_retailers,\n",
    "        SUM(pso.purchased_item_count) as total_packing_units_sold,\n",
    "        SUM(pso.purchased_item_count * pso.basic_unit_count) as total_basic_units_sold,\n",
    "        \n",
    "        -- Revenue and margin\n",
    "        SUM(pso.total_price) as total_nmv,\n",
    "        SUM(COALESCE(f.wac_p, 0) * pso.purchased_item_count * pso.basic_unit_count) as total_cogs,\n",
    "        (SUM(pso.total_price) - SUM(COALESCE(f.wac_p, 0) * pso.purchased_item_count * pso.basic_unit_count)) / \n",
    "            NULLIF(SUM(pso.total_price), 0) as blended_margin,\n",
    "        \n",
    "        -- Average order metrics\n",
    "        AVG(pso.purchased_item_count) as avg_packing_units_per_order,\n",
    "        \n",
    "        -- Velocity metrics (units per day)\n",
    "        SUM(pso.purchased_item_count) / 120.0 as packing_units_per_day\n",
    "        \n",
    "    FROM product_sales_order pso\n",
    "    JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "    JOIN base ON base.retailer_id = so.retailer_id\n",
    "    JOIN products ON products.id = pso.product_id\n",
    "    JOIN brands ON products.brand_id = brands.id\n",
    "    JOIN categories ON products.category_id = categories.id \n",
    "        AND categories.name_ar NOT LIKE '%سايب%'\n",
    "    JOIN finance.all_cogs f ON f.product_id = pso.product_id\n",
    "        AND f.from_date::date <= so.created_at::date\n",
    "        AND f.to_date::date > so.created_at::date\n",
    "    JOIN product_units ON product_units.id = products.unit_id\n",
    "\tjoin warehouses w on w.id = pso.warehouse_id\n",
    "    \n",
    "    WHERE TRUE\n",
    "        AND so.created_at::date BETWEEN current_date - 60 AND CURRENT_DATE - 1\n",
    "        AND so.sales_order_status_id NOT IN (7, 12)\n",
    "        AND so.channel IN ('telesales', 'retailer')\n",
    "        AND pso.purchased_item_count <> 0\n",
    "        AND products.activation = 'true'\n",
    "        AND w.id NOT IN (6, 9, 10)\n",
    "    \n",
    "    GROUP BY All\n",
    "),\n",
    "\n",
    "-- Add retailer penetration\n",
    "product_performance_with_penetration AS (\n",
    "    SELECT \n",
    "        pp.*,\n",
    "        wrc.total_warehouse_retailers,\n",
    "        (pp.total_retailers * 100.0 / NULLIF(wrc.total_warehouse_retailers, 0)) as retailer_penetration_pct\n",
    "    FROM product_performance pp\n",
    "    LEFT JOIN warehouse_retailer_counts wrc ON wrc.warehouse_id = pp.warehouse_id\n",
    "),\n",
    "\n",
    "-- Add pricing information at warehouse level\n",
    "product_performance_with_price AS (\n",
    "    SELECT \n",
    "        pp.*,\n",
    "        COALESCE(wp.price, 0) as product_price,\n",
    "        COALESCE(wp.basic_unit_count, 1) as basic_unit_count\n",
    "    FROM product_performance_with_penetration pp\n",
    "    LEFT JOIN warehouse_prices wp ON wp.warehouse_id = pp.warehouse_id\n",
    "        AND wp.product_id = pp.product_id \n",
    "        AND wp.packing_unit_id = pp.packing_unit_id\n",
    "),\n",
    "\n",
    "-- Add quality filters to focus on high-potential products\n",
    "qualified_products AS (\n",
    "    SELECT \n",
    "        pp.warehouse,\n",
    "        pp.warehouse_id,\n",
    "        pp.product_id,\n",
    "        pp.packing_unit_id,\n",
    "        pp.sku,\n",
    "        pp.brand,\n",
    "        pp.category,\n",
    "        pp.total_orders,\n",
    "        pp.total_retailers,\n",
    "        pp.total_packing_units_sold,\n",
    "        pp.total_basic_units_sold,\n",
    "        pp.total_nmv,\n",
    "        pp.blended_margin,\n",
    "        pp.avg_packing_units_per_order,\n",
    "        pp.packing_units_per_day,\n",
    "        pp.retailer_penetration_pct,\n",
    "        pp.product_price,\n",
    "        pp.basic_unit_count,\n",
    "        s.doh,\n",
    "        s.stocks,\n",
    "        \n",
    "        -- Calculate a simple volume-based score\n",
    "        (pp.total_nmv * pp.blended_margin) as gross_profit,\n",
    "        \n",
    "        -- Rank by gross profit within warehouse\n",
    "        ROW_NUMBER() OVER (PARTITION BY pp.warehouse_id ORDER BY (pp.total_nmv * pp.blended_margin) DESC) as gp_rank,\n",
    "        \n",
    "        -- Rank by velocity\n",
    "        ROW_NUMBER() OVER (PARTITION BY pp.warehouse_id ORDER BY pp.packing_units_per_day DESC) as velocity_rank,\n",
    "        \n",
    "        -- Rank by orders\n",
    "        ROW_NUMBER() OVER (PARTITION BY pp.warehouse_id ORDER BY pp.total_orders DESC) as order_rank,\n",
    "        \n",
    "        -- Rank by number of retailers\n",
    "        ROW_NUMBER() OVER (PARTITION BY pp.warehouse_id ORDER BY pp.total_retailers DESC) as retailer_rank\n",
    "        \n",
    "    FROM product_performance_with_price pp\n",
    "    JOIN stocks s ON s.product_id = pp.product_id \n",
    "        AND s.warehouse_id = pp.warehouse_id\n",
    "\n",
    "),\n",
    "\n",
    "-- Select top products using a combined scoring approach\n",
    "top_products AS (\n",
    "    SELECT \n",
    "        warehouse,\n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        sku,\n",
    "        brand,\n",
    "        category,\n",
    "        total_orders,\n",
    "        total_retailers,\n",
    "        total_packing_units_sold,\n",
    "        total_basic_units_sold,\n",
    "        ROUND(total_nmv, 2) as total_nmv,\n",
    "        ROUND(blended_margin * 100, 2) as margin_pct,\n",
    "        ROUND(avg_packing_units_per_order, 2) as avg_order_qty,\n",
    "        ROUND(packing_units_per_day, 2) as units_per_day,\n",
    "        ROUND(retailer_penetration_pct, 1) as retailer_penetration_pct,\n",
    "        ROUND(gross_profit, 2) as gross_profit,\n",
    "        ROUND(product_price, 2) as packing_unit_price,\n",
    "        basic_unit_count,\n",
    "        ROUND(product_price / NULLIF(basic_unit_count, 0), 2) as price_per_basic_unit,\n",
    "        gp_rank,\n",
    "        velocity_rank,\n",
    "        order_rank,\n",
    "        retailer_rank,\n",
    "        ROUND(doh, 2) as days_on_hand,\n",
    "        stocks as available_stock,\n",
    "        \n",
    "        -- Combined score: weighted average of ranks (lower is better)\n",
    "        (gp_rank * 0.15 + velocity_rank * 0.20 + order_rank * 0.30 + retailer_rank * 0.35) as combined_rank_score\n",
    "        \n",
    "    FROM qualified_products\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    warehouse,\n",
    "    warehouse_id,\n",
    "    product_id,\n",
    "    packing_unit_id,\n",
    "    sku,\n",
    "    brand,\n",
    "    category as cat,\n",
    "    total_orders,\n",
    "    total_retailers,\n",
    "    total_packing_units_sold,\n",
    "    total_basic_units_sold,\n",
    "    total_nmv,\n",
    "    margin_pct,\n",
    "    avg_order_qty,\n",
    "    units_per_day,\n",
    "    retailer_penetration_pct,\n",
    "    gross_profit,\n",
    "    packing_unit_price,\n",
    "    basic_unit_count,\n",
    "    price_per_basic_unit,\n",
    "    days_on_hand,\n",
    "    available_stock,\n",
    "    gp_rank as gross_profit_rank,\n",
    "    velocity_rank,\n",
    "    order_rank,\n",
    "    retailer_rank,\n",
    "    ROUND(combined_rank_score, 2) as combined_score,\n",
    "    ROW_NUMBER() OVER (PARTITION BY warehouse ORDER BY combined_rank_score) as final_rank\n",
    "FROM top_products\n",
    "WHERE combined_rank_score <= 500  -- Adjust this to get more/fewer products\n",
    "qualify final_rank<=200\n",
    "ORDER BY warehouse, combined_rank_score;\n",
    "'''\n",
    "\n",
    "# Execute query and convert numeric columns\n",
    "print(\"Fetching product selection data...\")\n",
    "selected_products = snowflake_query(\"Egypt\", query)\n",
    "\n",
    "for col in selected_products.columns:\n",
    "    selected_products[col] = pd.to_numeric(selected_products[col], errors='ignore')\n",
    "\n",
    "print(f\"✓ Retrieved {len(selected_products)} products from {selected_products['warehouse_id'].nunique()} warehouses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489552af-531c-49c3-b495-88dce04f56ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove excluded products\n",
    "selected_products = selected_products[~selected_products['product_id'].isin(PRODUCTS_TO_REMOVE)]\n",
    "print(f\"✓ Selected {len(selected_products)} products after exclusions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e424fc72",
   "metadata": {},
   "source": [
    "## 3. Quantity Tier Calculation\n",
    "\n",
    "Calculate tier 1 and tier 2 quantities based on:\n",
    "- Order history from frequent buyers (2+ orders)\n",
    "- Statistical analysis (median, Q3, P85, P90, P95)\n",
    "- IQR outlier removal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5d7495-748e-4337-92ae-aaa77e3b8398",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_df = selected_products[['warehouse_id', 'product_id', 'packing_unit_id']].values.tolist()\n",
    "tuples_string = ','.join([f\"({int(wh_id)}, {int(prod_id)}, {int(pu_id)})\" for wh_id, prod_id, pu_id in selected_df])\n",
    "query = f'''\n",
    "WITH selected_products AS (\n",
    "    SELECT warehouse_id, product_id, packing_unit_id\n",
    "    FROM (VALUES\n",
    "      {tuples_string}\n",
    "    ) AS x(warehouse_id, product_id, packing_unit_id)\n",
    "),\n",
    "\n",
    "-- Same base filtering as product selection query\n",
    "-- Retailers in QD cohorts AND in specific dynamic tags\n",
    "base AS (\n",
    "    SELECT *, ROW_NUMBER() OVER (PARTITION BY retailer_id ORDER BY priority) as rnk \n",
    "    FROM (\n",
    "        SELECT x.*, TAGGABLE_ID as retailer_id \n",
    "        FROM (\n",
    "            SELECT id as cohort_id, name as cohort_name, priority, dynamic_tag_id \n",
    "            FROM cohorts \n",
    "            WHERE is_active = 'true'\n",
    "                AND id IN (700,701,702,703,704,1123,1124,1125,1126)\n",
    "        ) x \n",
    "        JOIN DYNAMIC_TAGgables dt ON x.dynamic_tag_id = dt.dynamic_tag_id\n",
    "        WHERE dt.taggable_id not IN (\n",
    "            SELECT taggable_id FROM DYNAMIC_TAGgables \n",
    "            WHERE dynamic_tag_id IN (2807, 2808, 2809, 2810, 2811, 2812)\n",
    "        )\n",
    "    )\n",
    "    QUALIFY rnk = 1 \n",
    "),\n",
    "\n",
    "raw_order_quantities AS (\n",
    "    SELECT \n",
    "        whs.wh as warehouse,\n",
    "        whs.warehouse_id,\n",
    "        pso.product_id,\n",
    "        pso.packing_unit_id,\n",
    "        CONCAT(products.name_ar, ' ', products.size, ' ', product_units.name_ar) as sku,\n",
    "        brands.name_ar as brand,\n",
    "        categories.name_ar as category,\n",
    "        so.parent_sales_order_id,\n",
    "        so.retailer_id,\n",
    "        so.created_at::date as order_date,\n",
    "        SUM(pso.purchased_item_count) as order_qty,\n",
    "        SUM(pso.total_price) as order_value,\n",
    "        -- ADD RECENCY WEIGHT: Recent orders get higher weight (exponential decay)\n",
    "        EXP(-0.02 * DATEDIFF('day', so.created_at::date, CURRENT_DATE)) as recency_weight\n",
    "        \n",
    "    FROM product_sales_order pso\n",
    "    JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "    -- Filter to only include retailers from base (same cohorts + tags as product selection)\n",
    "    JOIN base ON base.retailer_id = so.retailer_id\n",
    "    JOIN products ON products.id = pso.product_id\n",
    "    JOIN brands ON products.brand_id = brands.id\n",
    "    JOIN categories ON products.category_id = categories.id\n",
    "    JOIN product_units ON product_units.id = products.unit_id\n",
    "    JOIN materialized_views.retailer_polygon ON materialized_views.retailer_polygon.retailer_id = so.retailer_id\n",
    "    JOIN districts ON districts.id = materialized_views.retailer_polygon.district_id\n",
    "    JOIN cities ON cities.id = districts.city_id\n",
    "    JOIN states ON states.id = cities.state_id\n",
    "    JOIN regions ON regions.id = states.region_id\n",
    "    JOIN (SELECT * FROM (VALUES\n",
    "            ('Cairo', 'El-Marg', 38),\n",
    "            ('Cairo', 'Mostorod', 1),\n",
    "            ('Giza', 'Barageel', 236),\n",
    "            ('Giza', 'Sakkarah', 962),\n",
    "            ('Delta West', 'El-Mahala', 337),\n",
    "            ('Delta West', 'Tanta', 8),\n",
    "            ('Delta East', 'Mansoura FC', 339),\n",
    "            ('Delta East', 'Sharqya', 170),\n",
    "            ('Upper Egypt', 'Assiut FC', 501),\n",
    "            ('Upper Egypt', 'Bani sweif', 401),\n",
    "            ('Upper Egypt', 'Menya Samalot', 703),\n",
    "            ('Upper Egypt', 'Sohag', 632),\n",
    "            ('Alexandria', 'Khorshed Alex', 797)\n",
    "        ) x(region_name, wh, warehouse_id)\n",
    "    ) whs ON whs.region_name = CASE WHEN regions.id = 2 THEN states.name_en ELSE regions.name_en END\n",
    "    JOIN selected_products sp ON sp.warehouse_id = whs.warehouse_id \n",
    "        AND sp.product_id = pso.product_id\n",
    "        AND sp.packing_unit_id = pso.packing_unit_id\n",
    "    \n",
    "    WHERE TRUE\n",
    "        AND so.created_at::date BETWEEN DATE_TRUNC('month', CURRENT_DATE - INTERVAL '4 months') AND CURRENT_DATE - 1\n",
    "        AND so.sales_order_status_id NOT IN (7, 12)\n",
    "        AND so.channel IN ('telesales', 'retailer')\n",
    "        AND pso.purchased_item_count <> 0\n",
    "        AND products.activation = 'true'\n",
    "    \n",
    "    GROUP BY \n",
    "        whs.wh,\n",
    "        whs.warehouse_id,\n",
    "        pso.product_id,\n",
    "        pso.packing_unit_id,\n",
    "        products.name_ar,\n",
    "        products.size,\n",
    "        product_units.name_ar,\n",
    "        brands.name_ar,\n",
    "        categories.name_ar,\n",
    "        so.parent_sales_order_id,\n",
    "        so.retailer_id,\n",
    "        so.created_at::date\n",
    "),\n",
    "\n",
    "retailer_frequency AS (\n",
    "    SELECT \n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        retailer_id,\n",
    "        COUNT(DISTINCT parent_sales_order_id) as order_count,\n",
    "        COUNT(DISTINCT DATE_TRUNC('week', order_date)) as weeks_ordered,\n",
    "        MIN(order_date) as first_order_date,\n",
    "        MAX(order_date) as last_order_date,\n",
    "        DATEDIFF('day', MIN(order_date), MAX(order_date)) as days_span,\n",
    "        CASE \n",
    "            WHEN COUNT(DISTINCT parent_sales_order_id) > 1 \n",
    "            THEN DATEDIFF('day', MIN(order_date), MAX(order_date)) / (COUNT(DISTINCT parent_sales_order_id) - 1)\n",
    "            ELSE NULL \n",
    "        END as avg_days_between_orders\n",
    "    FROM raw_order_quantities\n",
    "    GROUP BY warehouse_id, product_id, packing_unit_id, retailer_id\n",
    "),\n",
    "\n",
    "frequent_buyers AS (\n",
    "    SELECT \n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        retailer_id,\n",
    "        order_count,\n",
    "        weeks_ordered,\n",
    "        avg_days_between_orders\n",
    "    FROM retailer_frequency\n",
    "    WHERE order_count >= 2 \n",
    "       OR weeks_ordered >= 2\n",
    "),\n",
    "\n",
    "filtered_orders AS (\n",
    "    SELECT roq.*\n",
    "    FROM raw_order_quantities roq\n",
    "    JOIN frequent_buyers fb \n",
    "        ON fb.warehouse_id = roq.warehouse_id\n",
    "        AND fb.product_id = roq.product_id\n",
    "        AND fb.packing_unit_id = roq.packing_unit_id\n",
    "        AND fb.retailer_id = roq.retailer_id\n",
    "),\n",
    "\n",
    "initial_stats AS (\n",
    "    SELECT \n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY order_qty) as q1,\n",
    "        PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY order_qty) as q3,\n",
    "        MEDIAN(order_qty) as median_qty,\n",
    "        STDDEV_POP(order_qty) as stddev_qty,\n",
    "        AVG(order_qty) as avg_qty\n",
    "    FROM filtered_orders\n",
    "    GROUP BY warehouse_id, product_id, packing_unit_id\n",
    "),\n",
    "\n",
    "cleaned_orders AS (\n",
    "    SELECT fo.*\n",
    "    FROM filtered_orders fo\n",
    "    JOIN initial_stats ist \n",
    "        ON ist.warehouse_id = fo.warehouse_id\n",
    "        AND ist.product_id = fo.product_id\n",
    "        AND ist.packing_unit_id = fo.packing_unit_id\n",
    "    WHERE TRUE\n",
    "        AND fo.order_qty >= ist.q1 - 1.5 * (ist.q3 - ist.q1)\n",
    "        AND fo.order_qty <= ist.q3 + 1.5 * (ist.q3 - ist.q1)\n",
    "        AND (ist.stddev_qty = 0 \n",
    "             OR ABS(fo.order_qty - ist.avg_qty) <= 3 * ist.stddev_qty)\n",
    "),\n",
    "\n",
    "-- MODIFIED: Recent orders stats (last 15 days)\n",
    "recent_trends AS (\n",
    "    SELECT \n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        -- Weighted average gives more importance to recent orders\n",
    "        SUM(order_qty * recency_weight) / NULLIF(SUM(recency_weight), 0) as weighted_avg_qty,\n",
    "        -- Last 15 days statistics\n",
    "        AVG(CASE WHEN order_date >= CURRENT_DATE - 15 THEN order_qty END) as last_15d_avg,\n",
    "        MEDIAN(CASE WHEN order_date >= CURRENT_DATE - 15 THEN order_qty END) as last_15d_median,\n",
    "        MAX(CASE WHEN order_date >= CURRENT_DATE - 15 THEN order_qty END) as last_15d_max,\n",
    "        COUNT(CASE WHEN order_date >= CURRENT_DATE - 15 THEN 1 END) as last_15d_orders\n",
    "    FROM cleaned_orders\n",
    "    GROUP BY warehouse_id, product_id, packing_unit_id\n",
    "),\n",
    "\n",
    "quantity_stats AS (\n",
    "    SELECT \n",
    "        warehouse,\n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        sku,\n",
    "        brand,\n",
    "        category,\n",
    "        \n",
    "        COUNT(DISTINCT parent_sales_order_id) as total_orders,\n",
    "        COUNT(DISTINCT retailer_id) as total_retailers,\n",
    "        \n",
    "        MIN(order_qty) as min_qty,\n",
    "        MAX(order_qty) as max_qty,\n",
    "        AVG(order_qty) as avg_qty,\n",
    "        MEDIAN(order_qty) as median_qty,\n",
    "        STDDEV_POP(order_qty) as stddev_qty,\n",
    "        \n",
    "        PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY order_qty) as q1_qty,\n",
    "        PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY order_qty) as q2_qty,\n",
    "        PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY order_qty) as q3_qty,\n",
    "        PERCENTILE_CONT(0.85) WITHIN GROUP (ORDER BY order_qty) as p85_qty,\n",
    "        PERCENTILE_CONT(0.90) WITHIN GROUP (ORDER BY order_qty) as p90_qty,\n",
    "        PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY order_qty) as p95_qty,\n",
    "        \n",
    "        SUM(order_value) as total_revenue,\n",
    "        AVG(order_value) as avg_order_value\n",
    "        \n",
    "    FROM cleaned_orders\n",
    "    GROUP BY \n",
    "        warehouse,\n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        sku,\n",
    "        brand,\n",
    "        category\n",
    "),\n",
    "\n",
    "frequency_table AS (\n",
    "    SELECT\n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        order_qty,\n",
    "        COUNT(DISTINCT parent_sales_order_id) AS freq\n",
    "    FROM cleaned_orders\n",
    "    GROUP BY warehouse_id, product_id, packing_unit_id, order_qty\n",
    "),\n",
    "\n",
    "lag_lead AS (\n",
    "    SELECT\n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        order_qty,\n",
    "        freq,\n",
    "        LAG(freq) OVER (PARTITION BY warehouse_id, product_id, packing_unit_id ORDER BY order_qty) AS prev_freq,\n",
    "        LEAD(freq) OVER (PARTITION BY warehouse_id, product_id, packing_unit_id ORDER BY order_qty) AS next_freq\n",
    "    FROM frequency_table\n",
    "),\n",
    "\n",
    "most_frequent_qty AS (\n",
    "    SELECT \n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        order_qty as mode_qty,\n",
    "        freq as mode_freq,\n",
    "        freq * 1.0 / SUM(freq) OVER (PARTITION BY warehouse_id, product_id, packing_unit_id) as mode_contribution\n",
    "    FROM (\n",
    "        SELECT *,\n",
    "               ROW_NUMBER() OVER (PARTITION BY warehouse_id, product_id, packing_unit_id ORDER BY freq DESC, order_qty DESC) as rn\n",
    "        FROM lag_lead\n",
    "        WHERE (freq > COALESCE(prev_freq, -1))\n",
    "          AND (freq > COALESCE(next_freq, -1))\n",
    "    )\n",
    "    WHERE rn = 1\n",
    "),\n",
    "\n",
    "frequency_metrics AS (\n",
    "    SELECT \n",
    "        fb.warehouse_id,\n",
    "        fb.product_id,\n",
    "        fb.packing_unit_id,\n",
    "        COUNT(DISTINCT fb.retailer_id) as frequent_retailer_count,\n",
    "        AVG(fb.order_count) as avg_orders_per_retailer,\n",
    "        AVG(fb.avg_days_between_orders) as avg_refill_days,\n",
    "        MEDIAN(fb.avg_days_between_orders) as median_refill_days\n",
    "    FROM frequent_buyers fb\n",
    "    GROUP BY fb.warehouse_id, fb.product_id, fb.packing_unit_id\n",
    "),\n",
    "\n",
    "tier_calculations AS (\n",
    "    SELECT \n",
    "        qs.*,\n",
    "        COALESCE(mf.mode_qty, qs.median_qty) as mode_qty,\n",
    "        COALESCE(mf.mode_freq, 0) as mode_freq,\n",
    "        COALESCE(mf.mode_contribution, 0) as mode_contribution,\n",
    "        COALESCE(fm.frequent_retailer_count, 0) as frequent_retailer_count,\n",
    "        COALESCE(fm.avg_orders_per_retailer, 0) as avg_orders_per_retailer,\n",
    "        COALESCE(fm.avg_refill_days, 0) as avg_refill_days,\n",
    "        COALESCE(fm.median_refill_days, 0) as median_refill_days,\n",
    "        \n",
    "        -- ADD: Recency metrics\n",
    "        rt.weighted_avg_qty,\n",
    "        rt.last_15d_avg,\n",
    "        rt.last_15d_median,\n",
    "        rt.last_15d_max,\n",
    "        rt.last_15d_orders,\n",
    "        \n",
    "        -- MODIFIED: Tier 1 with 15-day recency factor\n",
    "        -- Blends historical median with recent trends (70% historical, 30% recent)\n",
    "        CEIL(GREATEST(\n",
    "            (0.7 * qs.median_qty + 0.3 * COALESCE(rt.weighted_avg_qty, qs.median_qty)) + 1.0 * COALESCE(qs.stddev_qty, 1),\n",
    "            qs.q3_qty,\n",
    "            COALESCE(mf.mode_qty, qs.median_qty) + GREATEST(3, qs.median_qty * 0.3),\n",
    "            -- If recent 15 days show growth, adjust upward\n",
    "            CASE \n",
    "                WHEN rt.last_15d_orders >= 3 AND rt.last_15d_median > qs.median_qty \n",
    "                THEN rt.last_15d_median * 1.2\n",
    "                ELSE qs.median_qty * 1.4\n",
    "            END,\n",
    "            qs.median_qty + 3\n",
    "        )) as tier_1_qty,\n",
    "        \n",
    "        -- MODIFIED: Tier 2 with 15-day recency factor\n",
    "        CEIL(GREATEST(\n",
    "            qs.q3_qty + 1.5 * COALESCE(qs.stddev_qty, 1),\n",
    "            qs.p85_qty + 1.0 * COALESCE(qs.stddev_qty, 1),\n",
    "            qs.p90_qty + 0.5 * COALESCE(qs.stddev_qty, 1),\n",
    "            qs.p95_qty,\n",
    "            -- Blend historical and weighted average\n",
    "            (0.6 * qs.median_qty + 0.4 * COALESCE(rt.weighted_avg_qty, qs.median_qty)) * 2.0,\n",
    "            -- If last 15 days show higher demand, adjust tier 2 upward\n",
    "            CASE \n",
    "                WHEN rt.last_15d_orders >= 3 AND rt.last_15d_max > qs.p90_qty \n",
    "                THEN rt.last_15d_max * 1.1\n",
    "                ELSE qs.median_qty * 2.0\n",
    "            END\n",
    "        )) as tier_2_qty_base\n",
    "        \n",
    "    FROM quantity_stats qs\n",
    "    LEFT JOIN most_frequent_qty mf \n",
    "        ON mf.warehouse_id = qs.warehouse_id \n",
    "        AND mf.product_id = qs.product_id\n",
    "        AND mf.packing_unit_id = qs.packing_unit_id\n",
    "    LEFT JOIN frequency_metrics fm\n",
    "        ON fm.warehouse_id = qs.warehouse_id\n",
    "        AND fm.product_id = qs.product_id\n",
    "        AND fm.packing_unit_id = qs.packing_unit_id\n",
    "    LEFT JOIN recent_trends rt\n",
    "        ON rt.warehouse_id = qs.warehouse_id\n",
    "        AND rt.product_id = qs.product_id\n",
    "        AND rt.packing_unit_id = qs.packing_unit_id\n",
    "),\n",
    "\n",
    "tier_adjustments AS (\n",
    "    SELECT \n",
    "        warehouse,\n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        sku,\n",
    "        brand,\n",
    "        category,\n",
    "        total_orders,\n",
    "        total_retailers,\n",
    "        min_qty,\n",
    "        avg_qty,\n",
    "        median_qty,\n",
    "        stddev_qty,\n",
    "        q1_qty,\n",
    "        q3_qty,\n",
    "        p85_qty,\n",
    "        p90_qty,\n",
    "        p95_qty,\n",
    "        max_qty,\n",
    "        mode_qty,\n",
    "        mode_freq,\n",
    "        mode_contribution,\n",
    "        frequent_retailer_count,\n",
    "        avg_orders_per_retailer,\n",
    "        avg_refill_days,\n",
    "        median_refill_days,\n",
    "        total_revenue,\n",
    "        avg_order_value,\n",
    "        \n",
    "        -- ADD: Recency metrics to output\n",
    "        weighted_avg_qty,\n",
    "        last_15d_avg,\n",
    "        last_15d_median,\n",
    "        last_15d_max,\n",
    "        last_15d_orders,\n",
    "        \n",
    "        tier_1_qty,\n",
    "        LEAST(\n",
    "            CEIL(GREATEST(\n",
    "                tier_2_qty_base,\n",
    "                tier_1_qty * 1.6\n",
    "            )),\n",
    "            GREATEST(\n",
    "                tier_1_qty * 3.5,\n",
    "                tier_1_qty + 20\n",
    "            )\n",
    "        ) as tier_2_qty\n",
    "        \n",
    "    FROM tier_calculations\n",
    "),\n",
    "\n",
    "retailer_distribution AS (\n",
    "    SELECT \n",
    "        co.warehouse_id,\n",
    "        co.product_id,\n",
    "        co.packing_unit_id,\n",
    "        ta.tier_1_qty,\n",
    "        ta.tier_2_qty,\n",
    "        COUNT(DISTINCT CASE \n",
    "            WHEN co.order_qty < ta.tier_1_qty THEN co.retailer_id \n",
    "        END) as retailers_below_t1,\n",
    "        COUNT(DISTINCT CASE \n",
    "            WHEN co.order_qty >= ta.tier_1_qty AND co.order_qty < ta.tier_2_qty THEN co.retailer_id \n",
    "        END) as retailers_at_t1,\n",
    "        COUNT(DISTINCT CASE \n",
    "            WHEN co.order_qty >= ta.tier_2_qty THEN co.retailer_id \n",
    "        END) as retailers_at_t2,\n",
    "        COUNT(CASE \n",
    "            WHEN co.order_qty < ta.tier_1_qty THEN 1 \n",
    "        END) as orders_below_t1,\n",
    "        COUNT(CASE \n",
    "            WHEN co.order_qty >= ta.tier_1_qty AND co.order_qty < ta.tier_2_qty THEN 1 \n",
    "        END) as orders_at_t1,\n",
    "        COUNT(CASE \n",
    "            WHEN co.order_qty >= ta.tier_2_qty THEN 1 \n",
    "        END) as orders_at_t2\n",
    "    FROM cleaned_orders co\n",
    "    JOIN tier_adjustments ta \n",
    "        ON ta.warehouse_id = co.warehouse_id \n",
    "        AND ta.product_id = co.product_id\n",
    "        AND ta.packing_unit_id = co.packing_unit_id\n",
    "    GROUP BY \n",
    "        co.warehouse_id,\n",
    "        co.product_id,\n",
    "        co.packing_unit_id,\n",
    "        ta.tier_1_qty,\n",
    "        ta.tier_2_qty\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    ta.warehouse,\n",
    "    ta.warehouse_id,\n",
    "    ta.product_id,\n",
    "    ta.packing_unit_id,\n",
    "    ta.sku,\n",
    "    ta.brand,\n",
    "    ta.category,\n",
    "    \n",
    "    ta.frequent_retailer_count,\n",
    "    ROUND(ta.avg_orders_per_retailer, 2) as avg_orders_per_retailer,\n",
    "    ROUND(ta.avg_refill_days, 1) as avg_refill_days,\n",
    "    ROUND(ta.median_refill_days, 1) as median_refill_days,\n",
    "    \n",
    "    ta.total_orders,\n",
    "    ta.total_retailers,\n",
    "    \n",
    "    ta.min_qty,\n",
    "    ROUND(ta.avg_qty, 2) as avg_qty,\n",
    "    ta.median_qty,\n",
    "    ROUND(ta.weighted_avg_qty, 2) as weighted_avg_qty,\n",
    "    ta.q1_qty as q1_25_qty,\n",
    "    ta.q3_qty as q3_75_qty,\n",
    "    ta.p85_qty,\n",
    "    ta.p90_qty,\n",
    "    ta.p95_qty,\n",
    "    ta.max_qty,\n",
    "    ROUND(ta.stddev_qty, 2) as stddev_qty,\n",
    "    ta.mode_qty,\n",
    "    ta.mode_freq,\n",
    "    ROUND(ta.mode_contribution * 100, 1) as mode_pct,\n",
    "    \n",
    "    -- MODIFIED: 15-day trend metrics\n",
    "    ROUND(ta.last_15d_avg, 2) as last_15d_avg,\n",
    "    ta.last_15d_median,\n",
    "    ta.last_15d_max,\n",
    "    ta.last_15d_orders,\n",
    "    \n",
    "    ta.tier_1_qty,\n",
    "    ta.tier_2_qty,\n",
    "    ROUND((ta.tier_1_qty - ta.median_qty) * 100.0 / NULLIF(ta.median_qty, 0), 1) as tier_1_increase_pct,\n",
    "    ROUND((ta.tier_2_qty - ta.median_qty) * 100.0 / NULLIF(ta.median_qty, 0), 1) as tier_2_increase_pct,\n",
    "    ROUND(ta.tier_2_qty * 1.0 / NULLIF(ta.tier_1_qty, 0), 2) as tier_2_to_tier_1_ratio,\n",
    "    \n",
    "    rd.retailers_below_t1,\n",
    "    rd.retailers_at_t1,\n",
    "    rd.retailers_at_t2,\n",
    "    \n",
    "    rd.orders_below_t1,\n",
    "    rd.orders_at_t1,\n",
    "    rd.orders_at_t2,\n",
    "    \n",
    "    ROUND(100.0 * rd.retailers_below_t1 / NULLIF(ta.total_retailers, 0), 1) as pct_retailers_below_t1,\n",
    "    ROUND(100.0 * rd.retailers_at_t1 / NULLIF(ta.total_retailers, 0), 1) as pct_retailers_at_t1,\n",
    "    ROUND(100.0 * rd.retailers_at_t2 / NULLIF(ta.total_retailers, 0), 1) as pct_retailers_at_t2,\n",
    "    \n",
    "    ROUND(100.0 * rd.orders_below_t1 / NULLIF(ta.total_orders, 0), 1) as pct_orders_below_t1,\n",
    "    ROUND(100.0 * rd.orders_at_t1 / NULLIF(ta.total_orders, 0), 1) as pct_orders_at_t1,\n",
    "    ROUND(100.0 * rd.orders_at_t2 / NULLIF(ta.total_orders, 0), 1) as pct_orders_at_t2,\n",
    "    \n",
    "    ROUND(ta.total_revenue, 2) as total_revenue,\n",
    "    ROUND(ta.avg_order_value, 2) as avg_order_value\n",
    "\n",
    "FROM tier_adjustments ta\n",
    "JOIN retailer_distribution rd \n",
    "    ON rd.warehouse_id = ta.warehouse_id \n",
    "    AND rd.product_id = ta.product_id\n",
    "    AND rd.packing_unit_id = ta.packing_unit_id\n",
    "ORDER BY ta.warehouse, ta.total_orders DESC\n",
    "'''\n",
    "\n",
    "# Execute query and convert numeric columns\n",
    "print(\"Fetching quantity tier data...\")\n",
    "tiers_selection = snowflake_query(\"Egypt\", query)\n",
    "\n",
    "for col in tiers_selection.columns:\n",
    "    tiers_selection[col] = pd.to_numeric(tiers_selection[col], errors='ignore')\n",
    "\n",
    "print(f\"✓ Calculated tiers for {len(tiers_selection)} product-warehouse combinations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc05aac1",
   "metadata": {},
   "source": [
    "### SKU Information & Cost Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8045f25-1b32-437c-81e8-a04f9771908f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f'''\n",
    "SELECT DISTINCT  \n",
    "    products.id as product_id,\n",
    "    CONCAT(products.name_ar, ' ', products.size, ' ', product_units.name_ar) as sku,\n",
    "    brands.name_ar as brand, \n",
    "    categories.name_ar as cat,\n",
    "    f.wac_p\n",
    "FROM products \n",
    "JOIN brands ON products.brand_id = brands.id \n",
    "JOIN categories ON products.category_id = categories.id\n",
    "JOIN finance.all_cogs f ON f.product_id = products.id \n",
    "    AND CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMESTAMP()) \n",
    "        BETWEEN f.from_date AND f.to_date \n",
    "JOIN product_units ON product_units.id = products.unit_id \n",
    "'''\n",
    "\n",
    "print(\"Fetching SKU information and WAC data...\")\n",
    "sku_info = snowflake_query(\"Egypt\", query)\n",
    "sku_info['product_id'] = pd.to_numeric(sku_info['product_id'])\n",
    "sku_info['wac_p'] = pd.to_numeric(sku_info['wac_p'])\n",
    "\n",
    "print(f\"✓ Retrieved cost data for {len(sku_info)} SKUs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ff38bd-bbc5-4ca6-b152-c283255d067a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Market Prices\n",
    "\n",
    "Gather competitive pricing data from multiple sources:\n",
    "- **Marketplace prices** - Regional marketplace data with fallbacks\n",
    "- **Ben Soliman prices** - Competitor pricing\n",
    "- **Scraped prices** - Web-scraped competitor data\n",
    "- **Product statistics** - Historical margin boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd94d09",
   "metadata": {},
   "source": [
    "### 4.1 Marketplace Prices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873a9cfe-76fb-486c-b71c-b2c3c447b7af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f'''\n",
    "WITH whs as (SELECT *\n",
    "             FROM   (values\n",
    "                            ('Cairo', 'El-Marg', 38,700),\n",
    "                            ('Cairo', 'Mostorod', 1,700),\n",
    "                            ('Giza', 'Barageel', 236,701),\n",
    "                            ('Delta West', 'El-Mahala', 337,703),\n",
    "                            ('Delta West', 'Tanta', 8,703),\n",
    "                            ('Delta East', 'Mansoura FC', 339,704),\n",
    "                            ('Delta East', 'Sharqya', 170,704),\n",
    "                            ('Upper Egypt', 'Assiut FC', 501,1124),\n",
    "                            ('Upper Egypt', 'Bani sweif', 401,1126),\n",
    "                            ('Upper Egypt', 'Menya Samalot', 703,1123),\n",
    "                            ('Upper Egypt', 'Sohag', 632,1125),\n",
    "                            ('Alexandria', 'Khorshed Alex', 797,702),\n",
    "\t\t\t\t\t\t\t('Giza', 'Sakkarah', 962,701)\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t)\n",
    "                    x(region, wh, warehouse_id,cohort_id)),\n",
    "full_data as (\n",
    "select products.id as product_id, region,warehouse_id\n",
    "from products , whs \n",
    "where activation = 'true'\n",
    "),\t\t\t\t\n",
    "\n",
    "MP as (\n",
    "select region,product_id,\n",
    "min(min_price) as min_price,\n",
    "min(max_price) as max_price,\n",
    "min(mod_price) as mod_price,\n",
    "min(true_min) as true_min,\n",
    "min(true_max) as true_max\n",
    "\n",
    "from (\n",
    "select mp.region,mp.product_id,mp.pu_id,\n",
    "min_price/BASIC_UNIT_COUNT as min_price,\n",
    "max_price/BASIC_UNIT_COUNT as max_price,\n",
    "mod_price/BASIC_UNIT_COUNT as mod_price,\n",
    "TRUE_MIN_PRICE/BASIC_UNIT_COUNT as true_min,\n",
    "TRUE_MAX_PRICE/BASIC_UNIT_COUNT as true_max\n",
    "from materialized_views.marketplace_prices mp \n",
    "join packing_unit_products pup on pup.product_id = mp.product_id and pup.packing_unit_id = mp.pu_id\n",
    "join finance.all_cogs f on f.product_id = mp.product_id and CURRENT_TIMESTAMP between f.from_date and f.to_date\n",
    "where  least(min_price,mod_price) between wac_p*0.9 and wac_p*1.3 \n",
    ")\n",
    "group by all \n",
    "),\n",
    "region_mapping AS (\n",
    "    SELECT * \n",
    "\tFROM \n",
    "\t(\tVALUES\n",
    "        ('Delta East', 'Delta West'),\n",
    "        ('Delta West', 'Delta East'),\n",
    "        ('Alexandria', 'Cairo'),\n",
    "        ('Alexandria', 'Giza'),\n",
    "        ('Upper Egypt', 'Cairo'),\n",
    "        ('Upper Egypt', 'Giza'),\n",
    "\t\t('Cairo','Giza'),\n",
    "\t\t('Giza','Cairo'),\n",
    "\t\t('Delta West', 'Cairo'),\n",
    "\t\t('Delta East', 'Cairo'),\n",
    "\t\t('Delta West', 'Giza'),\n",
    "\t\t('Delta East', 'Giza')\n",
    "\t\t)\n",
    "    AS region_mapping(region, fallback_region)\n",
    ")\n",
    "\n",
    "\n",
    "select region,warehouse_id,product_id,\n",
    "min(final_min_price) as final_min_price,\n",
    "min(final_max_price) as final_max_price,\n",
    "min(final_mod_price) as final_mod_price,\n",
    "min(final_true_min) as final_true_min,\n",
    "min(final_true_max) as final_true_max\n",
    "\n",
    "from (\n",
    "SELECT\n",
    "distinct \n",
    "\tw.region,\n",
    "    w.warehouse_id,\n",
    "\tw.product_id,\n",
    "    COALESCE(m1.min_price, m2.min_price) AS final_min_price,\n",
    "    COALESCE(m1.max_price, m2.max_price) AS final_max_price,\n",
    "    COALESCE(m1.mod_price, m2.mod_price) AS final_mod_price,\n",
    "\tCOALESCE(m1.true_min, m2.true_min) AS final_true_min,\n",
    "\tCOALESCE(m1.true_max, m2.true_max) AS final_true_max,\n",
    "FROM full_data w\n",
    "LEFT JOIN MP m1\n",
    "    ON w.region = m1.region and w.product_id = m1.product_id\n",
    "JOIN region_mapping rm\n",
    "    ON w.region = rm.region\n",
    "LEFT JOIN MP m2\n",
    "    ON rm.fallback_region = m2.region\n",
    "   AND w.product_id = m2.product_id\n",
    ")\n",
    "where final_min_price is not null \n",
    "group by all \n",
    "'''\n",
    "\n",
    "print(\"Fetching marketplace prices...\")\n",
    "marketplace = snowflake_query(\"Egypt\", query)\n",
    "marketplace.columns = marketplace.columns.str.lower()\n",
    "\n",
    "for col in marketplace.columns:\n",
    "    marketplace[col] = pd.to_numeric(marketplace[col], errors='ignore')\n",
    "\n",
    "print(f\"✓ Retrieved marketplace prices for {len(marketplace)} products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd2f7ff",
   "metadata": {},
   "source": [
    "### 4.2 Ben Soliman (Competitor) Prices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede9cdec-a69d-4579-afcc-57df08a5f753",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f'''\n",
    "select z.* \n",
    "from (\n",
    "select maxab_product_id as product_id,avg(bs_final_price) as ben_soliman_price\n",
    "from (\n",
    "select * , row_number()over(partition by maxab_product_id order by diff) as rnk_2\n",
    "from (\n",
    "select *,(bs_final_price-wac_p)/wac_p as diff_2\n",
    "from (\n",
    "select * ,bs_price/maxab_basic_unit_count as bs_final_price\n",
    "from (\n",
    "select *,row_number()over(partition by maxab_product_id,maxab_pu order by diff) as rnk \n",
    "from (\n",
    "select sm.* ,max(INJECTION_DATE::date)over(partition by maxab_product_id,maxab_pu) as max_date,wac1,wac_p,abs(bs_price-(wac_p*maxab_basic_unit_count))/(wac_p*maxab_basic_unit_count) as diff \n",
    "from materialized_views.savvy_mapping sm \n",
    "join finance.all_cogs f on f.product_id = sm.maxab_product_id and current_timestamp between f.from_Date and f.to_date\n",
    "where bs_price is not null \n",
    "and INJECTION_DATE::date >= CURRENT_DATE- 5\n",
    "qualify INJECTION_DATE::date = max_date\n",
    ")\n",
    "qualify rnk = 1 \n",
    ")\n",
    ")\n",
    "where diff_2 between -0.5 and 0.5 \n",
    ")\n",
    "qualify rnk_2 = 1 \n",
    ")\n",
    "group by all\n",
    ")z \n",
    "join finance.all_cogs f on f.product_id = z.product_id and current_timestamp between f.from_Date and f.to_date\n",
    "\n",
    "where ben_soliman_price between f.wac_p*0.9 and f.wac_p*1.3\n",
    "'''\n",
    "\n",
    "print(\"Fetching Ben Soliman (competitor) prices...\")\n",
    "bensoliman = snowflake_query(\"Egypt\", query)\n",
    "bensoliman.columns = bensoliman.columns.str.lower()\n",
    "\n",
    "for col in bensoliman.columns:\n",
    "    bensoliman[col] = pd.to_numeric(bensoliman[col], errors='ignore')\n",
    "\n",
    "print(f\"✓ Retrieved competitor prices for {len(bensoliman)} products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd5c097",
   "metadata": {},
   "source": [
    "### 4.3 Scraped Competitor Prices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98943949-f930-4b02-aaf6-1e20144c674e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f'''\n",
    "WITH whs as (SELECT *\n",
    "             FROM   (values\n",
    "                            ('Cairo', 'El-Marg', 38,700),\n",
    "                            ('Cairo', 'Mostorod', 1,700),\n",
    "                            ('Giza', 'Barageel', 236,701),\n",
    "                            ('Delta West', 'El-Mahala', 337,703),\n",
    "                            ('Delta West', 'Tanta', 8,703),\n",
    "                            ('Delta East', 'Mansoura FC', 339,704),\n",
    "                            ('Delta East', 'Sharqya', 170,704),\n",
    "                            ('Upper Egypt', 'Assiut FC', 501,1124),\n",
    "                            ('Upper Egypt', 'Bani sweif', 401,1126),\n",
    "                            ('Upper Egypt', 'Menya Samalot', 703,1123),\n",
    "                            ('Upper Egypt', 'Sohag', 632,1125),\n",
    "                            ('Alexandria', 'Khorshed Alex', 797,702),\n",
    "\t\t\t\t\t\t\t('Giza', 'Sakkarah', 962,701)\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t)\n",
    "                    x(region, wh, warehouse_id,cohort_id))\n",
    "select product_id,x.region,warehouse_id,min(MARKET_PRICE) as min_scrapped,max(MARKET_PRICE) as max_scrapped,median(MARKET_PRICE) as median_scrapped\n",
    "from (\n",
    "select MATERIALIZED_VIEWS.CLEANED_MARKET_PRICES.*,max(date)over(partition by region,MATERIALIZED_VIEWS.CLEANED_MARKET_PRICES.product_id,competitor) as max_date\n",
    "from MATERIALIZED_VIEWS.CLEANED_MARKET_PRICES\n",
    "join finance.all_cogs f on f.product_id = MATERIALIZED_VIEWS.CLEANED_MARKET_PRICES.product_id and CURRENT_TIMESTAMP between f.from_date and f.to_date \n",
    "where date>= current_date -5\n",
    "and MARKET_PRICE between f.wac_p * 0.9 and wac_p*1.3\n",
    "qualify date = max_date \n",
    ") x \n",
    "left join whs on whs.region = x.region\n",
    "group by all \n",
    "'''\n",
    "\n",
    "print(\"Fetching scraped competitor prices...\")\n",
    "scrapped_prices = snowflake_query(\"Egypt\", query)\n",
    "scrapped_prices.columns = scrapped_prices.columns.str.lower()\n",
    "\n",
    "for col in scrapped_prices.columns:\n",
    "    scrapped_prices[col] = pd.to_numeric(scrapped_prices[col], errors='ignore')\n",
    "\n",
    "print(f\"✓ Retrieved scraped prices for {len(scrapped_prices)} products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ceaef4",
   "metadata": {},
   "source": [
    "### 4.4 Product Statistics (Margin Boundaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b418565-6266-4bac-9af4-6970ba53a3be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f'''\n",
    "SELECT \n",
    "    region,\n",
    "    product_id,\n",
    "    optimal_bm,\n",
    "    MIN_BOUNDARY,\n",
    "    MAX_BOUNDARY,\n",
    "    MEDIAN_BM\n",
    "FROM (\n",
    "    SELECT \n",
    "        region,\n",
    "        product_id,\n",
    "        target_bm,\n",
    "        optimal_bm,\n",
    "        MIN_BOUNDARY,\n",
    "        MAX_BOUNDARY,\n",
    "        MEDIAN_BM,\n",
    "        MAX(created_at) OVER (PARTITION BY product_id, region) as max_date,\n",
    "        created_at\n",
    "    FROM materialized_views.PRODUCT_STATISTICS\n",
    "    WHERE created_at::date >= DATE_TRUNC('month', CURRENT_DATE - 60)\n",
    "    QUALIFY max_date = created_at\n",
    ")\n",
    "'''\n",
    "\n",
    "print(\"Fetching product statistics (margin boundaries)...\")\n",
    "stats = snowflake_query(\"Egypt\", query)\n",
    "stats.columns = stats.columns.str.lower()\n",
    "\n",
    "for col in stats.columns:\n",
    "    stats[col] = pd.to_numeric(stats[col], errors='ignore')\n",
    "\n",
    "print(f\"✓ Retrieved margin statistics for {len(stats)} products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33648230",
   "metadata": {},
   "source": [
    "### 4.5 Warehouse-Region Mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb86c080-2a8c-4a63-8507-02ffcfe904f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f'''\n",
    "SELECT warehouse_id, region\n",
    "FROM (\n",
    "    SELECT *, ROW_NUMBER() OVER (PARTITION BY warehouse_id ORDER BY nmv DESC) as rnk \n",
    "    FROM (\n",
    "        SELECT \n",
    "            CASE WHEN regions.id = 2 THEN cities.name_en ELSE regions.name_en END as region,\n",
    "            pso.warehouse_id,\n",
    "            SUM(pso.total_price) as nmv\n",
    "        FROM product_sales_order pso\n",
    "        JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "        JOIN materialized_views.retailer_polygon ON materialized_views.retailer_polygon.retailer_id = so.retailer_id\n",
    "        JOIN districts ON districts.id = materialized_views.retailer_polygon.district_id\n",
    "        JOIN cities ON cities.id = districts.city_id\n",
    "        JOIN states ON states.id = cities.state_id\n",
    "        JOIN regions ON regions.id = states.region_id             \n",
    "        WHERE TRUE\n",
    "            AND so.created_at::date BETWEEN CURRENT_DATE - 31 AND CURRENT_DATE - 1\n",
    "            AND so.sales_order_status_id NOT IN (7, 12)\n",
    "            AND so.channel IN ('telesales', 'retailer')\n",
    "            AND pso.purchased_item_count <> 0\n",
    "        GROUP BY ALL\n",
    "    )\n",
    "    QUALIFY rnk = 1 \n",
    ")\n",
    "'''\n",
    "\n",
    "print(\"Fetching warehouse-region mapping...\")\n",
    "warehouse_region = snowflake_query(\"Egypt\", query)\n",
    "warehouse_region.columns = warehouse_region.columns.str.lower()\n",
    "\n",
    "for col in warehouse_region.columns:\n",
    "    warehouse_region[col] = pd.to_numeric(warehouse_region[col], errors='ignore')\n",
    "\n",
    "print(f\"✓ Mapped {len(warehouse_region)} warehouses to regions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d74ad7b",
   "metadata": {},
   "source": [
    "### 4.6 Target Margins (Brand/Category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e09273a-0aed-4e4a-8f82-739055c3047c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Brand-level target margins\n",
    "query = f'''\n",
    "SELECT DISTINCT cat, brand, margin as target_bm\n",
    "FROM performance.commercial_targets cplan\n",
    "QUALIFY \n",
    "    CASE \n",
    "        WHEN DATE_TRUNC('month', MAX(DATE) OVER()) = DATE_TRUNC('month', CURRENT_DATE) \n",
    "        THEN DATE_TRUNC('month', CURRENT_DATE)\n",
    "        ELSE DATE_TRUNC('month', CURRENT_DATE - INTERVAL '1 month') \n",
    "    END = DATE_TRUNC('month', date)\n",
    "'''\n",
    "\n",
    "print(\"Fetching brand target margins...\")\n",
    "brand_cat_target = snowflake_query(\"Egypt\", query)\n",
    "brand_cat_target['target_bm'] = pd.to_numeric(brand_cat_target['target_bm'])\n",
    "print(f\"✓ Retrieved targets for {len(brand_cat_target)} brand-category combinations\")\n",
    "\n",
    "# Category-level weighted target margins\n",
    "query = f'''\n",
    "SELECT cat, SUM(target_bm * (target_nmv / cat_total)) as cat_target_margin\n",
    "FROM (\n",
    "    SELECT *, SUM(target_nmv) OVER (PARTITION BY cat) as cat_total\n",
    "    FROM (\n",
    "        SELECT cat, brand, AVG(target_bm) as target_bm, SUM(target_nmv) as target_nmv\n",
    "        FROM (\n",
    "            SELECT DISTINCT \n",
    "                date, \n",
    "                city as region, \n",
    "                cat, \n",
    "                brand, \n",
    "                margin as target_bm, \n",
    "                nmv as target_nmv\n",
    "            FROM performance.commercial_targets cplan\n",
    "            QUALIFY \n",
    "                CASE \n",
    "                    WHEN DATE_TRUNC('month', MAX(DATE) OVER()) = DATE_TRUNC('month', CURRENT_DATE) \n",
    "                    THEN DATE_TRUNC('month', CURRENT_DATE)\n",
    "                    ELSE DATE_TRUNC('month', CURRENT_DATE - INTERVAL '1 month') \n",
    "                END = DATE_TRUNC('month', date)\n",
    "        )\n",
    "        GROUP BY ALL\n",
    "    )\n",
    ")\n",
    "GROUP BY ALL \n",
    "'''\n",
    "\n",
    "print(\"Fetching category target margins...\")\n",
    "cat_target = snowflake_query(\"Egypt\", query)\n",
    "cat_target['cat_target_margin'] = pd.to_numeric(cat_target['cat_target_margin'])\n",
    "print(f\"✓ Retrieved targets for {len(cat_target)} categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab0491b",
   "metadata": {},
   "source": [
    "### 4.7 Merge All Data Sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e97860-cb64-4212-857a-98e82a822abd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MERGE ALL DATA SOURCES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Merging all data sources...\")\n",
    "\n",
    "# Start with selected products + tier quantities\n",
    "final_data = selected_products.merge(\n",
    "    tiers_selection[[\n",
    "        'warehouse_id', 'product_id', 'packing_unit_id',\n",
    "        'tier_1_qty', 'tier_2_qty', 'median_qty',\n",
    "        'tier_1_increase_pct', 'tier_2_increase_pct'\n",
    "    ]],\n",
    "    on=['warehouse_id', 'product_id', 'packing_unit_id']\n",
    ")\n",
    "\n",
    "# Select relevant columns\n",
    "final_data = final_data[[\n",
    "    'warehouse_id', 'product_id', 'packing_unit_id', 'sku', 'brand', 'cat',\n",
    "    'packing_unit_price', 'basic_unit_count', \n",
    "    'tier_1_qty', 'tier_2_qty', 'median_qty',\n",
    "    'tier_1_increase_pct', 'tier_2_increase_pct', 'final_rank'\n",
    "]]\n",
    "\n",
    "# Add WAC (weighted average cost)\n",
    "final_data = final_data.merge(sku_info[['product_id', 'wac_p']], on='product_id')\n",
    "final_data['wac_p'] = (final_data['wac_p'] * final_data['basic_unit_count']).round(2)\n",
    "\n",
    "# Add marketplace prices\n",
    "final_data = final_data.merge(marketplace, on=['product_id', 'warehouse_id'], how='left')\n",
    "final_data = final_data.drop(columns='region')\n",
    "\n",
    "# Add competitor prices\n",
    "final_data = final_data.merge(bensoliman[['product_id', 'ben_soliman_price']], on=['product_id'], how='left')\n",
    "final_data = final_data.merge(scrapped_prices, on=['product_id', 'warehouse_id'], how='left')\n",
    "final_data = final_data.drop(columns='region')\n",
    "\n",
    "# Add region and margin data\n",
    "final_data = final_data.merge(warehouse_region, on=['warehouse_id'])\n",
    "final_data = final_data.merge(stats, on=['product_id', 'region'], how='left')\n",
    "final_data = final_data.merge(brand_cat_target, on=['brand', 'cat'], how='left')\n",
    "final_data = final_data.merge(cat_target, on=['cat'], how='left')\n",
    "\n",
    "# Use brand target margin, fall back to category target margin\n",
    "final_data['Target_margin'] = final_data['target_bm'].fillna(final_data['cat_target_margin'])\n",
    "\n",
    "print(f\"✓ Merged data: {len(final_data)} products with all pricing data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383d11b7-a8d2-4d08-9a09-db03195b9b5b",
   "metadata": {},
   "source": [
    "### LIVE CART Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec7109f-492b-414b-889d-66bb5fd78f4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cohort_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>packing_unit_id</th>\n",
       "      <th>basic_unit_count</th>\n",
       "      <th>current_cart_rule</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1124</td>\n",
       "      <td>8494</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1123</td>\n",
       "      <td>11148</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1126</td>\n",
       "      <td>6587</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1123</td>\n",
       "      <td>10026</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1125</td>\n",
       "      <td>13060</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108945</th>\n",
       "      <td>700</td>\n",
       "      <td>5901</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108946</th>\n",
       "      <td>701</td>\n",
       "      <td>9340</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108947</th>\n",
       "      <td>701</td>\n",
       "      <td>7892</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108948</th>\n",
       "      <td>700</td>\n",
       "      <td>8956</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108949</th>\n",
       "      <td>700</td>\n",
       "      <td>3764</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108950 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        cohort_id  product_id  packing_unit_id  basic_unit_count  \\\n",
       "0            1124        8494                1                 6   \n",
       "1            1123       11148                1                 1   \n",
       "2            1126        6587                1                 1   \n",
       "3            1123       10026                1                 6   \n",
       "4            1125       13060                3                 1   \n",
       "...           ...         ...              ...               ...   \n",
       "108945        700        5901                1                48   \n",
       "108946        701        9340                2                 1   \n",
       "108947        701        7892                1                12   \n",
       "108948        700        8956                1                 1   \n",
       "108949        700        3764                1                24   \n",
       "\n",
       "        current_cart_rule  \n",
       "0                      25  \n",
       "1                      10  \n",
       "2                      25  \n",
       "3                      10  \n",
       "4                      25  \n",
       "...                   ...  \n",
       "108945                 25  \n",
       "108946                  5  \n",
       "108947                 25  \n",
       "108948                 25  \n",
       "108949                 25  \n",
       "\n",
       "[108950 rows x 5 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = '''\n",
    "SELECT \n",
    "    cppu.cohort_id,\n",
    "    product_id,\n",
    "    packing_unit_id,\n",
    "    basic_unit_count,\n",
    "    COALESCE(cppu.MAX_PER_SALES_ORDER, cppu2.MAX_PER_SALES_ORDER) as current_cart_rule\n",
    "FROM COHORT_PRODUCT_PACKING_UNITS cppu \n",
    "JOIN PACKING_UNIT_PRODUCTS pup ON cppu.PRODUCT_PACKING_UNIT_ID = pup.id \n",
    "JOIN cohorts c ON c.id = cppu.cohort_id\n",
    "JOIN COHORT_PRODUCT_PACKING_UNITS cppu2 \n",
    "    ON cppu.PRODUCT_PACKING_UNIT_ID = cppu2.PRODUCT_PACKING_UNIT_ID \n",
    "    AND cppu2.cohort_id = c.FALLBACK_COHORT_ID \n",
    "WHERE cppu.cohort_id IN (700, 701, 702, 703, 704, 1123, 1124, 1125, 1126)\n",
    "'''\n",
    "\n",
    "print(\"Fetching live cart rules...\")\n",
    "live_cart_rules = snowflake_query(\"Egypt\", query) \n",
    "live_cart_rules.columns = live_cart_rules.columns.str.lower()\n",
    "\n",
    "for col in live_cart_rules.columns:\n",
    "    live_cart_rules[col] = pd.to_numeric(live_cart_rules[col], errors='ignore')\n",
    "\n",
    "print(f\"✓ Retrieved {len(live_cart_rules)} cart rules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045aceb9-ff2c-45bd-8aae-d931e43fc0ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>wh</th>\n",
       "      <th>warehouse_id</th>\n",
       "      <th>cohort_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cairo</td>\n",
       "      <td>El-Marg</td>\n",
       "      <td>38</td>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cairo</td>\n",
       "      <td>Mostorod</td>\n",
       "      <td>1</td>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Giza</td>\n",
       "      <td>Barageel</td>\n",
       "      <td>236</td>\n",
       "      <td>701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Delta West</td>\n",
       "      <td>El-Mahala</td>\n",
       "      <td>337</td>\n",
       "      <td>703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Delta West</td>\n",
       "      <td>Tanta</td>\n",
       "      <td>8</td>\n",
       "      <td>703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Delta East</td>\n",
       "      <td>Mansoura FC</td>\n",
       "      <td>339</td>\n",
       "      <td>704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Delta East</td>\n",
       "      <td>Sharqya</td>\n",
       "      <td>170</td>\n",
       "      <td>704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Upper Egypt</td>\n",
       "      <td>Assiut FC</td>\n",
       "      <td>501</td>\n",
       "      <td>1124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Upper Egypt</td>\n",
       "      <td>Bani sweif</td>\n",
       "      <td>401</td>\n",
       "      <td>1126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Upper Egypt</td>\n",
       "      <td>Menya Samalot</td>\n",
       "      <td>703</td>\n",
       "      <td>1123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Upper Egypt</td>\n",
       "      <td>Sohag</td>\n",
       "      <td>632</td>\n",
       "      <td>1125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Alexandria</td>\n",
       "      <td>Khorshed Alex</td>\n",
       "      <td>797</td>\n",
       "      <td>702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Giza</td>\n",
       "      <td>Sakkarah</td>\n",
       "      <td>962</td>\n",
       "      <td>701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         region             wh  warehouse_id  cohort_id\n",
       "0         Cairo        El-Marg            38        700\n",
       "1         Cairo       Mostorod             1        700\n",
       "2          Giza       Barageel           236        701\n",
       "3    Delta West      El-Mahala           337        703\n",
       "4    Delta West          Tanta             8        703\n",
       "5    Delta East    Mansoura FC           339        704\n",
       "6    Delta East        Sharqya           170        704\n",
       "7   Upper Egypt      Assiut FC           501       1124\n",
       "8   Upper Egypt     Bani sweif           401       1126\n",
       "9   Upper Egypt  Menya Samalot           703       1123\n",
       "10  Upper Egypt          Sohag           632       1125\n",
       "11   Alexandria  Khorshed Alex           797        702\n",
       "12         Giza       Sakkarah           962        701"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cohort to Warehouse mapping\n",
    "mapping_coh_wh = pd.DataFrame({\n",
    "    'region':       ['Cairo', 'Cairo', 'Giza', 'Delta West', 'Delta West', 'Delta East', \n",
    "                     'Delta East', 'Upper Egypt', 'Upper Egypt', 'Upper Egypt', 'Upper Egypt', \n",
    "                     'Alexandria', 'Giza'],\n",
    "    'wh':           ['El-Marg', 'Mostorod', 'Barageel', 'El-Mahala', 'Tanta', 'Mansoura FC',\n",
    "                     'Sharqya', 'Assiut FC', 'Bani sweif', 'Menya Samalot', 'Sohag',\n",
    "                     'Khorshed Alex', 'Sakkarah'],\n",
    "    'warehouse_id': [38, 1, 236, 337, 8, 339, 170, 501, 401, 703, 632, 797, 962],\n",
    "    'cohort_id':    [700, 700, 701, 703, 703, 704, 704, 1124, 1126, 1123, 1125, 702, 701]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de132378-5bbb-4eff-a8d6-add1915ff206",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cohort_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>packing_unit_id</th>\n",
       "      <th>basic_unit_count</th>\n",
       "      <th>current_cart_rule</th>\n",
       "      <th>region</th>\n",
       "      <th>wh</th>\n",
       "      <th>warehouse_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1124</td>\n",
       "      <td>8494</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>Upper Egypt</td>\n",
       "      <td>Assiut FC</td>\n",
       "      <td>501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1123</td>\n",
       "      <td>11148</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>Upper Egypt</td>\n",
       "      <td>Menya Samalot</td>\n",
       "      <td>703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1126</td>\n",
       "      <td>6587</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>Upper Egypt</td>\n",
       "      <td>Bani sweif</td>\n",
       "      <td>401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1123</td>\n",
       "      <td>10026</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>Upper Egypt</td>\n",
       "      <td>Menya Samalot</td>\n",
       "      <td>703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1125</td>\n",
       "      <td>13060</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>Upper Egypt</td>\n",
       "      <td>Sohag</td>\n",
       "      <td>632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157387</th>\n",
       "      <td>701</td>\n",
       "      <td>7892</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>Giza</td>\n",
       "      <td>Sakkarah</td>\n",
       "      <td>962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157388</th>\n",
       "      <td>700</td>\n",
       "      <td>8956</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>Cairo</td>\n",
       "      <td>El-Marg</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157389</th>\n",
       "      <td>700</td>\n",
       "      <td>8956</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>Cairo</td>\n",
       "      <td>Mostorod</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157390</th>\n",
       "      <td>700</td>\n",
       "      <td>3764</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>Cairo</td>\n",
       "      <td>El-Marg</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157391</th>\n",
       "      <td>700</td>\n",
       "      <td>3764</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>Cairo</td>\n",
       "      <td>Mostorod</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>157392 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        cohort_id  product_id  packing_unit_id  basic_unit_count  \\\n",
       "0            1124        8494                1                 6   \n",
       "1            1123       11148                1                 1   \n",
       "2            1126        6587                1                 1   \n",
       "3            1123       10026                1                 6   \n",
       "4            1125       13060                3                 1   \n",
       "...           ...         ...              ...               ...   \n",
       "157387        701        7892                1                12   \n",
       "157388        700        8956                1                 1   \n",
       "157389        700        8956                1                 1   \n",
       "157390        700        3764                1                24   \n",
       "157391        700        3764                1                24   \n",
       "\n",
       "        current_cart_rule       region             wh  warehouse_id  \n",
       "0                      25  Upper Egypt      Assiut FC           501  \n",
       "1                      10  Upper Egypt  Menya Samalot           703  \n",
       "2                      25  Upper Egypt     Bani sweif           401  \n",
       "3                      10  Upper Egypt  Menya Samalot           703  \n",
       "4                      25  Upper Egypt          Sohag           632  \n",
       "...                   ...          ...            ...           ...  \n",
       "157387                 25         Giza       Sakkarah           962  \n",
       "157388                 25        Cairo        El-Marg            38  \n",
       "157389                 25        Cairo       Mostorod             1  \n",
       "157390                 25        Cairo        El-Marg            38  \n",
       "157391                 25        Cairo       Mostorod             1  \n",
       "\n",
       "[157392 rows x 8 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add warehouse mapping to cart rules\n",
    "live_cart_rules = live_cart_rules.merge(mapping_coh_wh, on='cohort_id')\n",
    "print(f\"✓ Cart rules mapped to {live_cart_rules['warehouse_id'].nunique()} warehouses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545252e6",
   "metadata": {},
   "source": [
    "## 5. Price Tier Calculation\n",
    "\n",
    "Calculate tier 1 and tier 2 prices with constraints:\n",
    "- **Max discount**: 5% from current price\n",
    "- **Min discount**: 0.35% from current price  \n",
    "- **Ratio bounds**: discount-to-quantity ratio between 1.3 and 3.5\n",
    "- **Price ordering**: WAC < Tier 2 < Tier 1 < Current Price\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c14fe97",
   "metadata": {},
   "source": [
    "### 5.1 Price Calculation Functions\n",
    "\n",
    "The `calculate_tier_prices` function uses multiple strategies:\n",
    "1. **Market prices strategy** - Use competitive pricing data if available\n",
    "2. **Margin range strategy** - Calculate from margin boundaries if no market data\n",
    "3. **Ratio adjustment** - Adjust tier_2 price to meet discount-to-quantity ratio bounds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4250db-739f-4602-9fcb-fd3638c7343c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_tier_prices(row, max_discount_pct=5.0, min_discount_pct=0.35, min_ratio=1.1, max_ratio=3.5):\n",
    "    \"\"\"\n",
    "    Calculate tier 1 and tier 2 prices for a single row.\n",
    "    \n",
    "    Parameters:\n",
    "    - max_discount_pct: Maximum allowed discount from current price (default: 5%)\n",
    "    - min_discount_pct: Minimum required discount from current price (default: 0.35%)\n",
    "    - min_ratio: Minimum discount-to-quantity ratio (default: 1.3)\n",
    "    - max_ratio: Maximum discount-to-quantity ratio (default: 3.5)\n",
    "    \n",
    "    Constraints:\n",
    "    - Tier prices must not go below price calculated with 0.3 * target_margin\n",
    "    - Ensure: WAC < Tier 2 < Tier 1 < Current Price\n",
    "    - Ensure: BOTH tiers must be valid or BOTH are None\n",
    "    - Ensure: discount_qty_ratio = (tier_2_discount/tier_1_discount) / (tier_2_qty/tier_1_qty) is between min_ratio and max_ratio\n",
    "    \"\"\"\n",
    "    \n",
    "    current_price = row['packing_unit_price']\n",
    "    wac = row['wac_p']\n",
    "    \n",
    "    # Get basic_unit_count for converting market prices\n",
    "    basic_unit_count = row.get('basic_unit_count', 1)\n",
    "    if pd.isna(basic_unit_count) or basic_unit_count <= 0:\n",
    "        basic_unit_count = 1\n",
    "    \n",
    "    # Validation\n",
    "    if pd.isna(current_price) or current_price <= 0:\n",
    "        return pd.Series({'tier_1_price': np.nan, 'tier_2_price': np.nan, 'price_source': 'invalid_current_price'})\n",
    "    \n",
    "    if pd.isna(wac) or wac <= 0:\n",
    "        return pd.Series({'tier_1_price': np.nan, 'tier_2_price': np.nan, 'price_source': 'invalid_wac'})\n",
    "    \n",
    "    if current_price <= wac:\n",
    "        return pd.Series({'tier_1_price': np.nan, 'tier_2_price': np.nan, 'price_source': 'current_price_below_wac'})\n",
    "    \n",
    "    # Calculate discount bounds\n",
    "    max_discount_price = current_price * (1 - max_discount_pct / 100)  # Minimum allowed price\n",
    "    min_discount_price = current_price * (1 - min_discount_pct / 100)  # Maximum allowed price\n",
    "    \n",
    "    # Calculate absolute minimum price based on target_margin\n",
    "    # Price must maintain at least 30% of target margin\n",
    "    absolute_min_price = wac  # Default to WAC if no target_margin\n",
    "    \n",
    "    if 'target_margin' in row.index and pd.notna(row['target_margin']) and 0 < row['target_margin'] < 1:\n",
    "        target_margin = row['target_margin']\n",
    "        # Minimum margin is 30% of target margin\n",
    "        min_margin = target_margin * 0.3\n",
    "        # Calculate minimum price: price = wac / (1 - min_margin)\n",
    "        absolute_min_price = wac / (1 - min_margin)\n",
    "    else:\n",
    "        # Fallback: use wac_cushion_pct\n",
    "        wac_cushion_pct = 0.25\n",
    "        absolute_min_price = wac / (1 - (wac_cushion_pct / 100))\n",
    "    \n",
    "    # Market price columns (these are per basic unit)\n",
    "    market_cols = [\n",
    "        'final_mod_price', 'median_scrapped', 'final_max_price', \n",
    "        'ben_soliman_price', 'max_scrapped', 'final_true_max',\n",
    "        'final_min_price', 'min_scrapped', 'final_true_min'\n",
    "    ]\n",
    "    \n",
    "    # Extract valid market prices (multiply by basic_unit_count, above absolute_min_price, within discount bounds)\n",
    "    valid_market_prices = []\n",
    "    for col in market_cols:\n",
    "        if col in row.index and pd.notna(row[col]) and row[col] > 0:\n",
    "            # Convert basic unit price to packing unit price\n",
    "            packing_price = row[col] * basic_unit_count\n",
    "            \n",
    "            # Must be: above absolute_min_price AND within discount bounds\n",
    "            if absolute_min_price < packing_price and max_discount_price <= packing_price <= min_discount_price:\n",
    "                valid_market_prices.append(packing_price)\n",
    "    \n",
    "    # Remove duplicates and sort descending\n",
    "    valid_market_prices = sorted(list(set(valid_market_prices)), reverse=True)\n",
    "    \n",
    "    tier_1 = None\n",
    "    tier_2 = None\n",
    "    source = ''\n",
    "    \n",
    "    min_gap_pct = 0.25\n",
    "    \n",
    "    # Strategy 1: Use market prices\n",
    "    if len(valid_market_prices) >= 3:\n",
    "        # Select from available prices\n",
    "        tier_1 = valid_market_prices[0]  # Highest price\n",
    "        \n",
    "        # Find tier 2 with minimum gap\n",
    "        for price in valid_market_prices[1:]:\n",
    "            if price < tier_1 * (1 - min_gap_pct / 100):\n",
    "                tier_2 = price\n",
    "                break\n",
    "        \n",
    "        # If no suitable tier 2 found, take second highest\n",
    "        if tier_2 is None and len(valid_market_prices) > 1:\n",
    "            tier_2 = valid_market_prices[1]\n",
    "        \n",
    "        if tier_1 is not None and tier_2 is not None:\n",
    "            source = 'market_prices'\n",
    "    \n",
    "    elif len(valid_market_prices) == 2:\n",
    "        tier_1 = valid_market_prices[0]\n",
    "        tier_2 = valid_market_prices[1]\n",
    "        source = 'market_prices'\n",
    "    \n",
    "    elif len(valid_market_prices) == 1:\n",
    "        # Only one market price - use margin range for the other\n",
    "        market_price = valid_market_prices[0]\n",
    "        \n",
    "        # Calculate which tier this should be based on its position\n",
    "        price_position = (market_price - max_discount_price) / (min_discount_price - max_discount_price)\n",
    "        \n",
    "        # If in upper half (>0.5), use as tier 1 and calculate tier 2\n",
    "        # If in lower half (<=0.5), use as tier 2 and calculate tier 1\n",
    "        if price_position > 0.5:\n",
    "            tier_1 = market_price\n",
    "            tier_2 = calculate_from_margin_range(row, wac, current_price, tier_1, tier=2, \n",
    "                                                 max_discount_price=max_discount_price,\n",
    "                                                 min_discount_price=min_discount_price,\n",
    "                                                 absolute_min_price=absolute_min_price)\n",
    "            if tier_2 is not None:\n",
    "                source = 'market_tier1_margin_tier2'\n",
    "        else:\n",
    "            tier_2 = market_price\n",
    "            tier_1 = calculate_from_margin_range(row, wac, current_price, tier_2, tier=1,\n",
    "                                                 max_discount_price=max_discount_price,\n",
    "                                                 min_discount_price=min_discount_price,\n",
    "                                                 absolute_min_price=absolute_min_price)\n",
    "            if tier_1 is not None:\n",
    "                source = 'margin_tier1_market_tier2'\n",
    "    \n",
    "    # Strategy 2: No market prices - use margin range method\n",
    "    if tier_1 is None or tier_2 is None:\n",
    "        tier_1, tier_2 = calculate_both_from_margin_range(row, wac, current_price,\n",
    "                                                          max_discount_price=max_discount_price,\n",
    "                                                          min_discount_price=min_discount_price,\n",
    "                                                          absolute_min_price=absolute_min_price)\n",
    "        if tier_1 is not None and tier_2 is not None:\n",
    "            source = 'margin_range_based'\n",
    "    \n",
    "    # CRITICAL: Final validation - BOTH must be valid or BOTH are None\n",
    "    if tier_1 is not None and tier_2 is not None:\n",
    "        # Ensure correct ordering\n",
    "        if tier_2 >= tier_1:\n",
    "            tier_1, tier_2 = max(tier_1, tier_2), min(tier_1, tier_2)\n",
    "        \n",
    "        # Apply discount bounds\n",
    "        tier_1 = max(tier_1, max_discount_price)\n",
    "        tier_1 = min(tier_1, min_discount_price)\n",
    "        tier_2 = max(tier_2, max_discount_price)\n",
    "        tier_2 = min(tier_2, min_discount_price)\n",
    "        \n",
    "        # Check if both above absolute minimum price\n",
    "        if tier_1 <= absolute_min_price or tier_2 <= absolute_min_price:\n",
    "            tier_1 = None\n",
    "            tier_2 = None\n",
    "            source = 'prices_below_minimum_margin'\n",
    "        else:\n",
    "            # Ensure minimum gap between tiers\n",
    "            if tier_2 > tier_1 * (1 - min_gap_pct / 100):\n",
    "                tier_2 = tier_1 * (1 - min_gap_pct / 100)\n",
    "                if tier_2 <= absolute_min_price:\n",
    "                    tier_1 = None\n",
    "                    tier_2 = None\n",
    "                    source = 'insufficient_gap_between_tiers'\n",
    "            \n",
    "            # Final check: both still valid?\n",
    "            if tier_1 is not None and tier_2 is not None:\n",
    "                if not (wac < tier_2 < tier_1 < current_price):\n",
    "                    tier_1 = None\n",
    "                    tier_2 = None\n",
    "                    source = 'invalid_tier_ordering'\n",
    "                elif not (max_discount_price <= tier_2 and tier_1 <= min_discount_price):\n",
    "                    tier_1 = None\n",
    "                    tier_2 = None\n",
    "                    source = 'tiers_outside_discount_bounds'\n",
    "                else:\n",
    "                    tier_1 = round(tier_1, 2)\n",
    "                    tier_2 = round(tier_2, 2)\n",
    "                    \n",
    "                    # Validate and adjust discount-to-quantity ratio\n",
    "                    tier_1_qty = row.get('tier_1_qty', None)\n",
    "                    tier_2_qty = row.get('tier_2_qty', None)\n",
    "                    \n",
    "                    if tier_1_qty is not None and tier_2_qty is not None and tier_1_qty > 0:\n",
    "                        tier_1_discount = current_price - tier_1\n",
    "                        tier_2_discount = current_price - tier_2\n",
    "                        \n",
    "                        if tier_1_discount > 0:\n",
    "                            diff_quantity = tier_2_qty / tier_1_qty\n",
    "                            diff_discount = tier_2_discount / tier_1_discount\n",
    "                            \n",
    "                            if diff_quantity > 0:\n",
    "                                discount_qty_ratio = diff_discount / diff_quantity\n",
    "                                \n",
    "                                # Adjust tier_2_price if ratio is outside bounds\n",
    "                                if discount_qty_ratio < min_ratio:\n",
    "                                    # Ratio too low - need more discount at tier 2\n",
    "                                    # tier_2 = current_price - (target_ratio * diff_quantity * tier_1_discount)\n",
    "                                    target_tier_2_discount = min_ratio * diff_quantity * tier_1_discount\n",
    "                                    adjusted_tier_2 = current_price - target_tier_2_discount\n",
    "                                    \n",
    "                                    # Ensure adjusted price is still valid (above WAC and absolute_min_price)\n",
    "                                    if adjusted_tier_2 > wac and adjusted_tier_2 > absolute_min_price and adjusted_tier_2 < tier_1:\n",
    "                                        tier_2 = round(adjusted_tier_2, 2)\n",
    "                                        source = source + '_ratio_adjusted_up'\n",
    "                                    else:\n",
    "                                        tier_1 = None\n",
    "                                        tier_2 = None\n",
    "                                        source = f'cannot_adjust_ratio_{discount_qty_ratio:.2f}_min_bound'\n",
    "                                \n",
    "                                elif discount_qty_ratio > max_ratio:\n",
    "                                    # Ratio too high - need less discount at tier 2\n",
    "                                    # tier_2 = current_price - (target_ratio * diff_quantity * tier_1_discount)\n",
    "                                    target_tier_2_discount = max_ratio * diff_quantity * tier_1_discount\n",
    "                                    adjusted_tier_2 = current_price - target_tier_2_discount\n",
    "                                    \n",
    "                                    # Ensure adjusted price is still valid (below tier_1 and above WAC)\n",
    "                                    if adjusted_tier_2 > wac and adjusted_tier_2 > absolute_min_price and adjusted_tier_2 < tier_1:\n",
    "                                        tier_2 = round(adjusted_tier_2, 2)\n",
    "                                        source = source + '_ratio_adjusted_down'\n",
    "                                    else:\n",
    "                                        tier_1 = None\n",
    "                                        tier_2 = None\n",
    "                                        source = f'cannot_adjust_ratio_{discount_qty_ratio:.2f}_max_bound'\n",
    "    \n",
    "    # FINAL CHECK: If only one tier exists, invalidate both\n",
    "    if (tier_1 is None and tier_2 is not None) or (tier_1 is not None and tier_2 is None):\n",
    "        tier_1 = None\n",
    "        tier_2 = None\n",
    "        source = 'incomplete_tier_pair'\n",
    "    \n",
    "    # If both are None and no source set, mark it\n",
    "    if tier_1 is None and tier_2 is None and source == '':\n",
    "        source = 'no_valid_prices'\n",
    "    \n",
    "    return pd.Series({\n",
    "        'tier_1_price': tier_1,\n",
    "        'tier_2_price': tier_2,\n",
    "        'price_source': source\n",
    "    })\n",
    "\n",
    "\n",
    "def calculate_both_from_margin_range(row, wac, current_price, max_discount_price, min_discount_price, absolute_min_price):\n",
    "    \"\"\"\n",
    "    Calculate both tier prices using margin range from minimum of (min_boundary, optimal_bm) to current margin.\n",
    "    Returns (tier_1_price, tier_2_price) or (None, None)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate current margin: margin = (price - wac) / price\n",
    "    current_margin = (current_price - wac) / current_price\n",
    "    \n",
    "    # Get min_boundary margin\n",
    "    min_boundary_margin = None\n",
    "    if 'min_boundary' in row.index and pd.notna(row['min_boundary']) and 0 < row['min_boundary'] < 1:\n",
    "        min_boundary_margin = row['min_boundary']\n",
    "    \n",
    "    # Get optimal_bm margin\n",
    "    optimal_margin = None\n",
    "    if 'optimal_bm' in row.index and pd.notna(row['optimal_bm']) and 0 < row['optimal_bm'] < 1:\n",
    "        optimal_margin = row['optimal_bm']\n",
    "    \n",
    "    # Determine starting margin: minimum of (min_boundary, optimal_bm)\n",
    "    start_margin = None\n",
    "    \n",
    "    if min_boundary_margin is not None and optimal_margin is not None:\n",
    "        start_margin = min(min_boundary_margin, optimal_margin)\n",
    "    elif min_boundary_margin is not None:\n",
    "        start_margin = min_boundary_margin\n",
    "    elif optimal_margin is not None:\n",
    "        start_margin = optimal_margin\n",
    "    else:\n",
    "        # Fallback: use 50% of current margin\n",
    "        start_margin = current_margin * 0.85\n",
    "    \n",
    "    # Ensure start_margin is less than current margin\n",
    "    if start_margin >= current_margin:\n",
    "        start_margin = current_margin * 0.85\n",
    "    \n",
    "    # Generate margin points in the range (10 points)\n",
    "    num_points = 10\n",
    "    margin_range = np.linspace(start_margin, current_margin, num_points)\n",
    "    \n",
    "    # Calculate prices from these margins: price = wac / (1 - margin)\n",
    "    price_candidates = []\n",
    "    for margin in margin_range:\n",
    "        if 0 < margin < 1:\n",
    "            price = wac / (1 - margin)\n",
    "            # Only keep prices within discount bounds and above absolute_min_price\n",
    "            if absolute_min_price < price and max_discount_price <= price <= min_discount_price:\n",
    "                price_candidates.append(price)\n",
    "    \n",
    "    if len(price_candidates) < 2:\n",
    "        return None, None\n",
    "    \n",
    "    # Sort prices descending\n",
    "    price_candidates = sorted(price_candidates, reverse=True)\n",
    "    \n",
    "    # Select Tier 1: closer to the top (less discount)\n",
    "    # Select Tier 2: further down (more discount)\n",
    "    tier_1_idx = int(len(price_candidates) * 0.25)  # 25% from top\n",
    "    tier_2_idx = int(len(price_candidates) * 0.65)  # 65% from top\n",
    "    \n",
    "    # Ensure valid indices\n",
    "    tier_1_idx = max(0, min(tier_1_idx, len(price_candidates) - 2))\n",
    "    tier_2_idx = max(tier_1_idx + 1, min(tier_2_idx, len(price_candidates) - 1))\n",
    "    \n",
    "    tier_1 = price_candidates[tier_1_idx]\n",
    "    tier_2 = price_candidates[tier_2_idx]\n",
    "    \n",
    "    # Ensure meaningful gap (at least 0.5%)\n",
    "    min_gap_pct = 0.25\n",
    "    if tier_2 > tier_1 * (1 - min_gap_pct / 100):\n",
    "        # Try to find better tier_2\n",
    "        for i in range(tier_2_idx + 1, len(price_candidates)):\n",
    "            if price_candidates[i] < tier_1 * (1 - min_gap_pct / 100):\n",
    "                tier_2 = price_candidates[i]\n",
    "                break\n",
    "    \n",
    "    # Final validation\n",
    "    if tier_2 >= tier_1 or tier_1 <= absolute_min_price or tier_2 <= absolute_min_price:\n",
    "        return None, None\n",
    "    \n",
    "    return tier_1, tier_2\n",
    "\n",
    "\n",
    "def calculate_from_margin_range(row, wac, current_price, other_tier_price, tier, \n",
    "                                max_discount_price, min_discount_price, absolute_min_price):\n",
    "    \"\"\"\n",
    "    Calculate single tier price using margin range.\n",
    "    Used when one tier is from market and we need to calculate the other.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate current margin\n",
    "    current_margin = (current_price - wac) / current_price\n",
    "    \n",
    "    # Get min_boundary margin\n",
    "    min_boundary_margin = None\n",
    "    if 'min_boundary' in row.index and pd.notna(row['min_boundary']) and 0 < row['min_boundary'] < 1:\n",
    "        min_boundary_margin = row['min_boundary']\n",
    "    \n",
    "    # Get optimal_bm margin\n",
    "    optimal_margin = None\n",
    "    if 'optimal_bm' in row.index and pd.notna(row['optimal_bm']) and 0 < row['optimal_bm'] < 1:\n",
    "        optimal_margin = row['optimal_bm']\n",
    "    \n",
    "    # Determine starting margin: minimum of (min_boundary, optimal_bm)\n",
    "    start_margin = None\n",
    "    \n",
    "    if min_boundary_margin is not None and optimal_margin is not None:\n",
    "        start_margin = min(min_boundary_margin, optimal_margin)\n",
    "    elif min_boundary_margin is not None:\n",
    "        start_margin = min_boundary_margin\n",
    "    elif optimal_margin is not None:\n",
    "        start_margin = optimal_margin\n",
    "    else:\n",
    "        start_margin = current_margin * 0.5\n",
    "    \n",
    "    # Ensure start_margin is less than current margin\n",
    "    if start_margin >= current_margin:\n",
    "        start_margin = current_margin * 0.7\n",
    "    \n",
    "    # Generate margin range (10 points)\n",
    "    num_points = 10\n",
    "    margin_range = np.linspace(start_margin, current_margin, num_points)\n",
    "    \n",
    "    # Calculate prices\n",
    "    price_candidates = []\n",
    "    for margin in margin_range:\n",
    "        if 0 < margin < 1:\n",
    "            price = wac / (1 - margin)\n",
    "            if absolute_min_price < price and max_discount_price <= price <= min_discount_price:\n",
    "                price_candidates.append(price)\n",
    "    \n",
    "    if len(price_candidates) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Sort prices descending\n",
    "    price_candidates = sorted(price_candidates, reverse=True)\n",
    "    \n",
    "    min_gap_pct = 0.5\n",
    "    \n",
    "    if tier == 1:\n",
    "        # Need tier 1 (higher price), we have tier 2 (lower price)\n",
    "        # Find prices above tier 2 with proper gap\n",
    "        target_candidates = [p for p in price_candidates \n",
    "                           if p > other_tier_price * (1 + min_gap_pct / 100)]\n",
    "        if target_candidates:\n",
    "            # Take from upper portion (25% position)\n",
    "            idx = int(len(target_candidates) * 0.25)\n",
    "            return target_candidates[idx]\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        # Need tier 2 (lower price), we have tier 1 (higher price)\n",
    "        # Find prices below tier 1 with proper gap\n",
    "        target_candidates = [p for p in price_candidates \n",
    "                           if p < other_tier_price * (1 - min_gap_pct / 100)]\n",
    "        if target_candidates:\n",
    "            # Take from lower portion (65% position)\n",
    "            idx = int(len(target_candidates) * 0.65)\n",
    "            idx = min(idx, len(target_candidates) - 1)\n",
    "            return target_candidates[idx]\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4895a40",
   "metadata": {},
   "source": [
    "### 5.2 Apply Price Calculations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d485d098-925d-4be2-8e44-18b33438c529",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2400 SKUs...\n",
      "Ratio adjusted up (was below 1.1): 1362 SKUs\n",
      "Ratio adjusted down (was above 3): 46 SKUs\n",
      "Could not adjust (constraints violated): 61 SKUs\n",
      "Final SKUs with valid tier prices: 1963\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# APPLY PRICE CALCULATIONS\n",
    "# =============================================================================\n",
    "\n",
    "# Normalize column names\n",
    "final_data.columns = final_data.columns.str.lower()\n",
    "\n",
    "print(f\"Processing {len(final_data)} SKUs...\")\n",
    "print(f\"Parameters: MAX_DISCOUNT={MAX_DISCOUNT_PCT}%, MIN_DISCOUNT={MIN_DISCOUNT_PCT}%, RATIO=[{MIN_RATIO}, {MAX_RATIO}]\")\n",
    "\n",
    "# Apply price calculation to each row\n",
    "result = final_data.apply(\n",
    "    lambda row: calculate_tier_prices(\n",
    "        row, \n",
    "        max_discount_pct=MAX_DISCOUNT_PCT,\n",
    "        min_discount_pct=MIN_DISCOUNT_PCT,\n",
    "        min_ratio=MIN_RATIO,\n",
    "        max_ratio=MAX_RATIO\n",
    "    ), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Merge results back to dataframe\n",
    "final_data = pd.concat([final_data, result], axis=1)\n",
    "\n",
    "# Summary of ratio adjustments\n",
    "ratio_adjusted_up = final_data['price_source'].str.contains('ratio_adjusted_up', na=False).sum()\n",
    "ratio_adjusted_down = final_data['price_source'].str.contains('ratio_adjusted_down', na=False).sum()\n",
    "cannot_adjust = final_data['price_source'].str.contains('cannot_adjust_ratio', na=False).sum()\n",
    "\n",
    "print(f\"\\n--- Ratio Adjustment Summary ---\")\n",
    "print(f\"  Adjusted up (was below {MIN_RATIO}):      {ratio_adjusted_up} SKUs\")\n",
    "print(f\"  Adjusted down (was above {MAX_RATIO}):    {ratio_adjusted_down} SKUs\")\n",
    "print(f\"  Could not adjust (constraints violated): {cannot_adjust} SKUs\")\n",
    "\n",
    "# Filter to only products with valid tier prices\n",
    "final_data = final_data[\n",
    "    (~final_data['tier_1_price'].isna()) & \n",
    "    (~final_data['tier_2_price'].isna())\n",
    "]\n",
    "\n",
    "print(f\"\\n✓ Final SKUs with valid tier prices: {len(final_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b0ceb8",
   "metadata": {},
   "source": [
    "## 6. Wholesale Pricing\n",
    "\n",
    "Calculate wholesale prices based on:\n",
    "- Vehicle capacity (quarter truck)\n",
    "- Rank-based margin tiers (20%, 25%, 40%, 60% of target margin)\n",
    "- Must be below tier_2_price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15518b2-699f-4c12-b551-f125b8abd4c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PREPARE DELIVERY FEE DATA\n",
    "# =============================================================================\n",
    "\n",
    "# Set delivery fees based on region\n",
    "final_data['delivery_fees'] = DELIVERY_FEE_OTHER\n",
    "final_data.loc[final_data['region'].isin(['Cairo', 'Giza']), 'delivery_fees'] = DELIVERY_FEE_CAIRO_GIZA\n",
    "\n",
    "# Prepare query data for wholesale calculation\n",
    "query_data = final_data[['warehouse_id', 'product_id', 'packing_unit_id', 'delivery_fees']].values.tolist()\n",
    "query_info = ','.join([\n",
    "    f\"({int(wh_id)}, {int(prod_id)}, {int(pu_id)}, {int(delivery_fees)})\" \n",
    "    for wh_id, prod_id, pu_id, delivery_fees in query_data\n",
    "])\n",
    "\n",
    "print(f\"✓ Prepared {len(query_data)} products for wholesale calculation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ec0273-9d56-4dc6-b4bc-2ba8093792b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f'''\n",
    "with chosen_products as (\n",
    "select *\n",
    "from (\n",
    "values \n",
    "{query_info}\n",
    ")x(warehouse_id,product_id,packing_unit_id,delivery_fees)\n",
    "\n",
    "),\n",
    "vec as (\n",
    "select  vt.id as vehicle_id,name_en as vehicle_name,vc.weight as vehicle_weight,vc.cbm as vehicle_cbm,900 as vehicle_cost\n",
    "from VEHICLE_TYPES  vt \n",
    "join  RETOOL.VEHICLE_CAPACITIES vc on vc.vehicle_id = vt.id\n",
    "where vehicle_id = 1\n",
    "),\n",
    "selected_products as (\n",
    "select x.*,\t(long*width*height)/1000000 AS cbm,weight/1000 AS weight,\n",
    "from chosen_products x\n",
    "join packing_unit_products on x.product_id = packing_unit_products.product_id and packing_unit_products.packing_unit_id = x.packing_unit_id\n",
    "),\n",
    "main_cte as (\n",
    "select warehouse_id,product_id,packing_unit_id,delivery_fees,\n",
    "ceil(least(quart_dababa_wht,quart_dababa_cbm)) as quart_dababa,\n",
    "vehicle_cost\n",
    "from (\n",
    "select * ,\n",
    "((vehicle_weight*0.9)/4)/weight as quart_dababa_wht , \n",
    "((vehicle_cbm*0.9)/4)/cbm as quart_dababa_cbm  \n",
    "from (\n",
    "select selected_products.*, vehicle_weight,vehicle_cbm,vehicle_cost\n",
    "from selected_products,vec\n",
    ")\n",
    ")\n",
    ")\n",
    "select mc.*, f.wac_p , \n",
    "(f.wac_p*quart_dababa)+(((vehicle_cost-(delivery_fees*4))*0.9)/4) as quart_cost,\n",
    "quart_cost/quart_dababa as unit_cost\n",
    "\n",
    "\n",
    "from main_cte mc \n",
    "join finance.all_cogs f on f.product_id = mc.product_id and CURRENT_TIMEstamp between from_date and to_date \n",
    "\n",
    "'''\n",
    "\n",
    "print(\"Fetching wholesale cost data (quarter truck calculations)...\")\n",
    "ws_data = snowflake_query(\"Egypt\", query)\n",
    "ws_data.columns = ws_data.columns.str.lower()\n",
    "\n",
    "for col in ws_data.columns:\n",
    "    ws_data[col] = pd.to_numeric(ws_data[col], errors='ignore')\n",
    "\n",
    "# Select and rename columns\n",
    "ws_data = ws_data[['warehouse_id', 'product_id', 'packing_unit_id', 'quart_dababa', 'unit_cost']]\n",
    "ws_data.columns = ['warehouse_id', 'product_id', 'packing_unit_id', 'WS_tier', 'WS_wac']\n",
    "\n",
    "print(f\"✓ Calculated wholesale data for {len(ws_data)} products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854c4e65-6403-480f-9380-519e106a5ae4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD FORCED BRANDS/CATEGORIES FROM GOOGLE SHEETS\n",
    "# =============================================================================\n",
    "\n",
    "scope = [\n",
    "    \"https://spreadsheets.google.com/feeds\",\n",
    "    \"https://www.googleapis.com/auth/spreadsheets\",\n",
    "    \"https://www.googleapis.com/auth/drive.file\",\n",
    "    \"https://www.googleapis.com/auth/drive\"\n",
    "]\n",
    "\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_dict(\n",
    "    json.loads(setup_environment_2.get_secret(\"prod/maxab-sheets\")), \n",
    "    scope\n",
    ")\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "# Load forced brands and categories\n",
    "force_brands = client.open('Wholesales_exec').worksheet('brands')\n",
    "force_cats = client.open('Wholesales_exec').worksheet('cats')\n",
    "force_brands_df = pd.DataFrame(force_brands.get_all_records())\n",
    "force_cats_df = pd.DataFrame(force_cats.get_all_records())\n",
    "\n",
    "# Extract unique lists\n",
    "forced_brand_list = force_brands_df.brand.unique() if not force_brands_df.empty else []\n",
    "forced_cat_list = force_cats_df.cat.unique() if not force_cats_df.empty else []\n",
    "\n",
    "print(f\"✓ Loaded {len(forced_brand_list)} forced brands, {len(forced_cat_list)} forced categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4830c5dc-d49e-47b4-93f0-2645896e8e7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT product_id, new_pp, forecasted_date\n",
    "FROM materialized_views.DBDP_PRICE_UPS\n",
    "WHERE region = 'Cairo'\n",
    "'''\n",
    "\n",
    "print(\"Fetching price-up forecasts...\")\n",
    "price_ups = snowflake_query(\"Egypt\", query)\n",
    "price_ups.columns = price_ups.columns.str.lower()\n",
    "\n",
    "for col in price_ups.columns:\n",
    "    price_ups[col] = pd.to_numeric(price_ups[col], errors='ignore')\n",
    "\n",
    "print(f\"✓ Retrieved {len(price_ups)} price-up forecasts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82f21df-6b2c-4910-82f9-c7b758a3ac22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge wholesale data and price-ups with final data\n",
    "final_data = final_data.merge(ws_data, on=['warehouse_id', 'product_id', 'packing_unit_id'], how='left')\n",
    "final_data['WS_wac'] = final_data['WS_wac'] * final_data['basic_unit_count']\n",
    "final_data = final_data.merge(price_ups, on='product_id', how='left')\n",
    "\n",
    "print(f\"✓ Added wholesale and price-up data to {len(final_data)} products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fb77deb3-b68a-4c4a-acc8-012272cf7e7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wholesales_margin(x):\n",
    "    \"\"\"\n",
    "    Calculate wholesale price based on margins and product tiers.\n",
    "    \"\"\"\n",
    "    # Extract key variables\n",
    "    wac = x['WS_wac']\n",
    "    target_margin = x['target_margin']\n",
    "    tier_2_price = x['tier_2_price']\n",
    "    final_rank = x['final_rank']\n",
    "    new_pp = x['new_pp']\n",
    "    brand = x['brand']\n",
    "    category = x['cat']\n",
    "    margin = ((x['packing_unit_price'] - x['wac_p']) / x['packing_unit_price'])\n",
    "    \n",
    "    # Update target margin if new_pp exists\n",
    "    if not pd.isna(new_pp):\n",
    "        target_margin =  margin* 0.9\n",
    "    \n",
    "    # Define constants\n",
    "    MIN_MARGIN = 0.01\n",
    "    TOTAL_RANKS = 133\n",
    "    \n",
    "    # Special brand handling\n",
    "    if brand in forced_brand_list:\n",
    "        return _calculate_forced_brand_price(x, wac, target_margin)\n",
    "    \n",
    "    # Fiori brand special case\n",
    "    if brand == 'فيوري':\n",
    "        return wac / (1 - (margin * 0.9))\n",
    "    \n",
    "    # Paper products special case\n",
    "    if category == 'ورقيات':\n",
    "        margin = np.minimum(np.maximum(0.6 * target_margin, 0.015), target_margin)\n",
    "        return wac / (1 - margin)\n",
    "    \n",
    "    # Standard tier-based pricing\n",
    "    tier = _determine_tier(final_rank, TOTAL_RANKS)\n",
    "    price = _calculate_tier_price(wac, target_margin, tier)\n",
    "    \n",
    "    # Adjust if price exceeds tier 2 price\n",
    "    if price >= tier_2_price:\n",
    "        price = (wac + tier_2_price) / 2\n",
    "    \n",
    "    # Ensure minimum margin\n",
    "    return np.maximum(price, wac / (1 - MIN_MARGIN))\n",
    "\n",
    "\n",
    "def _calculate_forced_brand_price(x, wac, target_margin):\n",
    "    \"\"\"Calculate price for forced brands with special margin rules.\"\"\"\n",
    "    brand = x['brand']\n",
    "    margin = ((x['packing_unit_price'] - x['wac_p']) / x['packing_unit_price'])\n",
    "    min_target = 0.25 * target_margin\n",
    "    \n",
    "    if brand in ['كوكا كولا', 'شويبس']:\n",
    "        return np.maximum(wac / (1 - (margin * 0.65)), min_target)\n",
    "    elif brand == 'جود كير':\n",
    "        return np.maximum(wac / (1 - (margin * 0.5)), min_target)\n",
    "    else:\n",
    "        return wac / (1 - (margin * 0.8))\n",
    "\n",
    "\n",
    "def _determine_tier(rank, total_ranks):\n",
    "    \"\"\"Determine product tier based on ranking.\"\"\"\n",
    "    if rank <= 0.25 * total_ranks:\n",
    "        return 1\n",
    "    elif rank <= 0.5 * total_ranks:\n",
    "        return 2\n",
    "    elif rank <= 0.75 * total_ranks:\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "\n",
    "def _calculate_tier_price(wac, target_margin, tier):\n",
    "    \"\"\"Calculate price based on tier with appropriate margin adjustments.\"\"\"\n",
    "    tier_config = {\n",
    "        1: {'multiplier': 0.2, 'min_margin': 0.01},\n",
    "        2: {'multiplier': 0.25, 'min_margin': 0.015},\n",
    "        3: {'multiplier': 0.4, 'min_margin': 0.015},\n",
    "        4: {'multiplier': 0.6, 'min_margin': 0.015}\n",
    "    }\n",
    "    \n",
    "    config = tier_config[tier]\n",
    "    adjusted_margin = config['multiplier'] * target_margin\n",
    "    margin = np.minimum(np.maximum(adjusted_margin, config['min_margin']), target_margin)\n",
    "    \n",
    "    return wac / (1 - margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1ed61e-384a-48f0-9f2d-e70593bde88a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate wholesale prices\n",
    "print(\"Calculating wholesale prices...\")\n",
    "final_data['WS_price'] = final_data.apply(wholesales_margin, axis=1)\n",
    "\n",
    "# Validate: WS price must be below tier 2 price\n",
    "final_data['valid'] = final_data['WS_price'] < final_data['tier_2_price']\n",
    "final_data.loc[final_data['valid'] == False, 'WS_price'] = np.nan\n",
    "\n",
    "valid_ws = final_data['WS_price'].notna().sum()\n",
    "print(f\"✓ Valid wholesale prices: {valid_ws} / {len(final_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8658cd",
   "metadata": {},
   "source": [
    "### 6.2 Wholesale NEW Logic (Delivery Savings Based)\n",
    "\n",
    "New wholesale pricing based on delivery cost savings:\n",
    "- **Car cost**: 900 EGP per delivery\n",
    "- **Car capacity**: 1.8 tons max\n",
    "- **Max ticket size**: 50,000 EGP\n",
    "- **Logic**: If retailer orders multiples of average ticket size, they save deliveries\n",
    "  - 2x avg TS = 1 delivery saved → discount = delivery cost savings\n",
    "  - 3x avg TS = 2 deliveries saved → more discount\n",
    "- **Goal**: Find optimal quantity that gives retailer max savings while price stays above WAC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "08a2d8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== WAREHOUSE TICKET SIZE STATISTICS ===\n",
      "warehouse_name  avg_ticket_size  median_ticket_size  avg_order_weight_kg\n",
      "      Mostorod          6221.75             4650.00               139.59\n",
      "         Tanta          4798.51             3507.63               120.62\n",
      "       El-Marg          6221.75             4650.00               139.59\n",
      "       Sharqya          5050.65             3845.75               117.40\n",
      "      Barageel          6765.23             4905.50               137.76\n",
      "     El-Mahala          4798.51             3507.63               120.62\n",
      "   Mansoura FC          5050.65             3845.75               117.40\n",
      "    Bani sweif          4956.83             3665.25               137.77\n",
      "     Assiut FC          4956.83             3665.25               137.77\n",
      "         Sohag          4956.83             3665.25               137.77\n",
      " Menya Samalot          4956.83             3665.25               137.77\n",
      " Khorshed Alex          5029.32             3497.58               113.70\n",
      "      Sakkarah          6765.23             4905.50               137.76\n",
      "\n",
      "Overall average ticket size: 5425.30 EGP\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# WHOLESALE NEW LOGIC - Configuration\n",
    "# =============================================================================\n",
    "WS_CAR_COST = 1100           # Cost per delivery (EGP)\n",
    "WS_CAR_CAPACITY_TONS = 1.8  # Max car capacity in tons\n",
    "WS_MAX_TICKET_SIZE = 40000  # Maximum ticket size (EGP)\n",
    "WS_MIN_MARGIN = 0.01        # Minimum margin (1%) above WAC\n",
    "\n",
    "# Query to get average ticket size per warehouse\n",
    "query = f'''\n",
    "WITH base AS (\n",
    "    SELECT *, ROW_NUMBER() OVER (PARTITION BY retailer_id ORDER BY priority) as rnk \n",
    "    FROM (\n",
    "        SELECT x.*, TAGGABLE_ID as retailer_id \n",
    "        FROM (\n",
    "            SELECT id as cohort_id, name as cohort_name, priority, dynamic_tag_id \n",
    "            FROM cohorts \n",
    "            WHERE is_active = 'true'\n",
    "                AND id IN (700,701,702,703,704,1123,1124,1125,1126)\n",
    "        ) x \n",
    "        JOIN DYNAMIC_TAGgables dt ON x.dynamic_tag_id = dt.dynamic_tag_id\n",
    "        WHERE dt.taggable_id IN (\n",
    "            SELECT taggable_id FROM DYNAMIC_TAGgables \n",
    "            WHERE dynamic_tag_id IN (2807, 2808, 2809, 2810, 2811, 2812)\n",
    "        )\n",
    "    )\n",
    "    QUALIFY rnk = 1 \n",
    "),\n",
    "\n",
    "-- Map regions to warehouses\n",
    "whs AS (\n",
    "    SELECT * FROM (VALUES\n",
    "        ('Cairo', 'El-Marg', 38),\n",
    "        ('Cairo', 'Mostorod', 1),\n",
    "        ('Giza', 'Barageel', 236),\n",
    "        ('Giza', 'Sakkarah', 962),\n",
    "        ('Delta West', 'El-Mahala', 337),\n",
    "        ('Delta West', 'Tanta', 8),\n",
    "        ('Delta East', 'Mansoura FC', 339),\n",
    "        ('Delta East', 'Sharqya', 170),\n",
    "        ('Upper Egypt', 'Assiut FC', 501),\n",
    "        ('Upper Egypt', 'Bani sweif', 401),\n",
    "        ('Upper Egypt', 'Menya Samalot', 703),\n",
    "        ('Upper Egypt', 'Sohag', 632),\n",
    "        ('Alexandria', 'Khorshed Alex', 797)\n",
    "    ) x(region_name, wh, warehouse_id)\n",
    "),\n",
    "\n",
    "-- Get ticket sizes (order values) for last 4 months\n",
    "ticket_sizes AS (\n",
    "    SELECT \n",
    "        whs.warehouse_id,\n",
    "        whs.wh as warehouse_name,\n",
    "        so.parent_sales_order_id,\n",
    "        so.retailer_id,\n",
    "        SUM(pso.total_price) as ticket_size,\n",
    "        SUM(pso.purchased_item_count * pup.weight / 1000) as order_weight_kg\n",
    "    FROM product_sales_order pso\n",
    "    JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "    JOIN base ON base.retailer_id = so.retailer_id\n",
    "    JOIN packing_unit_products pup ON pup.product_id = pso.product_id \n",
    "        AND pup.packing_unit_id = pso.packing_unit_id\n",
    "    JOIN materialized_views.retailer_polygon rp ON rp.retailer_id = so.retailer_id\n",
    "    JOIN districts ON districts.id = rp.district_id\n",
    "    JOIN cities ON cities.id = districts.city_id\n",
    "    JOIN states ON states.id = cities.state_id\n",
    "    JOIN regions ON regions.id = states.region_id\n",
    "    JOIN whs ON whs.region_name = CASE WHEN regions.id = 2 THEN states.name_en ELSE regions.name_en END\n",
    "    WHERE so.created_at::date BETWEEN DATE_TRUNC('month', CURRENT_DATE - INTERVAL '4 months') AND CURRENT_DATE - 1\n",
    "        AND so.sales_order_status_id NOT IN (7, 12)\n",
    "        AND so.channel IN ('telesales', 'retailer')\n",
    "        AND pso.purchased_item_count > 0\n",
    "    GROUP BY whs.warehouse_id, whs.wh, so.parent_sales_order_id, so.retailer_id\n",
    "),\n",
    "\n",
    "-- Calculate warehouse-level statistics\n",
    "warehouse_stats AS (\n",
    "    SELECT \n",
    "        warehouse_id,\n",
    "        warehouse_name,\n",
    "        COUNT(DISTINCT parent_sales_order_id) as total_orders,\n",
    "        COUNT(DISTINCT retailer_id) as total_retailers,\n",
    "        AVG(ticket_size) as avg_ticket_size,\n",
    "        MEDIAN(ticket_size) as median_ticket_size,\n",
    "        PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY ticket_size) as p75_ticket_size,\n",
    "        PERCENTILE_CONT(0.90) WITHIN GROUP (ORDER BY ticket_size) as p90_ticket_size,\n",
    "        MAX(ticket_size) as max_ticket_size,\n",
    "        AVG(order_weight_kg) as avg_order_weight_kg,\n",
    "        MEDIAN(order_weight_kg) as median_order_weight_kg\n",
    "    FROM ticket_sizes\n",
    "    WHERE ticket_size > 0\n",
    "    GROUP BY warehouse_id, warehouse_name\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    warehouse_id,\n",
    "    warehouse_name,\n",
    "    total_orders,\n",
    "    total_retailers,\n",
    "    ROUND(avg_ticket_size, 2) as avg_ticket_size,\n",
    "    ROUND(median_ticket_size, 2) as median_ticket_size,\n",
    "    ROUND(p75_ticket_size, 2) as p75_ticket_size,\n",
    "    ROUND(p90_ticket_size, 2) as p90_ticket_size,\n",
    "    ROUND(max_ticket_size, 2) as max_ticket_size,\n",
    "    ROUND(avg_order_weight_kg, 2) as avg_order_weight_kg,\n",
    "    ROUND(median_order_weight_kg, 2) as median_order_weight_kg,\n",
    "    -- Calculate how many orders fit in one car based on weight\n",
    "    ROUND({WS_CAR_CAPACITY_TONS * 1000} / NULLIF(avg_order_weight_kg, 0), 1) as orders_per_car_by_weight\n",
    "FROM warehouse_stats\n",
    "ORDER BY warehouse_id\n",
    "'''\n",
    "\n",
    "ws_ticket_data = snowflake_query(\"Egypt\", query)\n",
    "ws_ticket_data.columns = ws_ticket_data.columns.str.lower()\n",
    "for col in ws_ticket_data.columns:\n",
    "    ws_ticket_data[col] = pd.to_numeric(ws_ticket_data[col], errors='ignore')\n",
    "\n",
    "print(\"=== WAREHOUSE TICKET SIZE STATISTICS ===\")\n",
    "print(ws_ticket_data[['warehouse_name', 'avg_ticket_size', 'median_ticket_size', 'avg_order_weight_kg']].to_string(index=False))\n",
    "print(f\"\\nOverall average ticket size: {ws_ticket_data['avg_ticket_size'].mean():.2f} EGP\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "784f273b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating new wholesale logic based on delivery savings...\n",
      "\n",
      "=== NEW WHOLESALE LOGIC SUMMARY ===\n",
      "SKUs with valid WS new price: 1933 / 1963\n",
      "Total car cost: 1100 EGP\n",
      "Average orders per car: 14.0\n",
      "Average car cost per order: 78.66 EGP\n",
      "\n",
      "Order Consolidation:\n",
      "  Average multiplier: 6.9x of avg ticket size\n",
      "  Average order value needed: 36086.50 EGP\n",
      "  Average deliveries saved: 5.9\n",
      "\n",
      "Car Cost Savings:\n",
      "  Average car cost per order: 79.13 EGP\n",
      "  Average total savings: 463.62 EGP\n",
      "  Average discount per unit: 3.01 EGP\n",
      "\n",
      "Pricing:\n",
      "  Average WS new price margin: 3.07%\n",
      "  Average retailer savings: 1.27%\n",
      "\n",
      "Multiplier distribution:\n",
      "ws_new_multiplier\n",
      "2.0     32\n",
      "3.0     27\n",
      "4.0     28\n",
      "5.0    314\n",
      "6.0    154\n",
      "7.0    486\n",
      "8.0    892\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Merge ticket size data with final_data (including orders_per_car_by_weight)\n",
    "final_data = final_data.merge(\n",
    "    ws_ticket_data[['warehouse_id', 'avg_ticket_size', 'median_ticket_size', 'avg_order_weight_kg', 'orders_per_car_by_weight']], \n",
    "    on='warehouse_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "def calculate_ws_new_logic(row):\n",
    "    \"\"\"\n",
    "    Calculate wholesale pricing based on delivery savings.\n",
    "    \n",
    "    Logic:\n",
    "    - Car cost = 900 EGP, but car serves multiple orders per trip\n",
    "    - Car cost per order = 900 / orders_per_car\n",
    "    - If retailer consolidates, they save N orders worth of car cost\n",
    "    - Savings = deliveries_saved * (car_cost / orders_per_car)\n",
    "    - Calculate scenarios from 2x to max_multiplier (capped by max TS)\n",
    "    \n",
    "    Returns: Dict with optimal scenario\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get values\n",
    "    current_price = row['packing_unit_price']\n",
    "    wac = row['wac_p']\n",
    "    avg_ts = row.get('avg_ticket_size', 4000)  # Default 4000 if missing\n",
    "    tier_2_price = row['tier_2_price']\n",
    "    \n",
    "    # Get orders per car (how many orders fit in one car trip based on weight)\n",
    "    orders_per_car = row.get('orders_per_car_by_weight', 10)  # Default 10 if missing\n",
    "    if pd.isna(orders_per_car) or orders_per_car <= 0:\n",
    "        orders_per_car = 10\n",
    "    \n",
    "    # Calculate car cost per order\n",
    "    car_cost_per_order = WS_CAR_COST / orders_per_car\n",
    "    \n",
    "    if pd.isna(avg_ts) or avg_ts <= 0:\n",
    "        avg_ts = 4000\n",
    "    \n",
    "    if pd.isna(current_price) or pd.isna(wac) or current_price <= 0 or wac <= 0 or pd.isna(tier_2_price):\n",
    "        return pd.Series({\n",
    "            'ws_new_multiplier': None,\n",
    "            'ws_new_order_value': None,\n",
    "            'ws_new_qty': None,\n",
    "            'ws_new_deliveries_saved': None,\n",
    "            'ws_new_car_cost_per_order': None,\n",
    "            'ws_new_total_savings': None,\n",
    "            'ws_new_discount_per_unit': None,\n",
    "            'ws_new_price': None,\n",
    "            'ws_new_margin': None,\n",
    "            'ws_new_savings_pct': None\n",
    "        })\n",
    "    \n",
    "    # Calculate max multiplier based on constraints\n",
    "    # Max by ticket size: WS_MAX_TICKET_SIZE / avg_ts\n",
    "    # No arbitrary cap - let WS_MAX_TICKET_SIZE (50K) be the only limit\n",
    "    max_multiplier = int(WS_MAX_TICKET_SIZE / avg_ts)\n",
    "    \n",
    "    best_scenario = None\n",
    "    best_savings_pct = 0\n",
    "    \n",
    "    # Test scenarios from 2x to max_multiplier\n",
    "    for multiplier in range(2, max_multiplier + 1):\n",
    "        # Order value at this multiplier\n",
    "        order_value = avg_ts * multiplier\n",
    "        \n",
    "        # Deliveries saved = multiplier - 1 (consolidating multiple orders into one)\n",
    "        deliveries_saved = multiplier - 1\n",
    "        \n",
    "        # Total savings = deliveries_saved * car_cost_per_order\n",
    "        # This is the actual cost saving from consolidating orders\n",
    "        total_savings = deliveries_saved * car_cost_per_order\n",
    "        \n",
    "        # How many units of this SKU fit in this order value?\n",
    "        qty_at_current_price = order_value / current_price\n",
    "        \n",
    "        if qty_at_current_price <= 0:\n",
    "            continue\n",
    "        \n",
    "        # Discount per unit from car cost savings\n",
    "        discount_per_unit = total_savings / qty_at_current_price\n",
    "        \n",
    "        # New price after passing car cost savings\n",
    "        new_price = tier_2_price - discount_per_unit\n",
    "        \n",
    "        # Check if price stays above WAC with minimum margin\n",
    "        min_acceptable_price = wac * (1 + WS_MIN_MARGIN)\n",
    "        \n",
    "        if new_price >= min_acceptable_price:\n",
    "            # Calculate margin at new price\n",
    "            margin = (new_price - wac) / new_price\n",
    "            \n",
    "            # Savings percentage for retailer\n",
    "            savings_pct = (discount_per_unit / current_price) * 100\n",
    "            \n",
    "            # Keep track of best scenario (highest savings while valid)\n",
    "            if savings_pct > best_savings_pct:\n",
    "                best_savings_pct = savings_pct\n",
    "                best_scenario = {\n",
    "                    'ws_new_multiplier': multiplier,\n",
    "                    'ws_new_order_value': round(order_value, 2),\n",
    "                    'ws_new_qty': round(qty_at_current_price, 0),\n",
    "                    'ws_new_deliveries_saved': deliveries_saved,\n",
    "                    'ws_new_car_cost_per_order': round(car_cost_per_order, 2),\n",
    "                    'ws_new_total_savings': round(total_savings, 2),\n",
    "                    'ws_new_discount_per_unit': round(discount_per_unit, 2),\n",
    "                    'ws_new_price': round(new_price, 2),\n",
    "                    'ws_new_margin': round(margin, 4),\n",
    "                    'ws_new_savings_pct': round(savings_pct, 2)\n",
    "                }\n",
    "    \n",
    "    if best_scenario:\n",
    "        return pd.Series(best_scenario)\n",
    "    else:\n",
    "        return pd.Series({\n",
    "            'ws_new_multiplier': None,\n",
    "            'ws_new_order_value': None,\n",
    "            'ws_new_qty': None,\n",
    "            'ws_new_deliveries_saved': None,\n",
    "            'ws_new_car_cost_per_order': None,\n",
    "            'ws_new_total_savings': None,\n",
    "            'ws_new_discount_per_unit': None,\n",
    "            'ws_new_price': None,\n",
    "            'ws_new_margin': None,\n",
    "            'ws_new_savings_pct': None\n",
    "        })\n",
    "\n",
    "# Apply the new wholesale logic\n",
    "print(\"Calculating new wholesale logic based on delivery savings...\")\n",
    "ws_new_results = final_data.apply(calculate_ws_new_logic, axis=1)\n",
    "final_data = pd.concat([final_data, ws_new_results], axis=1)\n",
    "\n",
    "# Summary statistics\n",
    "valid_ws_new = final_data['ws_new_price'].notna().sum()\n",
    "print(f\"\\n=== NEW WHOLESALE LOGIC SUMMARY ===\")\n",
    "print(f\"SKUs with valid WS new price: {valid_ws_new} / {len(final_data)}\")\n",
    "print(f\"Total car cost: {WS_CAR_COST} EGP\")\n",
    "print(f\"Average orders per car: {final_data['orders_per_car_by_weight'].mean():.1f}\")\n",
    "print(f\"Average car cost per order: {WS_CAR_COST / final_data['orders_per_car_by_weight'].mean():.2f} EGP\")\n",
    "\n",
    "if valid_ws_new > 0:\n",
    "    print(f\"\\nOrder Consolidation:\")\n",
    "    print(f\"  Average multiplier: {final_data['ws_new_multiplier'].mean():.1f}x of avg ticket size\")\n",
    "    print(f\"  Average order value needed: {final_data['ws_new_order_value'].mean():.2f} EGP\")\n",
    "    print(f\"  Average deliveries saved: {final_data['ws_new_deliveries_saved'].mean():.1f}\")\n",
    "    \n",
    "    print(f\"\\nCar Cost Savings:\")\n",
    "    print(f\"  Average car cost per order: {final_data['ws_new_car_cost_per_order'].mean():.2f} EGP\")\n",
    "    print(f\"  Average total savings: {final_data['ws_new_total_savings'].mean():.2f} EGP\")\n",
    "    print(f\"  Average discount per unit: {final_data['ws_new_discount_per_unit'].mean():.2f} EGP\")\n",
    "    \n",
    "    print(f\"\\nPricing:\")\n",
    "    print(f\"  Average WS new price margin: {final_data['ws_new_margin'].mean()*100:.2f}%\")\n",
    "    print(f\"  Average retailer savings: {final_data['ws_new_savings_pct'].mean():.2f}%\")\n",
    "    \n",
    "    # Distribution of multipliers\n",
    "    print(f\"\\nMultiplier distribution:\")\n",
    "    print(final_data['ws_new_multiplier'].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef736fbd-c796-473d-aaa1-b8e725768f4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FINAL RANKING FILTER\n",
    "# =============================================================================\n",
    "\n",
    "# Re-rank within each warehouse and filter to top products\n",
    "final_data['new_rank'] = final_data.groupby(['warehouse_id'])['final_rank'].rank(method='dense', ascending=True)\n",
    "final_data = final_data[final_data['new_rank'] <= FINAL_PRODUCTS_PER_WAREHOUSE]\n",
    "\n",
    "print(f\"✓ Filtered to top {FINAL_PRODUCTS_PER_WAREHOUSE} products per warehouse: {len(final_data)} total SKUs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6cc89a6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== METRICS SUMMARY ===\n",
      "\n",
      "Stretch Analysis (how much retailers need to increase orders):\n",
      "  Average stretch to Tier 1: 253.3%\n",
      "  Average stretch to Tier 2: 514.9%\n",
      "\n",
      "Margin Analysis:\n",
      "  Current margin:  5.83%\n",
      "  Tier 1 margin:   5.11%\n",
      "  Tier 2 margin:   4.25%\n",
      "  WS margin:       3.17%\n",
      "\n",
      "Discount Analysis:\n",
      "  Average Tier 1 discount: 0.76%\n",
      "  Average Tier 2 discount: 1.65%\n",
      "\n",
      "Elasticity Analysis (discount increase vs quantity increase):\n",
      "  Average qty ratio (T2/T1): 1.74x\n",
      "  Average discount ratio (D2/D1): 2.20x\n",
      "  Average elasticity ratio: 1.27\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CALCULATE ADDITIONAL METRICS\n",
    "# =============================================================================\n",
    "\n",
    "# --- Stretch Percentages (how much retailers need to increase to reach each tier) ---\n",
    "# Already included from tiers_selection: tier_1_increase_pct, tier_2_increase_pct\n",
    "# These show: (tier_qty - median_qty) / median_qty * 100\n",
    "\n",
    "# Rename for clarity\n",
    "final_data['stretch_to_tier_1_pct'] = final_data['tier_1_increase_pct']\n",
    "final_data['stretch_to_tier_2_pct'] = final_data['tier_2_increase_pct']\n",
    "\n",
    "# --- Margins for each price tier ---\n",
    "# Margin = (price - wac) / price\n",
    "final_data['tier_1_margin'] = ((final_data['tier_1_price'] - final_data['wac_p']) / final_data['tier_1_price']).round(4)\n",
    "final_data['tier_2_margin'] = ((final_data['tier_2_price'] - final_data['wac_p']) / final_data['tier_2_price']).round(4)\n",
    "final_data['WS_margin'] = ((final_data['WS_price'] - final_data['wac_p']) / final_data['wac_p']).round(4)\n",
    "final_data['current_margin'] = ((final_data['packing_unit_price'] - final_data['wac_p']) / final_data['packing_unit_price']).round(4)\n",
    "\n",
    "# --- Discount calculations ---\n",
    "# Absolute discounts (price reduction from current price)\n",
    "final_data['discount_1'] = (final_data['packing_unit_price'] - final_data['tier_1_price']).round(2)\n",
    "final_data['discount_2'] = (final_data['packing_unit_price'] - final_data['tier_2_price']).round(2)\n",
    "\n",
    "# Discount percentages\n",
    "final_data['discount_1_pct'] = ((final_data['discount_1'] / final_data['packing_unit_price']) * 100).round(2)\n",
    "final_data['discount_2_pct'] = ((final_data['discount_2'] / final_data['packing_unit_price']) * 100).round(2)\n",
    "\n",
    "# --- Quantity and Discount Ratios ---\n",
    "# Quantity ratio (tier_2_qty / tier_1_qty)\n",
    "final_data['qty_ratio'] = (final_data['tier_2_qty'] / final_data['tier_1_qty']).round(2)\n",
    "\n",
    "# Discount ratio (discount_2 / discount_1)\n",
    "final_data['discount_ratio'] = (final_data['discount_2'] / final_data['discount_1']).round(2)\n",
    "\n",
    "# Elasticity ratio = discount_ratio / qty_ratio\n",
    "# This shows how much extra discount per unit of quantity increase\n",
    "final_data['elasticity_ratio'] = (final_data['discount_ratio'] / final_data['qty_ratio']).round(2)\n",
    "\n",
    "print(\"=== METRICS SUMMARY ===\")\n",
    "print(f\"\\nStretch Analysis (how much retailers need to increase orders):\")\n",
    "print(f\"  Average stretch to Tier 1: {final_data['stretch_to_tier_1_pct'].mean():.1f}%\")\n",
    "print(f\"  Average stretch to Tier 2: {final_data['stretch_to_tier_2_pct'].mean():.1f}%\")\n",
    "\n",
    "print(f\"\\nMargin Analysis:\")\n",
    "print(f\"  Current margin:  {final_data['current_margin'].mean()*100:.2f}%\")\n",
    "print(f\"  Tier 1 margin:   {final_data['tier_1_margin'].mean()*100:.2f}%\")\n",
    "print(f\"  Tier 2 margin:   {final_data['tier_2_margin'].mean()*100:.2f}%\")\n",
    "print(f\"  WS margin:       {final_data['WS_margin'].mean()*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nDiscount Analysis:\")\n",
    "print(f\"  Average Tier 1 discount: {final_data['discount_1_pct'].mean():.2f}%\")\n",
    "print(f\"  Average Tier 2 discount: {final_data['discount_2_pct'].mean():.2f}%\")\n",
    "\n",
    "print(f\"\\nElasticity Analysis (discount increase vs quantity increase):\")\n",
    "print(f\"  Average qty ratio (T2/T1): {final_data['qty_ratio'].mean():.2f}x\")\n",
    "print(f\"  Average discount ratio (D2/D1): {final_data['discount_ratio'].mean():.2f}x\")\n",
    "print(f\"  Average elasticity ratio: {final_data['elasticity_ratio'].mean():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ac640f",
   "metadata": {},
   "source": [
    "## 7. Final Ranking & Export\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "65ee57d5-5719-4d50-a45a-d3a3cbf79803",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload format created: 12 warehouse rows\n",
      "\n",
      "Per warehouse breakdown:\n",
      "  WH 501: Group 1 = 200 items, Group 2 = 196 items\n",
      "  WH 401: Group 1 = 200 items, Group 2 = 196 items\n",
      "  WH 236: Group 1 = 200 items, Group 2 = 199 items\n",
      "  WH 337: Group 1 = 200 items, Group 2 = 196 items\n",
      "  WH 797: Group 1 = 200 items, Group 2 = 198 items\n",
      "  WH 339: Group 1 = 200 items, Group 2 = 197 items\n",
      "  WH 703: Group 1 = 200 items, Group 2 = 197 items\n",
      "  WH 1: Group 1 = 200 items, Group 2 = 199 items\n",
      "  WH 962: Group 1 = 200 items, Group 2 = 199 items\n",
      "  WH 170: Group 1 = 200 items, Group 2 = 197 items\n",
      "  WH 632: Group 1 = 200 items, Group 2 = 196 items\n",
      "  WH 8: Group 1 = 200 items, Group 2 = 197 items\n",
      "\n",
      "=== DETAILED FILE ===\n",
      "Saved 1596 SKUs to 'QD_detailed.xlsx'\n",
      "\n",
      "=== UPLOAD FILE ===\n",
      "Saved 12 rows to 'QD_upload.xlsx'\n",
      "Columns: ['warehouse_id', 'Discounts Group 1', 'Discounts Group 2', 'Description']\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CREATE UPLOAD FORMAT\n",
    "# =============================================================================\n",
    "# Format: ONE row per warehouse_id\n",
    "# - Discounts Group 1: List of [tier 1 items + wholesale items] (max 200, overflow goes to Group 2)\n",
    "# - Discounts Group 2: List of [tier 2 items + overflow from Group 1]\n",
    "# Each item format: [product_id, packing_unit_id, quantity, discount_pct]\n",
    "\n",
    "MAX_GROUP_SIZE = 200\n",
    "MAX_DISCOUNT_CAP = 6.0  # Maximum discount capped at 6%\n",
    "\n",
    "final_quantity_discount = pd.DataFrame(columns=['warehouse_id', 'Discounts Group 1', 'Discounts Group 2', 'Description'])\n",
    "\n",
    "for wh_id in final_data.warehouse_id.unique():\n",
    "    warehouse_data = final_data[final_data['warehouse_id'] == wh_id]\n",
    "    warehouse_id = int(wh_id)\n",
    "    \n",
    "    # Collect all tier 1 items\n",
    "    tier_1_items = []\n",
    "    # Collect all tier 2 items\n",
    "    tier_2_items = []\n",
    "    # Collect all wholesale items\n",
    "    ws_items = []\n",
    "    \n",
    "    for i, r in warehouse_data.iterrows():\n",
    "        product_id = int(r['product_id'])\n",
    "        packing_unit_id = int(r['packing_unit_id'])\n",
    "        current_price = r['packing_unit_price']\n",
    "        \n",
    "        # Tier 1 (cap discount at MAX_DISCOUNT_CAP)\n",
    "        q_1 = int(r['tier_1_qty'])\n",
    "        d_1 = min(round(r['discount_1_pct'], 2), MAX_DISCOUNT_CAP)\n",
    "        tier_1_items.append([product_id, packing_unit_id, q_1, d_1])\n",
    "        \n",
    "        # Tier 2 (cap discount at MAX_DISCOUNT_CAP)\n",
    "        q_2 = int(r['tier_2_qty'])\n",
    "        d_2 = min(round(r['discount_2_pct'], 2), MAX_DISCOUNT_CAP)\n",
    "        tier_2_items.append([product_id, packing_unit_id, q_2, d_2])\n",
    "        \n",
    "        # Wholesale (new logic) - cap discount at MAX_DISCOUNT_CAP\n",
    "        ws_qty = r.get('ws_new_qty', None)\n",
    "        ws_price = r.get('ws_new_price', None)\n",
    "        \n",
    "        if pd.notna(ws_qty) and pd.notna(ws_price) and ws_qty > 0 and current_price > 0:\n",
    "            q_ws = int(ws_qty)\n",
    "            d_ws = min(round(((current_price - ws_price) / current_price) * 100, 2), MAX_DISCOUNT_CAP)\n",
    "            ws_items.append([product_id, packing_unit_id, q_ws, d_ws])\n",
    "    \n",
    "    # Group 1: Tier 1 + Wholesale (max 200)\n",
    "    group_1_items = tier_1_items + ws_items\n",
    "    \n",
    "    # Group 2: Tier 2 + overflow from Group 1\n",
    "    if len(group_1_items) > MAX_GROUP_SIZE:\n",
    "        # Overflow goes to Group 2\n",
    "        overflow = group_1_items[MAX_GROUP_SIZE:]\n",
    "        group_1_items = group_1_items[:MAX_GROUP_SIZE]\n",
    "        group_2_items = tier_2_items + overflow\n",
    "    else:\n",
    "        group_2_items = tier_2_items\n",
    "    \n",
    "    new_row = {\n",
    "        'warehouse_id': warehouse_id,\n",
    "        'Discounts Group 1': group_1_items,\n",
    "        'Discounts Group 2': group_2_items,\n",
    "        'Description': f'{warehouse_id}QD'\n",
    "    }\n",
    "    final_quantity_discount = pd.concat([final_quantity_discount, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "# Summary\n",
    "print(f\"Upload format created: {len(final_quantity_discount)} warehouse rows\")\n",
    "print(f\"\\nPer warehouse breakdown:\")\n",
    "for idx, row in final_quantity_discount.iterrows():\n",
    "    wh = row['warehouse_id']\n",
    "    g1_count = len(row['Discounts Group 1'])\n",
    "    g2_count = len(row['Discounts Group 2'])\n",
    "    print(f\"  WH {wh}: Group 1 = {g1_count} items, Group 2 = {g2_count} items\")\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE FILES\n",
    "# =============================================================================\n",
    "\n",
    "# Save detailed data\n",
    "detailed_file = 'QD_detailed.xlsx'\n",
    "final_data.to_excel(detailed_file, index=False)\n",
    "print(f\"\\n=== DETAILED FILE ===\")\n",
    "print(f\"Saved {len(final_data)} SKUs to '{detailed_file}'\")\n",
    "\n",
    "# Save upload format\n",
    "upload_file = 'QD_upload.xlsx'\n",
    "final_quantity_discount.to_excel(upload_file, index=False)\n",
    "print(f\"\\n=== UPLOAD FILE ===\")\n",
    "print(f\"Saved {len(final_quantity_discount)} rows to '{upload_file}'\")\n",
    "print(f\"Columns: {list(final_quantity_discount.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72902879-a6b9-469a-aa6b-bd903e2fd8c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Warehouse to Tag ID mapping for upload\n",
    "df_warehouse_mapping = pd.DataFrame({\n",
    "    'warehouse_name': ['Assiut FC', 'Bani sweif', 'Barageel', 'El-Mahala', 'Khorshed Alex', \n",
    "                       'Mansoura FC', 'Menya Samalot', 'Mostorod', 'Sakkarah', 'Sharqya', \n",
    "                       'Sohag', 'Tanta'],\n",
    "    'warehouse_id':   [501, 401, 236, 337, 797, 339, 703, 1, 962, 170, 632, 8],\n",
    "    'tag_id':         [3301, 3302, 3303, 3304, 3305, 3306, 3307, 3308, 3309, 3310, 3311, 3312]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7d74cd-a60a-4778-b642-2d5c5723700e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge upload data with warehouse mapping\n",
    "to_upload = final_quantity_discount.merge(df_warehouse_mapping, on='warehouse_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe7d893-03eb-421e-b704-de77e375c062",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tag ID</th>\n",
       "      <th>Description</th>\n",
       "      <th>Start Date/Time</th>\n",
       "      <th>End Date/Time</th>\n",
       "      <th>Discounts Group 1</th>\n",
       "      <th>Discounts Group 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3301</td>\n",
       "      <td>Assiut FC_QD</td>\n",
       "      <td>03/12/2025 15:24</td>\n",
       "      <td>06/12/2025 23:59</td>\n",
       "      <td>[[[2778, 2, 5, 0.57], [6935, 2, 7, 1.44], [693...</td>\n",
       "      <td>[[[2778, 2, 11, 1.38], [6935, 2, 14, 3.61], [6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3302</td>\n",
       "      <td>Bani sweif_QD</td>\n",
       "      <td>03/12/2025 15:24</td>\n",
       "      <td>06/12/2025 23:59</td>\n",
       "      <td>[[[141, 2, 4, 0.68], [2328, 2, 5, 0.39], [8650...</td>\n",
       "      <td>[[[141, 2, 7, 1.31], [2328, 2, 8, 1.23], [8650...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3303</td>\n",
       "      <td>Barageel_QD</td>\n",
       "      <td>03/12/2025 15:24</td>\n",
       "      <td>06/12/2025 23:59</td>\n",
       "      <td>[[[6935, 2, 8, 0.97], [362, 2, 4, 0.55], [141,...</td>\n",
       "      <td>[[[6935, 2, 16, 2.58], [362, 2, 7, 1.06], [141...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3304</td>\n",
       "      <td>El-Mahala_QD</td>\n",
       "      <td>03/12/2025 15:24</td>\n",
       "      <td>06/12/2025 23:59</td>\n",
       "      <td>[[[88, 10, 8, 0.93], [8915, 2, 5, 0.6], [6935,...</td>\n",
       "      <td>[[[88, 10, 16, 2.06], [8915, 2, 8, 1.05], [693...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3305</td>\n",
       "      <td>Khorshed Alex_QD</td>\n",
       "      <td>03/12/2025 15:24</td>\n",
       "      <td>06/12/2025 23:59</td>\n",
       "      <td>[[[6935, 2, 7, 0.64], [141, 2, 4, 0.55], [2778...</td>\n",
       "      <td>[[[6935, 2, 18, 2.84], [141, 2, 7, 1.06], [277...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3306</td>\n",
       "      <td>Mansoura FC_QD</td>\n",
       "      <td>03/12/2025 15:24</td>\n",
       "      <td>06/12/2025 23:59</td>\n",
       "      <td>[[[6935, 2, 6, 1.87], [305, 2, 4, 1.74], [2778...</td>\n",
       "      <td>[[[6935, 2, 10, 3.91], [305, 2, 7, 3.34], [277...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3307</td>\n",
       "      <td>Menya Samalot_QD</td>\n",
       "      <td>03/12/2025 15:24</td>\n",
       "      <td>06/12/2025 23:59</td>\n",
       "      <td>[[[2328, 2, 5, 0.39], [6935, 2, 7, 1.44], [141...</td>\n",
       "      <td>[[[2328, 2, 8, 1.23], [6935, 2, 14, 3.61], [14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3308</td>\n",
       "      <td>Mostorod_QD</td>\n",
       "      <td>03/12/2025 15:24</td>\n",
       "      <td>06/12/2025 23:59</td>\n",
       "      <td>[[[3, 2, 4, 0.47], [8672, 2, 6, 0.76], [2049, ...</td>\n",
       "      <td>[[[3, 2, 7, 0.91], [8672, 2, 10, 1.4], [2049, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3309</td>\n",
       "      <td>Sakkarah_QD</td>\n",
       "      <td>03/12/2025 15:24</td>\n",
       "      <td>06/12/2025 23:59</td>\n",
       "      <td>[[[305, 2, 4, 1.25], [6935, 2, 8, 0.97], [326,...</td>\n",
       "      <td>[[[305, 2, 7, 2.4], [6935, 2, 16, 2.74], [326,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3310</td>\n",
       "      <td>Sharqya_QD</td>\n",
       "      <td>03/12/2025 15:24</td>\n",
       "      <td>06/12/2025 23:59</td>\n",
       "      <td>[[[11685, 2, 5, 1.6], [130, 1, 4, 1.36], [6935...</td>\n",
       "      <td>[[[11685, 2, 8, 3.15], [130, 1, 7, 2.63], [693...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3311</td>\n",
       "      <td>Sohag_QD</td>\n",
       "      <td>03/12/2025 15:24</td>\n",
       "      <td>06/12/2025 23:59</td>\n",
       "      <td>[[[8672, 2, 5, 0.46], [2778, 2, 5, 0.5], [205,...</td>\n",
       "      <td>[[[8672, 2, 9, 0.92], [2778, 2, 11, 1.2], [205...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3312</td>\n",
       "      <td>Tanta_QD</td>\n",
       "      <td>03/12/2025 15:24</td>\n",
       "      <td>06/12/2025 23:59</td>\n",
       "      <td>[[[305, 2, 4, 0.5], [6935, 2, 5, 2.61], [2778,...</td>\n",
       "      <td>[[[305, 2, 7, 1.89], [6935, 2, 11, 6.0], [2778...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Tag ID       Description   Start Date/Time     End Date/Time  \\\n",
       "0     3301      Assiut FC_QD  03/12/2025 15:24  06/12/2025 23:59   \n",
       "1     3302     Bani sweif_QD  03/12/2025 15:24  06/12/2025 23:59   \n",
       "2     3303       Barageel_QD  03/12/2025 15:24  06/12/2025 23:59   \n",
       "3     3304      El-Mahala_QD  03/12/2025 15:24  06/12/2025 23:59   \n",
       "4     3305  Khorshed Alex_QD  03/12/2025 15:24  06/12/2025 23:59   \n",
       "5     3306    Mansoura FC_QD  03/12/2025 15:24  06/12/2025 23:59   \n",
       "6     3307  Menya Samalot_QD  03/12/2025 15:24  06/12/2025 23:59   \n",
       "7     3308       Mostorod_QD  03/12/2025 15:24  06/12/2025 23:59   \n",
       "8     3309       Sakkarah_QD  03/12/2025 15:24  06/12/2025 23:59   \n",
       "9     3310        Sharqya_QD  03/12/2025 15:24  06/12/2025 23:59   \n",
       "10    3311          Sohag_QD  03/12/2025 15:24  06/12/2025 23:59   \n",
       "11    3312          Tanta_QD  03/12/2025 15:24  06/12/2025 23:59   \n",
       "\n",
       "                                    Discounts Group 1  \\\n",
       "0   [[[2778, 2, 5, 0.57], [6935, 2, 7, 1.44], [693...   \n",
       "1   [[[141, 2, 4, 0.68], [2328, 2, 5, 0.39], [8650...   \n",
       "2   [[[6935, 2, 8, 0.97], [362, 2, 4, 0.55], [141,...   \n",
       "3   [[[88, 10, 8, 0.93], [8915, 2, 5, 0.6], [6935,...   \n",
       "4   [[[6935, 2, 7, 0.64], [141, 2, 4, 0.55], [2778...   \n",
       "5   [[[6935, 2, 6, 1.87], [305, 2, 4, 1.74], [2778...   \n",
       "6   [[[2328, 2, 5, 0.39], [6935, 2, 7, 1.44], [141...   \n",
       "7   [[[3, 2, 4, 0.47], [8672, 2, 6, 0.76], [2049, ...   \n",
       "8   [[[305, 2, 4, 1.25], [6935, 2, 8, 0.97], [326,...   \n",
       "9   [[[11685, 2, 5, 1.6], [130, 1, 4, 1.36], [6935...   \n",
       "10  [[[8672, 2, 5, 0.46], [2778, 2, 5, 0.5], [205,...   \n",
       "11  [[[305, 2, 4, 0.5], [6935, 2, 5, 2.61], [2778,...   \n",
       "\n",
       "                                    Discounts Group 2  \n",
       "0   [[[2778, 2, 11, 1.38], [6935, 2, 14, 3.61], [6...  \n",
       "1   [[[141, 2, 7, 1.31], [2328, 2, 8, 1.23], [8650...  \n",
       "2   [[[6935, 2, 16, 2.58], [362, 2, 7, 1.06], [141...  \n",
       "3   [[[88, 10, 16, 2.06], [8915, 2, 8, 1.05], [693...  \n",
       "4   [[[6935, 2, 18, 2.84], [141, 2, 7, 1.06], [277...  \n",
       "5   [[[6935, 2, 10, 3.91], [305, 2, 7, 3.34], [277...  \n",
       "6   [[[2328, 2, 8, 1.23], [6935, 2, 14, 3.61], [14...  \n",
       "7   [[[3, 2, 7, 0.91], [8672, 2, 10, 1.4], [2049, ...  \n",
       "8   [[[305, 2, 7, 2.4], [6935, 2, 16, 2.74], [326,...  \n",
       "9   [[[11685, 2, 8, 3.15], [130, 1, 7, 2.63], [693...  \n",
       "10  [[[8672, 2, 9, 0.92], [2778, 2, 11, 1.2], [205...  \n",
       "11  [[[305, 2, 7, 1.89], [6935, 2, 11, 6.0], [2778...  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PREPARE FINAL UPLOAD FILE\n",
    "# =============================================================================\n",
    "\n",
    "# Set description and date/time fields\n",
    "to_upload['Description'] = to_upload['warehouse_name'].astype(str) + \"_QD\"\n",
    "\n",
    "start_date = datetime.now() + timedelta(minutes=10)\n",
    "start_date_str = start_date.strftime('%d/%m/%Y %H:%M')\n",
    "\n",
    "end_date = datetime.now() + timedelta(days=3)\n",
    "end_date = end_date.replace(hour=23, minute=59, second=0, microsecond=0)\n",
    "end_date_str = end_date.strftime('%d/%m/%Y %H:%M')\n",
    "\n",
    "to_upload['Start Date/Time'] = start_date_str\n",
    "to_upload['End Date/Time'] = end_date_str\n",
    "to_upload = to_upload.rename(columns={'tag_id': 'Tag ID'})\n",
    "\n",
    "# Aggregate by Tag ID\n",
    "to_upload = to_upload.groupby(\n",
    "    ['Tag ID', 'Description', 'Start Date/Time', 'End Date/Time'], \n",
    "    as_index=False\n",
    ").agg({\n",
    "    'Discounts Group 1': list,\n",
    "    'Discounts Group 2': list\n",
    "})\n",
    "\n",
    "# Save upload file\n",
    "to_upload.to_excel('QD_upload.xlsx', index=False)\n",
    "print(f\"✓ Saved upload file: QD_upload.xlsx ({len(to_upload)} warehouses)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1e89eb-6eba-4c3f-a7de-25433787171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# UPLOAD TO API\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Uploading QD file to API...\")\n",
    "response = post_QD('QD_upload.xlsx')\n",
    "\n",
    "if response.ok:\n",
    "    print(f\"✓ Upload succeeded (status: {response.status_code})\")\n",
    "else:\n",
    "    print(f\"❌ Upload failed (status: {response.status_code})\")\n",
    "    print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310698a1-a266-492a-a5cb-5e861571a55c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cohort_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>packing_unit_id</th>\n",
       "      <th>tier_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>702</td>\n",
       "      <td>11491</td>\n",
       "      <td>2</td>\n",
       "      <td>170.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>702</td>\n",
       "      <td>6936</td>\n",
       "      <td>2</td>\n",
       "      <td>334.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>702</td>\n",
       "      <td>8650</td>\n",
       "      <td>2</td>\n",
       "      <td>272.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>702</td>\n",
       "      <td>2438</td>\n",
       "      <td>2</td>\n",
       "      <td>753.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>702</td>\n",
       "      <td>8853</td>\n",
       "      <td>2</td>\n",
       "      <td>334.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1591</th>\n",
       "      <td>1123</td>\n",
       "      <td>8915</td>\n",
       "      <td>2</td>\n",
       "      <td>260.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1592</th>\n",
       "      <td>1124</td>\n",
       "      <td>416</td>\n",
       "      <td>1</td>\n",
       "      <td>118.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1593</th>\n",
       "      <td>1123</td>\n",
       "      <td>1413</td>\n",
       "      <td>15</td>\n",
       "      <td>134.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>1124</td>\n",
       "      <td>2703</td>\n",
       "      <td>3</td>\n",
       "      <td>439.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>702</td>\n",
       "      <td>346</td>\n",
       "      <td>3</td>\n",
       "      <td>761.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1566 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cohort_id  product_id  packing_unit_id  tier_2\n",
       "0           702       11491                2   170.0\n",
       "1           702        6936                2   334.0\n",
       "2           702        8650                2   272.0\n",
       "3           702        2438                2   753.0\n",
       "4           702        8853                2   334.0\n",
       "...         ...         ...              ...     ...\n",
       "1591       1123        8915                2   260.0\n",
       "1592       1124         416                1   118.0\n",
       "1593       1123        1413               15   134.0\n",
       "1594       1124        2703                3   439.0\n",
       "1595        702         346                3   761.0\n",
       "\n",
       "[1566 rows x 4 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PREPARE CART RULES UPDATE\n",
    "# =============================================================================\n",
    "\n",
    "# Merge current cart rules with new tier data\n",
    "cart_rules_update = live_cart_rules.merge(\n",
    "    final_data[['warehouse_id', 'product_id', 'packing_unit_id', 'tier_2_qty', 'ws_new_qty']],\n",
    "    on=['warehouse_id', 'product_id', 'packing_unit_id']\n",
    ")\n",
    "cart_rules_update = cart_rules_update.fillna(0)\n",
    "\n",
    "# New cart rule = max of tier_2_qty and ws_new_qty\n",
    "cart_rules_update['tier_2'] = np.maximum(cart_rules_update['tier_2_qty'], cart_rules_update['ws_new_qty'])\n",
    "\n",
    "# Only update rules that need to increase\n",
    "cart_rules_update = cart_rules_update[cart_rules_update['tier_2'] > cart_rules_update['current_cart_rule']]\n",
    "cart_rules_update = cart_rules_update[['cohort_id', 'product_id', 'packing_unit_id', 'tier_2']]\n",
    "\n",
    "print(f\"✓ Cart rules to update: {len(cart_rules_update)} products across {cart_rules_update['cohort_id'].nunique()} cohorts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee06fc22-b921-4fad-bcb6-9a7c44e66cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# UPLOAD CART RULES BY COHORT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Uploading cart rules by cohort...\")\n",
    "\n",
    "for cohort in cart_rules_update.cohort_id.unique():\n",
    "    req_data = cart_rules_update[cart_rules_update['cohort_id'] == cohort]\n",
    "    \n",
    "    if len(req_data) > 0:\n",
    "        # Prepare data for upload\n",
    "        req_data = req_data[['product_id', 'packing_unit_id', 'tier_2']]\n",
    "        req_data.columns = ['Product ID', 'Packing Unit ID', 'Cart Rules']\n",
    "        \n",
    "        # Save and upload\n",
    "        filename = f'CartRules_{cohort}.xlsx'\n",
    "        req_data.to_excel(filename, index=False, engine='xlsxwriter')\n",
    "        \n",
    "        time.sleep(5)\n",
    "        response = post_cart_rules(cohort, filename)\n",
    "        \n",
    "        if response.ok:\n",
    "            print(f\"  ✓ Cohort {cohort}: {len(req_data)} rules uploaded\")\n",
    "        else:\n",
    "            print(f\"  ❌ Cohort {cohort}: Upload failed\")\n",
    "            print(response.content)\n",
    "            break\n",
    "\n",
    "print(\"\\n✓ Cart rules upload complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
