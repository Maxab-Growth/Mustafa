{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2e32a9-ef1a-474a-a2f7-e291561f6da3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Upgrade pip\n",
    "!pip install --upgrade pip\n",
    "# Connectivity\n",
    "!pip install psycopg2-binary  # PostgreSQL adapter\n",
    "# !pip install snowflake-connector-python  # Snowflake connector\n",
    "!pip install snowflake-connector-python==3.15.0 # Snowflake connector Older Version\n",
    "!pip install snowflake-sqlalchemy  # Snowflake SQLAlchemy connector\n",
    "!pip install warnings # Warnings management\n",
    "# !pip install pyarrow # Serialization\n",
    "!pip install keyring==23.11.0 # Key management\n",
    "!pip install sqlalchemy==1.4.46 # SQLAlchemy\n",
    "!pip install requests # HTTP requests\n",
    "!pip install boto3 # AWS SDK\n",
    "# !pip install slackclient # Slack API\n",
    "!pip install oauth2client # Google Sheets API\n",
    "!pip install gspread==5.9.0 # Google Sheets API\n",
    "!pip install gspread_dataframe # Google Sheets API\n",
    "!pip install google.cloud # Google Cloud\n",
    "# Data manipulation and analysis\n",
    "!pip install polars\n",
    "!pip install pandas==2.2.1\n",
    "!pip install numpy\n",
    "# !pip install fastparquet\n",
    "!pip install openpyxl # Excel file handling\n",
    "!pip install xlsxwriter # Excel file handling\n",
    "# Linear programming\n",
    "!pip install pulp\n",
    "# Date and time handling\n",
    "!pip install --upgrade datetime\n",
    "!pip install python-time\n",
    "!pip install --upgrade pytz\n",
    "# Progress bar\n",
    "!pip install tqdm\n",
    "# Database data types\n",
    "!pip install db-dtypes\n",
    "# Geospatial data handling\n",
    "# !pip install geopandas\n",
    "# !pip install shapely\n",
    "# !pip install fiona\n",
    "# !pip install haversine\n",
    "# Plotting\n",
    "\n",
    "# Modeling\n",
    "!pip install statsmodels\n",
    "!pip install scikit-learn\n",
    "\n",
    "!pip install import-ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16b73aac-2b7a-491b-80f9-674d0b9a67a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.Renviron\n",
      "/home/ec2-user/service_account_key.json\n",
      "/home/ec2-user/.Renviron\n",
      "/home/ec2-user/service_account_key.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "import json\n",
    "from datetime import date, timedelta\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import setup_environment_2\n",
    "import importlib\n",
    "import import_ipynb\n",
    "import warnings\n",
    "import demand_sku_cntrb\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "importlib.reload(setup_environment_2)\n",
    "setup_environment_2.initialize_env()\n",
    "import gspread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cc41c3e-734d-43f3-8964-1c9fd8e676bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_snowflake(query, columns=[]):\n",
    "    import os\n",
    "    import snowflake.connector\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    con = snowflake.connector.connect(\n",
    "        user =  os.environ[\"SNOWFLAKE_USERNAME\"],\n",
    "        account= os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "        password= os.environ[\"SNOWFLAKE_PASSWORD\"],\n",
    "        database =os.environ[\"SNOWFLAKE_DATABASE\"]\n",
    "    )\n",
    "    try:\n",
    "        cur = con.cursor()\n",
    "        cur.execute(\"USE WAREHOUSE COMPUTE_WH\")\n",
    "        cur.execute(query)\n",
    "        if len(columns) == 0:\n",
    "            out = pd.DataFrame(np.array(cur.fetchall()))\n",
    "        else:\n",
    "            out = pd.DataFrame(np.array(cur.fetchall()),columns=columns)\n",
    "        return out\n",
    "    except Exception as e:\n",
    "        print(\"Error: \", e)\n",
    "    finally:\n",
    "        cur.close()\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4843f389-c610-4185-bec0-0978a722587f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'America/Los_Angeles'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = '''\n",
    "SHOW PARAMETERS LIKE 'TIMEZONE'\n",
    "'''\n",
    "x  = query_snowflake(query)\n",
    "zone_to_use = x[1].values[0]\n",
    "zone_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5198399c-507c-4f5b-b61e-b030873c2694",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scope = [\"https://spreadsheets.google.com/feeds\",\n",
    "         'https://www.googleapis.com/auth/spreadsheets',\n",
    "         \"https://www.googleapis.com/auth/drive.file\",\n",
    "         \"https://www.googleapis.com/auth/drive\"]\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_dict(json.loads(setup_environment_2.get_secret(\"prod/maxab-sheets\")), scope)\n",
    "client = gspread.authorize(creds)\n",
    "min_max = client.open('Demand Based Dynamic Pricing').worksheet('min_max_margin_cohort')\n",
    "min_max_df = pd.DataFrame(min_max.get_all_records())\n",
    "for col in min_max_df.columns:\n",
    "    min_max_df[col] = pd.to_numeric(min_max_df[col], errors='ignore')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ade36b5b-e61d-4d32-a310-6f5f610f838b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f'''\n",
    "WITH whs as (SELECT *\n",
    "             FROM   (values\n",
    "                            ('Cairo', 'El-Marg', 38,700),\n",
    "                            ('Cairo', 'Mostorod', 1,700),\n",
    "                            ('Giza', 'Barageel', 236,701),\n",
    "                            ('Delta West', 'El-Mahala', 337,703),\n",
    "                            ('Delta West', 'Tanta', 8,703),\n",
    "                            ('Delta East', 'Mansoura FC', 339,704),\n",
    "                            ('Delta East', 'Sharqya', 170,704),\n",
    "                            ('Upper Egypt', 'Assiut FC', 501,1124),\n",
    "                            ('Upper Egypt', 'Bani sweif', 401,1126),\n",
    "                            ('Upper Egypt', 'Menya Samalot', 703,1123),\n",
    "                            ('Upper Egypt', 'Sohag', 632,1125),\n",
    "                            ('Alexandria', 'Khorshed Alex', 797,702),\n",
    "\t\t\t\t\t\t\t('Giza', 'Sakkarah', 962,701)\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t)\n",
    "                    x(region, wh, warehouse_id,cohort_id)),\n",
    "full_data as (\n",
    "select products.id as product_id, region\n",
    "from products , whs \n",
    "where activation = 'true'\n",
    "),\t\t\t\t\n",
    "\n",
    "MP as (\n",
    "select region,product_id,\n",
    "min(min_price) as min_price,\n",
    "min(max_price) as max_price,\n",
    "min(mod_price) as mod_price,\n",
    "min(true_min) as true_min,\n",
    "min(true_max) as true_max\n",
    "\n",
    "from (\n",
    "select mp.region,mp.product_id,mp.pu_id,\n",
    "min_price/BASIC_UNIT_COUNT as min_price,\n",
    "max_price/BASIC_UNIT_COUNT as max_price,\n",
    "mod_price/BASIC_UNIT_COUNT as mod_price,\n",
    "TRUE_MIN_PRICE/BASIC_UNIT_COUNT as true_min,\n",
    "TRUE_MAX_PRICE/BASIC_UNIT_COUNT as true_max\n",
    "from materialized_views.marketplace_prices mp \n",
    "join packing_unit_products pup on pup.product_id = mp.product_id and pup.packing_unit_id = mp.pu_id\n",
    "join finance.all_cogs f on f.product_id = mp.product_id and CURRENT_TIMESTAMP between f.from_date and f.to_date\n",
    "where  least(min_price,mod_price) between wac_p*0.9 and wac_p*1.3 \n",
    ")\n",
    "group by all \n",
    "),\n",
    "region_mapping AS (\n",
    "    SELECT * \n",
    "\tFROM \n",
    "\t(\tVALUES\n",
    "        ('Delta East', 'Delta West'),\n",
    "        ('Delta West', 'Delta East'),\n",
    "        ('Alexandria', 'Cairo'),\n",
    "        ('Alexandria', 'Giza'),\n",
    "        ('Upper Egypt', 'Cairo'),\n",
    "        ('Upper Egypt', 'Giza'),\n",
    "\t\t('Cairo','Giza'),\n",
    "\t\t('Giza','Cairo'),\n",
    "\t\t('Delta West', 'Cairo'),\n",
    "\t\t('Delta East', 'Cairo'),\n",
    "\t\t('Delta West', 'Giza'),\n",
    "\t\t('Delta East', 'Giza')\n",
    "\t\t)\n",
    "    AS region_mapping(region, fallback_region)\n",
    "),\n",
    "final_mp as (\n",
    "select region,product_id,\n",
    "min(final_min_price) as final_min_price,\n",
    "min(final_max_price) as final_max_price,\n",
    "min(final_mod_price) as final_mod_price,\n",
    "min(final_true_min) as final_true_min,\n",
    "min(final_true_max) as final_true_max\n",
    "\n",
    "from (\n",
    "SELECT\n",
    "distinct \n",
    "\tw.region,\n",
    "\tw.product_id,\n",
    "    COALESCE(m1.min_price, m2.min_price) AS final_min_price,\n",
    "    COALESCE(m1.max_price, m2.max_price) AS final_max_price,\n",
    "    COALESCE(m1.mod_price, m2.mod_price) AS final_mod_price,\n",
    "\tCOALESCE(m1.true_min, m2.true_min) AS final_true_min,\n",
    "\tCOALESCE(m1.true_max, m2.true_max) AS final_true_max,\n",
    "FROM full_data w\n",
    "LEFT JOIN MP m1\n",
    "    ON w.region = m1.region and w.product_id = m1.product_id\n",
    "JOIN region_mapping rm\n",
    "    ON w.region = rm.region\n",
    "LEFT JOIN MP m2\n",
    "    ON rm.fallback_region = m2.region\n",
    "   AND w.product_id = m2.product_id\n",
    ")\n",
    "where final_min_price is not null \n",
    "group by all \n",
    "),\n",
    "ben_soliman as (\n",
    "select z.* \n",
    "from (\n",
    "select maxab_product_id as product_id,maxab_sku as sku,avg(bs_final_price) as ben_soliman_price\n",
    "from (\n",
    "select * , row_number()over(partition by maxab_product_id order by diff) as rnk_2\n",
    "from (\n",
    "select *,(bs_final_price-wac_p)/wac_p as diff_2\n",
    "from (\n",
    "select * ,bs_price/maxab_basic_unit_count as bs_final_price\n",
    "from (\n",
    "select *,row_number()over(partition by maxab_product_id,maxab_pu order by diff) as rnk \n",
    "from (\n",
    "select sm.* ,max(INJECTION_DATE::date)over(partition by maxab_product_id,maxab_pu) as max_date,wac1,wac_p,abs(bs_price-(wac_p*maxab_basic_unit_count))/(wac_p*maxab_basic_unit_count) as diff \n",
    "from materialized_views.savvy_mapping sm \n",
    "join finance.all_cogs f on f.product_id = sm.maxab_product_id and current_timestamp between f.from_Date and f.to_date\n",
    "where bs_price is not null \n",
    "and INJECTION_DATE::date >= CURRENT_DATE- 5\n",
    "qualify INJECTION_DATE::date = max_date\n",
    ")\n",
    "qualify rnk = 1 \n",
    ")\n",
    ")\n",
    "where diff_2 between -0.5 and 0.5 \n",
    ")\n",
    "qualify rnk_2 = 1 \n",
    ")\n",
    "group by all\n",
    ")z \n",
    "join finance.all_cogs f on f.product_id = z.product_id and current_timestamp between f.from_Date and f.to_date\n",
    "\n",
    "where ben_soliman_price between f.wac_p*0.9 and f.wac_p*1.3\n",
    "),\n",
    "scrapped_data as (\n",
    "select product_id,cat,brand,region,max_date,min(MARKET_PRICE) as min_scrapped,max(MARKET_PRICE) as max_scrapped,median(MARKET_PRICE) as median_scrapped\n",
    "from (\n",
    "select MATERIALIZED_VIEWS.CLEANED_MARKET_PRICES.*,max(date)over(partition by region,MATERIALIZED_VIEWS.CLEANED_MARKET_PRICES.product_id,competitor) as max_date\n",
    "from MATERIALIZED_VIEWS.CLEANED_MARKET_PRICES\n",
    "join finance.all_cogs f on f.product_id = MATERIALIZED_VIEWS.CLEANED_MARKET_PRICES.product_id and CURRENT_TIMESTAMP between f.from_date and f.to_date \n",
    "where date>= current_date -5\n",
    "and MARKET_PRICE between f.wac_p * 0.9 and wac_p*1.3\n",
    "qualify date = max_date \n",
    ")\n",
    "group by all \n",
    "),\n",
    "local_prices as (\n",
    "SELECT  case when cpu.cohort_id in (700) then 'Cairo'\n",
    "             when cpu.cohort_id in (701) then 'Giza'\n",
    "             when cpu.cohort_id in (704) then 'Delta East'\n",
    "             when cpu.cohort_id in (703) then 'Delta West'\n",
    "             when cpu.cohort_id in (1123,1124,1125,1126) then 'Upper Egypt'\n",
    "             when cpu.cohort_id in (702) then 'Alexandria'\n",
    "        end as region,\n",
    "\t\tcohort_id,\n",
    "        pu.product_id,\n",
    "\t\tpu.packing_unit_id as packing_unit_id,\n",
    "\t\tpu.basic_unit_count,\n",
    "        avg(cpu.price) as price\n",
    "FROM    cohort_product_packing_units cpu\n",
    "join    PACKING_UNIT_PRODUCTS pu on pu.id = cpu.product_packing_unit_id\n",
    "WHERE   cpu.cohort_id in (700,701,702,703,704,1123,1124,1125,1126)\n",
    "    and cpu.created_at::date<>'2023-07-31'\n",
    "    and cpu.is_customized = true\n",
    "\tgroup by all \n",
    "),\n",
    "live_prices as (\n",
    "select region,cohort_id,product_id,pu_id as packing_unit_id,buc as basic_unit_count,NEW_PRICE as price\n",
    "from materialized_views.DBDP_PRICES\n",
    "where created_at = CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date\n",
    "and DATE_PART('hour', CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMESTAMP())::time) BETWEEN SPLIT_PART(time_slot, '-', 1)::int AND SPLIT_PART(time_slot, '-', 2)::int\n",
    "and cohort_id in (700,701,702,703,704,696,695,698,697,699,1123,1124,1125,1126)\n",
    "),\n",
    "prices as (\n",
    "select *\n",
    "from (\n",
    "    SELECT *, 1 AS priority FROM live_prices\n",
    "    UNION ALL\n",
    "    SELECT *, 2 AS priority FROM local_prices\n",
    ")\n",
    "QUALIFY ROW_NUMBER() OVER (PARTITION BY region,cohort_id,product_id,packing_unit_id ORDER BY priority) = 1\n",
    "),\n",
    "\n",
    "maxab_prices as (\n",
    "select region,cohort_id,product_id,price \n",
    "from prices \n",
    "where basic_unit_count = 1 \n",
    "),\n",
    "sales as (\n",
    "SELECT  DISTINCT\n",
    "\t\tcpc.cohort_id,\n",
    "\t\tpso.product_id,\n",
    "        sum(pso.total_price) as nmv\n",
    "FROM product_sales_order pso\n",
    "JOIN sales_orders so ON so.id = pso.sales_order_id   \n",
    "join COHORT_PRICING_CHANGES cpc on cpc.id = pso.cohort_pricing_change_id\n",
    "WHERE   True\n",
    "    AND so.created_at::date between date_trunc('month', CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - 90) and CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - 1\n",
    "    AND so.sales_order_status_id not in (7,12)\n",
    "    AND so.channel IN ('telesales','retailer')\n",
    "    AND pso.purchased_item_count <> 0\n",
    "GROUP BY ALL\n",
    "),\n",
    "margin_change as (\n",
    "select product_id,cohort_id,(0.6*product_std) +(0.3*brand_std) + (0.1*cat_std) as std,avg_margin\n",
    "from (\n",
    "select product_id,cohort_id,stddev(product_margin) as product_std , stddev(brand_margin) as brand_std,stddev(cat_margin) as cat_std,avg(product_margin) as avg_margin\n",
    "from (\n",
    "select distinct product_id,order_date,cohort_id,(nmv-cogs_p)/nmv as product_margin,(brand_nmv-brand_cogs)/brand_nmv as brand_margin,(cat_nmv-cat_cogs)/cat_nmv as cat_margin\n",
    "from(\n",
    "SELECT  DISTINCT\n",
    "\t\tso.created_at::date as order_date,\n",
    "\t\tcpc.cohort_id,\n",
    "\t\tpso.product_id,\n",
    "\t\tbrands.name_ar as brand, \n",
    "\t\tcategories.name_ar as cat,\n",
    "       sum(COALESCE(f.wac_p,0) * pso.purchased_item_count * pso.basic_unit_count) as cogs_p,\n",
    "    \tsum(pso.total_price) as nmv,\n",
    "\t\tsum(nmv) over(partition by order_date,cat,brand) as brand_nmv,\n",
    "\t\tsum(cogs_p) over(partition by order_date,cat,brand) as brand_cogs,\n",
    "\t\tsum(nmv) over(partition by order_date,cat) as cat_nmv,\n",
    "\t\tsum(cogs_p) over(partition by order_date,cat) as cat_cogs\n",
    "FROM product_sales_order pso\n",
    "JOIN sales_orders so ON so.id = pso.sales_order_id   \n",
    "join COHORT_PRICING_CHANGES cpc on cpc.id = pso.cohort_pricing_change_id\n",
    "JOIN products on products.id=pso.product_id\n",
    "JOIN brands on products.brand_id = brands.id \n",
    "JOIN categories ON products.category_id = categories.id\n",
    "JOIN finance.all_cogs f  ON f.product_id = pso.product_id\n",
    "                        AND f.from_date::date <= so.created_at::date\n",
    "                        AND f.to_date::date > so.created_at::date\n",
    "\t\t\t\t\t\t\n",
    "WHERE  so.created_at::date between date_trunc('month', CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - 120) and CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date\n",
    "    AND so.sales_order_status_id not in (7,12)\n",
    "    AND so.channel IN ('telesales','retailer')\n",
    "    AND pso.purchased_item_count <> 0\n",
    "GROUP BY ALL\n",
    ")\n",
    ")\n",
    "\n",
    "group by all \n",
    ")\n",
    "),\n",
    "cat_brand_target as (\n",
    "SELECT DISTINCT cat, brand, margin as target_bm\n",
    "FROM    performance.commercial_targets cplan\n",
    "QUALIFY CASE WHEN DATE_TRUNC('month', MAX(DATE)OVER()) = DATE_TRUNC('month', CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date) THEN DATE_TRUNC('month', CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date)\n",
    "ELSE DATE_TRUNC('month', CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - INTERVAL '1 month') END = DATE_TRUNC('month', date)\n",
    "),\n",
    "cat_target as (\n",
    "\n",
    "select cat,sum(target_bm *(target_nmv/cat_total)) as cat_target_margin\n",
    "from (\n",
    "select *,sum(target_nmv)over(partition by cat) as cat_total\n",
    "from (\n",
    "select cat,brand,avg(target_bm) as target_bm , sum(target_nmv) as target_nmv\n",
    "from (\n",
    "SELECT DISTINCT date,city as region,cat, brand, margin as target_bm,nmv as target_nmv\n",
    "FROM    performance.commercial_targets cplan\n",
    "QUALIFY CASE WHEN DATE_TRUNC('month', MAX(DATE)OVER()) = DATE_TRUNC('month', CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date) THEN DATE_TRUNC('month', CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date)\n",
    "ELSE DATE_TRUNC('month', CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - INTERVAL '1 month') END = DATE_TRUNC('month', date)\n",
    ")\n",
    "group by all\n",
    ")\n",
    ")\n",
    "group by all \n",
    ")\n",
    "\n",
    "select cohort_id,product_id,sku,cat,brand,\n",
    "case when min_status = 1 and new_min_status = 1 then  min_margin when min_status = 0 and new_min_status = 1 then new_min_margin else min_margin end as min_margin,\n",
    "case when new_min_status = 0 then min_margin else new_min_margin end as new_min_margin,\n",
    "avg_market_margin,\n",
    "new_avg_market_margin,\n",
    "target_margin,\n",
    "current_margin\n",
    "from (\n",
    "select *,\n",
    "(all_mp_mins-wac_p)/all_mp_mins as min_margin,\n",
    "(new_min-wac_p)/new_min as new_min_margin,\n",
    "(average_price-wac_p)/average_price as avg_market_margin,\n",
    "(average_new-wac_p)/average_new as new_avg_market_margin,\n",
    "(maxab_price-wac_p)/maxab_price as current_margin,\n",
    "greatest(least(current_margin - (1.5*std) , avg_margin - (0.5*std),target_margin-(2*std)),0.005) as lower_bound,\n",
    "greatest(current_margin + (3*std) , avg_margin + (4*std),target_margin+(3*std)) as upper_bound,\n",
    "case when min_margin between lower_bound and upper_bound then 1 else 0 end as min_status,\n",
    "case when new_min_margin between lower_bound and upper_bound then 1 else 0 end as new_min_status\n",
    "\n",
    "from (\n",
    "select distinct maxab.cohort_id,\n",
    "maxab.product_id,\n",
    "CONCAT(products.name_ar,' ',products.size,' ',product_units.name_ar) as sku,\n",
    "brands.name_ar as brand, \n",
    "categories.name_ar as cat,\n",
    "maxab.price as maxab_price,bs.ben_soliman_price,\n",
    "final_min_price,\n",
    "final_max_price,\n",
    "final_mod_price,\n",
    "min_scrapped,\n",
    "median_scrapped,\n",
    "max_scrapped,\n",
    "wac_p,\n",
    "NULLIF(\n",
    "    LEAST(\n",
    "        CASE WHEN final_min_price <> 0 AND final_min_price IS NOT NULL \n",
    "            THEN final_min_price ELSE 999999999 END,\n",
    "        CASE WHEN final_mod_price <> 0 AND final_mod_price IS NOT NULL \n",
    "            THEN final_mod_price ELSE 999999999 END,\n",
    "        CASE WHEN ben_soliman_price <> 0 AND ben_soliman_price IS NOT NULL \n",
    "            THEN ben_soliman_price ELSE 999999999 END,\n",
    "        CASE WHEN min_scrapped <> 0 AND min_scrapped IS NOT NULL \n",
    "            THEN min_scrapped ELSE 999999999 END\n",
    "    ), \n",
    "    999999999\n",
    ") AS all_mp_mins,\n",
    "\n",
    "(\n",
    "    COALESCE(ben_soliman_price, 0) * 0.4 * (CASE WHEN ben_soliman_price IS NOT NULL AND ben_soliman_price <> 0 THEN 1 ELSE 0 END) + \n",
    "    COALESCE(NULLIF(LEAST(\n",
    "        CASE WHEN final_min_price <> 0 AND final_min_price IS NOT NULL \n",
    "            THEN final_min_price ELSE 999999999 END,\n",
    "        CASE WHEN final_mod_price <> 0 AND final_mod_price IS NOT NULL \n",
    "            THEN final_mod_price ELSE 999999999 END\n",
    "    ), 999999999), 0) * 0.25 * (CASE WHEN (final_min_price IS NOT NULL AND final_min_price <> 0) \n",
    "                                          OR (final_mod_price IS NOT NULL AND final_mod_price <> 0) THEN 1 ELSE 0 END) + \n",
    "    COALESCE(min_scrapped, 0) * 0.35 * (CASE WHEN min_scrapped IS NOT NULL AND min_scrapped <> 0 THEN 1 ELSE 0 END)\n",
    ") / NULLIF(\n",
    "    (0.4 * (CASE WHEN ben_soliman_price IS NOT NULL AND ben_soliman_price <> 0 THEN 1 ELSE 0 END) +\n",
    "     0.25 * (CASE WHEN (final_min_price IS NOT NULL AND final_min_price <> 0) \n",
    "                   OR (final_mod_price IS NOT NULL AND final_mod_price <> 0) THEN 1 ELSE 0 END) +\n",
    "     0.35 * (CASE WHEN min_scrapped IS NOT NULL AND min_scrapped <> 0 THEN 1 ELSE 0 END)),\n",
    "    0\n",
    ") AS new_min,\n",
    "\n",
    "    (COALESCE((final_min_price + final_max_price) / 2, 0) + \n",
    "     COALESCE(ben_soliman_price, 0) + \n",
    "     COALESCE(final_mod_price, 0)+\n",
    "\t coalesce(median_scrapped,0)\n",
    "\t ) / \n",
    "    NULLIF(\n",
    "        (CASE WHEN (final_min_price + final_max_price) / 2 IS NOT NULL and final_min_price >0 and final_max_price >0 THEN 1 ELSE 0 END +\n",
    "         CASE WHEN ben_soliman_price IS NOT NULL and ben_soliman_price >0  THEN 1 ELSE 0 END +\n",
    "\t\t CASE WHEN median_scrapped IS NOT NULL and median_scrapped >0  THEN 1 ELSE 0 END +\n",
    "         CASE WHEN final_mod_price IS NOT NULL and final_mod_price > 0 THEN 1 ELSE 0 END), \n",
    "        0\n",
    "    ) AS average_price,\n",
    "\t(\n",
    "    COALESCE((final_min_price + final_max_price) / 2, 0) * 0.25 * \n",
    "        (CASE WHEN final_min_price > 0 AND final_max_price > 0 THEN 1 ELSE 0 END) + \n",
    "    COALESCE(ben_soliman_price, 0) * 0.4 * \n",
    "        (CASE WHEN ben_soliman_price IS NOT NULL AND ben_soliman_price > 0 THEN 1 ELSE 0 END) + \n",
    "    COALESCE(final_mod_price, 0) * 0.25 * \n",
    "        (CASE WHEN final_mod_price IS NOT NULL AND final_mod_price > 0 THEN 1 ELSE 0 END) +\n",
    "    COALESCE(median_scrapped, 0) * 0.1 * \n",
    "        (CASE WHEN median_scrapped IS NOT NULL AND median_scrapped > 0 THEN 1 ELSE 0 END)\n",
    ") / \n",
    "NULLIF(\n",
    "    (0.25 * (CASE WHEN final_min_price > 0 AND final_max_price > 0 THEN 1 ELSE 0 END) +\n",
    "     0.4 * (CASE WHEN ben_soliman_price IS NOT NULL AND ben_soliman_price > 0 THEN 1 ELSE 0 END) +\n",
    "     0.25 * (CASE WHEN final_mod_price IS NOT NULL AND final_mod_price > 0 THEN 1 ELSE 0 END) +\n",
    "     0.1 * (CASE WHEN median_scrapped IS NOT NULL AND median_scrapped > 0 THEN 1 ELSE 0 END)),\n",
    "    0\n",
    ") AS average_new,\n",
    "\t\n",
    "coalesce(nmv,0) as nmv,\n",
    "coalesce(mc.std,0.01) as std,\n",
    "coalesce(coalesce(cbt.target_bm , ct.cat_target_margin),0) as target_margin,\n",
    "coalesce(avg_margin,0) as avg_margin\n",
    "\n",
    "from maxab_prices maxab\n",
    "left join ben_soliman bs on bs.product_id = maxab.product_id\n",
    "left join final_mp fmp on fmp.product_id = maxab.product_id and fmp.region = maxab.region\n",
    "left join sales s on s.product_id = maxab.product_id and s.cohort_id = maxab.cohort_id\n",
    "left join scrapped_data  sd on sd.product_id = maxab.product_id and sd.region = maxab.region\n",
    "join finance.all_cogs f on f.product_id = maxab.product_id and CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMESTAMP()) between f.from_date and f.to_date\n",
    "JOIN products on products.id=maxab.product_id\n",
    "JOIN brands on products.brand_id = brands.id \n",
    "JOIN categories ON products.category_id = categories.id\n",
    "JOIN product_units ON product_units.id = products.unit_id \n",
    "left join margin_change mc on mc.product_id = maxab.product_id and mc.cohort_id = maxab.cohort_id\n",
    "left join cat_brand_target cbt on cbt.brand = brands.name_ar and cbt.cat = categories.name_ar \n",
    "left join cat_target ct on ct.cat = categories.name_ar \n",
    ")\n",
    "where all_mp_mins is not null \n",
    "and (min_status = 1 or new_min_status = 1)\n",
    ")\n",
    "order by NMV desc\n",
    "'''\n",
    "market_data   = query_snowflake(query, columns = ['cohort_id','product_id','sku','cat','brand','min_market_margin','new_min_margin','avg_market_margin','new_avg_margin','target_margin','current_margin'])\n",
    "market_data.columns = market_data.columns.str.lower()\n",
    "for col in market_data.columns:\n",
    "    market_data[col] = pd.to_numeric(market_data[col], errors='ignore')   \n",
    "market_data = market_data[['cohort_id','product_id','min_market_margin','new_min_margin','avg_market_margin','new_avg_margin','target_margin','current_margin']]   \n",
    "market_data = market_data[(market_data['min_market_margin'] > 0)&(market_data['avg_market_margin'] > 0) ]\n",
    "market_data=market_data.drop_duplicates(subset=['cohort_id','product_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1a60ce8-9362-4bbd-8d62-e969f2135d8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query ='''\n",
    "select region , product_id,new_pp,forecasted_date\n",
    "from materialized_views.DBDP_PRICE_UPS\n",
    "'''\n",
    "price_ups  = query_snowflake(query, columns = ['region','product_id','new_pp','forcasted_date'])\n",
    "price_ups.columns = price_ups.columns.str.lower()\n",
    "for col in price_ups.columns:\n",
    "    price_ups[col] = pd.to_numeric(price_ups[col], errors='ignore')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "986e7772-bc1f-4e0c-8165-8fcac88753ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT  DISTINCT\n",
    "\t\tcpc.cohort_id,  \n",
    "\t\tpso.product_id,\n",
    "\t\tCONCAT(products.name_ar,' ',products.size,' ',product_units.name_ar) as sku,\n",
    "\t\tbrands.name_ar as brand, \n",
    "\t\tcategories.name_ar as cat,\n",
    "        sum(pso.total_price) as nmv\n",
    "\n",
    "\n",
    "FROM product_sales_order pso\n",
    "JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "join COHORT_PRICING_CHANGES cpc on cpc.id = pso.COHORT_PRICING_CHANGE_id\n",
    "JOIN products on products.id=pso.product_id\n",
    "JOIN brands on products.brand_id = brands.id \n",
    "JOIN categories ON products.category_id = categories.id\n",
    "JOIN product_units ON product_units.id = products.unit_id \n",
    "          \n",
    "\n",
    "WHERE   True\n",
    "    AND so.created_at::date between  current_date - 60 and current_date -1 \n",
    "    AND so.sales_order_status_id not in (7,12)\n",
    "    AND so.channel IN ('telesales','retailer')\n",
    "    AND pso.purchased_item_count <> 0\n",
    "    and cpc.cohort_id in (700,701,702,703,704,1123,1124,1125,1126)\n",
    "\n",
    "GROUP BY ALL\n",
    "'''\n",
    "sales  = query_snowflake(query, columns = ['cohort_id','product_id','sku','brand','cat','nmv'])\n",
    "sales.columns = sales.columns.str.lower()\n",
    "for col in sales.columns:\n",
    "    sales[col] = pd.to_numeric(sales[col], errors='ignore')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f390dc2b-c772-4d2e-b7cc-bdcc2695a411",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def assign_tier(cumulative_contribution):\n",
    "    if cumulative_contribution <= 40:\n",
    "        return 1\n",
    "    elif cumulative_contribution <= 70:\n",
    "        return 2\n",
    "    elif cumulative_contribution <= 90:\n",
    "        return 3\n",
    "    else:\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aab29e4-c8fc-42d9-a816-616d55f867da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#sales['products'] =  1 \n",
    "sales['total_nmv'] = sales.groupby('cohort_id')['nmv'].transform(sum)\n",
    "#sales['total_products'] = sales.groupby('cohort_id')['products'].transform(sum)\n",
    "sales['cntrb_nmv'] = sales['nmv']/sales['total_nmv']\n",
    "#sales['cntrb_products'] = sales['products']/sales['total_products']\n",
    "sales = sales.sort_values(['cohort_id', 'nmv'], ascending=[True, False])\n",
    "sales['nmv_cumulative_cntrb'] = sales.groupby('cohort_id')['cntrb_nmv'].cumsum()\n",
    "#sales['products_cumulative_cntrb'] = sales.groupby('cohort_id')['cntrb_products'].cumsum()\n",
    "#sales['top_80_pct'] = sales['nmv_cumulative_cntrb'] <= 0.8\n",
    "sales['tier'] = sales['nmv_cumulative_cntrb'].apply(assign_tier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98a4fda-c93f-44f0-838d-5e5c2738fa0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "found = min_max_df.merge(market_data,on=['cohort_id','product_id'])\n",
    "found = found.merge(sales[['cohort_id','product_id','tier']],on=['cohort_id','product_id'])\n",
    "found['new_min'] = found['min_market_margin']\n",
    "found['min_change'] = (found['new_min_margin']-found['min_market_margin'])/found['min_market_margin']\n",
    "\n",
    "found.loc[(found['current_margin']>=found['min_market_margin'])&(found['tier'] == 1),'new_min'] = found['min_market_margin']\n",
    "found.loc[(found['current_margin']>=found['min_market_margin'])&(found['tier'] == 2),'new_min'] = found['new_min_margin']\n",
    "found.loc[(found['current_margin']>=found['min_market_margin'])&(found['tier'] == 3),'new_min'] = found['avg_market_margin']\n",
    "found.loc[(found['current_margin']>=found['min_market_margin'])&(found['tier'] == 4),'new_min'] = found['new_avg_margin']\n",
    "\n",
    "found.loc[(found['new_min'] == found['min_market_margin'])&(found['min_change']>=0.5),'new_min']= found.loc[(found['new_min'] == found['min_market_margin'])&(found['min_change'])>=0.5,'new_min_margin']\n",
    "found.loc[found['current_margin']<0,'new_min'] = found['current_margin']\n",
    "\n",
    "found.loc[found['new_min'] < found['min_market_margin'],'new_min'] = found['min_market_margin']\n",
    "\n",
    "found['diff'] = (found['max_margin'] - found['min_margin'])/found['min_margin']\n",
    "\n",
    "found['new_max']= found['new_min'] + np.minimum(np.maximum((found['diff']*found['new_min']),0.01),0.04)\n",
    "\n",
    "found.loc[found['max_margin'].isna(),'new_max'] = np.nan\n",
    "\n",
    "found=found[['cohort_id','product_id','new_min','new_max']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0552f8b0-e9db-47fa-9e35-46e039dd0d96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "market_data[market_data['product_id']==201]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead04dca-cb4f-46bc-825d-372eee1b1d65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_max_df['flag']=1\n",
    "not_found = market_data.merge(min_max_df[['cohort_id','product_id','flag']],on=['cohort_id','product_id'],how='left')\n",
    "not_found = not_found.merge(sales[['cohort_id','product_id','tier']],on=['cohort_id','product_id'])\n",
    "not_found = not_found[not_found['flag'].isna()]\n",
    "\n",
    "not_found['new_min'] = not_found['min_market_margin']\n",
    "not_found['min_change'] = (not_found['new_min_margin']-not_found['min_market_margin'])/not_found['min_market_margin']\n",
    "\n",
    "not_found.loc[(not_found['current_margin']>=not_found['min_market_margin'])&(not_found['tier'] == 1),'new_min'] = not_found['min_market_margin']\n",
    "not_found.loc[(not_found['current_margin']>=not_found['min_market_margin'])&(not_found['tier'] == 2),'new_min'] = not_found['new_min_margin']\n",
    "not_found.loc[(not_found['current_margin']>=not_found['min_market_margin'])&(not_found['tier'] == 3),'new_min'] = not_found['avg_market_margin']\n",
    "not_found.loc[(not_found['current_margin']>=not_found['min_market_margin'])&(not_found['tier'] == 4),'new_min'] = not_found['new_avg_margin']\n",
    "\n",
    "not_found.loc[(not_found['new_min'] == not_found['min_market_margin'])&(not_found['min_change'])>=0.5,'new_min']=not_found['new_min_margin']\n",
    "\n",
    "not_found['diff'] = np.minimum(np.maximum(0.3*not_found['target_margin'],0.01),0.04)\n",
    "not_found['new_max'] = not_found['new_min']+not_found['diff'] \n",
    "not_found=not_found[['cohort_id','product_id','new_min','new_max']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a07a63-cf1f-490e-b6ce-004595e2973c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "market_data['flag_2'] = 1 \n",
    "main_found = min_max_df.merge(market_data,on=['cohort_id','product_id'],how='left')\n",
    "main_found=main_found[main_found['flag_2'].isna()]\n",
    "main_found = main_found[['cohort_id','product_id','min_margin','max_margin','enforce']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283b3ac9-7f3c-48d0-b4d2-d52c2189de6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_df = pd.concat([found,not_found],axis=0)\n",
    "final_df=final_df.drop_duplicates()\n",
    "regions = pd.DataFrame({\n",
    "    'region': ['Cairo', 'Giza', 'Delta West', 'Delta East', 'Upper Egypt', \n",
    "               'Upper Egypt', 'Upper Egypt', 'Upper Egypt', 'Alexandria'],\n",
    "    'cohort_id': [700, 701, 703, 704, 1124, 1126, 1123, 1125, 702]\n",
    "})\n",
    "final_df=final_df.merge(regions,on=['cohort_id'])\n",
    "final_df=final_df[['cohort_id','product_id','new_min','new_max']]\n",
    "final_df=final_df.drop_duplicates()\n",
    "final_df.columns = ['cohort_id','product_id','min_margin','max_margin']\n",
    "final_df = pd.concat([final_df,main_found[['cohort_id','product_id','min_margin','max_margin']]],axis=0)\n",
    "price_ups=price_ups.merge(regions,on=['region'])\n",
    "final_df=final_df.merge(price_ups,on=['product_id','cohort_id'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e083e57-7b2e-4003-b3c0-519f4d2662e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_df.loc[~final_df['new_pp'].isna(),'max_margin'] = np.minimum(final_df['max_margin']+0.15,final_df['min_margin']+0.2)\n",
    "final_df['enforce'] = np.nan\n",
    "final_df.loc[~final_df['new_pp'].isna(),'enforce']= 1  \n",
    "final_df=final_df.drop_duplicates()\n",
    "final_df = final_df.merge(sales,on=['cohort_id','product_id'])\n",
    "final_df = final_df[['cohort_id', 'product_id','sku', 'min_margin', 'max_margin', 'enforce','brand']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349d409e-d747-4f40-8dae-653ba21c4e5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_df.to_excel('min_max_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6ec6bb-b153-4c51-8f40-a5fa68d620dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_df[final_df['enforce']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71de92b0-2384-4b65-8fcd-af42a6f67ea1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c94bf1-7de2-478c-964b-6b1360968d67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
