{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca6b4ad-4811-4760-a562-56ed274f1b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68f8dcc3-7a17-4ad7-854a-45028c7d2e9b",
   "metadata": {},
   "source": [
    "## Market Prices Extraction Queries\n",
    "Queries for external market price data:\n",
    "1. **Ben Soliman Prices** - Competitor reference prices\n",
    "2. **Marketplace Prices** - Min, Max, Mod prices from marketplace\n",
    "3. **Scrapped Data** - Competitor prices from scraping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fea7036f-45f7-4b1e-b936-d7f6fd6f8077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. BEN SOLIMAN PRICES QUERY\n",
    "# =============================================================================\n",
    "BEN_SOLIMAN_QUERY = f'''\n",
    "WITH lower as (\n",
    "    select distinct product_id, sku, new_d*bs_price as ben_soliman_price, INJECTION_DATE\n",
    "    from (\n",
    "        select maxab_product_id as product_id, maxab_sku as sku, INJECTION_DATE, wac1, wac_p,\n",
    "            (bs_price/bs_unit_count) as bs_price, diff, cu_price,\n",
    "            case when p1 > 1 then child_quantity else 0 end as scheck,\n",
    "            round(p1/2)*2 as p1, p2,\n",
    "            case when (ROUND(p1 / scheck) * scheck) = 0 then p1 else (ROUND(p1 / scheck) * scheck) end as new_d\n",
    "        from (\n",
    "            select sm.*, wac1, wac_p, \n",
    "                abs((bs_price/bs_unit_count)-(wac_p*maxab_basic_unit_count))/(wac_p*maxab_basic_unit_count) as diff,\n",
    "                cpc.price as cu_price, pup.child_quantity,\n",
    "                round((cu_price/(bs_price/bs_unit_count))) as p1, \n",
    "                round(((bs_price/bs_unit_count)/cu_price)) as p2\n",
    "            from materialized_views.savvy_mapping sm \n",
    "            join finance.all_cogs f on f.product_id = sm.maxab_product_id \n",
    "                and CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP()) between f.from_Date and f.to_date\n",
    "            join PACKING_UNIT_PRODUCTS pu on pu.product_id = sm.maxab_product_id and pu.IS_BASIC_UNIT = 1 \n",
    "            join cohort_product_packing_units cpc on cpc.PRODUCT_PACKING_UNIT_ID = pu.id and cohort_id = 700 \n",
    "            join packing_unit_products pup on pup.product_id = sm.maxab_product_id and pup.is_basic_unit = 1  \n",
    "            where bs_price is not null \n",
    "                and INJECTION_DATE::date >= CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - 5 \n",
    "                and diff > 0.3 and p1 > 1\n",
    "        )\n",
    "    )\n",
    "    qualify max(INJECTION_DATE) over(partition by product_id) = INJECTION_DATE\n",
    "),\n",
    "\n",
    "m_bs as (\n",
    "    select z.* from (\n",
    "        select maxab_product_id as product_id, maxab_sku as sku, avg(bs_final_price) as ben_soliman_price, INJECTION_DATE\n",
    "        from (\n",
    "            select *, row_number() over(partition by maxab_product_id order by diff) as rnk_2 \n",
    "            from (\n",
    "                select *, (bs_final_price-wac_p)/wac_p as diff_2 \n",
    "                from (\n",
    "                    select *, bs_price/maxab_basic_unit_count as bs_final_price \n",
    "                    from (\n",
    "                        select *, row_number() over(partition by maxab_product_id, maxab_pu order by diff) as rnk \n",
    "                        from (\n",
    "                            select *, max(INJECTION_DATE::date) over(partition by maxab_product_id, maxab_pu) as max_date\n",
    "                            from (\n",
    "                                select sm.*, wac1, wac_p, \n",
    "                                    abs(bs_price-(wac_p*maxab_basic_unit_count))/(wac_p*maxab_basic_unit_count) as diff \n",
    "                                from materialized_views.savvy_mapping sm \n",
    "                                join finance.all_cogs f on f.product_id = sm.maxab_product_id \n",
    "                                    and CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP()) between f.from_Date and f.to_date\n",
    "                                where bs_price is not null \n",
    "                                    and INJECTION_DATE::date >= CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - 5 \n",
    "                                    and diff < 0.3\n",
    "                            )\n",
    "                            qualify max_date = INJECTION_DATE\n",
    "                        ) qualify rnk = 1 \n",
    "                    )\n",
    "                ) where diff_2 between -0.5 and 0.5 \n",
    "            ) qualify rnk_2 = 1 \n",
    "        ) group by all\n",
    "    ) z \n",
    "    join finance.all_cogs f on f.product_id = z.product_id \n",
    "        and CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP()) between f.from_Date and f.to_date\n",
    "    where ben_soliman_price between f.wac_p*0.8 and f.wac_p*1.3\n",
    ")\n",
    "\n",
    "select product_id, avg(ben_soliman_price) as ben_soliman_price\n",
    "from (\n",
    "    select * from (\n",
    "        select * from m_bs \n",
    "        union all\n",
    "        select * from lower\n",
    "    )\n",
    "    qualify max(INJECTION_DATE) over(partition by product_id) = INJECTION_DATE\n",
    ")\n",
    "group by all\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12c971ec-5f0f-42b4-9ce8-c065fbabe31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2. MARKETPLACE PRICES QUERY (with region fallback)\n",
    "# =============================================================================\n",
    "MARKETPLACE_PRICES_QUERY = f'''\n",
    "WITH MP as (\n",
    "    select region, product_id,\n",
    "        min(min_price) as min_price, min(max_price) as max_price,\n",
    "        min(mod_price) as mod_price, min(true_min) as true_min, min(true_max) as true_max\n",
    "    from (\n",
    "        select mp.region, mp.product_id, mp.pu_id,\n",
    "            min_price/BASIC_UNIT_COUNT as min_price,\n",
    "            max_price/BASIC_UNIT_COUNT as max_price,\n",
    "            mod_price/BASIC_UNIT_COUNT as mod_price,\n",
    "            TRUE_MIN_PRICE/BASIC_UNIT_COUNT as true_min,\n",
    "            TRUE_MAX_PRICE/BASIC_UNIT_COUNT as true_max\n",
    "        from materialized_views.marketplace_prices mp \n",
    "        join packing_unit_products pup on pup.product_id = mp.product_id and pup.packing_unit_id = mp.pu_id\n",
    "        join finance.all_cogs f on f.product_id = mp.product_id \n",
    "            and CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP()) between f.from_date and f.to_date\n",
    "        where least(min_price, mod_price) between wac_p*0.9 and wac_p*1.3 \n",
    "    )\n",
    "    group by all \n",
    "),\n",
    "\n",
    "region_mapping AS (\n",
    "    SELECT * FROM (VALUES\n",
    "        ('Delta East', 'Delta West'), ('Delta West', 'Delta East'),\n",
    "        ('Alexandria', 'Cairo'), ('Alexandria', 'Giza'),\n",
    "        ('Upper Egypt', 'Cairo'), ('Upper Egypt', 'Giza'),\n",
    "        ('Cairo', 'Giza'), ('Giza', 'Cairo'),\n",
    "        ('Delta West', 'Cairo'), ('Delta East', 'Cairo'),\n",
    "        ('Delta West', 'Giza'), ('Delta East', 'Giza')\n",
    "    ) AS region_mapping(region, fallback_region)\n",
    "),\n",
    "\n",
    "all_regions as (\n",
    "    SELECT * FROM (VALUES\n",
    "        ('Cairo'), ('Giza'), ('Delta West'), ('Delta East'), ('Upper Egypt'), ('Alexandria')\n",
    "    ) AS x(region)\n",
    "),\n",
    "\n",
    "full_data as (\n",
    "    select products.id as product_id, ar.region\n",
    "    from products, all_regions ar\n",
    "    where activation = 'true'\n",
    ")\n",
    "\n",
    "select region, product_id,\n",
    "    min(final_min_price) as final_min_price, \n",
    "    min(final_max_price) as final_max_price,\n",
    "    min(final_mod_price) as final_mod_price, \n",
    "    min(final_true_min) as final_true_min,\n",
    "    min(final_true_max) as final_true_max\n",
    "from (\n",
    "    SELECT distinct w.region, w.product_id,\n",
    "        COALESCE(m1.min_price, m2.min_price) AS final_min_price,\n",
    "        COALESCE(m1.max_price, m2.max_price) AS final_max_price,\n",
    "        COALESCE(m1.mod_price, m2.mod_price) AS final_mod_price,\n",
    "        COALESCE(m1.true_min, m2.true_min) AS final_true_min,\n",
    "        COALESCE(m1.true_max, m2.true_max) AS final_true_max\n",
    "    FROM full_data w\n",
    "    LEFT JOIN MP m1 ON w.region = m1.region and w.product_id = m1.product_id\n",
    "    LEFT JOIN region_mapping rm ON w.region = rm.region\n",
    "    LEFT JOIN MP m2 ON rm.fallback_region = m2.region AND w.product_id = m2.product_id\n",
    ")\n",
    "where final_min_price is not null \n",
    "group by all\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a8826aa-e36b-4f51-9d12-10abf3439fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3. SCRAPPED DATA QUERY (Competitor prices from scraping)\n",
    "# =============================================================================\n",
    "SCRAPPED_DATA_QUERY = f'''\n",
    "select product_id, region,\n",
    "    MIN(market_price) AS min_scrapped,\n",
    "    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY market_price) AS scrapped25,\n",
    "    PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY market_price) AS scrapped50,\n",
    "    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY market_price) AS scrapped75,\n",
    "    MAX(market_price) AS max_scrapped\n",
    "from (\n",
    "    select distinct cmp.*, max(date) over(partition by region, cmp.product_id, competitor) as max_date\n",
    "    from MATERIALIZED_VIEWS.CLEANED_MARKET_PRICES cmp\n",
    "    join finance.all_cogs f on f.product_id = cmp.product_id \n",
    "        and CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP()) between f.from_date and f.to_date \n",
    "    where date >= CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - 7 \n",
    "        and MARKET_PRICE between f.wac_p * 0.8 and wac_p * 1.3\n",
    "    qualify date = max_date \n",
    ")\n",
    "group by all\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e451d83-655b-4896-8059-3b217cae0c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Additional Data Queries (Sales, Groups, WAC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0872ecf0-4f26-4d66-8b9c-5f27be5bb580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4. PRODUCT BASE DATA QUERY (product_id, sku, brand, cat, wac1, wac_p, current_price)\n",
    "# =============================================================================\n",
    "PRODUCT_BASE_QUERY = f'''\n",
    "WITH skus_prices AS (\n",
    "    WITH local_prices AS (\n",
    "        SELECT  \n",
    "            CASE \n",
    "                WHEN cpu.cohort_id IN (700, 695) THEN 'Cairo'\n",
    "                WHEN cpu.cohort_id IN (701) THEN 'Giza'\n",
    "                WHEN cpu.cohort_id IN (704, 698) THEN 'Delta East'\n",
    "                WHEN cpu.cohort_id IN (703, 697) THEN 'Delta West'\n",
    "                WHEN cpu.cohort_id IN (696, 1123, 1124, 1125, 1126) THEN 'Upper Egypt'\n",
    "                WHEN cpu.cohort_id IN (702, 699) THEN 'Alexandria'\n",
    "            END AS region,\n",
    "            cohort_id,\n",
    "            pu.product_id,\n",
    "            pu.packing_unit_id,\n",
    "            pu.basic_unit_count,\n",
    "            AVG(cpu.price) AS price\n",
    "        FROM cohort_product_packing_units cpu\n",
    "        JOIN PACKING_UNIT_PRODUCTS pu ON pu.id = cpu.product_packing_unit_id\n",
    "        WHERE cpu.cohort_id IN (700,701,702,703,704,695,696,697,698,699,1123,1124,1125,1126)\n",
    "            AND cpu.created_at::date <> '2023-07-31'\n",
    "            AND cpu.is_customized = TRUE\n",
    "        GROUP BY ALL\n",
    "    ),\n",
    "    \n",
    "    live_prices AS (\n",
    "        SELECT \n",
    "            region, cohort_id, product_id, \n",
    "            pu_id AS packing_unit_id, \n",
    "            buc AS basic_unit_count, \n",
    "            NEW_PRICE AS price\n",
    "        FROM materialized_views.DBDP_PRICES\n",
    "        WHERE created_at = CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date\n",
    "            AND DATE_PART('hour', CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::time) \n",
    "                BETWEEN SPLIT_PART(time_slot, '-', 1)::int AND (SPLIT_PART(time_slot, '-', 1)::int) + 1\n",
    "            AND cohort_id IN (700,701,702,703,704,695,696,697,698,699,1123,1124,1125,1126)\n",
    "    ),\n",
    "    \n",
    "    prices AS (\n",
    "        SELECT *\n",
    "        FROM (\n",
    "            SELECT *, 1 AS priority FROM live_prices\n",
    "            UNION ALL\n",
    "            SELECT *, 2 AS priority FROM local_prices\n",
    "        )\n",
    "        QUALIFY ROW_NUMBER() OVER (PARTITION BY region, cohort_id, product_id, packing_unit_id ORDER BY priority) = 1\n",
    "    )\n",
    "    \n",
    "    SELECT region, cohort_id, product_id, price\n",
    "    FROM prices\n",
    "    WHERE basic_unit_count = 1\n",
    "        AND ((product_id = 1309 AND packing_unit_id = 2) OR (product_id <> 1309))\n",
    ")\n",
    "\n",
    "SELECT distinct\n",
    "    region, cohort_id, p.product_id,\n",
    "    CONCAT(products.name_ar, ' ', products.size, ' ', product_units.name_ar) AS sku,\n",
    "    b.name_ar AS brand,\n",
    "    cat.name_ar AS cat,\n",
    "    wac1, wac_p, p.price as current_price\n",
    "FROM skus_prices p\n",
    "JOIN finance.all_cogs c ON c.product_id = p.product_id \n",
    "    AND CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP()) BETWEEN c.from_date AND c.to_date\n",
    "JOIN products ON products.id = p.product_id\n",
    "JOIN categories cat ON cat.id = products.category_id\n",
    "JOIN brands b ON b.id = products.brand_id\n",
    "JOIN product_units ON product_units.id = products.unit_id\n",
    "WHERE wac1 > 0 AND wac_p > 0\n",
    "GROUP BY ALL\n",
    "'''\n",
    "\n",
    "# =============================================================================\n",
    "# 5. SALES DATA QUERY (120-day NMV by cohort/product)\n",
    "# =============================================================================\n",
    "SALES_QUERY = f'''\n",
    "SELECT DISTINCT cpc.cohort_id, pso.product_id,\n",
    "    CONCAT(products.name_ar,' ',products.size,' ',product_units.name_ar) as sku,\n",
    "    brands.name_ar as brand, categories.name_ar as cat,\n",
    "    sum(pso.total_price) as nmv\n",
    "FROM product_sales_order pso\n",
    "JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "JOIN COHORT_PRICING_CHANGES cpc ON cpc.id = pso.COHORT_PRICING_CHANGE_id\n",
    "JOIN products ON products.id = pso.product_id\n",
    "JOIN brands ON products.brand_id = brands.id \n",
    "JOIN categories ON products.category_id = categories.id\n",
    "JOIN product_units ON product_units.id = products.unit_id \n",
    "WHERE so.created_at::date BETWEEN CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - 120 \n",
    "    AND CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - 1 \n",
    "    AND so.sales_order_status_id NOT IN (7, 12)\n",
    "    AND so.channel IN ('telesales', 'retailer')\n",
    "    AND pso.purchased_item_count <> 0\n",
    "    AND cpc.cohort_id IN (700,701,702,703,704,1123,1124,1125,1126)\n",
    "GROUP BY ALL\n",
    "'''\n",
    "\n",
    "# =============================================================================\n",
    "# 6. MARGIN STATS QUERY (STD and average margins)  \n",
    "# =============================================================================\n",
    "MARGIN_STATS_QUERY = f'''\n",
    "select product_id, cohort_id, \n",
    "    (0.6*product_std) + (0.3*brand_std) + (0.1*cat_std) as std, \n",
    "    avg_margin\n",
    "from (\n",
    "    select product_id, cohort_id, \n",
    "        stddev(product_margin) as product_std, \n",
    "        stddev(brand_margin) as brand_std,\n",
    "        stddev(cat_margin) as cat_std, \n",
    "        avg(product_margin) as avg_margin\n",
    "    from (\n",
    "        select distinct product_id, order_date, cohort_id,\n",
    "            (nmv-cogs_p)/nmv as product_margin, \n",
    "            (brand_nmv-brand_cogs)/brand_nmv as brand_margin,\n",
    "            (cat_nmv-cat_cogs)/cat_nmv as cat_margin\n",
    "        from (\n",
    "            SELECT DISTINCT so.created_at::date as order_date, cpc.cohort_id, pso.product_id,\n",
    "                brands.name_ar as brand, categories.name_ar as cat,\n",
    "                sum(COALESCE(f.wac_p,0) * pso.purchased_item_count * pso.basic_unit_count) as cogs_p,\n",
    "                sum(pso.total_price) as nmv,\n",
    "                sum(nmv) over(partition by order_date, cat, brand) as brand_nmv,\n",
    "                sum(cogs_p) over(partition by order_date, cat, brand) as brand_cogs,\n",
    "                sum(nmv) over(partition by order_date, cat) as cat_nmv,\n",
    "                sum(cogs_p) over(partition by order_date, cat) as cat_cogs\n",
    "            FROM product_sales_order pso\n",
    "            JOIN sales_orders so ON so.id = pso.sales_order_id   \n",
    "            JOIN COHORT_PRICING_CHANGES cpc on cpc.id = pso.cohort_pricing_change_id\n",
    "            JOIN products on products.id = pso.product_id\n",
    "            JOIN brands on products.brand_id = brands.id \n",
    "            JOIN categories ON products.category_id = categories.id\n",
    "            JOIN finance.all_cogs f ON f.product_id = pso.product_id\n",
    "                AND f.from_date::date <= so.created_at::date AND f.to_date::date > so.created_at::date\n",
    "            WHERE so.created_at::date between \n",
    "                date_trunc('month', CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - 120) \n",
    "                and CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date\n",
    "                AND so.sales_order_status_id not in (7,12)\n",
    "                AND so.channel IN ('telesales','retailer')\n",
    "                AND pso.purchased_item_count <> 0\n",
    "            GROUP BY ALL\n",
    "        )\n",
    "    ) group by all \n",
    ")\n",
    "'''\n",
    "\n",
    "# =============================================================================\n",
    "# 7. TARGET MARGINS QUERY\n",
    "# =============================================================================\n",
    "TARGET_MARGINS_QUERY = f'''\n",
    "WITH cat_brand_target as (\n",
    "    SELECT DISTINCT cat, brand, margin as target_bm\n",
    "    FROM performance.commercial_targets cplan\n",
    "    QUALIFY CASE \n",
    "        WHEN DATE_TRUNC('month', MAX(DATE) OVER()) = DATE_TRUNC('month', CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date) \n",
    "        THEN DATE_TRUNC('month', CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date)\n",
    "        ELSE DATE_TRUNC('month', CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - INTERVAL '1 month') \n",
    "    END = DATE_TRUNC('month', date)\n",
    "),\n",
    "cat_target as (\n",
    "    select cat, sum(target_bm * (target_nmv/cat_total)) as cat_target_margin\n",
    "    from (\n",
    "        select *, sum(target_nmv) over(partition by cat) as cat_total\n",
    "        from (\n",
    "            select cat, brand, avg(target_bm) as target_bm, sum(target_nmv) as target_nmv\n",
    "            from (\n",
    "                SELECT DISTINCT date, city as region, cat, brand, margin as target_bm, nmv as target_nmv\n",
    "                FROM performance.commercial_targets cplan\n",
    "                QUALIFY CASE \n",
    "                    WHEN DATE_TRUNC('month', MAX(DATE) OVER()) = DATE_TRUNC('month', CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date) \n",
    "                    THEN DATE_TRUNC('month', CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date)\n",
    "                    ELSE DATE_TRUNC('month', CONVERT_TIMEZONE('{TIMEZONE}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - INTERVAL '1 month') \n",
    "                END = DATE_TRUNC('month', date)\n",
    "            ) group by all\n",
    "        )\n",
    "    ) group by all \n",
    ")\n",
    "SELECT DISTINCT cbt.cat, cbt.brand, cbt.target_bm, ct.cat_target_margin\n",
    "FROM cat_brand_target cbt\n",
    "LEFT JOIN cat_target ct ON ct.cat = cbt.cat\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86f4dbb1-c781-411d-985b-649e787a015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Execute All Queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82621082-0224-4e15-980d-c25aad73cf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from Snowflake...\n",
      "  1. Loading Ben Soliman prices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28989/3044313561.py:18: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Loaded 1600 Ben Soliman price records\n",
      "  2. Loading marketplace prices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28989/3044313561.py:18: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Loaded 11404 marketplace price records\n",
      "  3. Loading scrapped data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28989/3044313561.py:18: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Loaded 5189 scrapped price records\n",
      "  4. Loading product base data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28989/3044313561.py:18: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Loaded 101742 product base records\n",
      "  5. Loading sales data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28989/3044313561.py:18: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Loaded 20617 sales records\n",
      "  6. Loading margin stats...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28989/3044313561.py:18: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Loaded 28648 margin stat records\n",
      "  7. Loading target margins...\n",
      "     Loaded 478 target margin records\n",
      "  8. Loading product groups...\n",
      "     Loaded 1576 group records\n",
      "\n",
      "All queries completed!\n",
      "\n",
      "============================================================\n",
      "df_product_base DataFrame available with columns:\n",
      "  - region, cohort_id, product_id, sku, brand, cat, wac1, wac_p, current_price\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28989/3044313561.py:18: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')\n",
      "/tmp/ipykernel_28989/3044313561.py:18: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Execute all queries\n",
    "# =============================================================================\n",
    "print(\"Loading data from Snowflake...\")\n",
    "\n",
    "# 1. Ben Soliman Prices\n",
    "print(\"  1. Loading Ben Soliman prices...\")\n",
    "df_ben_soliman = query_snowflake(BEN_SOLIMAN_QUERY)\n",
    "df_ben_soliman = convert_to_numeric(df_ben_soliman)\n",
    "print(f\"     Loaded {len(df_ben_soliman)} Ben Soliman price records\")\n",
    "\n",
    "# 2. Marketplace Prices\n",
    "print(\"  2. Loading marketplace prices...\")\n",
    "df_marketplace = query_snowflake(MARKETPLACE_PRICES_QUERY)\n",
    "df_marketplace = convert_to_numeric(df_marketplace)\n",
    "print(f\"     Loaded {len(df_marketplace)} marketplace price records\")\n",
    "\n",
    "# 3. Scrapped Data\n",
    "print(\"  3. Loading scrapped data...\")\n",
    "df_scrapped = query_snowflake(SCRAPPED_DATA_QUERY)\n",
    "df_scrapped = convert_to_numeric(df_scrapped)\n",
    "print(f\"     Loaded {len(df_scrapped)} scrapped price records\")\n",
    "\n",
    "# 4. Product Base Data (product_id, sku, brand, cat, wac1, wac_p, current_price)\n",
    "print(\"  4. Loading product base data...\")\n",
    "df_product_base = query_snowflake(PRODUCT_BASE_QUERY)\n",
    "df_product_base = convert_to_numeric(df_product_base)\n",
    "print(f\"     Loaded {len(df_product_base)} product base records\")\n",
    "\n",
    "# 5. Sales Data\n",
    "print(\"  5. Loading sales data...\")\n",
    "df_sales = query_snowflake(SALES_QUERY)\n",
    "df_sales = convert_to_numeric(df_sales)\n",
    "print(f\"     Loaded {len(df_sales)} sales records\")\n",
    "\n",
    "# 6. Margin Stats\n",
    "print(\"  6. Loading margin stats...\")\n",
    "df_margin_stats = query_snowflake(MARGIN_STATS_QUERY)\n",
    "df_margin_stats = convert_to_numeric(df_margin_stats)\n",
    "print(f\"     Loaded {len(df_margin_stats)} margin stat records\")\n",
    "\n",
    "# 7. Target Margins\n",
    "print(\"  7. Loading target margins...\")\n",
    "df_targets = query_snowflake(TARGET_MARGINS_QUERY)\n",
    "df_targets = convert_to_numeric(df_targets)\n",
    "print(f\"     Loaded {len(df_targets)} target margin records\")\n",
    "\n",
    "# 8. Product Groups (from PostgreSQL)\n",
    "print(\"  8. Loading product groups...\")\n",
    "df_groups = setup_environment_2.dwh_pg_query(\n",
    "    \"SELECT * FROM materialized_views.sku_commercial_groups\", \n",
    "    columns=['product_id', 'group']\n",
    ")\n",
    "df_groups.columns = df_groups.columns.str.lower()\n",
    "df_groups = convert_to_numeric(df_groups)\n",
    "print(f\"     Loaded {len(df_groups)} group records\")\n",
    "\n",
    "print(\"\\nAll queries completed!\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"df_product_base DataFrame available with columns:\")\n",
    "print(\"  - region, cohort_id, product_id, sku, brand, cat, wac1, wac_p, current_price\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4b0717e7-f64e-4a4f-98cd-d73250f68542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building market_data DataFrame (market prices only)...\n",
      "  Step 1: Joining all market price sources (outer join)...\n",
      "     Market prices base: 16306 records\n",
      "  Step 2: Adding cohort IDs and supporting data for processing...\n",
      "\n",
      "============================================================\n",
      "MARKET DATA BASE READY FOR PROCESSING\n",
      "============================================================\n",
      "Total records: 24076\n",
      "  - With marketplace prices: 16747\n",
      "  - With scrapped prices: 7694\n",
      "  - With Ben Soliman prices: 14400\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PART A: Build market_data DataFrame - Process market prices SEPARATELY\n",
    "# =============================================================================\n",
    "print(\"Building market_data DataFrame (market prices only)...\")\n",
    "\n",
    "# Create region-cohort mapping\n",
    "REGION_COHORT_DF = pd.DataFrame({\n",
    "    'region': ['Cairo', 'Giza', 'Delta West', 'Delta East', \n",
    "               'Upper Egypt', 'Upper Egypt', 'Upper Egypt', 'Upper Egypt', 'Alexandria'],\n",
    "    'cohort_id': [700, 701, 703, 704, 1124, 1126, 1123, 1125, 702]\n",
    "})\n",
    "\n",
    "# =============================================================================\n",
    "# Step 1: Outer join all market price sources\n",
    "# =============================================================================\n",
    "print(\"  Step 1: Joining all market price sources (outer join)...\")\n",
    "\n",
    "# Start with marketplace prices (has region + product_id)\n",
    "market_data = df_marketplace.copy()\n",
    "\n",
    "# Outer join with scrapped data (by region + product_id)\n",
    "market_data = market_data.merge(df_scrapped, on=['region', 'product_id'], how='outer')\n",
    "\n",
    "# Outer join with Ben Soliman prices (by product_id only - expand to all regions)\n",
    "all_regions = pd.DataFrame({'region': ['Cairo', 'Giza', 'Delta West', 'Delta East', 'Upper Egypt', 'Alexandria']})\n",
    "df_ben_soliman_expanded = df_ben_soliman.merge(all_regions, how='cross')\n",
    "\n",
    "# Outer join with Ben Soliman\n",
    "market_data = market_data.merge(df_ben_soliman_expanded, on=['region', 'product_id'], how='outer')\n",
    "\n",
    "print(f\"     Market prices base: {len(market_data)} records\")\n",
    "\n",
    "# =============================================================================\n",
    "# Step 2: Add cohort_id and supporting data for market processing\n",
    "# =============================================================================\n",
    "print(\"  Step 2: Adding cohort IDs and supporting data for processing...\")\n",
    "market_data = market_data.merge(REGION_COHORT_DF, on='region')\n",
    "\n",
    "# Need sales data for group processing (weighted median)\n",
    "market_data = market_data.merge(\n",
    "    df_sales[['cohort_id', 'product_id', 'nmv']], \n",
    "    on=['cohort_id', 'product_id'], \n",
    "    how='left'\n",
    ")\n",
    "market_data['nmv'] = market_data['nmv'].fillna(0)\n",
    "\n",
    "# Need margin stats for price analysis\n",
    "market_data = market_data.merge(df_margin_stats, on=['cohort_id', 'product_id'], how='left')\n",
    "\n",
    "# Need WAC for price analysis - get from product base\n",
    "market_data = market_data.merge(\n",
    "    df_product_base[['cohort_id', 'product_id', 'wac_p', 'brand', 'cat']].drop_duplicates(), \n",
    "    on=['cohort_id', 'product_id'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Need target margins for price analysis\n",
    "market_data = market_data.merge(df_targets, on=['brand', 'cat'], how='left')\n",
    "market_data['target_margin'] = market_data['target_bm'].fillna(market_data['cat_target_margin']).fillna(0)\n",
    "market_data = market_data.drop(columns=['target_bm', 'cat_target_margin'], errors='ignore')\n",
    "\n",
    "# Fill NaN values with defaults\n",
    "market_data['std'] = market_data['std'].fillna(0.01)\n",
    "market_data['avg_margin'] = market_data['avg_margin'].fillna(0)\n",
    "\n",
    "# Merge product groups for group processing\n",
    "market_data = market_data.merge(df_groups, on='product_id', how='left')\n",
    "\n",
    "# Remove duplicates\n",
    "market_data = market_data.drop_duplicates(subset=['cohort_id', 'product_id'])\n",
    "\n",
    "# Filter out records without WAC (can't process prices without cost)\n",
    "market_data = market_data[~market_data['wac_p'].isna()]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"MARKET DATA BASE READY FOR PROCESSING\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total records: {len(market_data)}\")\n",
    "print(f\"  - With marketplace prices: {len(market_data[~market_data['final_min_price'].isna()])}\")\n",
    "print(f\"  - With scrapped prices: {len(market_data[~market_data['min_scrapped'].isna()])}\")\n",
    "print(f\"  - With Ben Soliman prices: {len(market_data[~market_data['ben_soliman_price'].isna()])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f9ab63-4186-4a16-9369-296ac5d8091d",
   "metadata": {},
   "source": [
    "## PART A: Market Data Processing\n",
    "Process market prices separately (group fill, coverage filter, price analysis, margin tiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "75a9e52c-094b-45c8-ac9a-08c5834d6ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market data after group processing: 24076 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28989/2745197058.py:32: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: pd.Series({\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Group Processing - Calculate group-level aggregated prices (on market_data)\n",
    "# =============================================================================\n",
    "\n",
    "# Calculate group-level aggregated prices for products with group assignments\n",
    "groups_data = market_data[~market_data['group'].isna()].copy()\n",
    "groups_data['group_nmv'] = groups_data.groupby(['group', 'cohort_id'])['nmv'].transform('sum')\n",
    "groups_data['cntrb'] = (groups_data['nmv'] / groups_data['group_nmv']).fillna(1)\n",
    "\n",
    "# Flag if any price/scrapped column is non-NaN\n",
    "price_cols = [\n",
    "    'ben_soliman_price', 'final_min_price', 'final_max_price', 'final_mod_price', 'final_true_min', 'final_true_max',\n",
    "    'min_scrapped', 'scrapped25', 'scrapped50', 'scrapped75', 'max_scrapped'\n",
    "]\n",
    "groups_data['flag_non_nan'] = groups_data[price_cols].notna().any(axis=1).astype(int)\n",
    "\n",
    "# Weighted Median Function\n",
    "def weighted_median(series, weights):\n",
    "    valid = ~series.isna() & ~weights.isna()\n",
    "    s = series[valid]\n",
    "    w = weights[valid]\n",
    "    if len(s) == 0:\n",
    "        return np.nan\n",
    "    order = np.argsort(s)\n",
    "    s, w = s.iloc[order], w.iloc[order]\n",
    "    return s.iloc[np.searchsorted(np.cumsum(w), w.sum() / 2)]\n",
    "\n",
    "# Perform Weighted Aggregation\n",
    "groups_agg = (\n",
    "    groups_data[groups_data['flag_non_nan'] == 1]\n",
    "    .groupby(['group', 'cohort_id'])\n",
    "    .apply(lambda g: pd.Series({\n",
    "        col: weighted_median(g[col], g['cntrb']) for col in price_cols\n",
    "    }))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Fill missing prices with group-level prices\n",
    "merged = market_data.merge(groups_agg, on=['group', 'cohort_id'], how='left', suffixes=('', '_group'))\n",
    "for col in price_cols:\n",
    "    merged[col] = merged[col].fillna(merged[f'{col}_group'])\n",
    "\n",
    "market_data = merged.drop(columns=[f'{c}_group' for c in price_cols])\n",
    "\n",
    "print(f\"Market data after group processing: {len(market_data)} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891bfa3b-ac80-43b6-84ec-3fe67dec0292",
   "metadata": {},
   "source": [
    "## Price Coverage Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c17d8a25-5300-4f1f-8a57-974ef869efeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market data after price coverage filtering: 13000 records\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Price Coverage Filtering - Filter products with sufficient price data (on market_data)\n",
    "# =============================================================================\n",
    "\n",
    "# Score price coverage\n",
    "market_data['ben'] = 0\n",
    "market_data['MP'] = 0\n",
    "market_data['sp'] = 0\n",
    "\n",
    "# Ben Soliman: 1 point if present\n",
    "market_data.loc[~market_data['ben_soliman_price'].isna(), 'ben'] = 1\n",
    "\n",
    "# Marketplace: 1 point if single price, 3 points if range\n",
    "market_data.loc[(market_data['final_min_price'] == market_data['final_max_price']) & \n",
    "                (~market_data['final_min_price'].isna()), 'MP'] = 1\n",
    "market_data.loc[(market_data['final_min_price'] != market_data['final_max_price']) & \n",
    "                (~market_data['final_min_price'].isna()), 'MP'] = 3\n",
    "\n",
    "# Scrapped: 1 point if single price, 5 points if range\n",
    "market_data.loc[(market_data['min_scrapped'] == market_data['max_scrapped']) & \n",
    "                (~market_data['min_scrapped'].isna()), 'sp'] = 1\n",
    "market_data.loc[(market_data['min_scrapped'] != market_data['max_scrapped']) & \n",
    "                (~market_data['min_scrapped'].isna()), 'sp'] = 5\n",
    "\n",
    "# Total price coverage score\n",
    "market_data['total_p'] = market_data['ben'] + market_data['MP'] + market_data['sp']\n",
    "\n",
    "# Filter: keep only products with total_p > 2\n",
    "market_data = market_data[market_data['total_p'] > 2]\n",
    "\n",
    "print(f\"Market data after price coverage filtering: {len(market_data)} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f353dc91-179d-4421-9668-ee089de8e479",
   "metadata": {},
   "source": [
    "## Price Analysis & Margin Calculation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "520a7849-a085-4196-a06c-e500c49c6f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Price Analysis Functions\n",
    "# =============================================================================\n",
    "\n",
    "def price_analysis(row):\n",
    "    \"\"\"Analyze prices and calculate percentiles for a product.\"\"\"\n",
    "    wac = row['wac_p']\n",
    "    avg_margin = row['avg_margin'] if row['avg_margin'] >= 0.01 else row['target_margin']\n",
    "    std = np.maximum(row['std'], 0.0025)\n",
    "    target_margin = row['target_margin']\n",
    "    max_marg = np.maximum(avg_margin, target_margin)\n",
    "    \n",
    "    # Collect all price points\n",
    "    price_list = [\n",
    "        row['ben_soliman_price'], row['final_min_price'], row['final_mod_price'],\n",
    "        row['final_max_price'], row['final_true_min'], row['final_true_max'],\n",
    "        row['min_scrapped'], row['scrapped25'], row['scrapped50'], row['scrapped75'], row['max_scrapped']\n",
    "    ]\n",
    "    \n",
    "    # Filter valid prices within acceptable range\n",
    "    valid_prices = sorted({\n",
    "        x for x in price_list \n",
    "        if x and not pd.isna(x) and x != 0 \n",
    "        and wac / (1 - (avg_margin - (10 * std))) <= x <= wac / (1 - (max_marg + 10 * std))\n",
    "        and x >= wac * 0.9\n",
    "    })\n",
    "    \n",
    "    if not valid_prices:\n",
    "        return np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "    \n",
    "    return (\n",
    "        np.min(valid_prices),\n",
    "        np.percentile(valid_prices, 25),\n",
    "        np.percentile(valid_prices, 50),\n",
    "        np.percentile(valid_prices, 75),\n",
    "        np.max(valid_prices)\n",
    "    )\n",
    "\n",
    "\n",
    "def calculate_step_bounds(row):\n",
    "    \"\"\"Calculate below/above market bounds based on price steps.\"\"\"\n",
    "    wac = row['wac_p']\n",
    "    std = row['std']\n",
    "    prices = [row['minimum'], row['percentile_25'], row['percentile_50'], row['percentile_75'], row['maximum']]\n",
    "    \n",
    "    # Calculate valid steps between price points\n",
    "    valid_steps = []\n",
    "    for i in range(len(prices) - 1):\n",
    "        step = prices[i + 1] - prices[i]\n",
    "        if (step / wac) <= std * 1.2:\n",
    "            valid_steps.append(step)\n",
    "    \n",
    "    avg_step = np.mean(valid_steps) if valid_steps else min(2 * std, 0.2 * row['target_margin'])\n",
    "    \n",
    "    new_min = prices[0] - avg_step if (prices[0] - avg_step) >= wac else prices[0]\n",
    "    new_max = prices[-1] + avg_step if (prices[-1] + avg_step) >= wac else prices[-1]\n",
    "    \n",
    "    return new_min, new_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "be580be7-22d1-46e2-813e-c8c3ab9b8e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market data after price analysis: 12329 records\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Apply Price Analysis & Margin Calculation (on market_data)\n",
    "# =============================================================================\n",
    "\n",
    "# Apply price analysis to calculate price percentiles\n",
    "market_data[['minimum', 'percentile_25', 'percentile_50', 'percentile_75', 'maximum']] = \\\n",
    "    market_data.apply(price_analysis, axis=1, result_type='expand')\n",
    "\n",
    "# Filter out records without valid price analysis\n",
    "market_data = market_data[~market_data['minimum'].isna()]\n",
    "\n",
    "# Calculate below/above market bounds\n",
    "market_data[['below_market', 'above_market']] = market_data.apply(calculate_step_bounds, axis=1, result_type='expand')\n",
    "\n",
    "print(f\"Market data after price analysis: {len(market_data)} records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "29e93a98-5e6e-41ae-9347-53cd27ac1413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MARKET DATA PROCESSING COMPLETE\n",
      "============================================================\n",
      "Total processed market records: 12329\n",
      "\n",
      "Market data columns:\n",
      "  - Price columns: ben_soliman_price, final_min_price, final_max_price, etc.\n",
      "  - Percentiles: minimum, percentile_25, percentile_50, percentile_75, maximum\n",
      "  - Margin tiers: below_market, market_min, market_25, market_50, market_75, market_max, above_market\n",
      "\n",
      "Sample processed market data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cohort_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>ben_soliman_price</th>\n",
       "      <th>final_min_price</th>\n",
       "      <th>final_max_price</th>\n",
       "      <th>final_mod_price</th>\n",
       "      <th>final_true_min</th>\n",
       "      <th>final_true_max</th>\n",
       "      <th>min_scrapped</th>\n",
       "      <th>scrapped25</th>\n",
       "      <th>...</th>\n",
       "      <th>percentile_50</th>\n",
       "      <th>percentile_75</th>\n",
       "      <th>maximum</th>\n",
       "      <th>below_market</th>\n",
       "      <th>market_min</th>\n",
       "      <th>market_25</th>\n",
       "      <th>market_50</th>\n",
       "      <th>market_75</th>\n",
       "      <th>market_max</th>\n",
       "      <th>above_market</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>702</td>\n",
       "      <td>3.0</td>\n",
       "      <td>258.5</td>\n",
       "      <td>255.0</td>\n",
       "      <td>279.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>254.630005</td>\n",
       "      <td>254.957504</td>\n",
       "      <td>...</td>\n",
       "      <td>255.448753</td>\n",
       "      <td>256.580002</td>\n",
       "      <td>279.0</td>\n",
       "      <td>0.050898</td>\n",
       "      <td>0.053320</td>\n",
       "      <td>0.054655</td>\n",
       "      <td>0.056355</td>\n",
       "      <td>0.060515</td>\n",
       "      <td>0.136011</td>\n",
       "      <td>0.138019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>702</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>835.4</td>\n",
       "      <td>838.6</td>\n",
       "      <td>835.0</td>\n",
       "      <td>835.0</td>\n",
       "      <td>839.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>837.000000</td>\n",
       "      <td>838.700000</td>\n",
       "      <td>839.0</td>\n",
       "      <td>0.018123</td>\n",
       "      <td>0.019299</td>\n",
       "      <td>0.019651</td>\n",
       "      <td>0.021643</td>\n",
       "      <td>0.023626</td>\n",
       "      <td>0.023975</td>\n",
       "      <td>0.025137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>702</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>272.0</td>\n",
       "      <td>288.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>288.500000</td>\n",
       "      <td>290.0</td>\n",
       "      <td>0.025238</td>\n",
       "      <td>0.030653</td>\n",
       "      <td>0.036009</td>\n",
       "      <td>0.065273</td>\n",
       "      <td>0.092812</td>\n",
       "      <td>0.097505</td>\n",
       "      <td>0.102149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>702</td>\n",
       "      <td>14.0</td>\n",
       "      <td>465.0</td>\n",
       "      <td>461.5</td>\n",
       "      <td>477.0</td>\n",
       "      <td>477.0</td>\n",
       "      <td>460.0</td>\n",
       "      <td>477.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>463.250000</td>\n",
       "      <td>468.000000</td>\n",
       "      <td>477.0</td>\n",
       "      <td>0.022650</td>\n",
       "      <td>0.028316</td>\n",
       "      <td>0.030687</td>\n",
       "      <td>0.035133</td>\n",
       "      <td>0.044926</td>\n",
       "      <td>0.062946</td>\n",
       "      <td>0.068156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>702</td>\n",
       "      <td>17.0</td>\n",
       "      <td>604.0</td>\n",
       "      <td>596.5</td>\n",
       "      <td>603.5</td>\n",
       "      <td>600.0</td>\n",
       "      <td>595.0</td>\n",
       "      <td>605.0</td>\n",
       "      <td>598.500000</td>\n",
       "      <td>598.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>600.000000</td>\n",
       "      <td>603.750000</td>\n",
       "      <td>605.0</td>\n",
       "      <td>0.018424</td>\n",
       "      <td>0.022548</td>\n",
       "      <td>0.026638</td>\n",
       "      <td>0.030693</td>\n",
       "      <td>0.036714</td>\n",
       "      <td>0.038704</td>\n",
       "      <td>0.042660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cohort_id  product_id  ben_soliman_price  final_min_price  final_max_price  \\\n",
       "0        702         3.0              258.5            255.0            279.0   \n",
       "1        702         9.0                NaN            835.4            838.6   \n",
       "2        702        10.0                NaN            272.0            288.0   \n",
       "4        702        14.0              465.0            461.5            477.0   \n",
       "5        702        17.0              604.0            596.5            603.5   \n",
       "\n",
       "   final_mod_price  final_true_min  final_true_max  min_scrapped  scrapped25  \\\n",
       "0            255.0           255.0           300.0    254.630005  254.957504   \n",
       "1            835.0           835.0           839.0           NaN         NaN   \n",
       "2            270.0           270.0           290.0           NaN         NaN   \n",
       "4            477.0           460.0           477.0           NaN         NaN   \n",
       "5            600.0           595.0           605.0    598.500000  598.500000   \n",
       "\n",
       "   ...  percentile_50  percentile_75  maximum  below_market  market_min  \\\n",
       "0  ...     255.448753     256.580002    279.0      0.050898    0.053320   \n",
       "1  ...     837.000000     838.700000    839.0      0.018123    0.019299   \n",
       "2  ...     280.000000     288.500000    290.0      0.025238    0.030653   \n",
       "4  ...     463.250000     468.000000    477.0      0.022650    0.028316   \n",
       "5  ...     600.000000     603.750000    605.0      0.018424    0.022548   \n",
       "\n",
       "   market_25  market_50  market_75  market_max  above_market  \n",
       "0   0.054655   0.056355   0.060515    0.136011      0.138019  \n",
       "1   0.019651   0.021643   0.023626    0.023975      0.025137  \n",
       "2   0.036009   0.065273   0.092812    0.097505      0.102149  \n",
       "4   0.030687   0.035133   0.044926    0.062946      0.068156  \n",
       "5   0.026638   0.030693   0.036714    0.038704      0.042660  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Convert prices to margins (on market_data) - FINALIZE market_data processing\n",
    "# =============================================================================\n",
    "\n",
    "market_data['below_market'] = (market_data['below_market'] - market_data['wac_p']) / market_data['below_market']\n",
    "market_data['market_min'] = (market_data['minimum'] - market_data['wac_p']) / market_data['minimum']\n",
    "market_data['market_25'] = (market_data['percentile_25'] - market_data['wac_p']) / market_data['percentile_25']\n",
    "market_data['market_50'] = (market_data['percentile_50'] - market_data['wac_p']) / market_data['percentile_50']\n",
    "market_data['market_75'] = (market_data['percentile_75'] - market_data['wac_p']) / market_data['percentile_75']\n",
    "market_data['market_max'] = (market_data['maximum'] - market_data['wac_p']) / market_data['maximum']\n",
    "market_data['above_market'] = (market_data['above_market'] - market_data['wac_p']) / market_data['above_market']\n",
    "\n",
    "# Select only the market-related columns to merge later\n",
    "market_columns = [\n",
    "    'cohort_id', 'product_id',\n",
    "    # Market Prices (raw)\n",
    "    'ben_soliman_price', \n",
    "    'final_min_price', 'final_max_price', 'final_mod_price', 'final_true_min', 'final_true_max',\n",
    "    'min_scrapped', 'scrapped25', 'scrapped50', 'scrapped75', 'max_scrapped',\n",
    "    # Price Percentiles\n",
    "    'minimum', 'percentile_25', 'percentile_50', 'percentile_75', 'maximum',\n",
    "    # Margin Tiers\n",
    "    'below_market', 'market_min', 'market_25', 'market_50', 'market_75', 'market_max', 'above_market'\n",
    "]\n",
    "market_data = market_data[[c for c in market_columns if c in market_data.columns]]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"MARKET DATA PROCESSING COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total processed market records: {len(market_data)}\")\n",
    "print(f\"\\nMarket data columns:\")\n",
    "print(\"  - Price columns: ben_soliman_price, final_min_price, final_max_price, etc.\")\n",
    "print(\"  - Percentiles: minimum, percentile_25, percentile_50, percentile_75, maximum\")\n",
    "print(\"  - Margin tiers: below_market, market_min, market_25, market_50, market_75, market_max, above_market\")\n",
    "print(f\"\\nSample processed market data:\")\n",
    "market_data.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
