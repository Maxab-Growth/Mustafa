{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c83eefb",
   "metadata": {},
   "source": [
    "# Quantity Discount (QD) Pricing System\n",
    "\n",
    "This notebook calculates tiered pricing and quantities for products across warehouses.\n",
    "\n",
    "## Workflow:\n",
    "1. **Setup** - Imports, connections, and configuration\n",
    "2. **Product Selection** - Select top products per warehouse based on performance\n",
    "3. **Quantity Tiers** - Calculate tier 1 and tier 2 quantities based on order history\n",
    "4. **Market Prices** - Gather competitive pricing data\n",
    "5. **Price Tiers** - Calculate discounted prices for each tier\n",
    "6. **Wholesale Pricing** - Calculate wholesale prices for bulk orders\n",
    "7. **Export** - Save results to Excel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d33827a",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2f577ce-0216-4712-b5bb-9db6f27f879b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# =============================================================================\n",
    "# Package Installation\n",
    "# =============================================================================\n",
    "\n",
    "# Core\n",
    "!pip install --upgrade pip\n",
    "\n",
    "# Database Connectivity\n",
    "!pip install psycopg2-binary\n",
    "!pip install snowflake-connector-python==3.15.0\n",
    "!pip install snowflake-sqlalchemy\n",
    "!pip install sqlalchemy==1.4.46\n",
    "\n",
    "# AWS & API\n",
    "!pip install boto3\n",
    "!pip install requests\n",
    "!pip install keyring==23.11.0\n",
    "\n",
    "# Google Sheets\n",
    "!pip install oauth2client\n",
    "!pip install gspread==5.9.0\n",
    "!pip install gspread_dataframe\n",
    "!pip install google.cloud\n",
    "\n",
    "# Data Manipulation\n",
    "!pip install pandas==2.2.1\n",
    "!pip install numpy\n",
    "!pip install polars\n",
    "!pip install openpyxl\n",
    "!pip install xlsxwriter\n",
    "\n",
    "# Utilities\n",
    "!pip install tqdm\n",
    "!pip install warnings\n",
    "!pip install --upgrade datetime\n",
    "!pip install python-time\n",
    "!pip install --upgrade pytz\n",
    "!pip install db-dtypes\n",
    "!pip install import-ipynb\n",
    "\n",
    "# Analytics\n",
    "!pip install statsmodels\n",
    "!pip install scikit-learn\n",
    "!pip install pulp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ef08d39-8955-4a26-a608-620c40df850b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from google.oauth2.service_account import Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3d0f8d0-7f4a-4468-b218-49241a56edc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/snowflake/connector/options.py:104: UserWarning: You have an incompatible version of 'pyarrow' installed (22.0.0), please install a version that adheres to: 'pyarrow<19.0.0; extra == \"pandas\"'\n",
      "  warn_incompatible_dep(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.Renviron\n",
      "/home/ec2-user/service_account_key.json\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STANDARD LIBRARY IMPORTS\n",
    "# =============================================================================\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import base64\n",
    "import calendar\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "# =============================================================================\n",
    "# THIRD-PARTY IMPORTS\n",
    "# =============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import requests\n",
    "import gspread\n",
    "import boto3\n",
    "import snowflake.connector\n",
    "from tqdm import tqdm\n",
    "from requests import get\n",
    "from botocore.exceptions import ClientError\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "# =============================================================================\n",
    "# LOCAL IMPORTS & ENVIRONMENT SETUP\n",
    "# =============================================================================\n",
    "import setup_environment_2\n",
    "import importlib\n",
    "import import_ipynb\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "importlib.reload(setup_environment_2)\n",
    "setup_environment_2.initialize_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0727d967",
   "metadata": {},
   "source": [
    "### Configuration Constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba7201f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - Modify these parameters as needed\n",
    "# =============================================================================\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Cohort IDs for QD program\n",
    "# -----------------------------------------------------------------------------\n",
    "COHORT_IDS = [700, 701, 702, 703, 704, 1123, 1124, 1125, 1126]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Warehouse mappings: (region, warehouse_name, warehouse_id, cohort_id)\n",
    "# -----------------------------------------------------------------------------\n",
    "WAREHOUSE_MAPPING = [\n",
    "    ('Cairo',       'Mostorod',      1,   700),\n",
    "    ('Giza',        'Barageel',      236, 701),\n",
    "    ('Giza',        'Sakkarah',      962, 701),\n",
    "    ('Delta West',  'El-Mahala',     337, 703),\n",
    "    ('Delta West',  'Tanta',         8,   703),\n",
    "    ('Delta East',  'Mansoura FC',   339, 704),\n",
    "    ('Delta East',  'Sharqya',       170, 704),\n",
    "    ('Upper Egypt', 'Assiut FC',     501, 1124),\n",
    "    ('Upper Egypt', 'Bani sweif',    401, 1126),\n",
    "    ('Upper Egypt', 'Menya Samalot', 703, 1123),\n",
    "    ('Upper Egypt', 'Sohag',         632, 1125),\n",
    "    ('Alexandria',  'Khorshed Alex', 797, 702),\n",
    "]\n",
    "\n",
    "# Excluded warehouse IDs\n",
    "EXCLUDED_WAREHOUSES = [6, 9, 10]\n",
    "\n",
    "# Products to exclude from selection\n",
    "PRODUCTS_TO_REMOVE = [7630,589]\n",
    "CATS_TO_REMOVE = []#['مرقة وخلطات','صلصة و صوص','أرز','بقوليات','مكرونة','مكرونة سايب','زيوت','بقوليات و حبوب سايب','سمنة','ارز سايب','سمنة سايب','بهارات سايب','ياميش']\n",
    "BRANDS_TO_REMOVE = ['بيتي عصاير']\n",
    "# -----------------------------------------------------------------------------\n",
    "# Pricing Parameters\n",
    "# -----------------------------------------------------------------------------\n",
    "MAX_DISCOUNT_PCT = 5.0    # Maximum discount allowed from current price (%)\n",
    "MIN_DISCOUNT_PCT = 0.35   # Minimum discount required from current price (%)\n",
    "MIN_RATIO        = 1.05    # Minimum discount-to-quantity ratio\n",
    "MAX_RATIO        = 3      # Maximum discount-to-quantity ratio\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Product Selection Thresholds\n",
    "# -----------------------------------------------------------------------------\n",
    "MIN_ORDERS    = 20    # Minimum orders in 4 months\n",
    "MIN_RETAILERS = 5     # Minimum unique retailers\n",
    "MIN_NMV       = 5000  # Minimum revenue (EGP)\n",
    "MIN_VELOCITY  = 0.5   # Minimum units per day\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Ranking Parameters\n",
    "# -----------------------------------------------------------------------------\n",
    "TOP_PRODUCTS_PER_WAREHOUSE   = 200  # Initial selection\n",
    "FINAL_PRODUCTS_PER_WAREHOUSE = 133  # Final output\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Delivery Fees\n",
    "# -----------------------------------------------------------------------------\n",
    "DELIVERY_FEE_CAIRO_GIZA = 25\n",
    "DELIVERY_FEE_OTHER      = 20\n",
    "\n",
    "print(\"✓ Configuration loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4092441e-bd5e-4b90-9b98-a4325a9757ec",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "995dfa24-7123-41e5-97fb-a45c125f3f15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_secret(secret_name):\n",
    "    \"\"\"\n",
    "    Retrieve secret from AWS Secrets Manager.\n",
    "    \n",
    "    Args:\n",
    "        secret_name: Name/ID of the secret to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        Secret string or decoded binary\n",
    "    \"\"\"\n",
    "    region_name = \"us-east-1\"\n",
    "    \n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(SecretId=secret_name)\n",
    "    except ClientError as e:\n",
    "        # Re-raise all AWS Secrets Manager exceptions\n",
    "        raise e\n",
    "    \n",
    "    # Return decrypted secret (string or binary)\n",
    "    if 'SecretString' in get_secret_value_response:\n",
    "        return get_secret_value_response['SecretString']\n",
    "    else:\n",
    "        return base64.b64decode(get_secret_value_response['SecretBinary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa2e910e-65d9-4a69-9922-988548748eb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ API credentials loaded\n"
     ]
    }
   ],
   "source": [
    "# Load API credentials from AWS Secrets Manager\n",
    "pricing_api_secret = json.loads(get_secret(\"prod/pricing/api/\"))\n",
    "username = pricing_api_secret[\"egypt_username\"]\n",
    "password = pricing_api_secret[\"egypt_password\"]\n",
    "secret   = pricing_api_secret[\"egypt_secret\"]\n",
    "\n",
    "print(\"✓ API credentials loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01f29f77-2e72-4064-9503-ec9be748e835",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_access_token(url, client_id, client_secret):\n",
    "    \"\"\"\n",
    "    Get OAuth access token for MaxAB APIs.\n",
    "    \n",
    "    Args:\n",
    "        url: Token endpoint URL\n",
    "        client_id: OAuth client ID\n",
    "        client_secret: OAuth client secret\n",
    "        \n",
    "    Returns:\n",
    "        Access token string\n",
    "    \"\"\"\n",
    "    response = requests.post(\n",
    "        url,\n",
    "        data={\n",
    "            \"grant_type\": \"password\",\n",
    "            \"username\": username,\n",
    "            \"password\": password\n",
    "        },\n",
    "        auth=(client_id, client_secret),\n",
    "    )\n",
    "    return response.json()[\"access_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bfb8760-ddf2-4192-beaf-cf52c6c92ff2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def post_QD(file_name):\n",
    "    \"\"\"\n",
    "    Upload Quantity Discount file to MaxAB API.\n",
    "    \n",
    "    Args:\n",
    "        file_name: Path to the Excel file to upload\n",
    "        \n",
    "    Returns:\n",
    "        API response object\n",
    "    \"\"\"\n",
    "    token = get_access_token(\n",
    "        'https://sso.maxab.info/auth/realms/maxab/protocol/openid-connect/token',\n",
    "        'main-system-externals',\n",
    "        secret\n",
    "    )\n",
    "    \n",
    "    url = \"https://api.maxab.info/commerce/api/admins/v1/quantity-discounts\"\n",
    "    \n",
    "    files = [\n",
    "        ('file', (file_name, open(file_name, 'rb'), \n",
    "                  'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'))\n",
    "    ]\n",
    "    headers = {'Authorization': f'bearer {token}'}\n",
    "    \n",
    "    response = requests.request(\"POST\", url, headers=headers, data={}, files=files)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d44c2b13-8395-42c1-b902-ec404ba1bca1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def post_cart_rules(cohort_id, file_name):\n",
    "    \"\"\"\n",
    "    Upload Cart Rules file for a specific cohort.\n",
    "    \n",
    "    Args:\n",
    "        cohort_id: ID of the cohort to update\n",
    "        file_name: Path to the Excel file to upload\n",
    "        \n",
    "    Returns:\n",
    "        API response object\n",
    "    \"\"\"\n",
    "    token = get_access_token(\n",
    "        'https://sso.maxab.info/auth/realms/maxab/protocol/openid-connect/token',\n",
    "        'main-system-externals',\n",
    "        secret\n",
    "    )\n",
    "    \n",
    "    url = f\"https://api.maxab.info/main-system/api/admin-portal/cohorts/{cohort_id}/cart-rules\"\n",
    "    \n",
    "    files = [\n",
    "        ('sheet', (file_name, open(file_name, 'rb'),\n",
    "                   'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'))\n",
    "    ]\n",
    "    headers = {'Authorization': f'bearer {token}'}\n",
    "    \n",
    "    response = requests.request(\"POST\", url, headers=headers, data={}, files=files)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2601556c",
   "metadata": {},
   "source": [
    "### Database Connection Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dda92e1-ffd3-4d47-80a7-b65fc9a64ee4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def snowflake_query(country, query, warehouse=None, columns=[], conn=None):\n",
    "    \"\"\"\n",
    "    Execute a query against Snowflake and return results as DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        country: Country identifier (e.g., \"Egypt\")\n",
    "        query: SQL query string to execute\n",
    "        warehouse: Snowflake warehouse (optional)\n",
    "        columns: Custom column names (optional)\n",
    "        conn: Existing connection (optional)\n",
    "        \n",
    "    Returns:\n",
    "        pandas DataFrame with query results\n",
    "    \"\"\"\n",
    "    con = snowflake.connector.connect(\n",
    "        user     = os.environ[\"SNOWFLAKE_USERNAME\"],\n",
    "        account  = os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "        password = os.environ[\"SNOWFLAKE_PASSWORD\"],\n",
    "        database = os.environ[\"SNOWFLAKE_DATABASE\"]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        cur = con.cursor()\n",
    "        cur.execute(\"USE WAREHOUSE COMPUTE_WH\")\n",
    "        cur.execute(query)\n",
    "        \n",
    "        column_names = [col[0] for col in cur.description]\n",
    "        results = cur.fetchall()\n",
    "        \n",
    "        if not results:\n",
    "            out = pd.DataFrame(columns=[name.lower() for name in column_names])\n",
    "        else:\n",
    "            if len(columns) == 0:\n",
    "                out = pd.DataFrame(np.array(results), columns=column_names)\n",
    "                out.columns = out.columns.str.lower()\n",
    "            else:\n",
    "                out = pd.DataFrame(np.array(results), columns=columns)\n",
    "                out.columns = out.columns.str.lower()\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Query error: {e}\")\n",
    "        raise\n",
    "        \n",
    "    finally:\n",
    "        cur.close()\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "febda4a3-f2e9-4601-b101-0dd290924eb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using timezone: America/Los_Angeles\n"
     ]
    }
   ],
   "source": [
    "# Get Snowflake timezone for consistent date/time handling\n",
    "query = \"SHOW PARAMETERS LIKE 'TIMEZONE'\"\n",
    "timezone_result = snowflake_query(\"Egypt\", query)\n",
    "zone_to_use = timezone_result['value'].values[0]\n",
    "print(f\"✓ Using timezone: {zone_to_use}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376408a9",
   "metadata": {},
   "source": [
    "### Feedback Loop - Previous QD Cycle Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b3788ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching feedback from previous QD cycle...\n",
      "✓ Got feedback for 1582 SKUs\n",
      "  - With recommendations: 1409\n",
      "  - T1 changes: 580\n",
      "  - T2 changes: 1036\n",
      "  - T3 changes: 519\n",
      "  - Margin status: {'CRITICAL': 892, 'HIGH': 312, 'HEALTHY': 272, 'LOW': 106}\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FEEDBACK LOOP: Get recommendations from previous QD cycle\n",
    "# =============================================================================\n",
    "\n",
    "feedback_query = '''\n",
    "-- ============================================================\n",
    "-- QD FEEDBACK LOOP QUERY\n",
    "-- Based on PREVIOUS QD cycle performance\n",
    "-- ============================================================\n",
    "\n",
    "WITH warehouse_mapping AS (\n",
    "    SELECT * \n",
    "    FROM (VALUES\n",
    "        ('Assiut FC', 501, 3301),\n",
    "        ('Bani sweif', 401, 3302),\n",
    "        ('Barageel', 236, 3303),\n",
    "        ('El-Mahala', 337, 3304),\n",
    "        ('Khorshed Alex', 797, 3305),\n",
    "        ('Mansoura FC', 339, 3306),\n",
    "        ('Menya Samalot', 703, 3307),\n",
    "        ('Mostorod', 1, 3308),\n",
    "        ('Sakkarah', 962, 3309),\n",
    "        ('Sharqya', 170, 3310),\n",
    "        ('Sohag', 632, 3311),\n",
    "        ('Tanta', 8, 3312)\n",
    "    ) AS x(warehouse_name, warehouse_id, tag_id)\n",
    "), \n",
    "\n",
    "-- Get the PREVIOUS (completed) QD cycle\n",
    "previous_qd_cycle AS (\n",
    "    SELECT \n",
    "        qd.id AS qd_id,\n",
    "        qd.start_at AS start_at,\n",
    "        qd.end_at AS end_at,\n",
    "        qd.dynamic_tag_id,\n",
    "        RANK() OVER (PARTITION BY qd.dynamic_tag_id ORDER BY qd.start_at desc ,qd.end_at DESC) AS cycle_rank\n",
    "    FROM quantity_discounts qd\n",
    "    WHERE qd.start_at::date < current_date\n",
    "\tand qd.start_at::date >= current_date - interval '10 days'\n",
    "\tand dynamic_tag_id >3300\n",
    "    QUALIFY cycle_rank = 1\n",
    "),\n",
    "\n",
    "qd_products AS (\n",
    "    SELECT \n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        sku,\n",
    "        packing_unit_id,\n",
    "        qd_id,\n",
    "        MAX(CASE WHEN tier = 1 THEN quantity END) AS tier_1_qty,\n",
    "        MAX(CASE WHEN tier = 1 THEN discount_percentage END) AS tier_1_discount_pct,\n",
    "        MAX(CASE WHEN tier = 2 THEN quantity END) AS tier_2_qty,\n",
    "        MAX(CASE WHEN tier = 2 THEN discount_percentage END) AS tier_2_discount_pct,\n",
    "        MAX(CASE WHEN tier = 3 THEN quantity END) AS tier_3_qty,\n",
    "        MAX(CASE WHEN tier = 3 THEN discount_percentage END) AS tier_3_discount_pct,\n",
    "        start_at,\n",
    "        end_at\n",
    "    FROM (\n",
    "        SELECT \n",
    "            wm.warehouse_id,\n",
    "            qd.id AS qd_id,\n",
    "            qdv.product_id,\n",
    "            CONCAT(p.name_ar, ' ', p.size, ' ', product_units.name_ar) AS sku,\n",
    "            qdv.packing_unit_id,\n",
    "            qdv.quantity,\n",
    "            qdv.discount_percentage,\n",
    "            qd.start_at,\n",
    "            qd.end_at,\n",
    "            ROW_NUMBER() OVER (\n",
    "                PARTITION BY qdv.product_id, qdv.packing_unit_id, qd.id \n",
    "                ORDER BY qdv.quantity\n",
    "            ) AS tier\n",
    "        FROM quantity_discounts qd \n",
    "        JOIN quantity_discount_values qdv ON qd.id = qdv.quantity_discount_id \n",
    "        JOIN warehouse_mapping wm ON wm.tag_id = qd.dynamic_tag_id\n",
    "        JOIN products p ON p.id = qdv.product_id \n",
    "        JOIN product_units ON product_units.id = p.unit_id \n",
    "        JOIN previous_qd_cycle pqd ON qd.id = pqd.qd_id\n",
    "    )\n",
    "    GROUP BY ALL\n",
    "),\n",
    "\n",
    "excluded_rets AS (\n",
    "    SELECT dta.TAGGABLE_ID AS retailer_id\n",
    "    FROM DYNAMIC_TAGS dt \n",
    "    JOIN dynamic_taggables dta ON dt.id = dta.dynamic_tag_id \n",
    "    WHERE dt.name LIKE '%whole_sale%'\n",
    "        AND dt.id > 3000\n",
    "),\n",
    "\n",
    "orders_with_qd AS (\n",
    "    SELECT \n",
    "        w.id AS warehouse_id,\n",
    "        w.name AS warehouse,\n",
    "        pso.product_id,\n",
    "        sku,\n",
    "        pso.packing_unit_id,\n",
    "        pso.purchased_item_count AS qty,\n",
    "        pso.total_price AS nmv,\n",
    "        pso.basic_unit_count,\n",
    "        pso.item_quantity_discount_value AS qd_discount_per_item,\n",
    "        COALESCE(f.wac_p, 0) AS wac,\n",
    "        \n",
    "        -- Tier thresholds from previous cycle\n",
    "        qd.tier_1_qty,\n",
    "        qd.tier_2_qty,\n",
    "        qd.tier_3_qty,\n",
    "        qd.tier_1_discount_pct,\n",
    "        qd.tier_2_discount_pct,\n",
    "        qd.tier_3_discount_pct,\n",
    "        \n",
    "        -- Determine which tier was used\n",
    "        CASE \n",
    "            WHEN pso.item_quantity_discount_value = 0 THEN 'Base'\n",
    "            WHEN pso.purchased_item_count >= COALESCE(qd.tier_3_qty, 999999) AND qd.tier_3_qty IS NOT NULL THEN 'Tier 3'\n",
    "            WHEN pso.purchased_item_count >= COALESCE(qd.tier_2_qty, 999999) AND qd.tier_2_qty IS NOT NULL THEN 'Tier 2'\n",
    "            WHEN pso.purchased_item_count >= COALESCE(qd.tier_1_qty, 999999) THEN 'Tier 1'\n",
    "            ELSE 'Base'\n",
    "        END AS tier_used,\n",
    "        \n",
    "        -- Total discount\n",
    "        pso.item_quantity_discount_value * pso.purchased_item_count AS total_qd_discount,\n",
    "        \n",
    "        -- Near-miss flags for T1\n",
    "        CASE WHEN qd.tier_1_qty IS NOT NULL AND qty >= qd.tier_1_qty * 0.8 AND qty < qd.tier_1_qty THEN 1 ELSE 0 END AS near_miss_t1,\n",
    "        CASE WHEN qd.tier_1_qty IS NOT NULL AND qty >= qd.tier_1_qty * 0.9 AND qty < qd.tier_1_qty THEN 1 ELSE 0 END AS very_close_t1,\n",
    "        \n",
    "        -- Near-miss flags for T2 (among T1 achievers)\n",
    "        CASE WHEN qd.tier_2_qty IS NOT NULL AND qty >= qd.tier_1_qty AND qty >= qd.tier_2_qty * 0.8 AND qty < qd.tier_2_qty THEN 1 ELSE 0 END AS near_miss_t2,\n",
    "        CASE WHEN qd.tier_2_qty IS NOT NULL AND qty >= qd.tier_1_qty AND qty >= qd.tier_2_qty * 0.9 AND qty < qd.tier_2_qty THEN 1 ELSE 0 END AS very_close_t2,\n",
    "        \n",
    "        -- Near-miss flags for T3 (among T2 achievers)\n",
    "        CASE WHEN qd.tier_3_qty IS NOT NULL AND qty >= qd.tier_2_qty AND qty >= qd.tier_3_qty * 0.8 AND qty < qd.tier_3_qty THEN 1 ELSE 0 END AS near_miss_t3,\n",
    "        CASE WHEN qd.tier_3_qty IS NOT NULL AND qty >= qd.tier_2_qty AND qty >= qd.tier_3_qty * 0.9 AND qty < qd.tier_3_qty THEN 1 ELSE 0 END AS very_close_t3\n",
    "        \n",
    "    FROM product_sales_order pso\n",
    "    JOIN sales_orders so ON so.id = pso.sales_order_id \n",
    "    JOIN warehouses w ON w.id = pso.warehouse_id\n",
    "    JOIN qd_products qd ON qd.product_id = pso.product_id \n",
    "        AND qd.packing_unit_id = pso.packing_unit_id \n",
    "        AND qd.warehouse_id = pso.warehouse_id\n",
    "    JOIN finance.all_cogs f ON f.product_id = pso.product_id \n",
    "        AND so.created_at BETWEEN f.from_date AND f.to_date\n",
    "    WHERE so.created_at BETWEEN (SELECT MIN(start_at) FROM previous_qd_cycle) AND (SELECT MAX(end_at) FROM previous_qd_cycle)\n",
    "        AND so.sales_order_status_id NOT IN (7, 12)\n",
    "        AND pso.purchased_item_count > 0 \n",
    "        AND so.retailer_id NOT IN (SELECT retailer_id FROM excluded_rets)\n",
    "),\n",
    "\n",
    "performance_metrics AS (\n",
    "    SELECT \n",
    "        warehouse_id,\n",
    "        warehouse,\n",
    "        product_id,\n",
    "        sku,\n",
    "        packing_unit_id,\n",
    "        \n",
    "        -- Current tier configuration\n",
    "        MAX(tier_1_qty) AS prev_tier_1_qty,\n",
    "        MAX(tier_1_discount_pct) AS prev_tier_1_discount,\n",
    "        MAX(tier_2_qty) AS prev_tier_2_qty,\n",
    "        MAX(tier_2_discount_pct) AS prev_tier_2_discount,\n",
    "        MAX(tier_3_qty) AS prev_tier_3_qty,\n",
    "        MAX(tier_3_discount_pct) AS prev_tier_3_discount,\n",
    "        \n",
    "        -- Order counts (for reference)\n",
    "        COUNT(*) AS total_orders,\n",
    "        COUNT(CASE WHEN tier_used = 'Base' THEN 1 END) AS base_orders,\n",
    "        COUNT(CASE WHEN tier_used = 'Tier 1' THEN 1 END) AS t1_orders,\n",
    "        COUNT(CASE WHEN tier_used = 'Tier 2' THEN 1 END) AS t2_orders,\n",
    "        COUNT(CASE WHEN tier_used = 'Tier 3' THEN 1 END) AS t3_orders,\n",
    "        \n",
    "        -- NMV by tier\n",
    "        SUM(nmv) AS total_nmv,\n",
    "        SUM(CASE WHEN tier_used = 'Base' THEN nmv ELSE 0 END) AS base_nmv,\n",
    "        SUM(CASE WHEN tier_used = 'Tier 1' THEN nmv ELSE 0 END) AS t1_nmv,\n",
    "        SUM(CASE WHEN tier_used = 'Tier 2' THEN nmv ELSE 0 END) AS t2_nmv,\n",
    "        SUM(CASE WHEN tier_used = 'Tier 3' THEN nmv ELSE 0 END) AS t3_nmv,\n",
    "        \n",
    "        -- CONVERSION RATES (NMV-based)\n",
    "        ROUND(SUM(CASE WHEN tier_used = 'Base' THEN nmv ELSE 0 END) * 100.0 / NULLIF(SUM(nmv), 0), 1) AS base_pct,\n",
    "        ROUND(SUM(CASE WHEN tier_used = 'Tier 1' THEN nmv ELSE 0 END) * 100.0 / NULLIF(SUM(nmv), 0), 1) AS t1_conversion_pct,\n",
    "        ROUND(SUM(CASE WHEN tier_used = 'Tier 2' THEN nmv ELSE 0 END) * 100.0 / NULLIF(SUM(nmv), 0), 1) AS t2_conversion_pct,\n",
    "        ROUND(SUM(CASE WHEN tier_used = 'Tier 3' THEN nmv ELSE 0 END) * 100.0 / NULLIF(SUM(nmv), 0), 1) AS t3_conversion_pct,\n",
    "        \n",
    "        -- Near-miss NMV\n",
    "        SUM(CASE WHEN near_miss_t1 = 1 THEN nmv ELSE 0 END) AS near_miss_t1_nmv,\n",
    "        SUM(CASE WHEN near_miss_t2 = 1 THEN nmv ELSE 0 END) AS near_miss_t2_nmv,\n",
    "        SUM(CASE WHEN near_miss_t3 = 1 THEN nmv ELSE 0 END) AS near_miss_t3_nmv,\n",
    "        \n",
    "        -- Near-miss as % of potential NMV\n",
    "        ROUND(SUM(CASE WHEN near_miss_t1 = 1 THEN nmv ELSE 0 END) * 100.0 \n",
    "            / NULLIF(SUM(CASE WHEN tier_used = 'Base' THEN nmv ELSE 0 END), 0), 1) AS near_miss_t1_pct,\n",
    "        ROUND(SUM(CASE WHEN near_miss_t2 = 1 THEN nmv ELSE 0 END) * 100.0 \n",
    "            / NULLIF(SUM(CASE WHEN tier_used = 'Tier 1' THEN nmv ELSE 0 END), 0), 1) AS near_miss_t2_pct,\n",
    "        ROUND(SUM(CASE WHEN near_miss_t3 = 1 THEN nmv ELSE 0 END) * 100.0 \n",
    "            / NULLIF(SUM(CASE WHEN tier_used = 'Tier 2' THEN nmv ELSE 0 END), 0), 1) AS near_miss_t3_pct,\n",
    "        \n",
    "        -- Quantity analysis\n",
    "        ROUND(AVG(qty), 1) AS avg_qty,\n",
    "        MEDIAN(qty) AS median_qty,\n",
    "        \n",
    "        -- Financial\n",
    "        SUM(qty * wac * basic_unit_count) AS total_cogs,\n",
    "        SUM(COALESCE(total_qd_discount, 0)) AS total_discount,\n",
    "        \n",
    "        -- Blended margin\n",
    "        ROUND((SUM(nmv) - SUM(qty * wac * basic_unit_count) - SUM(COALESCE(total_qd_discount, 0))) * 100.0 \n",
    "            / NULLIF(SUM(nmv), 0), 2) AS blended_margin_pct,\n",
    "        \n",
    "        -- Margin by tier\n",
    "        ROUND(SUM(CASE WHEN tier_used = 'Base' THEN nmv - (qty * wac * basic_unit_count) END) * 100.0\n",
    "            / NULLIF(SUM(CASE WHEN tier_used = 'Base' THEN nmv END), 0), 2) AS base_margin_pct,\n",
    "        ROUND(SUM(CASE WHEN tier_used = 'Tier 1' THEN nmv - (qty * wac * basic_unit_count) - total_qd_discount END) * 100.0\n",
    "            / NULLIF(SUM(CASE WHEN tier_used = 'Tier 1' THEN nmv END), 0), 2) AS t1_margin_pct,\n",
    "        ROUND(SUM(CASE WHEN tier_used = 'Tier 2' THEN nmv - (qty * wac * basic_unit_count) - total_qd_discount END) * 100.0\n",
    "            / NULLIF(SUM(CASE WHEN tier_used = 'Tier 2' THEN nmv END), 0), 2) AS t2_margin_pct,\n",
    "        ROUND(SUM(CASE WHEN tier_used = 'Tier 3' THEN nmv - (qty * wac * basic_unit_count) - total_qd_discount END) * 100.0\n",
    "            / NULLIF(SUM(CASE WHEN tier_used = 'Tier 3' THEN nmv END), 0), 2) AS t3_margin_pct\n",
    "            \n",
    "    FROM orders_with_qd\n",
    "    GROUP BY ALL\n",
    "    HAVING total_orders >= 5\n",
    "),\n",
    "\n",
    "with_targets AS (\n",
    "    SELECT \n",
    "        pm.*,\n",
    "        b.name_ar AS brand,\n",
    "        c.name_ar AS cat,\n",
    "        COALESCE(AVG(ct.margin), 0.05) AS target_margin\n",
    "    FROM performance_metrics pm\n",
    "    JOIN products p ON p.id = pm.product_id\n",
    "    JOIN brands b ON b.id = p.brand_id\n",
    "    JOIN categories c ON c.id = p.category_id\n",
    "    LEFT JOIN performance.commercial_targets ct \n",
    "        ON ct.cat = c.name_ar \n",
    "        AND ct.brand = b.name_ar \n",
    "        AND ct.date >= DATE_TRUNC('month', CURRENT_DATE)\n",
    "    GROUP BY ALL\n",
    "),\n",
    "\n",
    "feedback_recommendations AS (\n",
    "    SELECT \n",
    "        *,\n",
    "        \n",
    "        -- Margin health indicator\n",
    "        CASE \n",
    "            WHEN blended_margin_pct < target_margin * 100 * 0.90 THEN 'CRITICAL'\n",
    "            WHEN blended_margin_pct < target_margin * 100 * 0.95 THEN 'LOW'\n",
    "            WHEN blended_margin_pct > target_margin * 100 * 1.10 THEN 'HIGH'\n",
    "            ELSE 'HEALTHY'\n",
    "        END AS margin_status,\n",
    "        \n",
    "        -- TIER 1 RECOMMENDATIONS\n",
    "        CASE \n",
    "            WHEN t1_conversion_pct > 20 AND blended_margin_pct < target_margin * 100 * 0.95 \n",
    "                THEN 'INCREASE_QTY'\n",
    "            WHEN t1_conversion_pct < 8 AND blended_margin_pct > target_margin * 100 AND near_miss_t1_pct > 15 \n",
    "                THEN 'DECREASE_QTY'\n",
    "            WHEN t1_conversion_pct < 5 AND blended_margin_pct > target_margin * 100 * 1.05 AND near_miss_t1_pct <= 15 \n",
    "                THEN 'INCREASE_DISCOUNT'\n",
    "            WHEN t1_conversion_pct > 25 AND blended_margin_pct >= target_margin * 100 * 0.95 \n",
    "                THEN 'SLIGHT_INCREASE_QTY'\n",
    "            ELSE 'NO_CHANGE'\n",
    "        END AS t1_action,\n",
    "        \n",
    "        CASE \n",
    "            WHEN t1_conversion_pct > 20 AND blended_margin_pct < target_margin * 100 * 0.95 \n",
    "                THEN CEIL(prev_tier_1_qty * 1.20)\n",
    "            WHEN t1_conversion_pct < 8 AND blended_margin_pct > target_margin * 100 AND near_miss_t1_pct > 15 \n",
    "                THEN CEIL(prev_tier_1_qty * 0.9)\n",
    "            WHEN t1_conversion_pct > 25 AND blended_margin_pct >= target_margin * 100 * 0.95 \n",
    "                THEN CEIL(prev_tier_1_qty * 1.10)\n",
    "            ELSE prev_tier_1_qty\n",
    "        END AS suggested_t1_qty,\n",
    "        \n",
    "        CASE \n",
    "            WHEN blended_margin_pct < target_margin * 100 * 0.90 \n",
    "                THEN GREATEST(prev_tier_1_discount - 0.25, 0.2)\n",
    "            WHEN t1_conversion_pct < 5 AND blended_margin_pct > target_margin * 100 * 1.05 \n",
    "                THEN LEAST(prev_tier_1_discount + 0.25, 4.0)\n",
    "            ELSE prev_tier_1_discount\n",
    "        END AS suggested_t1_discount,\n",
    "        \n",
    "        -- TIER 2 RECOMMENDATIONS\n",
    "        CASE \n",
    "            WHEN t2_conversion_pct > 15 AND t2_margin_pct < target_margin * 100 * 0.80 \n",
    "                THEN 'INCREASE_QTY'\n",
    "            WHEN t1_conversion_pct > 10 AND t2_conversion_pct < 3 AND near_miss_t2_pct > 20 \n",
    "                THEN 'DECREASE_QTY'\n",
    "            WHEN t1_conversion_pct > 10 AND t2_conversion_pct < 3 AND near_miss_t2_pct <= 20 \n",
    "                THEN 'INCREASE_DISCOUNT'\n",
    "            WHEN prev_tier_2_qty > prev_tier_1_qty * 2 AND t2_conversion_pct < 5 \n",
    "                THEN 'REDUCE_RATIO'\n",
    "            ELSE 'NO_CHANGE'\n",
    "        END AS t2_action,\n",
    "        \n",
    "        CASE \n",
    "            WHEN t2_conversion_pct > 15 AND t2_margin_pct < target_margin * 100 * 0.80 \n",
    "                THEN CEIL(prev_tier_2_qty * 1.20)\n",
    "            WHEN t1_conversion_pct > 10 AND t2_conversion_pct < 3 AND near_miss_t2_pct > 20 \n",
    "                THEN CEIL(prev_tier_2_qty * 0.9)\n",
    "            WHEN prev_tier_2_qty > prev_tier_1_qty * 2 AND t2_conversion_pct < 5 \n",
    "                THEN CEIL(prev_tier_1_qty * 1.7)\n",
    "            ELSE prev_tier_2_qty\n",
    "        END AS suggested_t2_qty,\n",
    "        \n",
    "        CASE \n",
    "            WHEN t2_margin_pct < target_margin * 100 * 0.70 \n",
    "                THEN GREATEST(prev_tier_2_discount - 0.25, prev_tier_1_discount + 0.3)\n",
    "            WHEN t1_conversion_pct > 10 AND t2_conversion_pct < 3 AND near_miss_t2_pct <= 20 \n",
    "                THEN LEAST(prev_tier_2_discount + 0.25, 5.0)\n",
    "            ELSE prev_tier_2_discount\n",
    "        END AS suggested_t2_discount,\n",
    "        \n",
    "        -- TIER 3 RECOMMENDATIONS\n",
    "        CASE \n",
    "            WHEN t3_conversion_pct > 5 AND t3_margin_pct < target_margin * 100 * 0.50 \n",
    "                THEN 'INCREASE_QTY_OR_REDUCE_DISCOUNT'\n",
    "            WHEN t2_conversion_pct > 5 AND t3_conversion_pct < 1 AND near_miss_t3_pct > 25 \n",
    "                THEN 'DECREASE_QTY'\n",
    "            WHEN t3_conversion_pct = 0 AND prev_tier_3_qty IS NOT NULL \n",
    "                THEN 'DECREASE_QTY_OR_INCREASE_DISCOUNT'\n",
    "            ELSE 'NO_CHANGE'\n",
    "        END AS t3_action,\n",
    "        \n",
    "        CASE \n",
    "            WHEN t3_conversion_pct > 5 AND t3_margin_pct < target_margin * 100 * 0.50 \n",
    "                THEN CEIL(prev_tier_3_qty * 1.05)\n",
    "            WHEN t2_conversion_pct > 5 AND t3_conversion_pct < 1 AND near_miss_t3_pct > 25 \n",
    "                THEN CEIL(prev_tier_3_qty * 0.95)\n",
    "            WHEN t3_conversion_pct = 0 AND prev_tier_3_qty IS NOT NULL \n",
    "                THEN CEIL(prev_tier_3_qty * 0.9)\n",
    "            ELSE prev_tier_3_qty\n",
    "        END AS suggested_t3_qty,\n",
    "        \n",
    "        CASE \n",
    "            WHEN t3_margin_pct < target_margin * 100 * 0.30 \n",
    "                THEN GREATEST(prev_tier_3_discount - 0.25, prev_tier_2_discount + 0.3)\n",
    "            WHEN t3_conversion_pct = 0 AND prev_tier_3_qty IS NOT NULL \n",
    "                THEN LEAST(prev_tier_3_discount + 0.25, 6.0)\n",
    "            ELSE prev_tier_3_discount\n",
    "        END AS suggested_t3_discount\n",
    "        \n",
    "    FROM with_targets\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    warehouse_id,\n",
    "    product_id,\n",
    "    packing_unit_id,\n",
    "    sku,\n",
    "    brand,\n",
    "    cat,\n",
    "    total_orders,\n",
    "    total_nmv,\n",
    "    ROUND(target_margin * 100, 2) AS target_margin_pct,\n",
    "    blended_margin_pct,\n",
    "    margin_status,\n",
    "    base_pct,\n",
    "    t1_conversion_pct,\n",
    "    t2_conversion_pct,\n",
    "    t3_conversion_pct,\n",
    "    near_miss_t1_pct,\n",
    "    near_miss_t2_pct,\n",
    "    near_miss_t3_pct,\n",
    "    prev_tier_1_qty,\n",
    "    prev_tier_1_discount,\n",
    "    t1_action,\n",
    "    suggested_t1_qty,\n",
    "    suggested_t1_discount,\n",
    "    prev_tier_2_qty,\n",
    "    prev_tier_2_discount,\n",
    "    t2_action,\n",
    "    suggested_t2_qty,\n",
    "    suggested_t2_discount,\n",
    "    prev_tier_3_qty,\n",
    "    prev_tier_3_discount,\n",
    "    t3_action,\n",
    "    suggested_t3_qty,\n",
    "    suggested_t3_discount,\n",
    "    CASE \n",
    "        WHEN t1_action != 'NO_CHANGE' OR t2_action != 'NO_CHANGE' OR t3_action != 'NO_CHANGE' \n",
    "        THEN TRUE ELSE FALSE \n",
    "    END AS has_recommendation\n",
    "FROM feedback_recommendations\n",
    "ORDER BY \n",
    "    CASE margin_status \n",
    "        WHEN 'CRITICAL' THEN 1 \n",
    "        WHEN 'LOW' THEN 2 \n",
    "        WHEN 'HIGH' THEN 3 \n",
    "        ELSE 4 \n",
    "    END,\n",
    "    total_nmv DESC\n",
    "'''\n",
    "\n",
    "print(\"Fetching feedback from previous QD cycle...\")\n",
    "try:\n",
    "    feedback_data = snowflake_query(\"Egypt\", feedback_query)\n",
    "    \n",
    "    for col in feedback_data.columns:\n",
    "        feedback_data[col] = pd.to_numeric(feedback_data[col], errors='ignore')\n",
    "    \n",
    "    if len(feedback_data) > 0:\n",
    "        print(f\"✓ Got feedback for {len(feedback_data)} SKUs\")\n",
    "        print(f\"  - With recommendations: {feedback_data['has_recommendation'].sum()}\")\n",
    "        print(f\"  - T1 changes: {(feedback_data['t1_action'] != 'NO_CHANGE').sum()}\")\n",
    "        print(f\"  - T2 changes: {(feedback_data['t2_action'] != 'NO_CHANGE').sum()}\")\n",
    "        print(f\"  - T3 changes: {(feedback_data['t3_action'] != 'NO_CHANGE').sum()}\")\n",
    "        print(f\"  - Margin status: {feedback_data['margin_status'].value_counts().to_dict()}\")\n",
    "    else:\n",
    "        print(\"⚠ No feedback data available (no previous cycle ended today)\")\n",
    "        feedback_data = pd.DataFrame()\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Could not fetch feedback data: {e}\")\n",
    "    feedback_data = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74bfbf09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Feedback loop configuration loaded\n",
      "  - Minimum elasticity ratio: 1.1\n",
      "  - Constraints: T1 qty < T2 qty < T3 qty\n",
      "  - Constraints: T1 disc < T2 disc < T3 disc\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FEEDBACK LOOP CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Minimum elasticity ratio between tiers\n",
    "# Elasticity = (discount_ratio) / (qty_ratio) >= 1.1\n",
    "# This ensures discount increases proportionally more than quantity requirement\n",
    "MIN_ELASTICITY_RATIO = 1.1\n",
    "\n",
    "print(\"✓ Feedback loop configuration loaded\")\n",
    "print(f\"  - Minimum elasticity ratio: {MIN_ELASTICITY_RATIO}\")\n",
    "print(f\"  - Constraints: T1 qty < T2 qty < T3 qty\")\n",
    "print(f\"  - Constraints: T1 disc < T2 disc < T3 disc\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e25101",
   "metadata": {},
   "source": [
    "### Google Sheets Connection (Removed - No Longer Needed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5d55d8",
   "metadata": {},
   "source": [
    "## 2. Product Selection\n",
    "\n",
    "Select top-performing products per warehouse based on:\n",
    "- Gross profit ranking (40% weight)\n",
    "- Sales velocity ranking (25% weight)\n",
    "- Order count ranking (20% weight)\n",
    "- Retailer count ranking (15% weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "225abe85-5954-48b3-97ac-dd49d3672c38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching product selection data...\n",
      "✓ Retrieved 4205 products from 15 warehouses\n"
     ]
    }
   ],
   "source": [
    "query = f''' \n",
    "WITH rr AS (\n",
    "    SELECT product_id, warehouse_id, rr\n",
    "    FROM (\n",
    "        SELECT *, \n",
    "               MAX(date) OVER (PARTITION BY product_id, warehouse_id) as max_date\n",
    "        FROM finance.PREDICTED_RUNNING_RATES\n",
    "        QUALIFY date = max_date\n",
    "            AND date::date >= CURRENT_DATE - 14 \n",
    "    )\n",
    "),\n",
    "\n",
    "stocks AS (\n",
    "    SELECT \n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        SUM(stocks) as stocks,\n",
    "        CASE \n",
    "            WHEN SUM(rr) > 0 THEN SUM(stocks) / SUM(rr) \n",
    "            ELSE SUM(stocks) \n",
    "        END as doh\n",
    "    FROM (\n",
    "        SELECT DISTINCT \n",
    "            product_warehouse.warehouse_id,\n",
    "            product_warehouse.product_id,\n",
    "            (product_warehouse.available_stock)::integer as stocks,\n",
    "            COALESCE(rr.rr, 0) as rr \n",
    "        FROM product_warehouse\n",
    "        JOIN products ON product_warehouse.product_id = products.id\n",
    "        JOIN product_units ON products.unit_id = product_units.id\n",
    "        LEFT JOIN rr ON rr.product_id = products.id \n",
    "            AND rr.warehouse_id = product_warehouse.warehouse_id\n",
    "        WHERE product_warehouse.warehouse_id NOT IN (6, 9, 10)\n",
    "            AND product_warehouse.is_basic_unit = 1\n",
    "            AND product_warehouse.available_stock > 0 \n",
    "    )\n",
    "    GROUP BY warehouse_id, product_id\n",
    "    HAVING doh >= 1\n",
    "),\n",
    "\n",
    "base AS (\n",
    "    SELECT *, ROW_NUMBER() OVER (PARTITION BY retailer_id ORDER BY priority) as rnk \n",
    "    FROM (\n",
    "        SELECT x.*, TAGGABLE_ID as retailer_id \n",
    "        FROM (\n",
    "            SELECT id as cohort_id, name as cohort_name, priority, dynamic_tag_id \n",
    "            FROM cohorts \n",
    "            WHERE is_active = 'true'\n",
    "                AND id IN (700,701,702,703,704,1123,1124,1125,1126)\n",
    "        ) x \n",
    "        JOIN DYNAMIC_TAGgables dt ON x.dynamic_tag_id = dt.dynamic_tag_id\n",
    "        WHERE dt.taggable_id not IN (\n",
    "            SELECT taggable_id FROM DYNAMIC_TAGgables \n",
    "            WHERE dynamic_tag_id IN (2807, 2808, 2809, 2810, 2811, 2812)\n",
    "        )\n",
    "    )\n",
    "    QUALIFY rnk = 1 \n",
    "    ORDER BY cohort_id\n",
    "),\n",
    "\n",
    "warehouse_retailer_counts AS (\n",
    "    SELECT \n",
    "        whs.warehouse_id,\n",
    "        COUNT(DISTINCT base.retailer_id) as total_warehouse_retailers\n",
    "    FROM base\n",
    "    CROSS JOIN (SELECT DISTINCT warehouse_id FROM (VALUES\n",
    "            (38), (1), (236), (962), (337), (8), (339), (170), \n",
    "            (501), (401), (703), (632), (797)\n",
    "        ) x(warehouse_id)\n",
    "    ) whs\n",
    "    GROUP BY whs.warehouse_id\n",
    "),\n",
    "\n",
    "cohort_warehouse_map AS (\n",
    "    SELECT cohort_id, warehouse_id\n",
    "    FROM (VALUES\n",
    "        (700, 38), (700, 1), (701, 236), (701, 962),\n",
    "        (703, 337), (703, 8), (704, 339), (704, 170),\n",
    "        (1124, 501), (1126, 401), (1123, 703), (1125, 632), (702, 797)\n",
    "    ) x(cohort_id, warehouse_id)\n",
    "),\n",
    "\n",
    "cohort_prices AS (\n",
    "    SELECT  \n",
    "        cpu.cohort_id, pu.product_id, pu.packing_unit_id,\n",
    "        pu.basic_unit_count, AVG(cpu.price) as price\n",
    "    FROM cohort_product_packing_units cpu\n",
    "    JOIN PACKING_UNIT_PRODUCTS pu ON pu.id = cpu.product_packing_unit_id\n",
    "    WHERE cpu.cohort_id IN (700,701,702,703,704,1123,1124,1125,1126)\n",
    "        AND cpu.created_at::date <> '2023-07-31'\n",
    "        AND cpu.is_customized = true\n",
    "    GROUP BY cpu.cohort_id, pu.product_id, pu.packing_unit_id, pu.basic_unit_count\n",
    "),\n",
    "\n",
    "live_cohort_prices AS (\n",
    "select cohort_id, product_id, pu_id as packing_unit_id,\n",
    "        buc as basic_unit_count, NEW_PRICE as price\n",
    "from materialized_views.DBDP_PRICES\n",
    "where created_at = current_date\n",
    "and DATE_PART('hour',CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMEstamp())) BETWEEN SPLIT_PART(time_slot, '-', 1)::int AND (SPLIT_PART(time_slot, '-', 1)::int)+1\n",
    "and cohort_id in (700,701,702,703,704,696,695,698,697,699,1123,1124,1125,1126)\n",
    "),\n",
    "\n",
    "combined_cohort_prices AS (\n",
    "    SELECT *\n",
    "    FROM (\n",
    "        SELECT *, 1 AS priority FROM live_cohort_prices\n",
    "        UNION ALL\n",
    "        SELECT *, 2 AS priority FROM cohort_prices\n",
    "    )\n",
    "    QUALIFY ROW_NUMBER() OVER (PARTITION BY cohort_id, product_id, packing_unit_id ORDER BY priority) = 1\n",
    "),\n",
    "\n",
    "warehouse_prices AS (\n",
    "    SELECT \n",
    "        cwm.warehouse_id, ccp.product_id, ccp.packing_unit_id,\n",
    "        ccp.basic_unit_count, ccp.price\n",
    "    FROM combined_cohort_prices ccp\n",
    "    JOIN cohort_warehouse_map cwm ON cwm.cohort_id = ccp.cohort_id\n",
    "    WHERE ccp.price IS NOT NULL\n",
    "),\n",
    "\n",
    "product_performance AS (\n",
    "    SELECT \n",
    "        w.name as warehouse,\n",
    "        w.id as warehouse_id,\n",
    "        pso.product_id,\n",
    "        pso.packing_unit_id,\n",
    "        CONCAT(products.name_ar, ' ', products.size, ' ', product_units.name_ar) as sku,\n",
    "        brands.name_ar as brand,\n",
    "        categories.name_ar as category,\n",
    "        COUNT(DISTINCT so.parent_sales_order_id) as total_orders,\n",
    "        COUNT(DISTINCT so.retailer_id) as total_retailers,\n",
    "        SUM(pso.purchased_item_count) as total_packing_units_sold,\n",
    "        SUM(pso.purchased_item_count * pso.basic_unit_count) as total_basic_units_sold,\n",
    "        SUM(pso.total_price) as total_nmv,\n",
    "        SUM(COALESCE(f.wac_p, 0) * pso.purchased_item_count * pso.basic_unit_count) as total_cogs,\n",
    "        (SUM(pso.total_price) - SUM(COALESCE(f.wac_p, 0) * pso.purchased_item_count * pso.basic_unit_count)) / \n",
    "            NULLIF(SUM(pso.total_price), 0) as blended_margin,\n",
    "        AVG(pso.purchased_item_count) as avg_packing_units_per_order,\n",
    "        SUM(pso.purchased_item_count) / 120.0 as packing_units_per_day,\n",
    "        -- New metrics for qty potential\n",
    "        STDDEV(pso.purchased_item_count) as order_qty_stddev,\n",
    "        MAX(pso.purchased_item_count) as max_order_qty,\n",
    "        COUNT(DISTINCT so.parent_sales_order_id) / NULLIF(COUNT(DISTINCT so.retailer_id), 0) as orders_per_retailer\n",
    "    FROM product_sales_order pso\n",
    "    JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "    JOIN base ON base.retailer_id = so.retailer_id\n",
    "    JOIN products ON products.id = pso.product_id\n",
    "    JOIN brands ON products.brand_id = brands.id\n",
    "    JOIN categories ON products.category_id = categories.id \n",
    "        AND categories.name_ar NOT LIKE '%سايب%'\n",
    "    JOIN finance.all_cogs f ON f.product_id = pso.product_id\n",
    "        AND f.from_date::date <= so.created_at::date\n",
    "        AND f.to_date::date > so.created_at::date\n",
    "    JOIN product_units ON product_units.id = products.unit_id\n",
    "    JOIN warehouses w ON w.id = pso.warehouse_id\n",
    "    WHERE so.created_at::date BETWEEN current_date - 60 AND CURRENT_DATE - 1\n",
    "        AND so.sales_order_status_id NOT IN (7, 12)\n",
    "        AND so.channel IN ('telesales', 'retailer')\n",
    "        AND pso.purchased_item_count <> 0\n",
    "        AND products.activation = 'true'\n",
    "        and products.id <> 11794\n",
    "        AND w.id NOT IN (6, 9, 10)\n",
    "    GROUP BY ALL\n",
    "),\n",
    "\n",
    "-- Category benchmarks for qty potential comparison\n",
    "category_benchmarks AS (\n",
    "    SELECT \n",
    "        category,\n",
    "        AVG(avg_packing_units_per_order) as category_avg_qty\n",
    "    FROM product_performance\n",
    "    GROUP BY category\n",
    "),\n",
    "\n",
    "product_performance_with_penetration AS (\n",
    "    SELECT \n",
    "        pp.*,\n",
    "        wrc.total_warehouse_retailers,\n",
    "        (pp.total_retailers * 100.0 / NULLIF(wrc.total_warehouse_retailers, 0)) as retailer_penetration_pct,\n",
    "        cb.category_avg_qty\n",
    "    FROM product_performance pp\n",
    "    LEFT JOIN warehouse_retailer_counts wrc ON wrc.warehouse_id = pp.warehouse_id\n",
    "    LEFT JOIN category_benchmarks cb ON cb.category = pp.category\n",
    "),\n",
    "\n",
    "product_performance_with_price AS (\n",
    "    SELECT \n",
    "        pp.*,\n",
    "        COALESCE(wp.price, 0) as product_price,\n",
    "        COALESCE(wp.basic_unit_count, 1) as basic_unit_count\n",
    "    FROM product_performance_with_penetration pp\n",
    "    LEFT JOIN warehouse_prices wp ON wp.warehouse_id = pp.warehouse_id\n",
    "        AND wp.product_id = pp.product_id \n",
    "        AND wp.packing_unit_id = pp.packing_unit_id\n",
    "),\n",
    "\n",
    "qualified_products AS (\n",
    "    SELECT \n",
    "        pp.warehouse,\n",
    "        pp.warehouse_id,\n",
    "        pp.product_id,\n",
    "        pp.packing_unit_id,\n",
    "        pp.sku,\n",
    "        pp.brand,\n",
    "        pp.category,\n",
    "        pp.total_orders,\n",
    "        pp.total_retailers,\n",
    "        pp.total_packing_units_sold,\n",
    "        pp.total_basic_units_sold,\n",
    "        pp.total_nmv,\n",
    "        pp.blended_margin,\n",
    "        pp.avg_packing_units_per_order,\n",
    "        pp.packing_units_per_day,\n",
    "        pp.retailer_penetration_pct,\n",
    "        pp.product_price,\n",
    "        pp.basic_unit_count,\n",
    "        pp.order_qty_stddev,\n",
    "        pp.max_order_qty,\n",
    "        pp.orders_per_retailer,\n",
    "        pp.category_avg_qty,\n",
    "        s.doh,\n",
    "        s.stocks,\n",
    "        (pp.total_nmv * pp.blended_margin) as gross_profit,\n",
    "        \n",
    "        -- Qty Potential Score (max 3.0, min 1.0)\n",
    "        (\n",
    "            -- Factor 1: Room to grow vs category (30%)\n",
    "            (CASE \n",
    "                WHEN pp.avg_packing_units_per_order < pp.category_avg_qty * 0.7 THEN 3\n",
    "                WHEN pp.avg_packing_units_per_order < pp.category_avg_qty THEN 2\n",
    "                ELSE 1\n",
    "            END) * 0.30\n",
    "            \n",
    "            -- Factor 2: Repeat purchase pattern (30%)\n",
    "            + (CASE \n",
    "                WHEN pp.orders_per_retailer >= 3 THEN 3\n",
    "                WHEN pp.orders_per_retailer >= 1.5 THEN 2\n",
    "                ELSE 1\n",
    "            END) * 0.30\n",
    "            \n",
    "            -- Factor 3: Current avg qty headroom (25%)\n",
    "            + (CASE \n",
    "                WHEN pp.avg_packing_units_per_order < 2 THEN 3\n",
    "                WHEN pp.avg_packing_units_per_order < 4 THEN 2\n",
    "                ELSE 1\n",
    "            END) * 0.25\n",
    "            \n",
    "            -- Factor 4: Order qty variance - flexibility (15%)\n",
    "            + (CASE \n",
    "                WHEN pp.order_qty_stddev > pp.avg_packing_units_per_order * 0.5 THEN 3\n",
    "                WHEN pp.order_qty_stddev > pp.avg_packing_units_per_order * 0.25 THEN 2\n",
    "                ELSE 1\n",
    "            END) * 0.15\n",
    "        ) as qty_potential_score,\n",
    "        \n",
    "        ROW_NUMBER() OVER (PARTITION BY pp.warehouse_id ORDER BY (pp.total_nmv * pp.blended_margin) DESC) as gp_rank,\n",
    "        ROW_NUMBER() OVER (PARTITION BY pp.warehouse_id ORDER BY pp.packing_units_per_day DESC) as velocity_rank,\n",
    "        ROW_NUMBER() OVER (PARTITION BY pp.warehouse_id ORDER BY pp.total_orders DESC) as order_rank,\n",
    "        ROW_NUMBER() OVER (PARTITION BY pp.warehouse_id ORDER BY pp.total_retailers DESC) as retailer_rank\n",
    "    FROM product_performance_with_price pp\n",
    "    JOIN stocks s ON s.product_id = pp.product_id \n",
    "        AND s.warehouse_id = pp.warehouse_id\n",
    "),\n",
    "\n",
    "top_products AS (\n",
    "    SELECT \n",
    "        warehouse,\n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        sku,\n",
    "        brand,\n",
    "        category,\n",
    "        total_orders,\n",
    "        total_retailers,\n",
    "        total_packing_units_sold,\n",
    "        total_basic_units_sold,\n",
    "        ROUND(total_nmv, 2) as total_nmv,\n",
    "        ROUND(blended_margin * 100, 2) as margin_pct,\n",
    "        ROUND(avg_packing_units_per_order, 2) as avg_order_qty,\n",
    "        ROUND(packing_units_per_day, 2) as units_per_day,\n",
    "        ROUND(retailer_penetration_pct, 1) as retailer_penetration_pct,\n",
    "        ROUND(gross_profit, 2) as gross_profit,\n",
    "        ROUND(product_price, 2) as packing_unit_price,\n",
    "        basic_unit_count,\n",
    "        ROUND(product_price / NULLIF(basic_unit_count, 0), 2) as price_per_basic_unit,\n",
    "        gp_rank,\n",
    "        velocity_rank,\n",
    "        order_rank,\n",
    "        retailer_rank,\n",
    "        ROUND(doh, 2) as days_on_hand,\n",
    "        stocks as available_stock,\n",
    "        (gp_rank * 0.25 + velocity_rank * 0.1 + order_rank * 0.3 + retailer_rank * 0.35) as combined_rank_score,\n",
    "        \n",
    "        -- Qty potential metrics\n",
    "        ROUND(orders_per_retailer, 2) as orders_per_retailer,\n",
    "        ROUND(order_qty_stddev, 2) as order_qty_stddev,\n",
    "        max_order_qty,\n",
    "        ROUND(category_avg_qty, 2) as category_avg_qty,\n",
    "        ROUND(qty_potential_score, 2) as qty_potential_score,\n",
    "        \n",
    "        -- Brand rank\n",
    "        ROW_NUMBER() OVER (\n",
    "            PARTITION BY warehouse_id, brand \n",
    "            ORDER BY (gp_rank * 0.25 + velocity_rank * 0.1 + order_rank * 0.3 + retailer_rank * 0.35)\n",
    "        ) as brand_rank\n",
    "        \n",
    "    FROM qualified_products\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    warehouse,\n",
    "    warehouse_id,\n",
    "    product_id,\n",
    "    packing_unit_id,\n",
    "    sku,\n",
    "    brand,\n",
    "    category as cat,\n",
    "    total_orders,\n",
    "    total_retailers,\n",
    "    total_packing_units_sold,\n",
    "    total_basic_units_sold,\n",
    "    total_nmv,\n",
    "    margin_pct,\n",
    "    avg_order_qty,\n",
    "    units_per_day,\n",
    "    retailer_penetration_pct,\n",
    "    gross_profit,\n",
    "    packing_unit_price,\n",
    "    basic_unit_count,\n",
    "    price_per_basic_unit,\n",
    "    days_on_hand,\n",
    "    available_stock,\n",
    "    gp_rank as gross_profit_rank,\n",
    "    velocity_rank,\n",
    "    order_rank,\n",
    "    retailer_rank,\n",
    "    brand_rank,\n",
    "    -- Qty potential columns\n",
    "    orders_per_retailer,\n",
    "    order_qty_stddev,\n",
    "    max_order_qty,\n",
    "    category_avg_qty,\n",
    "    qty_potential_score,\n",
    "    ROUND(combined_rank_score, 2) as combined_score,\n",
    "    ROW_NUMBER() OVER (PARTITION BY warehouse ORDER BY combined_rank_score) as final_rank\n",
    "FROM top_products\n",
    "WHERE combined_rank_score <= 500\n",
    "  AND brand_rank <= 5  -- Maximum 5 products per brand per warehouse\n",
    "QUALIFY final_rank <= 500\n",
    "ORDER BY warehouse, combined_rank_score\n",
    "'''\n",
    "\n",
    "# Execute query and convert numeric columns\n",
    "print(\"Fetching product selection data...\")\n",
    "selected_products = snowflake_query(\"Egypt\", query)\n",
    "\n",
    "for col in selected_products.columns:\n",
    "    selected_products[col] = pd.to_numeric(selected_products[col], errors='ignore')\n",
    "\n",
    "print(f\"✓ Retrieved {len(selected_products)} products from {selected_products['warehouse_id'].nunique()} warehouses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "489552af-531c-49c3-b495-88dce04f56ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Selected 4138 products after exclusions\n"
     ]
    }
   ],
   "source": [
    "# Remove excluded products\n",
    "selected_products = selected_products[~selected_products['product_id'].isin(PRODUCTS_TO_REMOVE)]\n",
    "selected_products = selected_products[~selected_products['cat'].isin(CATS_TO_REMOVE)]\n",
    "selected_products = selected_products[~selected_products['brand'].isin(BRANDS_TO_REMOVE)]\n",
    "print(f\"✓ Selected {len(selected_products)} products after exclusions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e424fc72",
   "metadata": {},
   "source": [
    "## 3. Quantity Tier Calculation\n",
    "\n",
    "Calculate tier 1 and tier 2 quantities based on:\n",
    "- Order history from frequent buyers (2+ orders)\n",
    "- Statistical analysis (median, Q3, P85, P90, P95)\n",
    "- IQR outlier removal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e5d7495-748e-4337-92ae-aaa77e3b8398",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching quantity tier data...\n",
      "✓ Calculated tiers for 3955 product-warehouse combinations\n"
     ]
    }
   ],
   "source": [
    "selected_df = selected_products[['warehouse_id', 'product_id', 'packing_unit_id']].values.tolist()\n",
    "tuples_string = ','.join([f\"({int(wh_id)}, {int(prod_id)}, {int(pu_id)})\" for wh_id, prod_id, pu_id in selected_df])\n",
    "query = f'''\n",
    "WITH selected_products AS (\n",
    "    SELECT warehouse_id, product_id, packing_unit_id\n",
    "    FROM (VALUES\n",
    "      {tuples_string}\n",
    "    ) AS x(warehouse_id, product_id, packing_unit_id)\n",
    "),\n",
    "\n",
    "-- Same base filtering as product selection query\n",
    "-- Retailers in QD cohorts AND in specific dynamic tags\n",
    "base AS (\n",
    "    SELECT *, ROW_NUMBER() OVER (PARTITION BY retailer_id ORDER BY priority) as rnk \n",
    "    FROM (\n",
    "        SELECT x.*, TAGGABLE_ID as retailer_id \n",
    "        FROM (\n",
    "            SELECT id as cohort_id, name as cohort_name, priority, dynamic_tag_id \n",
    "            FROM cohorts \n",
    "            WHERE is_active = 'true'\n",
    "                AND id IN (700,701,702,703,704,1123,1124,1125,1126)\n",
    "        ) x \n",
    "        JOIN DYNAMIC_TAGgables dt ON x.dynamic_tag_id = dt.dynamic_tag_id\n",
    "        WHERE dt.taggable_id not IN (\n",
    "            SELECT taggable_id FROM DYNAMIC_TAGgables \n",
    "            WHERE dynamic_tag_id IN (2807, 2808, 2809, 2810, 2811, 2812)\n",
    "        )\n",
    "    )\n",
    "    QUALIFY rnk = 1 \n",
    "),\n",
    "\n",
    "raw_order_quantities AS (\n",
    "    SELECT \n",
    "        whs.wh as warehouse,\n",
    "        whs.warehouse_id,\n",
    "        pso.product_id,\n",
    "        pso.packing_unit_id,\n",
    "        CONCAT(products.name_ar, ' ', products.size, ' ', product_units.name_ar) as sku,\n",
    "        brands.name_ar as brand,\n",
    "        categories.name_ar as category,\n",
    "        so.parent_sales_order_id,\n",
    "        so.retailer_id,\n",
    "        so.created_at::date as order_date,\n",
    "        SUM(pso.purchased_item_count) as order_qty,\n",
    "        SUM(pso.total_price) as order_value,\n",
    "        -- ADD RECENCY WEIGHT: Recent orders get higher weight (exponential decay)\n",
    "        EXP(-0.02 * DATEDIFF('day', so.created_at::date, CURRENT_DATE)) as recency_weight\n",
    "        \n",
    "    FROM product_sales_order pso\n",
    "    JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "    -- Filter to only include retailers from base (same cohorts + tags as product selection)\n",
    "    JOIN base ON base.retailer_id = so.retailer_id\n",
    "    JOIN products ON products.id = pso.product_id\n",
    "    JOIN brands ON products.brand_id = brands.id\n",
    "    JOIN categories ON products.category_id = categories.id\n",
    "    JOIN product_units ON product_units.id = products.unit_id\n",
    "    JOIN materialized_views.retailer_polygon ON materialized_views.retailer_polygon.retailer_id = so.retailer_id\n",
    "    JOIN districts ON districts.id = materialized_views.retailer_polygon.district_id\n",
    "    JOIN cities ON cities.id = districts.city_id\n",
    "    JOIN states ON states.id = cities.state_id\n",
    "    JOIN regions ON regions.id = states.region_id\n",
    "    JOIN (SELECT * FROM (VALUES\n",
    "            ('Cairo', 'El-Marg', 38),\n",
    "            ('Cairo', 'Mostorod', 1),\n",
    "            ('Giza', 'Barageel', 236),\n",
    "            ('Giza', 'Sakkarah', 962),\n",
    "            ('Delta West', 'El-Mahala', 337),\n",
    "            ('Delta West', 'Tanta', 8),\n",
    "            ('Delta East', 'Mansoura FC', 339),\n",
    "            ('Delta East', 'Sharqya', 170),\n",
    "            ('Upper Egypt', 'Assiut FC', 501),\n",
    "            ('Upper Egypt', 'Bani sweif', 401),\n",
    "            ('Upper Egypt', 'Menya Samalot', 703),\n",
    "            ('Upper Egypt', 'Sohag', 632),\n",
    "            ('Alexandria', 'Khorshed Alex', 797)\n",
    "        ) x(region_name, wh, warehouse_id)\n",
    "    ) whs ON whs.region_name = CASE WHEN regions.id = 2 THEN states.name_en ELSE regions.name_en END\n",
    "    JOIN selected_products sp ON sp.warehouse_id = whs.warehouse_id \n",
    "        AND sp.product_id = pso.product_id\n",
    "        AND sp.packing_unit_id = pso.packing_unit_id\n",
    "    \n",
    "    WHERE TRUE\n",
    "        AND so.created_at::date BETWEEN DATE_TRUNC('month', CURRENT_DATE - INTERVAL '4 months') AND CURRENT_DATE - 1\n",
    "        AND so.sales_order_status_id NOT IN (7, 12)\n",
    "        AND so.channel IN ('telesales', 'retailer')\n",
    "        AND pso.purchased_item_count <> 0\n",
    "        AND products.activation = 'true'\n",
    "    \n",
    "    GROUP BY \n",
    "        whs.wh,\n",
    "        whs.warehouse_id,\n",
    "        pso.product_id,\n",
    "        pso.packing_unit_id,\n",
    "        products.name_ar,\n",
    "        products.size,\n",
    "        product_units.name_ar,\n",
    "        brands.name_ar,\n",
    "        categories.name_ar,\n",
    "        so.parent_sales_order_id,\n",
    "        so.retailer_id,\n",
    "        so.created_at::date\n",
    "),\n",
    "\n",
    "retailer_frequency AS (\n",
    "    SELECT \n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        retailer_id,\n",
    "        COUNT(DISTINCT parent_sales_order_id) as order_count,\n",
    "        COUNT(DISTINCT DATE_TRUNC('week', order_date)) as weeks_ordered,\n",
    "        MIN(order_date) as first_order_date,\n",
    "        MAX(order_date) as last_order_date,\n",
    "        DATEDIFF('day', MIN(order_date), MAX(order_date)) as days_span,\n",
    "        CASE \n",
    "            WHEN COUNT(DISTINCT parent_sales_order_id) > 1 \n",
    "            THEN DATEDIFF('day', MIN(order_date), MAX(order_date)) / (COUNT(DISTINCT parent_sales_order_id) - 1)\n",
    "            ELSE NULL \n",
    "        END as avg_days_between_orders\n",
    "    FROM raw_order_quantities\n",
    "    GROUP BY warehouse_id, product_id, packing_unit_id, retailer_id\n",
    "),\n",
    "\n",
    "frequent_buyers AS (\n",
    "    SELECT \n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        retailer_id,\n",
    "        order_count,\n",
    "        weeks_ordered,\n",
    "        avg_days_between_orders\n",
    "    FROM retailer_frequency\n",
    "    WHERE order_count >= 2 \n",
    "       OR weeks_ordered >= 2\n",
    "),\n",
    "\n",
    "filtered_orders AS (\n",
    "    SELECT roq.*\n",
    "    FROM raw_order_quantities roq\n",
    "    JOIN frequent_buyers fb \n",
    "        ON fb.warehouse_id = roq.warehouse_id\n",
    "        AND fb.product_id = roq.product_id\n",
    "        AND fb.packing_unit_id = roq.packing_unit_id\n",
    "        AND fb.retailer_id = roq.retailer_id\n",
    "),\n",
    "\n",
    "initial_stats AS (\n",
    "    SELECT \n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY order_qty) as q1,\n",
    "        PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY order_qty) as q3,\n",
    "        MEDIAN(order_qty) as median_qty,\n",
    "        STDDEV_POP(order_qty) as stddev_qty,\n",
    "        AVG(order_qty) as avg_qty\n",
    "    FROM filtered_orders\n",
    "    GROUP BY warehouse_id, product_id, packing_unit_id\n",
    "),\n",
    "\n",
    "cleaned_orders AS (\n",
    "    SELECT fo.*\n",
    "    FROM filtered_orders fo\n",
    "    JOIN initial_stats ist \n",
    "        ON ist.warehouse_id = fo.warehouse_id\n",
    "        AND ist.product_id = fo.product_id\n",
    "        AND ist.packing_unit_id = fo.packing_unit_id\n",
    "    WHERE TRUE\n",
    "        AND fo.order_qty >= ist.q1 - 1.5 * (ist.q3 - ist.q1)\n",
    "        AND fo.order_qty <= ist.q3 + 1.5 * (ist.q3 - ist.q1)\n",
    "        AND (ist.stddev_qty = 0 \n",
    "             OR ABS(fo.order_qty - ist.avg_qty) <= 3 * ist.stddev_qty)\n",
    "),\n",
    "\n",
    "-- MODIFIED: Recent orders stats (last 15 days)\n",
    "recent_trends AS (\n",
    "    SELECT \n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        -- Weighted average gives more importance to recent orders\n",
    "        SUM(order_qty * recency_weight) / NULLIF(SUM(recency_weight), 0) as weighted_avg_qty,\n",
    "        -- Last 15 days statistics\n",
    "        AVG(CASE WHEN order_date >= CURRENT_DATE - 15 THEN order_qty END) as last_15d_avg,\n",
    "        MEDIAN(CASE WHEN order_date >= CURRENT_DATE - 15 THEN order_qty END) as last_15d_median,\n",
    "        MAX(CASE WHEN order_date >= CURRENT_DATE - 15 THEN order_qty END) as last_15d_max,\n",
    "        COUNT(CASE WHEN order_date >= CURRENT_DATE - 15 THEN 1 END) as last_15d_orders\n",
    "    FROM cleaned_orders\n",
    "    GROUP BY warehouse_id, product_id, packing_unit_id\n",
    "),\n",
    "\n",
    "quantity_stats AS (\n",
    "    SELECT \n",
    "        warehouse,\n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        sku,\n",
    "        brand,\n",
    "        category,\n",
    "        \n",
    "        COUNT(DISTINCT parent_sales_order_id) as total_orders,\n",
    "        COUNT(DISTINCT retailer_id) as total_retailers,\n",
    "        \n",
    "        MIN(order_qty) as min_qty,\n",
    "        MAX(order_qty) as max_qty,\n",
    "        AVG(order_qty) as avg_qty,\n",
    "        MEDIAN(order_qty) as median_qty,\n",
    "        STDDEV_POP(order_qty) as stddev_qty,\n",
    "        \n",
    "        PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY order_qty) as q1_qty,\n",
    "        PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY order_qty) as q2_qty,\n",
    "        PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY order_qty) as q3_qty,\n",
    "        PERCENTILE_CONT(0.85) WITHIN GROUP (ORDER BY order_qty) as p85_qty,\n",
    "        PERCENTILE_CONT(0.90) WITHIN GROUP (ORDER BY order_qty) as p90_qty,\n",
    "        PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY order_qty) as p95_qty,\n",
    "        \n",
    "        SUM(order_value) as total_revenue,\n",
    "        AVG(order_value) as avg_order_value\n",
    "        \n",
    "    FROM cleaned_orders\n",
    "    GROUP BY \n",
    "        warehouse,\n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        sku,\n",
    "        brand,\n",
    "        category\n",
    "),\n",
    "\n",
    "frequency_table AS (\n",
    "    SELECT\n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        order_qty,\n",
    "        COUNT(DISTINCT parent_sales_order_id) AS freq\n",
    "    FROM cleaned_orders\n",
    "    GROUP BY warehouse_id, product_id, packing_unit_id, order_qty\n",
    "),\n",
    "\n",
    "lag_lead AS (\n",
    "    SELECT\n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        order_qty,\n",
    "        freq,\n",
    "        LAG(freq) OVER (PARTITION BY warehouse_id, product_id, packing_unit_id ORDER BY order_qty) AS prev_freq,\n",
    "        LEAD(freq) OVER (PARTITION BY warehouse_id, product_id, packing_unit_id ORDER BY order_qty) AS next_freq\n",
    "    FROM frequency_table\n",
    "),\n",
    "\n",
    "most_frequent_qty AS (\n",
    "    SELECT \n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        order_qty as mode_qty,\n",
    "        freq as mode_freq,\n",
    "        freq * 1.0 / SUM(freq) OVER (PARTITION BY warehouse_id, product_id, packing_unit_id) as mode_contribution\n",
    "    FROM (\n",
    "        SELECT *,\n",
    "               ROW_NUMBER() OVER (PARTITION BY warehouse_id, product_id, packing_unit_id ORDER BY freq DESC, order_qty DESC) as rn\n",
    "        FROM lag_lead\n",
    "        WHERE (freq > COALESCE(prev_freq, -1))\n",
    "          AND (freq > COALESCE(next_freq, -1))\n",
    "    )\n",
    "    WHERE rn = 1\n",
    "),\n",
    "\n",
    "frequency_metrics AS (\n",
    "    SELECT \n",
    "        fb.warehouse_id,\n",
    "        fb.product_id,\n",
    "        fb.packing_unit_id,\n",
    "        COUNT(DISTINCT fb.retailer_id) as frequent_retailer_count,\n",
    "        AVG(fb.order_count) as avg_orders_per_retailer,\n",
    "        AVG(fb.avg_days_between_orders) as avg_refill_days,\n",
    "        MEDIAN(fb.avg_days_between_orders) as median_refill_days\n",
    "    FROM frequent_buyers fb\n",
    "    GROUP BY fb.warehouse_id, fb.product_id, fb.packing_unit_id\n",
    "),\n",
    "\n",
    "tier_calculations AS (\n",
    "    SELECT \n",
    "        qs.*,\n",
    "        COALESCE(mf.mode_qty, qs.median_qty) as mode_qty,\n",
    "        COALESCE(mf.mode_freq, 0) as mode_freq,\n",
    "        COALESCE(mf.mode_contribution, 0) as mode_contribution,\n",
    "        COALESCE(fm.frequent_retailer_count, 0) as frequent_retailer_count,\n",
    "        COALESCE(fm.avg_orders_per_retailer, 0) as avg_orders_per_retailer,\n",
    "        COALESCE(fm.avg_refill_days, 0) as avg_refill_days,\n",
    "        COALESCE(fm.median_refill_days, 0) as median_refill_days,\n",
    "        \n",
    "        -- ADD: Recency metrics\n",
    "        rt.weighted_avg_qty,\n",
    "        rt.last_15d_avg,\n",
    "        rt.last_15d_median,\n",
    "        rt.last_15d_max,\n",
    "        rt.last_15d_orders,\n",
    "        \n",
    "        -- MODIFIED: Tier 1 with 15-day recency factor\n",
    "        -- Blends historical median with recent trends (70% historical, 30% recent)\n",
    "        CEIL(GREATEST(\n",
    "            (0.7 * qs.median_qty + 0.3 * COALESCE(rt.weighted_avg_qty, qs.median_qty)) + 1.0 * COALESCE(qs.stddev_qty, 1),\n",
    "            qs.q3_qty,\n",
    "            COALESCE(mf.mode_qty, qs.median_qty) + GREATEST(3, qs.median_qty * 0.3),\n",
    "            -- If recent 15 days show growth, adjust upward\n",
    "            CASE \n",
    "                WHEN rt.last_15d_orders >= 2 AND rt.last_15d_median > qs.median_qty \n",
    "                THEN rt.last_15d_median * 1.2\n",
    "                ELSE qs.median_qty * 1.3\n",
    "            END,\n",
    "            qs.median_qty + 2\n",
    "        )) as tier_1_qty,\n",
    "        \n",
    "        -- MODIFIED: Tier 2 with 15-day recency factor\n",
    "        CEIL(GREATEST(\n",
    "            qs.q3_qty + 1.5 * COALESCE(qs.stddev_qty, 1),\n",
    "            qs.p85_qty + 1.0 * COALESCE(qs.stddev_qty, 1),\n",
    "            qs.p90_qty + 0.5 * COALESCE(qs.stddev_qty, 1),\n",
    "            qs.p95_qty,\n",
    "            -- Blend historical and weighted average\n",
    "            (0.6 * qs.median_qty + 0.4 * COALESCE(rt.weighted_avg_qty, qs.median_qty)) * 2.0,\n",
    "            -- If last 15 days show higher demand, adjust tier 2 upward\n",
    "            CASE \n",
    "                WHEN rt.last_15d_orders >= 2 AND rt.last_15d_max > qs.p90_qty \n",
    "                THEN rt.last_15d_max * 1.1\n",
    "                ELSE qs.median_qty * 1.6\n",
    "            END,\n",
    "            tier_1_qty*1.3\n",
    "        )) as tier_2_qty_base\n",
    "        \n",
    "    FROM quantity_stats qs\n",
    "    LEFT JOIN most_frequent_qty mf \n",
    "        ON mf.warehouse_id = qs.warehouse_id \n",
    "        AND mf.product_id = qs.product_id\n",
    "        AND mf.packing_unit_id = qs.packing_unit_id\n",
    "    LEFT JOIN frequency_metrics fm\n",
    "        ON fm.warehouse_id = qs.warehouse_id\n",
    "        AND fm.product_id = qs.product_id\n",
    "        AND fm.packing_unit_id = qs.packing_unit_id\n",
    "    LEFT JOIN recent_trends rt\n",
    "        ON rt.warehouse_id = qs.warehouse_id\n",
    "        AND rt.product_id = qs.product_id\n",
    "        AND rt.packing_unit_id = qs.packing_unit_id\n",
    "),\n",
    "\n",
    "tier_adjustments AS (\n",
    "    SELECT \n",
    "        warehouse,\n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        sku,\n",
    "        brand,\n",
    "        category,\n",
    "        total_orders,\n",
    "        total_retailers,\n",
    "        min_qty,\n",
    "        avg_qty,\n",
    "        median_qty,\n",
    "        stddev_qty,\n",
    "        q1_qty,\n",
    "        q3_qty,\n",
    "        p85_qty,\n",
    "        p90_qty,\n",
    "        p95_qty,\n",
    "        max_qty,\n",
    "        mode_qty,\n",
    "        mode_freq,\n",
    "        mode_contribution,\n",
    "        frequent_retailer_count,\n",
    "        avg_orders_per_retailer,\n",
    "        avg_refill_days,\n",
    "        median_refill_days,\n",
    "        total_revenue,\n",
    "        avg_order_value,\n",
    "        \n",
    "        -- ADD: Recency metrics to output\n",
    "        weighted_avg_qty,\n",
    "        last_15d_avg,\n",
    "        last_15d_median,\n",
    "        last_15d_max,\n",
    "        last_15d_orders,\n",
    "        \n",
    "        tier_1_qty,\n",
    "        LEAST(\n",
    "            CEIL(GREATEST(\n",
    "                tier_2_qty_base,\n",
    "                tier_1_qty * 1.6\n",
    "            )),\n",
    "            GREATEST(\n",
    "                tier_1_qty * 3.5,\n",
    "                tier_1_qty + 20\n",
    "            )\n",
    "        ) as tier_2_qty\n",
    "        \n",
    "    FROM tier_calculations\n",
    "),\n",
    "\n",
    "retailer_distribution AS (\n",
    "    SELECT \n",
    "        co.warehouse_id,\n",
    "        co.product_id,\n",
    "        co.packing_unit_id,\n",
    "        ta.tier_1_qty,\n",
    "        ta.tier_2_qty,\n",
    "        COUNT(DISTINCT CASE \n",
    "            WHEN co.order_qty < ta.tier_1_qty THEN co.retailer_id \n",
    "        END) as retailers_below_t1,\n",
    "        COUNT(DISTINCT CASE \n",
    "            WHEN co.order_qty >= ta.tier_1_qty AND co.order_qty < ta.tier_2_qty THEN co.retailer_id \n",
    "        END) as retailers_at_t1,\n",
    "        COUNT(DISTINCT CASE \n",
    "            WHEN co.order_qty >= ta.tier_2_qty THEN co.retailer_id \n",
    "        END) as retailers_at_t2,\n",
    "        COUNT(CASE \n",
    "            WHEN co.order_qty < ta.tier_1_qty THEN 1 \n",
    "        END) as orders_below_t1,\n",
    "        COUNT(CASE \n",
    "            WHEN co.order_qty >= ta.tier_1_qty AND co.order_qty < ta.tier_2_qty THEN 1 \n",
    "        END) as orders_at_t1,\n",
    "        COUNT(CASE \n",
    "            WHEN co.order_qty >= ta.tier_2_qty THEN 1 \n",
    "        END) as orders_at_t2\n",
    "    FROM cleaned_orders co\n",
    "    JOIN tier_adjustments ta \n",
    "        ON ta.warehouse_id = co.warehouse_id \n",
    "        AND ta.product_id = co.product_id\n",
    "        AND ta.packing_unit_id = co.packing_unit_id\n",
    "    GROUP BY \n",
    "        co.warehouse_id,\n",
    "        co.product_id,\n",
    "        co.packing_unit_id,\n",
    "        ta.tier_1_qty,\n",
    "        ta.tier_2_qty\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    ta.warehouse,\n",
    "    ta.warehouse_id,\n",
    "    ta.product_id,\n",
    "    ta.packing_unit_id,\n",
    "    ta.sku,\n",
    "    ta.brand,\n",
    "    ta.category,\n",
    "    \n",
    "    ta.frequent_retailer_count,\n",
    "    ROUND(ta.avg_orders_per_retailer, 2) as avg_orders_per_retailer,\n",
    "    ROUND(ta.avg_refill_days, 1) as avg_refill_days,\n",
    "    ROUND(ta.median_refill_days, 1) as median_refill_days,\n",
    "    \n",
    "    ta.total_orders,\n",
    "    ta.total_retailers,\n",
    "    \n",
    "    ta.min_qty,\n",
    "    ROUND(ta.avg_qty, 2) as avg_qty,\n",
    "    ta.median_qty,\n",
    "    ROUND(ta.weighted_avg_qty, 2) as weighted_avg_qty,\n",
    "    ta.q1_qty as q1_25_qty,\n",
    "    ta.q3_qty as q3_75_qty,\n",
    "    ta.p85_qty,\n",
    "    ta.p90_qty,\n",
    "    ta.p95_qty,\n",
    "    ta.max_qty,\n",
    "    ROUND(ta.stddev_qty, 2) as stddev_qty,\n",
    "    ta.mode_qty,\n",
    "    ta.mode_freq,\n",
    "    ROUND(ta.mode_contribution * 100, 1) as mode_pct,\n",
    "    \n",
    "    -- MODIFIED: 15-day trend metrics\n",
    "    ROUND(ta.last_15d_avg, 2) as last_15d_avg,\n",
    "    ta.last_15d_median,\n",
    "    ta.last_15d_max,\n",
    "    ta.last_15d_orders,\n",
    "    \n",
    "    ta.tier_1_qty,\n",
    "    ta.tier_2_qty,\n",
    "    ROUND((ta.tier_1_qty - ta.median_qty) * 100.0 / NULLIF(ta.median_qty, 0), 1) as tier_1_increase_pct,\n",
    "    ROUND((ta.tier_2_qty - ta.median_qty) * 100.0 / NULLIF(ta.median_qty, 0), 1) as tier_2_increase_pct,\n",
    "    ROUND(ta.tier_2_qty * 1.0 / NULLIF(ta.tier_1_qty, 0), 2) as tier_2_to_tier_1_ratio,\n",
    "    \n",
    "    rd.retailers_below_t1,\n",
    "    rd.retailers_at_t1,\n",
    "    rd.retailers_at_t2,\n",
    "    \n",
    "    rd.orders_below_t1,\n",
    "    rd.orders_at_t1,\n",
    "    rd.orders_at_t2,\n",
    "    \n",
    "    ROUND(100.0 * rd.retailers_below_t1 / NULLIF(ta.total_retailers, 0), 1) as pct_retailers_below_t1,\n",
    "    ROUND(100.0 * rd.retailers_at_t1 / NULLIF(ta.total_retailers, 0), 1) as pct_retailers_at_t1,\n",
    "    ROUND(100.0 * rd.retailers_at_t2 / NULLIF(ta.total_retailers, 0), 1) as pct_retailers_at_t2,\n",
    "    \n",
    "    ROUND(100.0 * rd.orders_below_t1 / NULLIF(ta.total_orders, 0), 1) as pct_orders_below_t1,\n",
    "    ROUND(100.0 * rd.orders_at_t1 / NULLIF(ta.total_orders, 0), 1) as pct_orders_at_t1,\n",
    "    ROUND(100.0 * rd.orders_at_t2 / NULLIF(ta.total_orders, 0), 1) as pct_orders_at_t2,\n",
    "    \n",
    "    ROUND(ta.total_revenue, 2) as total_revenue,\n",
    "    ROUND(ta.avg_order_value, 2) as avg_order_value\n",
    "\n",
    "FROM tier_adjustments ta\n",
    "JOIN retailer_distribution rd \n",
    "    ON rd.warehouse_id = ta.warehouse_id \n",
    "    AND rd.product_id = ta.product_id\n",
    "    AND rd.packing_unit_id = ta.packing_unit_id\n",
    "ORDER BY ta.warehouse, ta.total_orders DESC\n",
    "'''\n",
    "\n",
    "# Execute query and convert numeric columns\n",
    "print(\"Fetching quantity tier data...\")\n",
    "tiers_selection = snowflake_query(\"Egypt\", query)\n",
    "\n",
    "for col in tiers_selection.columns:\n",
    "    tiers_selection[col] = pd.to_numeric(tiers_selection[col], errors='ignore')\n",
    "\n",
    "print(f\"✓ Calculated tiers for {len(tiers_selection)} product-warehouse combinations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10182061",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "APPLYING FEEDBACK LOOP ADJUSTMENTS\n",
      "================================================================================\n",
      "  T3 (wholesale) feedback stored for later: 521 SKUs\n",
      "\n",
      "  SKUs with feedback data: 1394 / 3955\n",
      "  ✓ Applied T1 qty adjustments: 1269 SKUs (avg change: +16.0%)\n",
      "  ⚠ Skipped T1 qty for 125 SKUs (script already better)\n",
      "  ✓ Applied T2 qty adjustments: 1130 SKUs (avg change: +22.2%)\n",
      "  ⚠ Skipped T2 qty for 264 SKUs (script already better)\n",
      "\n",
      "  Validating tier constraints...\n",
      "    Fixed 2 SKUs where T2 qty <= T1 qty\n",
      "\n",
      "  T1 Actions applied:\n",
      "    - INCREASE_QTY: 254\n",
      "    - SLIGHT_INCREASE_QTY: 135\n",
      "    - INCREASE_DISCOUNT: 129\n",
      "    - DECREASE_QTY: 8\n",
      "\n",
      "  T2 Actions applied:\n",
      "    - INCREASE_QTY: 639\n",
      "    - INCREASE_DISCOUNT: 250\n",
      "    - DECREASE_QTY: 20\n",
      "    - REDUCE_RATIO: 3\n",
      "\n",
      "  Margin Status of adjusted SKUs:\n",
      "    - CRITICAL: 789\n",
      "    - HIGH: 278\n",
      "    - HEALTHY: 231\n",
      "    - LOW: 96\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# APPLY FEEDBACK ADJUSTMENTS TO TIER QUANTITIES\n",
    "# =============================================================================\n",
    "\n",
    "# Store T3 (wholesale) feedback for later use (applied after ws_new calculation)\n",
    "feedback_t3_data = pd.DataFrame()\n",
    "\n",
    "if len(feedback_data) > 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"APPLYING FEEDBACK LOOP ADJUSTMENTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Store T3 feedback for later (will be applied after ws_new_qty is calculated)\n",
    "    t3_cols = ['warehouse_id', 'product_id', 'packing_unit_id', \n",
    "               'suggested_t3_qty', 'suggested_t3_discount', 't3_action']\n",
    "    available_t3_cols = [c for c in t3_cols if c in feedback_data.columns]\n",
    "    if len(available_t3_cols) > 3:  # Has T3 suggestions\n",
    "        feedback_t3_data = feedback_data[available_t3_cols].copy()\n",
    "        print(f\"  T3 (wholesale) feedback stored for later: {feedback_t3_data['suggested_t3_qty'].notna().sum()} SKUs\")\n",
    "    \n",
    "    # Merge feedback with tiers_selection\n",
    "    tiers_with_feedback = tiers_selection.merge(\n",
    "        feedback_data[['warehouse_id', 'product_id', 'packing_unit_id', \n",
    "                       'suggested_t1_qty', 'suggested_t2_qty', \n",
    "                       't1_action', 't2_action', 'margin_status']],\n",
    "        on=['warehouse_id', 'product_id', 'packing_unit_id'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n  SKUs with feedback data: {tiers_with_feedback['suggested_t1_qty'].notna().sum()} / {len(tiers_with_feedback)}\")\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # SMART T1 QUANTITY ADJUSTMENTS\n",
    "    # Only apply if recommendation actually improves the current value\n",
    "    # ==========================================================================\n",
    "    mask_t1 = tiers_with_feedback['suggested_t1_qty'].notna()\n",
    "    t1_applied = 0\n",
    "    t1_skipped_already_better = 0\n",
    "    \n",
    "    if mask_t1.sum() > 0:\n",
    "        original_t1 = tiers_with_feedback.loc[mask_t1, 'tier_1_qty'].copy()\n",
    "        suggested_t1 = tiers_with_feedback.loc[mask_t1, 'suggested_t1_qty']\n",
    "        t1_action = tiers_with_feedback.loc[mask_t1, 't1_action'].fillna('')\n",
    "        \n",
    "        # Determine which rows should be updated\n",
    "        # Skip if: action is INCREASE and script already >= suggested\n",
    "        # Skip if: action is DECREASE and script already <= suggested\n",
    "        skip_t1_increase = t1_action.str.contains('INCREASE', case=False, na=False) & (original_t1 >= suggested_t1)\n",
    "        skip_t1_decrease = t1_action.str.contains('DECREASE|REDUCE', case=False, na=False) & (original_t1 <= suggested_t1)\n",
    "        skip_t1 = skip_t1_increase | skip_t1_decrease\n",
    "        \n",
    "        # Apply only where NOT skipped\n",
    "        apply_t1 = mask_t1.copy()\n",
    "        apply_t1.loc[mask_t1] = ~skip_t1.values\n",
    "        \n",
    "        if apply_t1.sum() > 0:\n",
    "            tiers_with_feedback.loc[apply_t1, 'tier_1_qty'] = tiers_with_feedback.loc[apply_t1, 'suggested_t1_qty'].astype(int)\n",
    "            avg_change = ((tiers_with_feedback.loc[apply_t1, 'tier_1_qty'] - original_t1.loc[~skip_t1]) / original_t1.loc[~skip_t1] * 100).mean()\n",
    "            t1_applied = apply_t1.sum()\n",
    "            print(f\"  ✓ Applied T1 qty adjustments: {t1_applied} SKUs (avg change: {avg_change:+.1f}%)\")\n",
    "        \n",
    "        t1_skipped_already_better = skip_t1.sum()\n",
    "        if t1_skipped_already_better > 0:\n",
    "            print(f\"  ⚠ Skipped T1 qty for {t1_skipped_already_better} SKUs (script already better)\")\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # SMART T2 QUANTITY ADJUSTMENTS\n",
    "    # Only apply if recommendation actually improves the current value\n",
    "    # ==========================================================================\n",
    "    mask_t2 = tiers_with_feedback['suggested_t2_qty'].notna()\n",
    "    t2_applied = 0\n",
    "    t2_skipped_already_better = 0\n",
    "    \n",
    "    if mask_t2.sum() > 0:\n",
    "        original_t2 = tiers_with_feedback.loc[mask_t2, 'tier_2_qty'].copy()\n",
    "        suggested_t2 = tiers_with_feedback.loc[mask_t2, 'suggested_t2_qty']\n",
    "        t2_action = tiers_with_feedback.loc[mask_t2, 't2_action'].fillna('')\n",
    "        \n",
    "        # Determine which rows should be updated\n",
    "        skip_t2_increase = t2_action.str.contains('INCREASE', case=False, na=False) & (original_t2 >= suggested_t2)\n",
    "        skip_t2_decrease = t2_action.str.contains('DECREASE|REDUCE', case=False, na=False) & (original_t2 <= suggested_t2)\n",
    "        skip_t2 = skip_t2_increase | skip_t2_decrease\n",
    "        \n",
    "        # Apply only where NOT skipped\n",
    "        apply_t2 = mask_t2.copy()\n",
    "        apply_t2.loc[mask_t2] = ~skip_t2.values\n",
    "        \n",
    "        if apply_t2.sum() > 0:\n",
    "            tiers_with_feedback.loc[apply_t2, 'tier_2_qty'] = tiers_with_feedback.loc[apply_t2, 'suggested_t2_qty'].astype(int)\n",
    "            avg_change = ((tiers_with_feedback.loc[apply_t2, 'tier_2_qty'] - original_t2.loc[~skip_t2]) / original_t2.loc[~skip_t2] * 100).mean()\n",
    "            t2_applied = apply_t2.sum()\n",
    "            print(f\"  ✓ Applied T2 qty adjustments: {t2_applied} SKUs (avg change: {avg_change:+.1f}%)\")\n",
    "        \n",
    "        t2_skipped_already_better = skip_t2.sum()\n",
    "        if t2_skipped_already_better > 0:\n",
    "            print(f\"  ⚠ Skipped T2 qty for {t2_skipped_already_better} SKUs (script already better)\")\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # VALIDATE TIER QUANTITY CONSTRAINTS: T1 qty < T2 qty\n",
    "    # ==========================================================================\n",
    "    print(\"\\n  Validating tier constraints...\")\n",
    "    \n",
    "    invalid_qty = tiers_with_feedback['tier_2_qty'] <= tiers_with_feedback['tier_1_qty']\n",
    "    if invalid_qty.sum() > 0:\n",
    "        # Fix: Set T2 qty = T1 qty * 1.5\n",
    "        tiers_with_feedback.loc[invalid_qty, 'tier_2_qty'] = (\n",
    "            tiers_with_feedback.loc[invalid_qty, 'tier_1_qty'] * 1.5\n",
    "        ).astype(int)\n",
    "        print(f\"    Fixed {invalid_qty.sum()} SKUs where T2 qty <= T1 qty\")\n",
    "    \n",
    "    # Update tiers_selection with adjusted values\n",
    "    tiers_selection = tiers_with_feedback.drop(\n",
    "        columns=['suggested_t1_qty', 'suggested_t2_qty', 't1_action', 't2_action', 'margin_status'], \n",
    "        errors='ignore'\n",
    "    )\n",
    "    \n",
    "    # Summary by action\n",
    "    if 't1_action' in tiers_with_feedback.columns:\n",
    "        t1_actions = tiers_with_feedback[tiers_with_feedback['t1_action'].notna()]['t1_action'].value_counts()\n",
    "        print(f\"\\n  T1 Actions applied:\")\n",
    "        for action, count in t1_actions.items():\n",
    "            if action != 'NO_CHANGE':\n",
    "                print(f\"    - {action}: {count}\")\n",
    "    \n",
    "    if 't2_action' in tiers_with_feedback.columns:\n",
    "        t2_actions = tiers_with_feedback[tiers_with_feedback['t2_action'].notna()]['t2_action'].value_counts()\n",
    "        print(f\"\\n  T2 Actions applied:\")\n",
    "        for action, count in t2_actions.items():\n",
    "            if action != 'NO_CHANGE':\n",
    "                print(f\"    - {action}: {count}\")\n",
    "    \n",
    "    if 'margin_status' in tiers_with_feedback.columns:\n",
    "        margin_dist = tiers_with_feedback[tiers_with_feedback['margin_status'].notna()]['margin_status'].value_counts()\n",
    "        print(f\"\\n  Margin Status of adjusted SKUs:\")\n",
    "        for status, count in margin_dist.items():\n",
    "            print(f\"    - {status}: {count}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "else:\n",
    "    print(\"⚠ No feedback data available - using original tier calculations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc05aac1",
   "metadata": {},
   "source": [
    "### SKU Information & Cost Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8045f25-1b32-437c-81e8-a04f9771908f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching SKU information and WAC data...\n",
      "✓ Retrieved cost data for 8147 SKUs\n"
     ]
    }
   ],
   "source": [
    "query = f'''\n",
    "SELECT DISTINCT  \n",
    "    products.id as product_id,\n",
    "    CONCAT(products.name_ar, ' ', products.size, ' ', product_units.name_ar) as sku,\n",
    "    brands.name_ar as brand, \n",
    "    categories.name_ar as cat,\n",
    "    f.wac_p\n",
    "FROM products \n",
    "JOIN brands ON products.brand_id = brands.id \n",
    "JOIN categories ON products.category_id = categories.id\n",
    "JOIN finance.all_cogs f ON f.product_id = products.id \n",
    "    AND CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMESTAMP()) \n",
    "        BETWEEN f.from_date AND f.to_date \n",
    "JOIN product_units ON product_units.id = products.unit_id \n",
    "'''\n",
    "\n",
    "print(\"Fetching SKU information and WAC data...\")\n",
    "sku_info = snowflake_query(\"Egypt\", query)\n",
    "sku_info['product_id'] = pd.to_numeric(sku_info['product_id'])\n",
    "sku_info['wac_p'] = pd.to_numeric(sku_info['wac_p'])\n",
    "\n",
    "print(f\"✓ Retrieved cost data for {len(sku_info)} SKUs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ff38bd-bbc5-4ca6-b152-c283255d067a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Market Prices\n",
    "\n",
    "Gather competitive pricing data from multiple sources:\n",
    "- **Marketplace prices** - Regional marketplace data with fallbacks\n",
    "- **Ben Soliman prices** - Competitor pricing\n",
    "- **Scraped prices** - Web-scraped competitor data\n",
    "- **Product statistics** - Historical margin boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd94d09",
   "metadata": {},
   "source": [
    "### 4.1 Marketplace Prices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "873a9cfe-76fb-486c-b71c-b2c3c447b7af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching marketplace prices...\n",
      "✓ Retrieved marketplace prices for 24853 products\n"
     ]
    }
   ],
   "source": [
    "query = f'''\n",
    "WITH whs as (SELECT *\n",
    "             FROM   (values\n",
    "                            ('Cairo', 'El-Marg', 38,700),\n",
    "                            ('Cairo', 'Mostorod', 1,700),\n",
    "                            ('Giza', 'Barageel', 236,701),\n",
    "                            ('Delta West', 'El-Mahala', 337,703),\n",
    "                            ('Delta West', 'Tanta', 8,703),\n",
    "                            ('Delta East', 'Mansoura FC', 339,704),\n",
    "                            ('Delta East', 'Sharqya', 170,704),\n",
    "                            ('Upper Egypt', 'Assiut FC', 501,1124),\n",
    "                            ('Upper Egypt', 'Bani sweif', 401,1126),\n",
    "                            ('Upper Egypt', 'Menya Samalot', 703,1123),\n",
    "                            ('Upper Egypt', 'Sohag', 632,1125),\n",
    "                            ('Alexandria', 'Khorshed Alex', 797,702),\n",
    "\t\t\t\t\t\t\t('Giza', 'Sakkarah', 962,701)\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t)\n",
    "                    x(region, wh, warehouse_id,cohort_id)),\n",
    "full_data as (\n",
    "select products.id as product_id, region,warehouse_id\n",
    "from products , whs \n",
    "where activation = 'true'\n",
    "),\t\t\t\t\n",
    "\n",
    "MP as (\n",
    "select region,product_id,\n",
    "min(min_price) as min_price,\n",
    "min(max_price) as max_price,\n",
    "min(mod_price) as mod_price,\n",
    "min(true_min) as true_min,\n",
    "min(true_max) as true_max\n",
    "\n",
    "from (\n",
    "select mp.region,mp.product_id,mp.pu_id,\n",
    "min_price/BASIC_UNIT_COUNT as min_price,\n",
    "max_price/BASIC_UNIT_COUNT as max_price,\n",
    "mod_price/BASIC_UNIT_COUNT as mod_price,\n",
    "TRUE_MIN_PRICE/BASIC_UNIT_COUNT as true_min,\n",
    "TRUE_MAX_PRICE/BASIC_UNIT_COUNT as true_max\n",
    "from materialized_views.marketplace_prices mp \n",
    "join packing_unit_products pup on pup.product_id = mp.product_id and pup.packing_unit_id = mp.pu_id\n",
    "join finance.all_cogs f on f.product_id = mp.product_id and CURRENT_TIMESTAMP between f.from_date and f.to_date\n",
    "where  least(min_price,mod_price) between wac_p*0.9 and wac_p*1.3 \n",
    ")\n",
    "group by all \n",
    "),\n",
    "region_mapping AS (\n",
    "    SELECT * \n",
    "\tFROM \n",
    "\t(\tVALUES\n",
    "        ('Delta East', 'Delta West'),\n",
    "        ('Delta West', 'Delta East'),\n",
    "        ('Alexandria', 'Cairo'),\n",
    "        ('Alexandria', 'Giza'),\n",
    "        ('Upper Egypt', 'Cairo'),\n",
    "        ('Upper Egypt', 'Giza'),\n",
    "\t\t('Cairo','Giza'),\n",
    "\t\t('Giza','Cairo'),\n",
    "\t\t('Delta West', 'Cairo'),\n",
    "\t\t('Delta East', 'Cairo'),\n",
    "\t\t('Delta West', 'Giza'),\n",
    "\t\t('Delta East', 'Giza')\n",
    "\t\t)\n",
    "    AS region_mapping(region, fallback_region)\n",
    ")\n",
    "\n",
    "\n",
    "select region,warehouse_id,product_id,\n",
    "min(final_min_price) as final_min_price,\n",
    "min(final_max_price) as final_max_price,\n",
    "min(final_mod_price) as final_mod_price,\n",
    "min(final_true_min) as final_true_min,\n",
    "min(final_true_max) as final_true_max\n",
    "\n",
    "from (\n",
    "SELECT\n",
    "distinct \n",
    "\tw.region,\n",
    "    w.warehouse_id,\n",
    "\tw.product_id,\n",
    "    COALESCE(m1.min_price, m2.min_price) AS final_min_price,\n",
    "    COALESCE(m1.max_price, m2.max_price) AS final_max_price,\n",
    "    COALESCE(m1.mod_price, m2.mod_price) AS final_mod_price,\n",
    "\tCOALESCE(m1.true_min, m2.true_min) AS final_true_min,\n",
    "\tCOALESCE(m1.true_max, m2.true_max) AS final_true_max,\n",
    "FROM full_data w\n",
    "LEFT JOIN MP m1\n",
    "    ON w.region = m1.region and w.product_id = m1.product_id\n",
    "JOIN region_mapping rm\n",
    "    ON w.region = rm.region\n",
    "LEFT JOIN MP m2\n",
    "    ON rm.fallback_region = m2.region\n",
    "   AND w.product_id = m2.product_id\n",
    ")\n",
    "where final_min_price is not null \n",
    "group by all \n",
    "'''\n",
    "\n",
    "print(\"Fetching marketplace prices...\")\n",
    "marketplace = snowflake_query(\"Egypt\", query)\n",
    "marketplace.columns = marketplace.columns.str.lower()\n",
    "\n",
    "for col in marketplace.columns:\n",
    "    marketplace[col] = pd.to_numeric(marketplace[col], errors='ignore')\n",
    "\n",
    "print(f\"✓ Retrieved marketplace prices for {len(marketplace)} products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd2f7ff",
   "metadata": {},
   "source": [
    "### 4.2 Ben Soliman (Competitor) Prices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ede9cdec-a69d-4579-afcc-57df08a5f753",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Ben Soliman (competitor) prices...\n",
      "✓ Retrieved competitor prices for 1566 products\n"
     ]
    }
   ],
   "source": [
    "query = f'''\n",
    "with lower as (\n",
    "select distinct product_id,sku,new_d*bs_price as ben_soliman_price,INJECTION_DATE\n",
    "from (\n",
    "select maxab_product_id as product_id,maxab_sku as sku,INJECTION_DATE,wac1,wac_p,(bs_price/bs_unit_count) as bs_price,diff,cu_price,case when p1 > 1 then child_quantity else 0 end as scheck,round(p1/2)*2 as p1,p2,case when (ROUND(p1 / scheck) * scheck) = 0 then p1 else (ROUND(p1 / scheck) * scheck) end as new_d\n",
    "from (\n",
    "select sm.*,wac1, wac_p, abs((bs_price/bs_unit_count)-(wac_p*maxab_basic_unit_count))/(wac_p*maxab_basic_unit_count) as diff,cpc.price as cu_price,pup.child_quantity , round((cu_price/(bs_price/bs_unit_count))) as p1, round(((bs_price/bs_unit_count)/cu_price)) as p2\n",
    "from materialized_views.savvy_mapping sm \n",
    "join finance.all_cogs f on f.product_id = sm.maxab_product_id and CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMESTAMP()) between f.from_Date and f.to_date\n",
    "join   PACKING_UNIT_PRODUCTS pu on pu.product_id = sm.maxab_product_id and pu.IS_BASIC_UNIT = 1 \n",
    "join cohort_product_packing_units cpc on cpc.PRODUCT_PACKING_UNIT_ID = pu.id and cohort_id = 700 \n",
    "join packing_unit_products pup on pup.product_id = sm.maxab_product_id and pup.is_basic_unit = 1  \n",
    "where bs_price is not null and INJECTION_DATE::date >= CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - 5 \n",
    "and diff > 0.3\n",
    "and p1 > 1\n",
    ")\n",
    ")\n",
    "qualify max(INJECTION_DATE)over(partition by product_id)  = INJECTION_DATE\n",
    "),\n",
    "m_bs as (\n",
    "select z.* from (\n",
    "\tselect maxab_product_id as product_id, maxab_sku as sku, avg(bs_final_price) as ben_soliman_price,INJECTION_DATE\n",
    "\tfrom (\n",
    "\t\tselect *, row_number() over(partition by maxab_product_id order by diff) as rnk_2 from (\n",
    "\t\t\tselect *, (bs_final_price-wac_p)/wac_p as diff_2 from (\n",
    "\t\t\t\tselect *, bs_price/maxab_basic_unit_count as bs_final_price from (\n",
    "\t\t\t\t\tselect *, row_number() over(partition by maxab_product_id, maxab_pu order by diff) as rnk from (\n",
    "\t\t\t\t\t\tselect * ,max(INJECTION_DATE::date) over(partition by maxab_product_id, maxab_pu) as max_date,\n",
    "\t\t\t\t\t\tfrom (\n",
    "\t\t\t\t\t\t\tselect sm.*,wac1, wac_p, abs(bs_price-(wac_p*maxab_basic_unit_count))/(wac_p*maxab_basic_unit_count) as diff \n",
    "\t\t\t\t\tfrom materialized_views.savvy_mapping sm \n",
    "\t\t\t\t\tjoin finance.all_cogs f on f.product_id = sm.maxab_product_id and CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMESTAMP()) between f.from_Date and f.to_date\n",
    "\t\t\t\t\twhere bs_price is not null and INJECTION_DATE::date >= CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - 5 \n",
    "\t\t\t\t\tand diff < 0.3\n",
    "\t\t\t\t\t)\n",
    "\t\t\t\t\tqualify max_date = INJECTION_DATE\n",
    "\t\t\t\t\t) qualify rnk = 1 \n",
    "\t\t\t\t)\n",
    "\t\t\t) where diff_2 between -0.5 and 0.5 \n",
    "\t\t) qualify rnk_2 = 1 \n",
    "\t) group by all\n",
    ") z \n",
    "join finance.all_cogs f on f.product_id = z.product_id and CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMESTAMP()) between f.from_Date and f.to_date\n",
    "where ben_soliman_price between f.wac_p*0.7 and f.wac_p*1.3\n",
    ")\n",
    "select product_id,avg(ben_soliman_price) as ben_soliman_price\n",
    "from (\n",
    "select *\n",
    "from (\n",
    "select * \n",
    "from m_bs \n",
    "\n",
    "union all\n",
    "\n",
    " select *\n",
    " from lower\n",
    " )\n",
    " qualify max(INJECTION_DATE) over(partition by product_id) = INJECTION_DATE\n",
    " )\n",
    " group by all\n",
    "'''\n",
    "\n",
    "print(\"Fetching Ben Soliman (competitor) prices...\")\n",
    "bensoliman = snowflake_query(\"Egypt\", query)\n",
    "bensoliman.columns = bensoliman.columns.str.lower()\n",
    "\n",
    "for col in bensoliman.columns:\n",
    "    bensoliman[col] = pd.to_numeric(bensoliman[col], errors='ignore')\n",
    "\n",
    "print(f\"✓ Retrieved competitor prices for {len(bensoliman)} products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd5c097",
   "metadata": {},
   "source": [
    "### 4.3 Scraped Competitor Prices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98943949-f930-4b02-aaf6-1e20144c674e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching scraped competitor prices...\n",
      "✓ Retrieved scraped prices for 10257 products\n"
     ]
    }
   ],
   "source": [
    "query = f'''\n",
    "WITH whs as (SELECT *\n",
    "             FROM   (values\n",
    "                            ('Cairo', 'El-Marg', 38,700),\n",
    "                            ('Cairo', 'Mostorod', 1,700),\n",
    "                            ('Giza', 'Barageel', 236,701),\n",
    "                            ('Delta West', 'El-Mahala', 337,703),\n",
    "                            ('Delta West', 'Tanta', 8,703),\n",
    "                            ('Delta East', 'Mansoura FC', 339,704),\n",
    "                            ('Delta East', 'Sharqya', 170,704),\n",
    "                            ('Upper Egypt', 'Assiut FC', 501,1124),\n",
    "                            ('Upper Egypt', 'Bani sweif', 401,1126),\n",
    "                            ('Upper Egypt', 'Menya Samalot', 703,1123),\n",
    "                            ('Upper Egypt', 'Sohag', 632,1125),\n",
    "                            ('Alexandria', 'Khorshed Alex', 797,702),\n",
    "\t\t\t\t\t\t\t('Giza', 'Sakkarah', 962,701)\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t)\n",
    "                    x(region, wh, warehouse_id,cohort_id))\n",
    "select product_id,x.region,warehouse_id,min(MARKET_PRICE) as min_scrapped,max(MARKET_PRICE) as max_scrapped,median(MARKET_PRICE) as median_scrapped\n",
    "from (\n",
    "select MATERIALIZED_VIEWS.CLEANED_MARKET_PRICES.*,max(date)over(partition by region,MATERIALIZED_VIEWS.CLEANED_MARKET_PRICES.product_id,competitor) as max_date\n",
    "from MATERIALIZED_VIEWS.CLEANED_MARKET_PRICES\n",
    "join finance.all_cogs f on f.product_id = MATERIALIZED_VIEWS.CLEANED_MARKET_PRICES.product_id and CURRENT_TIMESTAMP between f.from_date and f.to_date \n",
    "where date>= current_date -5\n",
    "and MARKET_PRICE between f.wac_p * 0.9 and wac_p*1.3\n",
    "qualify date = max_date \n",
    ") x \n",
    "left join whs on whs.region = x.region\n",
    "group by all \n",
    "'''\n",
    "\n",
    "print(\"Fetching scraped competitor prices...\")\n",
    "scrapped_prices = snowflake_query(\"Egypt\", query)\n",
    "scrapped_prices.columns = scrapped_prices.columns.str.lower()\n",
    "\n",
    "for col in scrapped_prices.columns:\n",
    "    scrapped_prices[col] = pd.to_numeric(scrapped_prices[col], errors='ignore')\n",
    "\n",
    "print(f\"✓ Retrieved scraped prices for {len(scrapped_prices)} products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ceaef4",
   "metadata": {},
   "source": [
    "### 4.4 Product Statistics (Margin Boundaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b418565-6266-4bac-9af4-6970ba53a3be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching product statistics (margin boundaries)...\n",
      "✓ Retrieved margin statistics for 18160 products\n"
     ]
    }
   ],
   "source": [
    "query = f'''\n",
    "SELECT \n",
    "    region,\n",
    "    product_id,\n",
    "    optimal_bm,\n",
    "    MIN_BOUNDARY,\n",
    "    MAX_BOUNDARY,\n",
    "    MEDIAN_BM\n",
    "FROM (\n",
    "    SELECT \n",
    "        region,\n",
    "        product_id,\n",
    "        target_bm,\n",
    "        optimal_bm,\n",
    "        MIN_BOUNDARY,\n",
    "        MAX_BOUNDARY,\n",
    "        MEDIAN_BM,\n",
    "        MAX(created_at) OVER (PARTITION BY product_id, region) as max_date,\n",
    "        created_at\n",
    "    FROM materialized_views.PRODUCT_STATISTICS\n",
    "    WHERE created_at::date >= DATE_TRUNC('month', CURRENT_DATE - 60)\n",
    "    QUALIFY max_date = created_at\n",
    ")\n",
    "'''\n",
    "\n",
    "print(\"Fetching product statistics (margin boundaries)...\")\n",
    "stats = snowflake_query(\"Egypt\", query)\n",
    "stats.columns = stats.columns.str.lower()\n",
    "\n",
    "for col in stats.columns:\n",
    "    stats[col] = pd.to_numeric(stats[col], errors='ignore')\n",
    "\n",
    "print(f\"✓ Retrieved margin statistics for {len(stats)} products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33648230",
   "metadata": {},
   "source": [
    "### 4.5 Warehouse-Region Mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb86c080-2a8c-4a63-8507-02ffcfe904f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching warehouse-region mapping...\n",
      "✓ Mapped 15 warehouses to regions\n"
     ]
    }
   ],
   "source": [
    "query = f'''\n",
    "SELECT warehouse_id, region\n",
    "FROM (\n",
    "    SELECT *, ROW_NUMBER() OVER (PARTITION BY warehouse_id ORDER BY nmv DESC) as rnk \n",
    "    FROM (\n",
    "        SELECT \n",
    "            CASE WHEN regions.id = 2 THEN cities.name_en ELSE regions.name_en END as region,\n",
    "            pso.warehouse_id,\n",
    "            SUM(pso.total_price) as nmv\n",
    "        FROM product_sales_order pso\n",
    "        JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "        JOIN materialized_views.retailer_polygon ON materialized_views.retailer_polygon.retailer_id = so.retailer_id\n",
    "        JOIN districts ON districts.id = materialized_views.retailer_polygon.district_id\n",
    "        JOIN cities ON cities.id = districts.city_id\n",
    "        JOIN states ON states.id = cities.state_id\n",
    "        JOIN regions ON regions.id = states.region_id             \n",
    "        WHERE TRUE\n",
    "            AND so.created_at::date BETWEEN CURRENT_DATE - 31 AND CURRENT_DATE - 1\n",
    "            AND so.sales_order_status_id NOT IN (7, 12)\n",
    "            AND so.channel IN ('telesales', 'retailer')\n",
    "            AND pso.purchased_item_count <> 0\n",
    "        GROUP BY ALL\n",
    "    )\n",
    "    QUALIFY rnk = 1 \n",
    ")\n",
    "'''\n",
    "\n",
    "print(\"Fetching warehouse-region mapping...\")\n",
    "warehouse_region = snowflake_query(\"Egypt\", query)\n",
    "warehouse_region.columns = warehouse_region.columns.str.lower()\n",
    "\n",
    "for col in warehouse_region.columns:\n",
    "    warehouse_region[col] = pd.to_numeric(warehouse_region[col], errors='ignore')\n",
    "\n",
    "print(f\"✓ Mapped {len(warehouse_region)} warehouses to regions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d74ad7b",
   "metadata": {},
   "source": [
    "### 4.6 Target Margins (Brand/Category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e09273a-0aed-4e4a-8f82-739055c3047c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching brand target margins...\n",
      "✓ Retrieved targets for 478 brand-category combinations\n",
      "Fetching category target margins...\n",
      "✓ Retrieved targets for 73 categories\n"
     ]
    }
   ],
   "source": [
    "# Brand-level target margins\n",
    "query = f'''\n",
    "SELECT DISTINCT cat, brand, margin as target_bm\n",
    "FROM performance.commercial_targets cplan\n",
    "QUALIFY \n",
    "    CASE \n",
    "        WHEN DATE_TRUNC('month', MAX(DATE) OVER()) = DATE_TRUNC('month', CURRENT_DATE) \n",
    "        THEN DATE_TRUNC('month', CURRENT_DATE)\n",
    "        ELSE DATE_TRUNC('month', CURRENT_DATE - INTERVAL '1 month') \n",
    "    END = DATE_TRUNC('month', date)\n",
    "'''\n",
    "\n",
    "print(\"Fetching brand target margins...\")\n",
    "brand_cat_target = snowflake_query(\"Egypt\", query)\n",
    "brand_cat_target['target_bm'] = pd.to_numeric(brand_cat_target['target_bm'])\n",
    "print(f\"✓ Retrieved targets for {len(brand_cat_target)} brand-category combinations\")\n",
    "\n",
    "# Category-level weighted target margins\n",
    "query = f'''\n",
    "SELECT cat, SUM(target_bm * (target_nmv / cat_total)) as cat_target_margin\n",
    "FROM (\n",
    "    SELECT *, SUM(target_nmv) OVER (PARTITION BY cat) as cat_total\n",
    "    FROM (\n",
    "        SELECT cat, brand, AVG(target_bm) as target_bm, SUM(target_nmv) as target_nmv\n",
    "        FROM (\n",
    "            SELECT DISTINCT \n",
    "                date, \n",
    "                city as region, \n",
    "                cat, \n",
    "                brand, \n",
    "                margin as target_bm, \n",
    "                nmv as target_nmv\n",
    "            FROM performance.commercial_targets cplan\n",
    "            QUALIFY \n",
    "                CASE \n",
    "                    WHEN DATE_TRUNC('month', MAX(DATE) OVER()) = DATE_TRUNC('month', CURRENT_DATE) \n",
    "                    THEN DATE_TRUNC('month', CURRENT_DATE)\n",
    "                    ELSE DATE_TRUNC('month', CURRENT_DATE - INTERVAL '1 month') \n",
    "                END = DATE_TRUNC('month', date)\n",
    "        )\n",
    "        GROUP BY ALL\n",
    "    )\n",
    ")\n",
    "GROUP BY ALL \n",
    "'''\n",
    "\n",
    "print(\"Fetching category target margins...\")\n",
    "cat_target = snowflake_query(\"Egypt\", query)\n",
    "cat_target['cat_target_margin'] = pd.to_numeric(cat_target['cat_target_margin'])\n",
    "print(f\"✓ Retrieved targets for {len(cat_target)} categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab0491b",
   "metadata": {},
   "source": [
    "### 4.7 Merge All Data Sources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383d11b7-a8d2-4d08-9a09-db03195b9b5b",
   "metadata": {},
   "source": [
    "### LIVE CART Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ec7109f-492b-414b-889d-66bb5fd78f4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching live cart rules...\n",
      "✓ Retrieved 110660 cart rules\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "SELECT \n",
    "    cppu.cohort_id,\n",
    "    product_id,\n",
    "    packing_unit_id,\n",
    "    basic_unit_count,\n",
    "    COALESCE(cppu.MAX_PER_SALES_ORDER, cppu2.MAX_PER_SALES_ORDER) as current_cart_rule\n",
    "FROM COHORT_PRODUCT_PACKING_UNITS cppu \n",
    "JOIN PACKING_UNIT_PRODUCTS pup ON cppu.PRODUCT_PACKING_UNIT_ID = pup.id \n",
    "JOIN cohorts c ON c.id = cppu.cohort_id\n",
    "JOIN COHORT_PRODUCT_PACKING_UNITS cppu2 \n",
    "    ON cppu.PRODUCT_PACKING_UNIT_ID = cppu2.PRODUCT_PACKING_UNIT_ID \n",
    "    AND cppu2.cohort_id = c.FALLBACK_COHORT_ID \n",
    "WHERE cppu.cohort_id IN (700, 701, 702, 703, 704, 1123, 1124, 1125, 1126)\n",
    "'''\n",
    "\n",
    "print(\"Fetching live cart rules...\")\n",
    "live_cart_rules = snowflake_query(\"Egypt\", query) \n",
    "live_cart_rules.columns = live_cart_rules.columns.str.lower()\n",
    "\n",
    "for col in live_cart_rules.columns:\n",
    "    live_cart_rules[col] = pd.to_numeric(live_cart_rules[col], errors='ignore')\n",
    "\n",
    "print(f\"✓ Retrieved {len(live_cart_rules)} cart rules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4e97860-cb64-4212-857a-98e82a822abd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging all data sources...\n",
      "✓ Merged data: 3955 products with all pricing data\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MERGE ALL DATA SOURCES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Merging all data sources...\")\n",
    "\n",
    "# Start with selected products + tier quantities\n",
    "final_data = selected_products.merge(\n",
    "    tiers_selection[[\n",
    "        'warehouse_id', 'product_id', 'packing_unit_id',\n",
    "        'tier_1_qty', 'tier_2_qty', 'median_qty', 'stddev_qty',\n",
    "        'tier_1_increase_pct', 'tier_2_increase_pct'\n",
    "    ]],\n",
    "    on=['warehouse_id', 'product_id', 'packing_unit_id']\n",
    ")\n",
    "\n",
    "# Select relevant columns\n",
    "final_data = final_data[[\n",
    "    'warehouse_id', 'product_id', 'packing_unit_id', 'sku', 'brand', 'cat',\n",
    "    'packing_unit_price', 'basic_unit_count', \n",
    "    'tier_1_qty', 'tier_2_qty', 'median_qty', 'stddev_qty',\n",
    "    'tier_1_increase_pct', 'tier_2_increase_pct', 'final_rank'\n",
    "]]\n",
    "\n",
    "# Add WAC (weighted average cost)\n",
    "final_data = final_data.merge(sku_info[['product_id', 'wac_p']], on='product_id')\n",
    "final_data['wac_p'] = (final_data['wac_p'] * final_data['basic_unit_count']).round(2)\n",
    "\n",
    "# Add marketplace prices\n",
    "final_data = final_data.merge(marketplace, on=['product_id', 'warehouse_id'], how='left')\n",
    "final_data = final_data.drop(columns='region')\n",
    "\n",
    "# Add competitor prices\n",
    "final_data = final_data.merge(bensoliman[['product_id', 'ben_soliman_price']], on=['product_id'], how='left')\n",
    "final_data = final_data.merge(scrapped_prices, on=['product_id', 'warehouse_id'], how='left')\n",
    "final_data = final_data.drop(columns='region')\n",
    "\n",
    "# Add region and margin data\n",
    "final_data = final_data.merge(warehouse_region, on=['warehouse_id'])\n",
    "final_data = final_data.merge(stats, on=['product_id', 'region'], how='left')\n",
    "final_data = final_data.merge(brand_cat_target, on=['brand', 'cat'], how='left')\n",
    "final_data = final_data.merge(cat_target, on=['cat'], how='left')\n",
    "\n",
    "# Use brand target margin, fall back to category target margin\n",
    "final_data['Target_margin'] = final_data['target_bm'].fillna(final_data['cat_target_margin'])\n",
    "\n",
    "print(f\"✓ Merged data: {len(final_data)} products with all pricing data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "045aceb9-ff2c-45bd-8aae-d931e43fc0ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cohort to Warehouse mapping\n",
    "mapping_coh_wh = pd.DataFrame({\n",
    "    'region':       ['Cairo', 'Cairo', 'Giza', 'Delta West', 'Delta West', 'Delta East', \n",
    "                     'Delta East', 'Upper Egypt', 'Upper Egypt', 'Upper Egypt', 'Upper Egypt', \n",
    "                     'Alexandria', 'Giza'],\n",
    "    'wh':           ['El-Marg', 'Mostorod', 'Barageel', 'El-Mahala', 'Tanta', 'Mansoura FC',\n",
    "                     'Sharqya', 'Assiut FC', 'Bani sweif', 'Menya Samalot', 'Sohag',\n",
    "                     'Khorshed Alex', 'Sakkarah'],\n",
    "    'warehouse_id': [38, 1, 236, 337, 8, 339, 170, 501, 401, 703, 632, 797, 962],\n",
    "    'cohort_id':    [700, 700, 701, 703, 703, 704, 704, 1124, 1126, 1123, 1125, 702, 701]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de132378-5bbb-4eff-a8d6-add1915ff206",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Cart rules mapped to 13 warehouses\n"
     ]
    }
   ],
   "source": [
    "# Add warehouse mapping to cart rules\n",
    "live_cart_rules = live_cart_rules.merge(mapping_coh_wh, on='cohort_id')\n",
    "print(f\"✓ Cart rules mapped to {live_cart_rules['warehouse_id'].nunique()} warehouses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545252e6",
   "metadata": {},
   "source": [
    "## 5. Price Tier Calculation\n",
    "\n",
    "Calculate tier 1 and tier 2 prices with constraints:\n",
    "- **Max discount**: 5% from current price\n",
    "- **Min discount**: 0.35% from current price  \n",
    "- **Ratio bounds**: discount-to-quantity ratio between 1.05 and 3\n",
    "- **Price ordering**: WAC < Tier 2 < Tier 1 < Current Price\n",
    "- **Tier 1 margin constraint**: margin >= 60% of current margin\n",
    "- **Tier 2 margin constraint**: margin >= 40% of current margin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c14fe97",
   "metadata": {},
   "source": [
    "### 5.1 Price Calculation Functions\n",
    "\n",
    "The `calculate_tier_prices` function uses multiple strategies:\n",
    "1. **Market prices strategy** - Use competitive pricing data if available\n",
    "2. **Margin range strategy** - Calculate from margin boundaries if no market data\n",
    "3. **Ratio adjustment** - Adjust tier_2 price to meet discount-to-quantity ratio bounds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a4250db-739f-4602-9fcb-fd3638c7343c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_tier_prices(row, max_discount_pct=5.0, min_discount_pct=0.35, min_ratio=1.1, max_ratio=3):\n",
    "    \"\"\"\n",
    "    Calculate tier 1 and tier 2 prices for a single row.\n",
    "    \n",
    "    Parameters:\n",
    "    - max_discount_pct: Maximum allowed discount from current price (default: 5%)\n",
    "    - min_discount_pct: Minimum required discount from current price (default: 0.35%)\n",
    "    - min_ratio: Minimum discount-to-quantity ratio (default: 1.3)\n",
    "    - max_ratio: Maximum discount-to-quantity ratio (default: 3.5)\n",
    "    \n",
    "    Constraints:\n",
    "    - Tier 1 margin must be >= 60% of current margin\n",
    "    - Tier 2 margin must be >= 40% of current margin\n",
    "    - Ensure: WAC < Tier 2 < Tier 1 < Current Price\n",
    "    - Ensure: BOTH tiers must be valid or BOTH are None\n",
    "    - Ensure: discount_qty_ratio = (tier_2_discount/tier_1_discount) / (tier_2_qty/tier_1_qty) is between min_ratio and max_ratio\n",
    "    \"\"\"\n",
    "    \n",
    "    current_price = row['packing_unit_price']\n",
    "    wac = row['wac_p']\n",
    "    \n",
    "    # Get basic_unit_count for converting market prices\n",
    "    basic_unit_count = row.get('basic_unit_count', 1)\n",
    "    if pd.isna(basic_unit_count) or basic_unit_count <= 0:\n",
    "        basic_unit_count = 1\n",
    "    \n",
    "    # Validation\n",
    "    if pd.isna(current_price) or current_price <= 0:\n",
    "        return pd.Series({'tier_1_price': np.nan, 'tier_2_price': np.nan, 'price_source': 'invalid_current_price'})\n",
    "    \n",
    "    if pd.isna(wac) or wac <= 0:\n",
    "        return pd.Series({'tier_1_price': np.nan, 'tier_2_price': np.nan, 'price_source': 'invalid_wac'})\n",
    "    \n",
    "    if current_price <= wac:\n",
    "        return pd.Series({'tier_1_price': np.nan, 'tier_2_price': np.nan, 'price_source': 'current_price_below_wac'})\n",
    "    \n",
    "    # Calculate discount bounds\n",
    "    max_discount_price = current_price * (1 - max_discount_pct / 100)  # Minimum allowed price\n",
    "    min_discount_price = current_price * (1 - min_discount_pct / 100)  # Maximum allowed price\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # MARGIN-BASED MINIMUM PRICES (NEW CONSTRAINT)\n",
    "    # ==========================================================================\n",
    "    # Calculate current margin: margin = (price - wac) / price\n",
    "    current_margin = (current_price - wac) / current_price\n",
    "    \n",
    "    # Tier 1: minimum margin = 60% of current margin\n",
    "    min_t1_margin = 0.60 * current_margin\n",
    "    # Calculate minimum T1 price: price = wac / (1 - margin)\n",
    "    min_t1_price = wac / (1 - min_t1_margin) if min_t1_margin < 1 else current_price\n",
    "    \n",
    "    # Tier 2: minimum margin = 40% of current margin\n",
    "    min_t2_margin = 0.40 * current_margin\n",
    "    # Calculate minimum T2 price: price = wac / (1 - margin)\n",
    "    min_t2_price = wac / (1 - min_t2_margin) if min_t2_margin < 1 else current_price\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # ABSOLUTE MINIMUM PRICE (legacy constraint - use higher of this or margin-based)\n",
    "    # ==========================================================================\n",
    "    absolute_min_price = wac*0.9  # Default to WAC if no target_margin\n",
    "    \n",
    "    if 'target_margin' in row.index and pd.notna(row['target_margin']) and 0 < row['target_margin'] < 1:\n",
    "        target_margin = row['target_margin']\n",
    "        # Minimum margin is 30% of target margin\n",
    "        min_margin = target_margin * 0.5\n",
    "        # Calculate minimum price: price = wac / (1 - min_margin)\n",
    "        absolute_min_price = wac / (1 - min_margin)\n",
    "    else:\n",
    "        # Fallback: use wac_cushion_pct\n",
    "        wac_cushion_pct = 0.25\n",
    "        absolute_min_price = wac / (1 - (wac_cushion_pct / 100))\n",
    "    \n",
    "    # Use the HIGHER of absolute_min_price or margin-based minimums\n",
    "    min_t1_price = max(min_t1_price, absolute_min_price)\n",
    "    min_t2_price = max(min_t2_price, absolute_min_price)\n",
    "    \n",
    "    # Market price columns (these are per basic unit)\n",
    "    market_cols = [\n",
    "        'final_mod_price', 'median_scrapped', 'final_max_price', \n",
    "        'ben_soliman_price', 'max_scrapped', 'final_true_max',\n",
    "        'final_min_price', 'min_scrapped', 'final_true_min'\n",
    "    ]\n",
    "    \n",
    "    # Extract valid market prices (multiply by basic_unit_count, above absolute_min_price, within discount bounds)\n",
    "    valid_market_prices = []\n",
    "    for col in market_cols:\n",
    "        if col in row.index and pd.notna(row[col]) and row[col] > 0:\n",
    "            # Convert basic unit price to packing unit price\n",
    "            packing_price = row[col] * basic_unit_count\n",
    "            \n",
    "            # Must be: above absolute_min_price AND within discount bounds\n",
    "            if max_discount_price <= packing_price <= min_discount_price and packing_price > (wac*basic_unit_count)*0.9:\n",
    "                valid_market_prices.append(packing_price)\n",
    "    \n",
    "    # Remove duplicates and sort descending\n",
    "    valid_market_prices = sorted(list(set(valid_market_prices)), reverse=True)\n",
    "    \n",
    "    tier_1 = None\n",
    "    tier_2 = None\n",
    "    source = ''\n",
    "    \n",
    "    min_gap_pct = 0.25\n",
    "    \n",
    "    # Strategy 1: Use market prices\n",
    "    if len(valid_market_prices) >= 3:\n",
    "        # Select from available prices\n",
    "        tier_1 = valid_market_prices[0]  # Highest price\n",
    "        \n",
    "        # Find tier 2 with minimum gap\n",
    "        for price in valid_market_prices[1:]:\n",
    "            if price < tier_1 * (1 - min_gap_pct / 100):\n",
    "                tier_2 = price\n",
    "                break\n",
    "        \n",
    "        # If no suitable tier 2 found, take second highest\n",
    "        if tier_2 is None and len(valid_market_prices) > 1:\n",
    "            tier_2 = valid_market_prices[1]\n",
    "        \n",
    "        if tier_1 is not None and tier_2 is not None:\n",
    "            source = 'market_prices'\n",
    "    \n",
    "    elif len(valid_market_prices) == 2:\n",
    "        tier_1 = valid_market_prices[0]\n",
    "        tier_2 = valid_market_prices[1]\n",
    "        source = 'market_prices'\n",
    "    \n",
    "    elif len(valid_market_prices) == 1:\n",
    "        # Only one market price - use margin range for the other\n",
    "        market_price = valid_market_prices[0]\n",
    "        \n",
    "        # Calculate which tier this should be based on its position\n",
    "        price_position = (market_price - max_discount_price) / (min_discount_price - max_discount_price)\n",
    "        \n",
    "        # If in upper half (>0.5), use as tier 1 and calculate tier 2\n",
    "        # If in lower half (<=0.5), use as tier 2 and calculate tier 1\n",
    "        if price_position > 0.5:\n",
    "            tier_1 = market_price\n",
    "            tier_2 = calculate_from_margin_range(row, wac, current_price, tier_1, tier=2, \n",
    "                                                 max_discount_price=max_discount_price,\n",
    "                                                 min_discount_price=min_discount_price,\n",
    "                                                 absolute_min_price=absolute_min_price)\n",
    "            if tier_2 is not None:\n",
    "                source = 'market_tier1_margin_tier2'\n",
    "        else:\n",
    "            tier_2 = market_price\n",
    "            tier_1 = calculate_from_margin_range(row, wac, current_price, tier_2, tier=1,\n",
    "                                                 max_discount_price=max_discount_price,\n",
    "                                                 min_discount_price=min_discount_price,\n",
    "                                                 absolute_min_price=absolute_min_price)\n",
    "            if tier_1 is not None:\n",
    "                source = 'margin_tier1_market_tier2'\n",
    "    \n",
    "    # Strategy 2: No market prices - use margin range method\n",
    "    if tier_1 is None or tier_2 is None:\n",
    "        tier_1, tier_2 = calculate_both_from_margin_range(row, wac, current_price,\n",
    "                                                          max_discount_price=max_discount_price,\n",
    "                                                          min_discount_price=min_discount_price,\n",
    "                                                          absolute_min_price=absolute_min_price)\n",
    "        if tier_1 is not None and tier_2 is not None:\n",
    "            source = 'margin_range_based'\n",
    "    \n",
    "    # CRITICAL: Final validation - BOTH must be valid or BOTH are None\n",
    "    if tier_1 is not None and tier_2 is not None:\n",
    "        # Ensure correct ordering\n",
    "        if tier_2 >= tier_1:\n",
    "            tier_1, tier_2 = max(tier_1, tier_2), min(tier_1, tier_2)\n",
    "        \n",
    "        # Apply discount bounds\n",
    "        tier_1 = max(tier_1, max_discount_price)\n",
    "        tier_1 = min(tier_1, min_discount_price)\n",
    "        tier_2 = max(tier_2, max_discount_price)\n",
    "        tier_2 = min(tier_2, min_discount_price)\n",
    "        \n",
    "        # ==========================================================================\n",
    "        # ENFORCE MARGIN-BASED MINIMUM PRICES (NEW CONSTRAINT)\n",
    "        # Tier 1: must maintain >= 60% of current margin\n",
    "        # Tier 2: must maintain >= 40% of current margin\n",
    "        # ==========================================================================\n",
    "        # Enforce T1 minimum (60% of current margin)\n",
    "        if tier_1 < min_t1_price:\n",
    "            tier_1 = min_t1_price\n",
    "        \n",
    "        # Enforce T2 minimum (40% of current margin)\n",
    "        if tier_2 < min_t2_price:\n",
    "            tier_2 = min_t2_price\n",
    "        \n",
    "        # Check if prices are still valid after enforcement\n",
    "        if tier_1 <= wac or tier_2 <= wac:\n",
    "            tier_1 = None\n",
    "            tier_2 = None\n",
    "            source = 'prices_below_wac_after_margin_enforcement'\n",
    "        elif tier_2 >= tier_1:\n",
    "            # T2 minimum pushed it above T1 - invalid\n",
    "            tier_1 = None\n",
    "            tier_2 = None\n",
    "            source = 'margin_constraints_conflict'\n",
    "        elif tier_1 > min_discount_price or tier_2 > min_discount_price:\n",
    "            # Prices pushed above max allowed (min discount)\n",
    "            tier_1 = None\n",
    "            tier_2 = None\n",
    "            source = 'margin_constraints_exceed_discount_bounds'\n",
    "        else:\n",
    "            # Ensure minimum gap between tiers\n",
    "            if tier_2 > tier_1 * (1 - min_gap_pct / 100):\n",
    "                tier_2 = tier_1 * (1 - min_gap_pct / 100)\n",
    "                # Re-check T2 minimum after gap adjustment\n",
    "                if tier_2 < min_t2_price:\n",
    "                    tier_1 = None\n",
    "                    tier_2 = None\n",
    "                    source = 'insufficient_gap_with_margin_constraint'\n",
    "            \n",
    "            # Final check: both still valid?\n",
    "            if tier_1 is not None and tier_2 is not None:\n",
    "                if not (wac < tier_2 < tier_1 < current_price):\n",
    "                    tier_1 = None\n",
    "                    tier_2 = None\n",
    "                    source = 'invalid_tier_ordering'\n",
    "                elif not (max_discount_price <= tier_2 and tier_1 <= min_discount_price):\n",
    "                    tier_1 = None\n",
    "                    tier_2 = None\n",
    "                    source = 'tiers_outside_discount_bounds'\n",
    "                else:\n",
    "                    tier_1 = round(tier_1, 2)\n",
    "                    tier_2 = round(tier_2, 2)\n",
    "                    \n",
    "                    # =================================================================\n",
    "                    # RATIO ADJUSTMENT (adjust tier_2 price if ratio out of bounds)\n",
    "                    # =================================================================\n",
    "                    tier_1_qty = row.get('tier_1_qty', None)\n",
    "                    tier_2_qty = row.get('tier_2_qty', None)\n",
    "                    \n",
    "                    if tier_1_qty is not None and tier_2_qty is not None and tier_1_qty > 0:\n",
    "                        tier_1_discount = current_price - tier_1\n",
    "                        tier_2_discount = current_price - tier_2\n",
    "                        \n",
    "                        if tier_1_discount > 0:\n",
    "                            diff_quantity = tier_2_qty / tier_1_qty\n",
    "                            diff_discount = tier_2_discount / tier_1_discount\n",
    "                            \n",
    "                            if diff_quantity > 0:\n",
    "                                discount_qty_ratio = diff_discount / diff_quantity\n",
    "                                \n",
    "                                # If ratio too high, reduce T2 discount\n",
    "                                if discount_qty_ratio > max_ratio:\n",
    "                                    target_tier_2_discount = max_ratio * diff_quantity * tier_1_discount\n",
    "                                    adjusted_tier_2 = current_price - target_tier_2_discount\n",
    "                                    \n",
    "                                    # Check against T2 minimum (40% of current margin)\n",
    "                                    if adjusted_tier_2 > wac and adjusted_tier_2 >= min_t2_price and adjusted_tier_2 < tier_1:\n",
    "                                        tier_2 = round(adjusted_tier_2, 2)\n",
    "                                        source = source + '_ratio_down'\n",
    "                                    else:\n",
    "                                        tier_1 = None\n",
    "                                        tier_2 = None\n",
    "                                        source = f'cannot_ratio_{discount_qty_ratio:.2f}_max_margin_constraint'\n",
    "                                \n",
    "                                # If ratio too low, increase T2 discount\n",
    "                                elif discount_qty_ratio < min_ratio:\n",
    "                                    target_tier_2_discount = min_ratio * diff_quantity * tier_1_discount\n",
    "                                    adjusted_tier_2 = current_price - target_tier_2_discount\n",
    "                                    \n",
    "                                    # Check against T2 minimum (40% of current margin)\n",
    "                                    if adjusted_tier_2 > wac and adjusted_tier_2 >= min_t2_price and adjusted_tier_2 < tier_1:\n",
    "                                        tier_2 = round(adjusted_tier_2, 2)\n",
    "                                        source = source + '_ratio_up'\n",
    "                                    else:\n",
    "                                        tier_1 = None\n",
    "                                        tier_2 = None\n",
    "                                        source = f'cannot_ratio_{discount_qty_ratio:.2f}_min_margin_constraint'\n",
    "    \n",
    "    # FINAL CHECK: If only one tier exists, invalidate both\n",
    "    if (tier_1 is None and tier_2 is not None) or (tier_1 is not None and tier_2 is None):\n",
    "        tier_1 = None\n",
    "        tier_2 = None\n",
    "        source = 'incomplete_tier_pair'\n",
    "    \n",
    "    # If both are None and no source set, mark it\n",
    "    if tier_1 is None and tier_2 is None and source == '':\n",
    "        source = 'no_valid_prices'\n",
    "    \n",
    "    return pd.Series({\n",
    "        'tier_1_price': tier_1,\n",
    "        'tier_2_price': tier_2,\n",
    "        'price_source': source\n",
    "    })\n",
    "\n",
    "\n",
    "def calculate_both_from_margin_range(row, wac, current_price, max_discount_price, min_discount_price, absolute_min_price):\n",
    "    \"\"\"\n",
    "    Calculate both tier prices using margin range from minimum of (min_boundary, optimal_bm) to current margin.\n",
    "    Returns (tier_1_price, tier_2_price) or (None, None)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate current margin: margin = (price - wac) / price\n",
    "    current_margin = (current_price - wac) / current_price\n",
    "    \n",
    "    # Get min_boundary margin\n",
    "    min_boundary_margin = None\n",
    "    if 'min_boundary' in row.index and pd.notna(row['min_boundary']) and 0 < row['min_boundary'] < 1:\n",
    "        min_boundary_margin = row['min_boundary']\n",
    "    \n",
    "    # Get optimal_bm margin\n",
    "    optimal_margin = None\n",
    "    if 'optimal_bm' in row.index and pd.notna(row['optimal_bm']) and 0 < row['optimal_bm'] < 1:\n",
    "        optimal_margin = row['optimal_bm']\n",
    "    \n",
    "    # Determine starting margin: minimum of (min_boundary, optimal_bm)\n",
    "    start_margin = None\n",
    "    \n",
    "    if min_boundary_margin is not None and optimal_margin is not None:\n",
    "        start_margin = min(min_boundary_margin, optimal_margin)\n",
    "    elif min_boundary_margin is not None:\n",
    "        start_margin = min_boundary_margin\n",
    "    elif optimal_margin is not None:\n",
    "        start_margin = optimal_margin\n",
    "    else:\n",
    "        # Fallback: use 50% of current margin\n",
    "        start_margin = current_margin * 0.85\n",
    "    \n",
    "    # Ensure start_margin is less than current margin\n",
    "    if start_margin >= current_margin:\n",
    "        start_margin = current_margin * 0.85\n",
    "    \n",
    "    # Generate margin points in the range (10 points)\n",
    "    num_points = 10\n",
    "    margin_range = np.linspace(start_margin, current_margin, num_points)\n",
    "    \n",
    "    # Calculate prices from these margins: price = wac / (1 - margin)\n",
    "    price_candidates = []\n",
    "    for margin in margin_range:\n",
    "        if margin < 1:\n",
    "            price = wac / (1 - margin)\n",
    "            # Only keep prices within discount bounds and above absolute_min_price\n",
    "            if  max_discount_price <= price <= min_discount_price and price > wac*0.9:\n",
    "                price_candidates.append(price)\n",
    "    \n",
    "    if len(price_candidates) < 2:\n",
    "        return None, None\n",
    "    \n",
    "    # Sort prices descending\n",
    "    price_candidates = sorted(price_candidates, reverse=True)\n",
    "    \n",
    "    # Select Tier 1: closer to the top (less discount)\n",
    "    # Select Tier 2: further down (more discount)\n",
    "    tier_1_idx = int(len(price_candidates) * 0.25)  # 25% from top\n",
    "    tier_2_idx = int(len(price_candidates) * 0.65)  # 65% from top\n",
    "    \n",
    "    # Ensure valid indices\n",
    "    tier_1_idx = max(0, min(tier_1_idx, len(price_candidates) - 2))\n",
    "    tier_2_idx = max(tier_1_idx + 1, min(tier_2_idx, len(price_candidates) - 1))\n",
    "    \n",
    "    tier_1 = price_candidates[tier_1_idx]\n",
    "    tier_2 = price_candidates[tier_2_idx]\n",
    "    \n",
    "    # Ensure meaningful gap (at least 0.5%)\n",
    "    min_gap_pct = 0.25\n",
    "    if tier_2 > tier_1 * (1 - min_gap_pct / 100):\n",
    "        # Try to find better tier_2\n",
    "        for i in range(tier_2_idx + 1, len(price_candidates)):\n",
    "            if price_candidates[i] < tier_1 * (1 - min_gap_pct / 100):\n",
    "                tier_2 = price_candidates[i]\n",
    "                break\n",
    "    \n",
    "    # Final validation\n",
    "    if tier_2 >= tier_1 or tier_1 <= wac*0.9 or tier_2 <= wac*0.9:\n",
    "        return None, None\n",
    "    \n",
    "    return tier_1, tier_2\n",
    "\n",
    "\n",
    "def calculate_from_margin_range(row, wac, current_price, other_tier_price, tier, \n",
    "                                max_discount_price, min_discount_price, absolute_min_price):\n",
    "    \"\"\"\n",
    "    Calculate single tier price using margin range.\n",
    "    Used when one tier is from market and we need to calculate the other.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate current margin\n",
    "    current_margin = (current_price - wac) / current_price\n",
    "    \n",
    "    # Get min_boundary margin\n",
    "    min_boundary_margin = None\n",
    "    if 'min_boundary' in row.index and pd.notna(row['min_boundary']) and 0 < row['min_boundary'] < 1:\n",
    "        min_boundary_margin = row['min_boundary']\n",
    "    \n",
    "    # Get optimal_bm margin\n",
    "    optimal_margin = None\n",
    "    if 'optimal_bm' in row.index and pd.notna(row['optimal_bm']) and 0 < row['optimal_bm'] < 1:\n",
    "        optimal_margin = row['optimal_bm']\n",
    "    \n",
    "    # Determine starting margin: minimum of (min_boundary, optimal_bm)\n",
    "    start_margin = None\n",
    "    \n",
    "    if min_boundary_margin is not None and optimal_margin is not None:\n",
    "        start_margin = min(min_boundary_margin, optimal_margin)\n",
    "    elif min_boundary_margin is not None:\n",
    "        start_margin = min_boundary_margin\n",
    "    elif optimal_margin is not None:\n",
    "        start_margin = optimal_margin\n",
    "    else:\n",
    "        start_margin = current_margin * 0.5\n",
    "    \n",
    "    # Ensure start_margin is less than current margin\n",
    "    if start_margin >= current_margin:\n",
    "        start_margin = current_margin * 0.7\n",
    "    \n",
    "    # Generate margin range (10 points)\n",
    "    num_points = 10\n",
    "    margin_range = np.linspace(start_margin, current_margin, num_points)\n",
    "    \n",
    "    # Calculate prices\n",
    "    price_candidates = []\n",
    "    for margin in margin_range:\n",
    "        if margin < 1:\n",
    "            price = wac / (1 - margin)\n",
    "            if max_discount_price <= price <= min_discount_price and price >= wac*0.9:\n",
    "                price_candidates.append(price)\n",
    "    \n",
    "    if len(price_candidates) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Sort prices descending\n",
    "    price_candidates = sorted(price_candidates, reverse=True)\n",
    "    \n",
    "    min_gap_pct = 0.5\n",
    "    \n",
    "    if tier == 1:\n",
    "        # Need tier 1 (higher price), we have tier 2 (lower price)\n",
    "        # Find prices above tier 2 with proper gap\n",
    "        target_candidates = [p for p in price_candidates \n",
    "                           if p > other_tier_price * (1 + min_gap_pct / 100)]\n",
    "        if target_candidates:\n",
    "            # Take from upper portion (25% position)\n",
    "            idx = int(len(target_candidates) * 0.25)\n",
    "            return target_candidates[idx]\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        # Need tier 2 (lower price), we have tier 1 (higher price)\n",
    "        # Find prices below tier 1 with proper gap\n",
    "        target_candidates = [p for p in price_candidates \n",
    "                           if p < other_tier_price * (1 - min_gap_pct / 100)]\n",
    "        if target_candidates:\n",
    "            # Take from lower portion (65% position)\n",
    "            idx = int(len(target_candidates) * 0.65)\n",
    "            idx = min(idx, len(target_candidates) - 1)\n",
    "            return target_candidates[idx]\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4895a40",
   "metadata": {},
   "source": [
    "### 5.2 Apply Price Calculations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d485d098-925d-4be2-8e44-18b33438c529",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 3955 SKUs...\n",
      "Parameters: MAX_DISCOUNT=5.0%, MIN_DISCOUNT=0.35%, RATIO=[1.05, 3]\n",
      "\n",
      "--- Ratio Adjustment Summary ---\n",
      "  Ratio adjusted down (was above 3): 119 SKUs\n",
      "  Ratio adjusted up (was below 1.05):   1512 SKUs\n",
      "  Could not adjust (constraints violated):  256 SKUs\n",
      "\n",
      "✓ Final SKUs with valid tier prices: 3086\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# APPLY PRICE CALCULATIONS\n",
    "# =============================================================================\n",
    "\n",
    "# Normalize column names\n",
    "final_data.columns = final_data.columns.str.lower()\n",
    "\n",
    "print(f\"Processing {len(final_data)} SKUs...\")\n",
    "print(f\"Parameters: MAX_DISCOUNT={MAX_DISCOUNT_PCT}%, MIN_DISCOUNT={MIN_DISCOUNT_PCT}%, RATIO=[{MIN_RATIO}, {MAX_RATIO}]\")\n",
    "\n",
    "# Apply price calculation to each row\n",
    "result = final_data.apply(\n",
    "    lambda row: calculate_tier_prices(\n",
    "        row, \n",
    "        max_discount_pct=MAX_DISCOUNT_PCT,\n",
    "        min_discount_pct=MIN_DISCOUNT_PCT,\n",
    "        min_ratio=MIN_RATIO,\n",
    "        max_ratio=MAX_RATIO\n",
    "    ), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Merge results back to dataframe\n",
    "final_data = pd.concat([final_data, result], axis=1)\n",
    "\n",
    "# Summary of ratio adjustments\n",
    "ratio_adjusted_down = final_data['price_source'].str.contains('ratio_down', na=False).sum()\n",
    "ratio_adjusted_up = final_data['price_source'].str.contains('ratio_up', na=False).sum()\n",
    "cannot_adjust = final_data['price_source'].str.contains('cannot', na=False).sum()\n",
    "\n",
    "print(f\"\\n--- Ratio Adjustment Summary ---\")\n",
    "print(f\"  Ratio adjusted down (was above {MAX_RATIO}): {ratio_adjusted_down} SKUs\")\n",
    "print(f\"  Ratio adjusted up (was below {MIN_RATIO}):   {ratio_adjusted_up} SKUs\")\n",
    "print(f\"  Could not adjust (constraints violated):  {cannot_adjust} SKUs\")\n",
    "\n",
    "# Filter to only products with valid tier prices\n",
    "final_data = final_data[\n",
    "    (~final_data['tier_1_price'].isna()) & \n",
    "    (~final_data['tier_2_price'].isna())\n",
    "]\n",
    "\n",
    "print(f\"\\n✓ Final SKUs with valid tier prices: {len(final_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ea38543",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "APPLYING DISCOUNT CONSTRAINTS FROM FEEDBACK\n",
      "================================================================================\n",
      "  ✓ Applied T1 discount adjustments: 1128 SKUs\n",
      "  ⚠ Skipped T1 discount for 50 SKUs (script already better)\n",
      "  ✓ Applied T2 discount adjustments: 1100 SKUs\n",
      "  ⚠ Skipped T2 discount for 78 SKUs (script already better)\n",
      "\n",
      "  Validating discount constraints...\n",
      "    Fixed 11 SKUs where T2 discount <= T1 discount\n",
      "    Fixed 1444 SKUs with elasticity < 1.1\n",
      "\n",
      "  Final elasticity statistics:\n",
      "    Min: 1.09\n",
      "    Mean: 1.56\n",
      "    Max: 12.93\n",
      "    SKUs below 1.1: 754\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# VALIDATE AND APPLY DISCOUNT CONSTRAINTS FROM FEEDBACK\n",
    "# =============================================================================\n",
    "# Constraints:\n",
    "# 1. T1 discount < T2 discount < T3 discount (WS discount)\n",
    "# 2. Elasticity ratio >= 1.1 between consecutive tiers\n",
    "\n",
    "if len(feedback_data) > 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"APPLYING DISCOUNT CONSTRAINTS FROM FEEDBACK\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Merge feedback discount suggestions\n",
    "    final_data = final_data.merge(\n",
    "        feedback_data[['warehouse_id', 'product_id', 'packing_unit_id',\n",
    "                       'suggested_t1_discount', 'suggested_t2_discount',\n",
    "                       't1_action', 't2_action']],\n",
    "        on=['warehouse_id', 'product_id', 'packing_unit_id'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # SMART T1 DISCOUNT ADJUSTMENTS\n",
    "    # Only apply if recommendation actually improves the current value\n",
    "    # ==========================================================================\n",
    "    mask_d1 = final_data['suggested_t1_discount'].notna()\n",
    "    d1_applied = 0\n",
    "    d1_skipped = 0\n",
    "    final_data['discount_1'] = (final_data['packing_unit_price'] - final_data['tier_1_price']).round(2)\n",
    "    final_data['discount_2'] = (final_data['packing_unit_price'] - final_data['tier_2_price']).round(2)\n",
    "    final_data['discount_1_pct'] = ((final_data['discount_1'] / final_data['packing_unit_price']) * 100).round(2)\n",
    "    final_data['discount_2_pct'] = ((final_data['discount_2'] / final_data['packing_unit_price']) * 100).round(2)\n",
    "    if mask_d1.sum() > 0:\n",
    "        original_d1 = final_data.loc[mask_d1, 'discount_1_pct'].copy()\n",
    "        suggested_d1 = final_data.loc[mask_d1, 'suggested_t1_discount']\n",
    "        d1_action = final_data.loc[mask_d1, 't1_action'].fillna('')\n",
    "        \n",
    "        # Skip if: action is INCREASE_DISCOUNT and script already >= suggested\n",
    "        # Skip if: action is DECREASE_DISCOUNT/REDUCE and script already <= suggested\n",
    "        skip_d1_increase = d1_action.str.contains('INCREASE_DISCOUNT', case=False, na=False) & (original_d1 >= suggested_d1)\n",
    "        skip_d1_decrease = d1_action.str.contains('DECREASE_DISCOUNT|REDUCE_DISCOUNT', case=False, na=False) & (original_d1 <= suggested_d1)\n",
    "        skip_d1 = skip_d1_increase | skip_d1_decrease\n",
    "        \n",
    "        # Apply only where NOT skipped\n",
    "        apply_d1 = mask_d1.copy()\n",
    "        apply_d1.loc[mask_d1] = ~skip_d1.values\n",
    "        \n",
    "        if apply_d1.sum() > 0:\n",
    "            final_data.loc[apply_d1, 'discount_1_pct'] = final_data.loc[apply_d1, 'suggested_t1_discount']\n",
    "            final_data.loc[apply_d1, 'tier_1_price'] = (\n",
    "                final_data.loc[apply_d1, 'packing_unit_price'] * \n",
    "                (1 - final_data.loc[apply_d1, 'suggested_t1_discount'] / 100)\n",
    "            ).round(2)\n",
    "            d1_applied = apply_d1.sum()\n",
    "            print(f\"  ✓ Applied T1 discount adjustments: {d1_applied} SKUs\")\n",
    "        \n",
    "        d1_skipped = skip_d1.sum()\n",
    "        if d1_skipped > 0:\n",
    "            print(f\"  ⚠ Skipped T1 discount for {d1_skipped} SKUs (script already better)\")\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # SMART T2 DISCOUNT ADJUSTMENTS\n",
    "    # Only apply if recommendation actually improves the current value\n",
    "    # ==========================================================================\n",
    "    mask_d2 = final_data['suggested_t2_discount'].notna()\n",
    "    d2_applied = 0\n",
    "    d2_skipped = 0\n",
    "    \n",
    "    if mask_d2.sum() > 0:\n",
    "        original_d2 = final_data.loc[mask_d2, 'discount_2_pct'].copy()\n",
    "        suggested_d2 = final_data.loc[mask_d2, 'suggested_t2_discount']\n",
    "        d2_action = final_data.loc[mask_d2, 't2_action'].fillna('')\n",
    "        \n",
    "        # Skip if: action is INCREASE_DISCOUNT and script already >= suggested\n",
    "        # Skip if: action is DECREASE_DISCOUNT/REDUCE and script already <= suggested\n",
    "        skip_d2_increase = d2_action.str.contains('INCREASE_DISCOUNT', case=False, na=False) & (original_d2 >= suggested_d2)\n",
    "        skip_d2_decrease = d2_action.str.contains('DECREASE_DISCOUNT|REDUCE_DISCOUNT', case=False, na=False) & (original_d2 <= suggested_d2)\n",
    "        skip_d2 = skip_d2_increase | skip_d2_decrease\n",
    "        \n",
    "        # Apply only where NOT skipped\n",
    "        apply_d2 = mask_d2.copy()\n",
    "        apply_d2.loc[mask_d2] = ~skip_d2.values\n",
    "        \n",
    "        if apply_d2.sum() > 0:\n",
    "            final_data.loc[apply_d2, 'discount_2_pct'] = final_data.loc[apply_d2, 'suggested_t2_discount']\n",
    "            final_data.loc[apply_d2, 'tier_2_price'] = (\n",
    "                final_data.loc[apply_d2, 'packing_unit_price'] * \n",
    "                (1 - final_data.loc[apply_d2, 'suggested_t2_discount'] / 100)\n",
    "            ).round(2)\n",
    "            d2_applied = apply_d2.sum()\n",
    "            print(f\"  ✓ Applied T2 discount adjustments: {d2_applied} SKUs\")\n",
    "        \n",
    "        d2_skipped = skip_d2.sum()\n",
    "        if d2_skipped > 0:\n",
    "            print(f\"  ⚠ Skipped T2 discount for {d2_skipped} SKUs (script already better)\")\n",
    "    \n",
    "    # Drop action columns (no longer needed)\n",
    "    final_data = final_data.drop(columns=['t1_action', 't2_action'], errors='ignore')\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # VALIDATE DISCOUNT CONSTRAINTS\n",
    "    # ==========================================================================\n",
    "    print(\"\\n  Validating discount constraints...\")\n",
    "    \n",
    "    # Constraint 1: T2 discount must be > T1 discount\n",
    "    invalid_disc = final_data['discount_2_pct'] <= final_data['discount_1_pct']\n",
    "    if invalid_disc.sum() > 0:\n",
    "        # Fix: Set T2 discount = T1 discount + 0.5\n",
    "        final_data.loc[invalid_disc, 'discount_2_pct'] = final_data.loc[invalid_disc, 'discount_1_pct'] + 0.5\n",
    "        final_data.loc[invalid_disc, 'tier_2_price'] = (\n",
    "            final_data.loc[invalid_disc, 'packing_unit_price'] * \n",
    "            (1 - final_data.loc[invalid_disc, 'discount_2_pct'] / 100)\n",
    "        ).round(2)\n",
    "        print(f\"    Fixed {invalid_disc.sum()} SKUs where T2 discount <= T1 discount\")\n",
    "    \n",
    "    # Constraint 2: Elasticity ratio >= MIN_ELASTICITY_RATIO\n",
    "    qty_ratio = final_data['tier_2_qty'] / final_data['tier_1_qty']\n",
    "    disc_ratio = final_data['discount_2_pct'] / final_data['discount_1_pct'].replace(0, np.nan)\n",
    "    elasticity = disc_ratio / qty_ratio\n",
    "    \n",
    "    low_elasticity = elasticity < MIN_ELASTICITY_RATIO\n",
    "    if low_elasticity.sum() > 0:\n",
    "        # Fix: Increase T2 discount to meet minimum elasticity\n",
    "        required_disc_ratio = MIN_ELASTICITY_RATIO * qty_ratio\n",
    "        new_d2 = final_data['discount_1_pct'] * required_disc_ratio\n",
    "        final_data.loc[low_elasticity, 'discount_2_pct'] = new_d2.loc[low_elasticity].round(2)\n",
    "        final_data.loc[low_elasticity, 'tier_2_price'] = (\n",
    "            final_data.loc[low_elasticity, 'packing_unit_price'] * \n",
    "            (1 - final_data.loc[low_elasticity, 'discount_2_pct'] / 100)\n",
    "        ).round(2)\n",
    "        print(f\"    Fixed {low_elasticity.sum()} SKUs with elasticity < {MIN_ELASTICITY_RATIO}\")\n",
    "    \n",
    "    # Drop temporary columns\n",
    "    final_data = final_data.drop(columns=['suggested_t1_discount', 'suggested_t2_discount'], errors='ignore')\n",
    "\n",
    "# Calculate final elasticity statistics\n",
    "qty_ratio_final = final_data['tier_2_qty'] / final_data['tier_1_qty']\n",
    "disc_ratio_final = final_data['discount_2_pct'] / final_data['discount_1_pct'].replace(0, np.nan)\n",
    "elasticity_final = disc_ratio_final / qty_ratio_final\n",
    "\n",
    "print(f\"\\n  Final elasticity statistics:\")\n",
    "print(f\"    Min: {elasticity_final.min():.2f}\")\n",
    "print(f\"    Mean: {elasticity_final.mean():.2f}\")\n",
    "print(f\"    Max: {elasticity_final.max():.2f}\")\n",
    "print(f\"    SKUs below {MIN_ELASTICITY_RATIO}: {(elasticity_final < MIN_ELASTICITY_RATIO).sum()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b0ceb8",
   "metadata": {},
   "source": [
    "## 6. Wholesale Pricing\n",
    "\n",
    "Calculate wholesale prices based on:\n",
    "- Vehicle capacity (quarter truck)\n",
    "- Rank-based margin tiers (20%, 25%, 40%, 60% of target margin)\n",
    "- Must be below tier_2_price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f15518b2-699f-4c12-b551-f125b8abd4c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Prepared 3086 products for wholesale calculation\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PREPARE DELIVERY FEE DATA\n",
    "# =============================================================================\n",
    "\n",
    "# Set delivery fees based on region\n",
    "final_data['delivery_fees'] = DELIVERY_FEE_OTHER\n",
    "final_data.loc[final_data['region'].isin(['Cairo', 'Giza']), 'delivery_fees'] = DELIVERY_FEE_CAIRO_GIZA\n",
    "\n",
    "# Prepare query data for wholesale calculation\n",
    "query_data = final_data[['warehouse_id', 'product_id', 'packing_unit_id', 'delivery_fees']].values.tolist()\n",
    "query_info = ','.join([\n",
    "    f\"({int(wh_id)}, {int(prod_id)}, {int(pu_id)}, {int(delivery_fees)})\" \n",
    "    for wh_id, prod_id, pu_id, delivery_fees in query_data\n",
    "])\n",
    "\n",
    "print(f\"✓ Prepared {len(query_data)} products for wholesale calculation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "55ec0273-9d56-4dc6-b4bc-2ba8093792b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching wholesale cost data (quarter truck calculations)...\n",
      "✓ Calculated wholesale data for 3086 products\n"
     ]
    }
   ],
   "source": [
    "query = f'''\n",
    "with chosen_products as (\n",
    "select *\n",
    "from (\n",
    "values \n",
    "{query_info}\n",
    ")x(warehouse_id,product_id,packing_unit_id,delivery_fees)\n",
    "\n",
    "),\n",
    "vec as (\n",
    "select  vt.id as vehicle_id,name_en as vehicle_name,vc.weight as vehicle_weight,vc.cbm as vehicle_cbm,900 as vehicle_cost\n",
    "from VEHICLE_TYPES  vt \n",
    "join  RETOOL.VEHICLE_CAPACITIES vc on vc.vehicle_id = vt.id\n",
    "where vehicle_id = 1\n",
    "),\n",
    "selected_products as (\n",
    "select x.*,\t(long*width*height)/1000000 AS cbm,weight/1000 AS weight,\n",
    "from chosen_products x\n",
    "join packing_unit_products on x.product_id = packing_unit_products.product_id and packing_unit_products.packing_unit_id = x.packing_unit_id\n",
    "),\n",
    "main_cte as (\n",
    "select warehouse_id,product_id,packing_unit_id,delivery_fees,\n",
    "ceil(least(quart_dababa_wht,quart_dababa_cbm)) as quart_dababa,\n",
    "vehicle_cost\n",
    "from (\n",
    "select * ,\n",
    "((vehicle_weight*0.9)/4)/weight as quart_dababa_wht , \n",
    "((vehicle_cbm*0.9)/4)/cbm as quart_dababa_cbm  \n",
    "from (\n",
    "select selected_products.*, vehicle_weight,vehicle_cbm,vehicle_cost\n",
    "from selected_products,vec\n",
    ")\n",
    ")\n",
    ")\n",
    "select mc.*, f.wac_p , \n",
    "(f.wac_p*quart_dababa)+(((vehicle_cost-(delivery_fees*4))*0.9)/4) as quart_cost,\n",
    "quart_cost/quart_dababa as unit_cost\n",
    "\n",
    "\n",
    "from main_cte mc \n",
    "join finance.all_cogs f on f.product_id = mc.product_id and CURRENT_TIMEstamp between from_date and to_date \n",
    "\n",
    "'''\n",
    "\n",
    "print(\"Fetching wholesale cost data (quarter truck calculations)...\")\n",
    "ws_data = snowflake_query(\"Egypt\", query)\n",
    "ws_data.columns = ws_data.columns.str.lower()\n",
    "\n",
    "for col in ws_data.columns:\n",
    "    ws_data[col] = pd.to_numeric(ws_data[col], errors='ignore')\n",
    "\n",
    "# Select and rename columns\n",
    "ws_data = ws_data[['warehouse_id', 'product_id', 'packing_unit_id', 'quart_dababa', 'unit_cost']]\n",
    "ws_data.columns = ['warehouse_id', 'product_id', 'packing_unit_id', 'WS_tier', 'WS_wac']\n",
    "\n",
    "print(f\"✓ Calculated wholesale data for {len(ws_data)} products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "854c4e65-6403-480f-9380-519e106a5ae4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Skipping forced brands/categories - not needed for delivery savings logic\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# NOTE: Forced brands/categories loading removed\n",
    "# This was only used by the old rank-based wholesale logic which has been removed.\n",
    "# Now using only the delivery savings-based wholesale logic.\n",
    "# =============================================================================\n",
    "\n",
    "print(\"✓ Skipping forced brands/categories - not needed for delivery savings logic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4830c5dc-d49e-47b4-93f0-2645896e8e7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching price-up forecasts...\n",
      "✓ Retrieved 205 price-up forecasts\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "SELECT product_id, new_pp, forecasted_date\n",
    "FROM materialized_views.DBDP_PRICE_UPS\n",
    "WHERE region = 'Cairo'\n",
    "'''\n",
    "\n",
    "print(\"Fetching price-up forecasts...\")\n",
    "price_ups = snowflake_query(\"Egypt\", query)\n",
    "price_ups.columns = price_ups.columns.str.lower()\n",
    "\n",
    "for col in price_ups.columns:\n",
    "    price_ups[col] = pd.to_numeric(price_ups[col], errors='ignore')\n",
    "\n",
    "print(f\"✓ Retrieved {len(price_ups)} price-up forecasts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d82f21df-6b2c-4910-82f9-c7b758a3ac22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Added wholesale and price-up data to 3086 products\n"
     ]
    }
   ],
   "source": [
    "# Merge wholesale data and price-ups with final data\n",
    "final_data = final_data.merge(ws_data, on=['warehouse_id', 'product_id', 'packing_unit_id'], how='left')\n",
    "final_data['WS_wac'] = final_data['WS_wac'] * final_data['basic_unit_count']\n",
    "final_data = final_data.merge(price_ups, on='product_id', how='left')\n",
    "\n",
    "print(f\"✓ Added wholesale and price-up data to {len(final_data)} products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fb77deb3-b68a-4c4a-acc8-012272cf7e7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using delivery savings-based wholesale logic only\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# NOTE: Old wholesale logic (rank-based margin tiers) has been removed.\n",
    "# Only using the delivery savings-based wholesale logic (see next cells).\n",
    "# =============================================================================\n",
    "print(\"✓ Using delivery savings-based wholesale logic only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a1ed61e-384a-48f0-9f2d-e70593bde88a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Skipping old wholesale logic - using delivery savings logic only\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# OLD WHOLESALE LOGIC REMOVED\n",
    "# Wholesale pricing now uses only the delivery savings-based logic below\n",
    "# =============================================================================\n",
    "print(\"✓ Skipping old wholesale logic - using delivery savings logic only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8658cd",
   "metadata": {},
   "source": [
    "### 6.2 Wholesale NEW Logic (Delivery Savings Based)\n",
    "\n",
    "New wholesale pricing based on delivery cost savings:\n",
    "- **Car cost**: 1400 EGP per delivery\n",
    "- **Car capacity**: 1.8 tons max\n",
    "- **Max ticket size**: 30,000 EGP\n",
    "- **Logic**: If retailer orders multiples of average ticket size, they save deliveries\n",
    "  - 2x avg TS = 1 delivery saved → discount = delivery cost savings\n",
    "  - 3x avg TS = 2 deliveries saved → more discount\n",
    "- **Wholesale margin constraint**: margin >= 25% of current margin\n",
    "- **Goal**: Find optimal quantity that gives retailer max savings while maintaining minimum margin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "08a2d8b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== WAREHOUSE TICKET SIZE STATISTICS ===\n",
      "warehouse_name  avg_ticket_size  median_ticket_size  avg_order_weight_kg\n",
      "      Mostorod          5265.42             3640.50               120.99\n",
      "         Tanta          3573.23             2403.00                87.53\n",
      "       El-Marg          5265.42             3640.50               120.99\n",
      "       Sharqya          3720.38             2580.25                85.96\n",
      "      Barageel          5397.95             3666.00               113.57\n",
      "     El-Mahala          3573.23             2403.00                87.53\n",
      "   Mansoura FC          3720.38             2580.25                85.96\n",
      "    Bani sweif          4304.67             2566.00               114.68\n",
      "     Assiut FC          4304.67             2566.00               114.68\n",
      "         Sohag          4304.67             2566.00               114.68\n",
      " Menya Samalot          4304.67             2566.00               114.68\n",
      " Khorshed Alex          3798.80             2287.75                98.01\n",
      "      Sakkarah          5397.95             3666.00               113.57\n",
      "\n",
      "Overall average ticket size: 4379.34 EGP\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# WHOLESALE NEW LOGIC - Configuration\n",
    "# =============================================================================\n",
    "WS_CAR_COST = 1400           # Cost per delivery (EGP)\n",
    "WS_CAR_CAPACITY_TONS = 1.8  # Max car capacity in tons\n",
    "WS_MAX_TICKET_SIZE = 35000  # Maximum ticket size (EGP)\n",
    "WS_MIN_MARGIN = 0.015        # Minimum margin (1%) above WAC\n",
    "\n",
    "# Query to get average ticket size per warehouse\n",
    "query = f'''\n",
    "WITH base AS (\n",
    "    SELECT *, ROW_NUMBER() OVER (PARTITION BY retailer_id ORDER BY priority) as rnk \n",
    "    FROM (\n",
    "        SELECT x.*, TAGGABLE_ID as retailer_id \n",
    "        FROM (\n",
    "            SELECT id as cohort_id, name as cohort_name, priority, dynamic_tag_id \n",
    "            FROM cohorts \n",
    "            WHERE is_active = 'true'\n",
    "                AND id IN (700,701,702,703,704,1123,1124,1125,1126)\n",
    "        ) x \n",
    "        JOIN DYNAMIC_TAGgables dt ON x.dynamic_tag_id = dt.dynamic_tag_id\n",
    "        WHERE dt.taggable_id not IN (\n",
    "            SELECT taggable_id FROM DYNAMIC_TAGgables \n",
    "            WHERE dynamic_tag_id IN (2807, 2808, 2809, 2810, 2811, 2812)\n",
    "        )\n",
    "    )\n",
    "    QUALIFY rnk = 1 \n",
    "),\n",
    "\n",
    "-- Map regions to warehouses\n",
    "whs AS (\n",
    "    SELECT * FROM (VALUES\n",
    "        ('Cairo', 'El-Marg', 38),\n",
    "        ('Cairo', 'Mostorod', 1),\n",
    "        ('Giza', 'Barageel', 236),\n",
    "        ('Giza', 'Sakkarah', 962),\n",
    "        ('Delta West', 'El-Mahala', 337),\n",
    "        ('Delta West', 'Tanta', 8),\n",
    "        ('Delta East', 'Mansoura FC', 339),\n",
    "        ('Delta East', 'Sharqya', 170),\n",
    "        ('Upper Egypt', 'Assiut FC', 501),\n",
    "        ('Upper Egypt', 'Bani sweif', 401),\n",
    "        ('Upper Egypt', 'Menya Samalot', 703),\n",
    "        ('Upper Egypt', 'Sohag', 632),\n",
    "        ('Alexandria', 'Khorshed Alex', 797)\n",
    "    ) x(region_name, wh, warehouse_id)\n",
    "),\n",
    "\n",
    "-- Get ticket sizes (order values) for last 4 months\n",
    "ticket_sizes AS (\n",
    "    SELECT \n",
    "        whs.warehouse_id,\n",
    "        whs.wh as warehouse_name,\n",
    "        so.parent_sales_order_id,\n",
    "        so.retailer_id,\n",
    "        SUM(pso.total_price) as ticket_size,\n",
    "        SUM(pso.purchased_item_count * pup.weight / 1000) as order_weight_kg\n",
    "    FROM product_sales_order pso\n",
    "    JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "    JOIN base ON base.retailer_id = so.retailer_id\n",
    "    JOIN packing_unit_products pup ON pup.product_id = pso.product_id \n",
    "        AND pup.packing_unit_id = pso.packing_unit_id\n",
    "    JOIN materialized_views.retailer_polygon rp ON rp.retailer_id = so.retailer_id\n",
    "    JOIN districts ON districts.id = rp.district_id\n",
    "    JOIN cities ON cities.id = districts.city_id\n",
    "    JOIN states ON states.id = cities.state_id\n",
    "    JOIN regions ON regions.id = states.region_id\n",
    "    JOIN whs ON whs.region_name = CASE WHEN regions.id = 2 THEN states.name_en ELSE regions.name_en END\n",
    "    WHERE so.created_at::date BETWEEN DATE_TRUNC('month', CURRENT_DATE - INTERVAL '4 months') AND CURRENT_DATE - 1\n",
    "        AND so.sales_order_status_id NOT IN (7, 12)\n",
    "        AND so.channel IN ('telesales', 'retailer')\n",
    "        AND pso.purchased_item_count > 0\n",
    "    GROUP BY whs.warehouse_id, whs.wh, so.parent_sales_order_id, so.retailer_id\n",
    "),\n",
    "\n",
    "-- Calculate warehouse-level statistics\n",
    "warehouse_stats AS (\n",
    "    SELECT \n",
    "        warehouse_id,\n",
    "        warehouse_name,\n",
    "        COUNT(DISTINCT parent_sales_order_id) as total_orders,\n",
    "        COUNT(DISTINCT retailer_id) as total_retailers,\n",
    "        AVG(ticket_size) as avg_ticket_size,\n",
    "        MEDIAN(ticket_size) as median_ticket_size,\n",
    "        PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY ticket_size) as p75_ticket_size,\n",
    "        PERCENTILE_CONT(0.90) WITHIN GROUP (ORDER BY ticket_size) as p90_ticket_size,\n",
    "        MAX(ticket_size) as max_ticket_size,\n",
    "        AVG(order_weight_kg) as avg_order_weight_kg,\n",
    "        MEDIAN(order_weight_kg) as median_order_weight_kg\n",
    "    FROM ticket_sizes\n",
    "    WHERE ticket_size > 0\n",
    "    GROUP BY warehouse_id, warehouse_name\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    warehouse_id,\n",
    "    warehouse_name,\n",
    "    total_orders,\n",
    "    total_retailers,\n",
    "    ROUND(avg_ticket_size, 2) as avg_ticket_size,\n",
    "    ROUND(median_ticket_size, 2) as median_ticket_size,\n",
    "    ROUND(p75_ticket_size, 2) as p75_ticket_size,\n",
    "    ROUND(p90_ticket_size, 2) as p90_ticket_size,\n",
    "    ROUND(max_ticket_size, 2) as max_ticket_size,\n",
    "    ROUND(avg_order_weight_kg, 2) as avg_order_weight_kg,\n",
    "    ROUND(median_order_weight_kg, 2) as median_order_weight_kg,\n",
    "    -- Calculate how many orders fit in one car based on weight\n",
    "    ROUND({WS_CAR_CAPACITY_TONS * 1000} / NULLIF(avg_order_weight_kg, 0), 1) as orders_per_car_by_weight\n",
    "FROM warehouse_stats\n",
    "ORDER BY warehouse_id\n",
    "'''\n",
    "\n",
    "ws_ticket_data = snowflake_query(\"Egypt\", query)\n",
    "ws_ticket_data.columns = ws_ticket_data.columns.str.lower()\n",
    "for col in ws_ticket_data.columns:\n",
    "    ws_ticket_data[col] = pd.to_numeric(ws_ticket_data[col], errors='ignore')\n",
    "\n",
    "print(\"=== WAREHOUSE TICKET SIZE STATISTICS ===\")\n",
    "print(ws_ticket_data[['warehouse_name', 'avg_ticket_size', 'median_ticket_size', 'avg_order_weight_kg']].to_string(index=False))\n",
    "print(f\"\\nOverall average ticket size: {ws_ticket_data['avg_ticket_size'].mean():.2f} EGP\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "784f273b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating new wholesale logic based on delivery savings...\n",
      "\n",
      "=== NEW WHOLESALE LOGIC SUMMARY ===\n",
      "SKUs with valid WS new price: 1128 / 3086\n",
      "Total car cost: 1400 EGP\n",
      "Average orders per car: 17.6\n",
      "Average car cost per order: 79.61 EGP\n",
      "\n",
      "Order Consolidation:\n",
      "  Average multiplier: 7.8x of avg ticket size\n",
      "  Average order value needed: 32324.96 EGP\n",
      "  Average deliveries saved: 6.8\n",
      "\n",
      "Car Cost Savings:\n",
      "  Average car cost per order: 80.73 EGP\n",
      "  Average total savings: 538.74 EGP\n",
      "  Average discount per unit: 4.74 EGP\n",
      "\n",
      "Pricing:\n",
      "  Average WS new price margin: 3.79%\n",
      "  Average retailer savings: 1.65%\n",
      "\n",
      "Multiplier distribution:\n",
      "ws_new_multiplier\n",
      "3.0     27\n",
      "4.0     16\n",
      "5.0     18\n",
      "6.0    232\n",
      "7.0      4\n",
      "8.0    364\n",
      "9.0    467\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Merge ticket size data with final_data (including orders_per_car_by_weight)\n",
    "final_data = final_data.merge(\n",
    "    ws_ticket_data[['warehouse_id', 'avg_ticket_size', 'median_ticket_size', 'avg_order_weight_kg', 'orders_per_car_by_weight']], \n",
    "    on='warehouse_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "def calculate_ws_new_logic(row):\n",
    "    \"\"\"\n",
    "    Calculate wholesale pricing based on delivery savings.\n",
    "    \n",
    "    Logic:\n",
    "    - Car cost = 900 EGP, but car serves multiple orders per trip\n",
    "    - Car cost per order = 900 / orders_per_car\n",
    "    - If retailer consolidates, they save N orders worth of car cost\n",
    "    - Savings = deliveries_saved * (car_cost / orders_per_car)\n",
    "    - Calculate scenarios from 2x to max_multiplier (capped by max TS)\n",
    "    \n",
    "    Returns: Dict with optimal scenario\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get values\n",
    "    current_price = row['packing_unit_price']\n",
    "    wac = row['wac_p']\n",
    "    avg_ts = row.get('avg_ticket_size', 4000)  # Default 4000 if missing\n",
    "    tier_2_price = row['tier_2_price']\n",
    "    brand = row['brand']\n",
    "    \n",
    "    # Get orders per car (how many orders fit in one car trip based on weight)\n",
    "    orders_per_car = row.get('orders_per_car_by_weight', 15)  # Default 10 if missing\n",
    "    if pd.isna(orders_per_car) or orders_per_car <= 0:\n",
    "        orders_per_car = 15\n",
    "    \n",
    "    # Calculate car cost per order\n",
    "    car_cost_per_order = WS_CAR_COST / orders_per_car\n",
    "    \n",
    "    if pd.isna(avg_ts) or avg_ts <= 0:\n",
    "        avg_ts = 4000\n",
    "    \n",
    "    if pd.isna(current_price) or pd.isna(wac) or current_price <= 0 or wac <= 0 or pd.isna(tier_2_price):\n",
    "        return pd.Series({\n",
    "            'ws_new_multiplier': None,\n",
    "            'ws_new_order_value': None,\n",
    "            'ws_new_qty': None,\n",
    "            'ws_new_deliveries_saved': None,\n",
    "            'ws_new_car_cost_per_order': None,\n",
    "            'ws_new_total_savings': None,\n",
    "            'ws_new_discount_per_unit': None,\n",
    "            'ws_new_price': None,\n",
    "            'ws_new_margin': None,\n",
    "            'ws_new_savings_pct': None\n",
    "        })\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # MARGIN-BASED MINIMUM PRICE FOR WHOLESALE (NEW CONSTRAINT)\n",
    "    # Wholesale: minimum margin = 25% of current margin\n",
    "    # ==========================================================================\n",
    "    current_margin = (current_price - wac) / current_price\n",
    "    min_ws_margin = 0.4 * current_margin\n",
    "    # Calculate minimum WS price: price = wac / (1 - margin)\n",
    "    min_ws_price_margin_based = wac / (1 - min_ws_margin) if min_ws_margin < 1 else current_price\n",
    "    \n",
    "    # Also keep legacy minimum (WAC + WS_MIN_MARGIN)\n",
    "    min_ws_price_legacy = wac / (1 - WS_MIN_MARGIN)\n",
    "    \n",
    "    # Use the HIGHER of the two constraints\n",
    "    min_acceptable_price = max(min_ws_price_margin_based, min_ws_price_legacy)\n",
    "    \n",
    "    # Calculate max multiplier based on constraints\n",
    "    # Max by ticket size: WS_MAX_TICKET_SIZE / avg_ts\n",
    "    # No arbitrary cap - let WS_MAX_TICKET_SIZE (50K) be the only limit\n",
    "    max_multiplier = int(WS_MAX_TICKET_SIZE / avg_ts)\n",
    "    \n",
    "    best_scenario = None\n",
    "    best_savings_pct = 0\n",
    "    \n",
    "    # Test scenarios from 2x to max_multiplier\n",
    "    for multiplier in range(3, int(orders_per_car) + 1):#max_multiplier\n",
    "        # Order value at this multiplier\n",
    "        order_value = avg_ts * multiplier\n",
    "        \n",
    "        # Deliveries saved = multiplier - 1 (consolidating multiple orders into one)\n",
    "        deliveries_saved = multiplier - 1\n",
    "        \n",
    "        # Total savings = deliveries_saved * car_cost_per_order\n",
    "        # This is the actual cost saving from consolidating orders\n",
    "        total_savings = (deliveries_saved * car_cost_per_order)\n",
    "        \n",
    "        # How many units of this SKU fit in this order value?\n",
    "        qty_at_current_price = order_value / current_price\n",
    "        \n",
    "        if qty_at_current_price <= 0:\n",
    "            continue\n",
    "        \n",
    "        # Discount per unit from car cost savings\n",
    "        discount_per_unit = total_savings / qty_at_current_price\n",
    "        \n",
    "        # New price after passing car cost savings\n",
    "        new_price = current_price - discount_per_unit\n",
    "        \n",
    "        # Check if price stays above minimum (25% of current margin OR legacy min)\n",
    "        if new_price >= min_acceptable_price and order_value <=WS_MAX_TICKET_SIZE and new_price < tier_2_price:\n",
    "            # Calculate margin at new price\n",
    "            margin = (new_price - wac) / new_price\n",
    "            \n",
    "            # Savings percentage for retailer\n",
    "            savings_pct = (discount_per_unit / current_price) * 100\n",
    "            \n",
    "            # Keep track of best scenario (highest savings while valid)\n",
    "            if savings_pct > best_savings_pct:\n",
    "                best_savings_pct = savings_pct\n",
    "                best_scenario = {\n",
    "                    'ws_new_multiplier': multiplier,\n",
    "                    'ws_new_order_value': round(order_value, 2),\n",
    "                    'ws_new_qty': round(qty_at_current_price, 0),\n",
    "                    'ws_new_deliveries_saved': deliveries_saved,\n",
    "                    'ws_new_car_cost_per_order': round(car_cost_per_order, 2),\n",
    "                    'ws_new_total_savings': round(total_savings, 2),\n",
    "                    'ws_new_discount_per_unit': round(discount_per_unit, 2),\n",
    "                    'ws_new_price': round(new_price, 2),\n",
    "                    'ws_new_margin': round(margin, 4),\n",
    "                    'ws_new_savings_pct': round(savings_pct, 2)\n",
    "                }\n",
    "    \n",
    "    if best_scenario:\n",
    "        return pd.Series(best_scenario)\n",
    "    else:\n",
    "        return pd.Series({\n",
    "            'ws_new_multiplier': None,\n",
    "            'ws_new_order_value': None,\n",
    "            'ws_new_qty': None,\n",
    "            'ws_new_deliveries_saved': None,\n",
    "            'ws_new_car_cost_per_order': None,\n",
    "            'ws_new_total_savings': None,\n",
    "            'ws_new_discount_per_unit': None,\n",
    "            'ws_new_price': None,\n",
    "            'ws_new_margin': None,\n",
    "            'ws_new_savings_pct': None\n",
    "        })\n",
    "\n",
    "# Apply the new wholesale logic\n",
    "print(\"Calculating new wholesale logic based on delivery savings...\")\n",
    "ws_new_results = final_data.apply(calculate_ws_new_logic, axis=1)\n",
    "final_data = pd.concat([final_data, ws_new_results], axis=1)\n",
    "\n",
    "# Summary statistics\n",
    "valid_ws_new = final_data['ws_new_price'].notna().sum()\n",
    "print(f\"\\n=== NEW WHOLESALE LOGIC SUMMARY ===\")\n",
    "print(f\"SKUs with valid WS new price: {valid_ws_new} / {len(final_data)}\")\n",
    "print(f\"Total car cost: {WS_CAR_COST} EGP\")\n",
    "print(f\"Average orders per car: {final_data['orders_per_car_by_weight'].mean():.1f}\")\n",
    "print(f\"Average car cost per order: {WS_CAR_COST / final_data['orders_per_car_by_weight'].mean():.2f} EGP\")\n",
    "\n",
    "if valid_ws_new > 0:\n",
    "    print(f\"\\nOrder Consolidation:\")\n",
    "    print(f\"  Average multiplier: {final_data['ws_new_multiplier'].mean():.1f}x of avg ticket size\")\n",
    "    print(f\"  Average order value needed: {final_data['ws_new_order_value'].mean():.2f} EGP\")\n",
    "    print(f\"  Average deliveries saved: {final_data['ws_new_deliveries_saved'].mean():.1f}\")\n",
    "    \n",
    "    print(f\"\\nCar Cost Savings:\")\n",
    "    print(f\"  Average car cost per order: {final_data['ws_new_car_cost_per_order'].mean():.2f} EGP\")\n",
    "    print(f\"  Average total savings: {final_data['ws_new_total_savings'].mean():.2f} EGP\")\n",
    "    print(f\"  Average discount per unit: {final_data['ws_new_discount_per_unit'].mean():.2f} EGP\")\n",
    "    \n",
    "    print(f\"\\nPricing:\")\n",
    "    print(f\"  Average WS new price margin: {final_data['ws_new_margin'].mean()*100:.2f}%\")\n",
    "    print(f\"  Average retailer savings: {final_data['ws_new_savings_pct'].mean():.2f}%\")\n",
    "    \n",
    "    # Distribution of multipliers\n",
    "    print(f\"\\nMultiplier distribution:\")\n",
    "    print(final_data['ws_new_multiplier'].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "546b6edd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "APPLYING T3 (WHOLESALE) FEEDBACK ADJUSTMENTS\n",
      "================================================================================\n",
      "  ⚠ Skipped T3 qty for 7 SKUs (script already better)\n",
      "  ⚠ Capped 36 SKUs with large decrease (applied -15% instead)\n",
      "  ✓ Applied T3 qty adjustments: 266 SKUs (avg change: -9.9%)\n",
      "  ✓ Applied T3 discount adjustments: 305 SKUs\n",
      "  ⚠ Skipped T3 discount for 4 SKUs (script already better)\n",
      "\n",
      "  Validating T3 constraints...\n",
      "    Fixed 3 SKUs where T3 discount <= T2 discount\n",
      "\n",
      "  T3 Actions applied:\n",
      "    - DECREASE_QTY_OR_INCREASE_DISCOUNT: 365\n",
      "    - INCREASE_QTY_OR_REDUCE_DISCOUNT: 2\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# APPLY T3 (WHOLESALE) FEEDBACK ADJUSTMENTS\n",
    "# =============================================================================\n",
    "\n",
    "if len(feedback_t3_data) > 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"APPLYING T3 (WHOLESALE) FEEDBACK ADJUSTMENTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Merge T3 feedback with final_data\n",
    "    final_data = final_data.merge(\n",
    "        feedback_t3_data,\n",
    "        on=['warehouse_id', 'product_id', 'packing_unit_id'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Calculate current WS discount percentage for comparison\n",
    "    final_data['current_ws_discount'] = (\n",
    "        (final_data['packing_unit_price'] - final_data['ws_new_price']) / \n",
    "        final_data['packing_unit_price'] * 100\n",
    "    ).round(2)\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # SMART T3 QUANTITY ADJUSTMENTS\n",
    "    # Only apply if recommendation actually improves the current value\n",
    "    # ==========================================================================\n",
    "    mask_t3 = (final_data['suggested_t3_qty'].notna() & final_data['ws_new_qty'].notna())\n",
    "    t3_qty_applied = 0\n",
    "    t3_qty_skipped_already_better = 0\n",
    "    t3_qty_capped = 0\n",
    "    \n",
    "    if mask_t3.sum() > 0:\n",
    "        original_t3_qty = final_data.loc[mask_t3, 'ws_new_qty'].copy()\n",
    "        suggested_t3_qty = final_data.loc[mask_t3, 'suggested_t3_qty']\n",
    "        t3_action = final_data.loc[mask_t3, 't3_action'].fillna('')\n",
    "        \n",
    "        # Check if script already achieves the recommendation goal\n",
    "        # Skip if: action is INCREASE and script already >= suggested\n",
    "        # Skip if: action is DECREASE and script already <= suggested\n",
    "        skip_t3_increase = t3_action.str.contains('INCREASE_QTY', case=False, na=False) & (original_t3_qty >= suggested_t3_qty)\n",
    "        skip_t3_decrease = t3_action.str.contains('DECREASE_QTY', case=False, na=False) & (original_t3_qty <= suggested_t3_qty)\n",
    "        skip_t3_already_better = skip_t3_increase | skip_t3_decrease\n",
    "        \n",
    "        t3_qty_skipped_already_better = skip_t3_already_better.sum()\n",
    "        if t3_qty_skipped_already_better > 0:\n",
    "            print(f\"  ⚠ Skipped T3 qty for {t3_qty_skipped_already_better} SKUs (script already better)\")\n",
    "        \n",
    "        # For remaining SKUs, apply with capping logic\n",
    "        apply_t3_mask = mask_t3.copy()\n",
    "        apply_t3_mask.loc[mask_t3] = ~skip_t3_already_better.values\n",
    "        \n",
    "        if apply_t3_mask.sum() > 0:\n",
    "            # Calculate percentage change for those we'll process\n",
    "            pct_change = (final_data.loc[apply_t3_mask, 'suggested_t3_qty'] - \n",
    "                          final_data.loc[apply_t3_mask, 'ws_new_qty']) / final_data.loc[apply_t3_mask, 'ws_new_qty']\n",
    "            \n",
    "            # Handle extreme reductions (suggested < 30% of original, i.e., >70% reduction)\n",
    "            mask_extreme = pct_change < -0.70\n",
    "            mask_extreme_full = apply_t3_mask.copy()\n",
    "            mask_extreme_full.loc[apply_t3_mask] = mask_extreme.values\n",
    "            \n",
    "            if mask_extreme_full.sum() > 0:\n",
    "                final_data.loc[mask_extreme_full, 'ws_new_qty'] = (\n",
    "                    final_data.loc[mask_extreme_full, 'ws_new_qty'] * 0.9\n",
    "                ).astype(int)\n",
    "                print(f\"  ⚠ Capped {mask_extreme_full.sum()} SKUs with extreme reduction (applied -15% instead)\")\n",
    "                t3_qty_capped += mask_extreme_full.sum()\n",
    "            \n",
    "            # Handle large decreases (-30% to -70%)\n",
    "            mask_large_dec = (pct_change < -0.15) & (pct_change >= -0.70)\n",
    "            mask_large_dec_full = apply_t3_mask.copy()\n",
    "            mask_large_dec_full.loc[apply_t3_mask] = mask_large_dec.values\n",
    "            \n",
    "            if mask_large_dec_full.sum() > 0:\n",
    "                final_data.loc[mask_large_dec_full, 'ws_new_qty'] = (\n",
    "                    final_data.loc[mask_large_dec_full, 'ws_new_qty'] * 0.95\n",
    "                ).astype(int)\n",
    "                print(f\"  ⚠ Capped {mask_large_dec_full.sum()} SKUs with large decrease (applied -15% instead)\")\n",
    "                t3_qty_capped += mask_large_dec_full.sum()\n",
    "            \n",
    "            # Handle large increases (> +30%)\n",
    "            mask_large_inc = pct_change > 0.15\n",
    "            mask_large_inc_full = apply_t3_mask.copy()\n",
    "            mask_large_inc_full.loc[apply_t3_mask] = mask_large_inc.values\n",
    "            \n",
    "            if mask_large_inc_full.sum() > 0:\n",
    "                final_data.loc[mask_large_inc_full, 'ws_new_qty'] = (\n",
    "                    final_data.loc[mask_large_inc_full, 'ws_new_qty'] * 1.15\n",
    "                ).astype(int)\n",
    "                print(f\"  ⚠ Capped {mask_large_inc_full.sum()} SKUs with large increase (applied +15% instead)\")\n",
    "                t3_qty_capped += mask_large_inc_full.sum()\n",
    "            \n",
    "            # Handle normal changes (within ±30%)\n",
    "            mask_normal = (pct_change >= -0.15) & (pct_change <= 0.15)\n",
    "            mask_normal_full = apply_t3_mask.copy()\n",
    "            mask_normal_full.loc[apply_t3_mask] = mask_normal.values\n",
    "            \n",
    "            if mask_normal_full.sum() > 0:\n",
    "                orig = final_data.loc[mask_normal_full, 'ws_new_qty'].copy()\n",
    "                final_data.loc[mask_normal_full, 'ws_new_qty'] = final_data.loc[mask_normal_full, 'suggested_t3_qty'].astype(int)\n",
    "                avg_change = ((final_data.loc[mask_normal_full, 'ws_new_qty'] - orig) / orig * 100).mean()\n",
    "                t3_qty_applied = mask_normal_full.sum()\n",
    "                print(f\"  ✓ Applied T3 qty adjustments: {t3_qty_applied} SKUs (avg change: {avg_change:+.1f}%)\")\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # SMART T3 DISCOUNT ADJUSTMENTS\n",
    "    # Only apply if recommendation actually improves the current value\n",
    "    # ==========================================================================\n",
    "    mask_t3_disc = (final_data['suggested_t3_discount'].notna() & \n",
    "                    final_data['ws_new_price'].notna() &\n",
    "                    final_data['ws_new_qty'].notna())\n",
    "    t3_disc_applied = 0\n",
    "    t3_disc_skipped = 0\n",
    "    \n",
    "    if mask_t3_disc.sum() > 0:\n",
    "        original_ws_disc = final_data.loc[mask_t3_disc, 'current_ws_discount'].copy()\n",
    "        suggested_ws_disc = final_data.loc[mask_t3_disc, 'suggested_t3_discount']\n",
    "        t3_action = final_data.loc[mask_t3_disc, 't3_action'].fillna('')\n",
    "        \n",
    "        # Skip if: action contains INCREASE_DISCOUNT and script already >= suggested\n",
    "        # Skip if: action contains DECREASE/REDUCE DISCOUNT and script already <= suggested\n",
    "        skip_disc_increase = t3_action.str.contains('INCREASE_DISCOUNT', case=False, na=False) & (original_ws_disc >= suggested_ws_disc)\n",
    "        skip_disc_decrease = t3_action.str.contains('DECREASE_DISCOUNT|REDUCE_DISCOUNT', case=False, na=False) & (original_ws_disc <= suggested_ws_disc)\n",
    "        skip_disc = skip_disc_increase | skip_disc_decrease\n",
    "        \n",
    "        # Apply only where NOT skipped\n",
    "        apply_disc = mask_t3_disc.copy()\n",
    "        apply_disc.loc[mask_t3_disc] = ~skip_disc.values\n",
    "        \n",
    "        if apply_disc.sum() > 0:\n",
    "            final_data.loc[apply_disc, 'ws_new_price'] = (\n",
    "                final_data.loc[apply_disc, 'packing_unit_price'] * \n",
    "                (1 - (final_data.loc[apply_disc, 'suggested_t3_discount'] / 100))\n",
    "            ).round(2)\n",
    "            t3_disc_applied = apply_disc.sum()\n",
    "            print(f\"  ✓ Applied T3 discount adjustments: {t3_disc_applied} SKUs\")\n",
    "        \n",
    "        t3_disc_skipped = skip_disc.sum()\n",
    "        if t3_disc_skipped > 0:\n",
    "            print(f\"  ⚠ Skipped T3 discount for {t3_disc_skipped} SKUs (script already better)\")\n",
    "    \n",
    "    # Drop temporary column\n",
    "    final_data = final_data.drop(columns=['current_ws_discount'], errors='ignore')\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # VALIDATE T3 CONSTRAINTS: T2 qty < T3 qty AND T2 discount < T3 discount\n",
    "    # ==========================================================================\n",
    "    print(\"\\n  Validating T3 constraints...\")\n",
    "    \n",
    "    # Constraint 1: T3 qty must be > T2 qty\n",
    "    invalid_t3_qty = (final_data['ws_new_qty'].notna() & \n",
    "                      (final_data['ws_new_qty'] <= final_data['tier_2_qty']))\n",
    "    if invalid_t3_qty.sum() > 0:\n",
    "        # Fix: Set T3 qty = T2 qty * 1.5\n",
    "        final_data.loc[invalid_t3_qty, 'ws_new_qty'] = (\n",
    "            final_data.loc[invalid_t3_qty, 'tier_2_qty'] * 10\n",
    "        ).astype(int)\n",
    "        print(f\"    Fixed {invalid_t3_qty.sum()} SKUs where T3 qty <= T2 qty\")\n",
    "    \n",
    "    # Constraint 2: T3 discount must be > T2 discount (T3 price < T2 price)\n",
    "    # Calculate T3 discount percentage\n",
    "    final_data['ws_discount_pct'] = (\n",
    "        (final_data['packing_unit_price'] - final_data['ws_new_price']) / \n",
    "        final_data['packing_unit_price'] * 100\n",
    "    ).round(2)\n",
    "    \n",
    "    invalid_t3_disc = (final_data['ws_new_price'].notna() & \n",
    "                       (final_data['ws_discount_pct'] <= final_data['discount_2_pct']))\n",
    "    if invalid_t3_disc.sum() > 0:\n",
    "        # Fix: Set T3 discount = T2 discount + 0.5\n",
    "        new_t3_disc = final_data.loc[invalid_t3_disc, 'discount_2_pct'] + 0.5\n",
    "        final_data.loc[invalid_t3_disc, 'ws_discount_pct'] = new_t3_disc\n",
    "        final_data.loc[invalid_t3_disc, 'ws_new_price'] = (\n",
    "            final_data.loc[invalid_t3_disc, 'packing_unit_price'] * \n",
    "            (1 - (new_t3_disc / 100))\n",
    "        ).round(2)\n",
    "        print(f\"    Fixed {invalid_t3_disc.sum()} SKUs where T3 discount <= T2 discount\")\n",
    "    \n",
    "    # Summary by T3 action\n",
    "    if 't3_action' in final_data.columns:\n",
    "        t3_actions = final_data[final_data['t3_action'].notna()]['t3_action'].value_counts()\n",
    "        print(f\"\\n  T3 Actions applied:\")\n",
    "        for action, count in t3_actions.items():\n",
    "            if action != 'NO_CHANGE':\n",
    "                print(f\"    - {action}: {count}\")\n",
    "    \n",
    "    # Drop T3 feedback columns\n",
    "    cols_to_drop = ['suggested_t3_qty', 'suggested_t3_discount', 't3_action']\n",
    "    final_data = final_data.drop(columns=[c for c in cols_to_drop if c in final_data.columns], errors='ignore')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "else:\n",
    "    print(\"⚠ No T3 (wholesale) feedback adjustments applied\")\n",
    "    # Still create ws_discount_pct column for later use\n",
    "    if 'ws_new_price' in final_data.columns:\n",
    "        final_data['ws_discount_pct'] = (\n",
    "            (final_data['packing_unit_price'] - final_data['ws_new_price']) / \n",
    "            final_data['packing_unit_price'] * 100\n",
    "        ).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "534215e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ENFORCING MARGIN CONSTRAINTS (FINAL VALIDATION)\n",
      "================================================================================\n",
      "  ✓ Fixed 3 SKUs with T1 margin < 60% of current margin\n",
      "  ✓ Fixed 71 SKUs with T2 margin < 40% of current margin\n",
      "  ✓ Fixed 7 SKUs with WS margin < 25% of current margin\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "MARGIN CONSTRAINT SUMMARY\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Current Margin:      7.72% (average)\n",
      "\n",
      "Tier 1 (min 60%):\n",
      "  - Valid SKUs:      3086\n",
      "  - Avg margin:      6.83%\n",
      "  - Avg retention:   88.0%\n",
      "  - Min retention:   60.0%\n",
      "\n",
      "Tier 2 (min 40%):\n",
      "  - Valid SKUs:      3086\n",
      "  - Avg margin:      5.56%\n",
      "  - Avg retention:   71.0%\n",
      "  - Min retention:   38.9%\n",
      "\n",
      "Wholesale (min 25%):\n",
      "  - Valid SKUs:      1128\n",
      "  - Avg margin:      3.69%\n",
      "  - Avg retention:   64.9%\n",
      "  - Min retention:   25.0%\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FINAL MARGIN CONSTRAINT ENFORCEMENT\n",
    "# =============================================================================\n",
    "# This ensures margin constraints are ALWAYS enforced, even after feedback adjustments\n",
    "#\n",
    "# Constraints:\n",
    "# - Tier 1: margin >= 60% of current margin\n",
    "# - Tier 2: margin >= 40% of current margin  \n",
    "# - Wholesale: margin >= 25% of current margin\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENFORCING MARGIN CONSTRAINTS (FINAL VALIDATION)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate current margin for all rows\n",
    "final_data['current_margin'] = (final_data['packing_unit_price'] - final_data['wac_p']) / final_data['packing_unit_price']\n",
    "\n",
    "# ==========================================================================\n",
    "# TIER 1 MARGIN CONSTRAINT: >= 60% of current margin\n",
    "# ==========================================================================\n",
    "final_data['tier_1_margin_actual'] = (final_data['tier_1_price'] - final_data['wac_p']) / final_data['tier_1_price']\n",
    "final_data['min_t1_margin_required'] = 0.60 * final_data['current_margin']\n",
    "final_data['min_t1_price_required'] = final_data['wac_p'] / (1 - final_data['min_t1_margin_required'])\n",
    "\n",
    "# Find violations\n",
    "t1_violations = (final_data['tier_1_price'].notna() & \n",
    "                 (final_data['tier_1_margin_actual'] < final_data['min_t1_margin_required']))\n",
    "\n",
    "if t1_violations.sum() > 0:\n",
    "    # Fix: Set tier_1_price to minimum required\n",
    "    final_data.loc[t1_violations, 'tier_1_price'] = final_data.loc[t1_violations, 'min_t1_price_required'].round(2)\n",
    "    # Update discount percentage\n",
    "    final_data.loc[t1_violations, 'discount_1'] = (\n",
    "        final_data.loc[t1_violations, 'packing_unit_price'] - final_data.loc[t1_violations, 'tier_1_price']\n",
    "    ).round(2)\n",
    "    final_data.loc[t1_violations, 'discount_1_pct'] = (\n",
    "        final_data.loc[t1_violations, 'discount_1'] / final_data.loc[t1_violations, 'packing_unit_price'] * 100\n",
    "    ).round(2)\n",
    "    print(f\"  ✓ Fixed {t1_violations.sum()} SKUs with T1 margin < 60% of current margin\")\n",
    "else:\n",
    "    print(f\"  ✓ All T1 prices meet 60% margin constraint\")\n",
    "\n",
    "# ==========================================================================\n",
    "# TIER 2 MARGIN CONSTRAINT: >= 40% of current margin\n",
    "# ==========================================================================\n",
    "final_data['tier_2_margin_actual'] = (final_data['tier_2_price'] - final_data['wac_p']) / final_data['tier_2_price']\n",
    "final_data['min_t2_margin_required'] = 0.40 * final_data['current_margin']\n",
    "final_data['min_t2_price_required'] = final_data['wac_p'] / (1 - final_data['min_t2_margin_required'])\n",
    "\n",
    "# Find violations\n",
    "t2_violations = (final_data['tier_2_price'].notna() & \n",
    "                 (final_data['tier_2_margin_actual'] < final_data['min_t2_margin_required']))\n",
    "\n",
    "if t2_violations.sum() > 0:\n",
    "    # Fix: Set tier_2_price to minimum required\n",
    "    final_data.loc[t2_violations, 'tier_2_price'] = final_data.loc[t2_violations, 'min_t2_price_required'].round(2)\n",
    "    # Update discount percentage\n",
    "    final_data.loc[t2_violations, 'discount_2'] = (\n",
    "        final_data.loc[t2_violations, 'packing_unit_price'] - final_data.loc[t2_violations, 'tier_2_price']\n",
    "    ).round(2)\n",
    "    final_data.loc[t2_violations, 'discount_2_pct'] = (\n",
    "        final_data.loc[t2_violations, 'discount_2'] / final_data.loc[t2_violations, 'packing_unit_price'] * 100\n",
    "    ).round(2)\n",
    "    print(f\"  ✓ Fixed {t2_violations.sum()} SKUs with T2 margin < 40% of current margin\")\n",
    "else:\n",
    "    print(f\"  ✓ All T2 prices meet 40% margin constraint\")\n",
    "\n",
    "# ==========================================================================\n",
    "# WHOLESALE MARGIN CONSTRAINT: >= 25% of current margin\n",
    "# ==========================================================================\n",
    "final_data['ws_margin_actual'] = (final_data['ws_new_price'] - final_data['wac_p']) / final_data['ws_new_price']\n",
    "final_data['min_ws_margin_required'] = 0.25 * final_data['current_margin']\n",
    "final_data['min_ws_price_required'] = final_data['wac_p'] / (1 - final_data['min_ws_margin_required'])\n",
    "\n",
    "# Find violations\n",
    "ws_violations = (final_data['ws_new_price'].notna() & \n",
    "                 (final_data['ws_margin_actual'] < final_data['min_ws_margin_required']))\n",
    "\n",
    "if ws_violations.sum() > 0:\n",
    "    # Fix: Set ws_new_price to minimum required\n",
    "    final_data.loc[ws_violations, 'ws_new_price'] = final_data.loc[ws_violations, 'min_ws_price_required'].round(2)\n",
    "    # Update discount percentage\n",
    "    final_data.loc[ws_violations, 'ws_discount_pct'] = (\n",
    "        (final_data.loc[ws_violations, 'packing_unit_price'] - final_data.loc[ws_violations, 'ws_new_price']) / \n",
    "        final_data.loc[ws_violations, 'packing_unit_price'] * 100\n",
    "    ).round(2)\n",
    "    print(f\"  ✓ Fixed {ws_violations.sum()} SKUs with WS margin < 25% of current margin\")\n",
    "else:\n",
    "    print(f\"  ✓ All WS prices meet 25% margin constraint\")\n",
    "\n",
    "# ==========================================================================\n",
    "# RE-VALIDATE TIER ORDERING AFTER MARGIN FIXES\n",
    "# ==========================================================================\n",
    "# Ensure: T2 price < T1 price (T2 discount > T1 discount)\n",
    "ordering_violations = (final_data['tier_2_price'].notna() & \n",
    "                       final_data['tier_1_price'].notna() &\n",
    "                       (final_data['tier_2_price'] >= final_data['tier_1_price']))\n",
    "\n",
    "if ordering_violations.sum() > 0:\n",
    "    # Invalidate both tiers for these SKUs (margin constraints conflict with ordering)\n",
    "    final_data.loc[ordering_violations, 'tier_1_price'] = np.nan\n",
    "    final_data.loc[ordering_violations, 'tier_2_price'] = np.nan\n",
    "    final_data.loc[ordering_violations, 'discount_1'] = np.nan\n",
    "    final_data.loc[ordering_violations, 'discount_2'] = np.nan\n",
    "    final_data.loc[ordering_violations, 'discount_1_pct'] = np.nan\n",
    "    final_data.loc[ordering_violations, 'discount_2_pct'] = np.nan\n",
    "    print(f\"  ⚠ Invalidated {ordering_violations.sum()} SKUs where T2 price >= T1 price after margin fix\")\n",
    "\n",
    "# Ensure: WS price < T2 price (WS discount > T2 discount)\n",
    "ws_ordering_violations = (final_data['ws_new_price'].notna() & \n",
    "                          final_data['tier_2_price'].notna() &\n",
    "                          (final_data['ws_new_price'] >= final_data['tier_2_price']))\n",
    "\n",
    "if ws_ordering_violations.sum() > 0:\n",
    "    # Invalidate WS for these SKUs\n",
    "    final_data.loc[ws_ordering_violations, 'ws_new_price'] = np.nan\n",
    "    final_data.loc[ws_ordering_violations, 'ws_new_qty'] = np.nan\n",
    "    final_data.loc[ws_ordering_violations, 'ws_discount_pct'] = np.nan\n",
    "    print(f\"  ⚠ Invalidated {ws_ordering_violations.sum()} SKUs where WS price >= T2 price after margin fix\")\n",
    "\n",
    "# Drop temporary columns\n",
    "temp_cols = ['min_t1_margin_required', 'min_t1_price_required', 'tier_1_margin_actual',\n",
    "             'min_t2_margin_required', 'min_t2_price_required', 'tier_2_margin_actual',\n",
    "             'min_ws_margin_required', 'min_ws_price_required', 'ws_margin_actual']\n",
    "final_data = final_data.drop(columns=[c for c in temp_cols if c in final_data.columns], errors='ignore')\n",
    "\n",
    "# ==========================================================================\n",
    "# FINAL SUMMARY\n",
    "# ==========================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"MARGIN CONSTRAINT SUMMARY\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Calculate final margins\n",
    "final_data['tier_1_margin_final'] = (final_data['tier_1_price'] - final_data['wac_p']) / final_data['tier_1_price']\n",
    "final_data['tier_2_margin_final'] = (final_data['tier_2_price'] - final_data['wac_p']) / final_data['tier_2_price']\n",
    "final_data['ws_margin_final'] = (final_data['ws_new_price'] - final_data['wac_p']) / final_data['ws_new_price']\n",
    "\n",
    "# Calculate margin retention percentages\n",
    "final_data['t1_margin_retention'] = (final_data['tier_1_margin_final'] / final_data['current_margin'] * 100).round(1)\n",
    "final_data['t2_margin_retention'] = (final_data['tier_2_margin_final'] / final_data['current_margin'] * 100).round(1)\n",
    "final_data['ws_margin_retention'] = (final_data['ws_margin_final'] / final_data['current_margin'] * 100).round(1)\n",
    "\n",
    "valid_t1 = final_data['tier_1_price'].notna()\n",
    "valid_t2 = final_data['tier_2_price'].notna()\n",
    "valid_ws = final_data['ws_new_price'].notna()\n",
    "\n",
    "print(f\"\\nCurrent Margin:      {final_data['current_margin'].mean()*100:.2f}% (average)\")\n",
    "print(f\"\\nTier 1 (min 60%):\")\n",
    "print(f\"  - Valid SKUs:      {valid_t1.sum()}\")\n",
    "print(f\"  - Avg margin:      {final_data.loc[valid_t1, 'tier_1_margin_final'].mean()*100:.2f}%\")\n",
    "print(f\"  - Avg retention:   {final_data.loc[valid_t1, 't1_margin_retention'].mean():.1f}%\")\n",
    "print(f\"  - Min retention:   {final_data.loc[valid_t1, 't1_margin_retention'].min():.1f}%\")\n",
    "\n",
    "print(f\"\\nTier 2 (min 40%):\")\n",
    "print(f\"  - Valid SKUs:      {valid_t2.sum()}\")\n",
    "print(f\"  - Avg margin:      {final_data.loc[valid_t2, 'tier_2_margin_final'].mean()*100:.2f}%\")\n",
    "print(f\"  - Avg retention:   {final_data.loc[valid_t2, 't2_margin_retention'].mean():.1f}%\")\n",
    "print(f\"  - Min retention:   {final_data.loc[valid_t2, 't2_margin_retention'].min():.1f}%\")\n",
    "\n",
    "print(f\"\\nWholesale (min 25%):\")\n",
    "print(f\"  - Valid SKUs:      {valid_ws.sum()}\")\n",
    "print(f\"  - Avg margin:      {final_data.loc[valid_ws, 'ws_margin_final'].mean()*100:.2f}%\")\n",
    "print(f\"  - Avg retention:   {final_data.loc[valid_ws, 'ws_margin_retention'].mean():.1f}%\")\n",
    "print(f\"  - Min retention:   {final_data.loc[valid_ws, 'ws_margin_retention'].min():.1f}%\")\n",
    "\n",
    "# Drop temporary columns\n",
    "temp_cols = ['tier_1_margin_final', 'tier_2_margin_final', 'ws_margin_final',\n",
    "             't1_margin_retention', 't2_margin_retention', 'ws_margin_retention']\n",
    "final_data = final_data.drop(columns=[c for c in temp_cols if c in final_data.columns], errors='ignore')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e1ad15d4-8b0f-4368-aa19-1ed86a94eeb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Filtered to top 133 products per warehouse: 2040 total SKUs\n"
     ]
    }
   ],
   "source": [
    "final_data['t1_f'] = final_data['tier_1_qty'].notna().astype(int)\n",
    "final_data['t2_f'] = final_data['tier_2_qty'].notna().astype(int)\n",
    "final_data['ws_f'] = final_data['ws_new_qty'].notna().astype(int)\n",
    "final_data['all_f'] = final_data['t1_f']+final_data['t2_f']+final_data['ws_f']\n",
    "final_data=final_data.sort_values(by =['warehouse_id','final_rank'],ascending = [True,True] )\n",
    "final_data['cumsum'] = final_data.groupby('warehouse_id')['all_f'].cumsum()\n",
    "final_data = final_data[final_data['cumsum'] <= 400]\n",
    "print(f\"✓ Filtered to top {FINAL_PRODUCTS_PER_WAREHOUSE} products per warehouse: {len(final_data)} total SKUs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6cc89a6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== METRICS SUMMARY ===\n",
      "\n",
      "Stretch Analysis (how much retailers need to increase orders):\n",
      "  Average stretch to Tier 1: 247.6%\n",
      "  Average stretch to Tier 2: 513.4%\n",
      "\n",
      "Margin Analysis:\n",
      "  Current margin:  7.82%\n",
      "  Tier 1 margin:   6.92%\n",
      "  Tier 2 margin:   5.58%\n",
      "  WS margin:       3.84%\n",
      "\n",
      "Discount Analysis:\n",
      "  Average Tier 1 discount: 0.97%\n",
      "  Average Tier 2 discount: 2.39%\n",
      "\n",
      "Elasticity Analysis (discount increase vs quantity increase):\n",
      "  Average qty ratio (T2/T1): 1.81x\n",
      "  Average discount ratio (D2/D1): 2.93x\n",
      "  Average elasticity ratio: 1.65\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CALCULATE ADDITIONAL METRICS\n",
    "# =============================================================================\n",
    "\n",
    "# --- Stretch Percentages (how much retailers need to increase to reach each tier) ---\n",
    "# Already included from tiers_selection: tier_1_increase_pct, tier_2_increase_pct\n",
    "# These show: (tier_qty - median_qty) / median_qty * 100\n",
    "\n",
    "# Rename for clarity\n",
    "final_data['stretch_to_tier_1_pct'] = final_data['tier_1_increase_pct']\n",
    "final_data['stretch_to_tier_2_pct'] = final_data['tier_2_increase_pct']\n",
    "\n",
    "# --- Margins for each price tier ---\n",
    "# Margin = (price - wac) / price\n",
    "final_data['tier_1_margin'] = ((final_data['tier_1_price'] - final_data['wac_p']) / final_data['tier_1_price']).round(4)\n",
    "final_data['tier_2_margin'] = ((final_data['tier_2_price'] - final_data['wac_p']) / final_data['tier_2_price']).round(4)\n",
    "final_data['WS_margin'] = ((final_data['ws_new_price'] - final_data['wac_p']) / final_data['wac_p']).round(4)\n",
    "final_data['current_margin'] = ((final_data['packing_unit_price'] - final_data['wac_p']) / final_data['packing_unit_price']).round(4)\n",
    "\n",
    "# --- Discount calculations ---\n",
    "# Absolute discounts (price reduction from current price)\n",
    "final_data['discount_1'] = (final_data['packing_unit_price'] - final_data['tier_1_price']).round(2)\n",
    "final_data['discount_2'] = (final_data['packing_unit_price'] - final_data['tier_2_price']).round(2)\n",
    "\n",
    "# Discount percentages\n",
    "final_data['discount_1_pct'] = ((final_data['discount_1'] / final_data['packing_unit_price']) * 100).round(2)\n",
    "final_data['discount_2_pct'] = ((final_data['discount_2'] / final_data['packing_unit_price']) * 100).round(2)\n",
    "\n",
    "# --- Quantity and Discount Ratios ---\n",
    "# Quantity ratio (tier_2_qty / tier_1_qty)\n",
    "final_data['qty_ratio'] = (final_data['tier_2_qty'] / final_data['tier_1_qty']).round(2)\n",
    "\n",
    "# Discount ratio (discount_2 / discount_1)\n",
    "final_data['discount_ratio'] = (final_data['discount_2'] / final_data['discount_1']).round(2)\n",
    "\n",
    "# Elasticity ratio = discount_ratio / qty_ratio\n",
    "# This shows how much extra discount per unit of quantity increase\n",
    "final_data['elasticity_ratio'] = (final_data['discount_ratio'] / final_data['qty_ratio']).round(2)\n",
    "\n",
    "print(\"=== METRICS SUMMARY ===\")\n",
    "print(f\"\\nStretch Analysis (how much retailers need to increase orders):\")\n",
    "print(f\"  Average stretch to Tier 1: {final_data['stretch_to_tier_1_pct'].mean():.1f}%\")\n",
    "print(f\"  Average stretch to Tier 2: {final_data['stretch_to_tier_2_pct'].mean():.1f}%\")\n",
    "\n",
    "print(f\"\\nMargin Analysis:\")\n",
    "print(f\"  Current margin:  {final_data['current_margin'].mean()*100:.2f}%\")\n",
    "print(f\"  Tier 1 margin:   {final_data['tier_1_margin'].mean()*100:.2f}%\")\n",
    "print(f\"  Tier 2 margin:   {final_data['tier_2_margin'].mean()*100:.2f}%\")\n",
    "print(f\"  WS margin:       {final_data['WS_margin'].mean()*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nDiscount Analysis:\")\n",
    "print(f\"  Average Tier 1 discount: {final_data['discount_1_pct'].mean():.2f}%\")\n",
    "print(f\"  Average Tier 2 discount: {final_data['discount_2_pct'].mean():.2f}%\")\n",
    "\n",
    "print(f\"\\nElasticity Analysis (discount increase vs quantity increase):\")\n",
    "print(f\"  Average qty ratio (T2/T1): {final_data['qty_ratio'].mean():.2f}x\")\n",
    "print(f\"  Average discount ratio (D2/D1): {final_data['discount_ratio'].mean():.2f}x\")\n",
    "print(f\"  Average elasticity ratio: {final_data['elasticity_ratio'].mean():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb449fce",
   "metadata": {},
   "source": [
    "## 7. Conversion Scenarios & Simulation\n",
    "\n",
    "Before uploading, analyze expected blended prices and margins:\n",
    "1. **Hypothetical Scenarios** - 10 different conversion rate assumptions\n",
    "2. **Historical Simulation** - Actual tier conversion from previous month data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3090e037",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "CONVERSION SCENARIOS ANALYSIS - NMV, Quantity & Gross Profit Impact\n",
      "========================================================================================================================\n",
      "\n",
      "Simulation based on 1,000 orders distributed across 2040 SKUs\n",
      "\n",
      "Scenario                             Base   T1   T2   WS |   NMV Δ%   Qty Δ%    GP Δ% | Blnd Margin     GP (EGP)\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Scenario 1 - Very Low Conversion      90%   7%   2%   1% |   +71.3%   +84.2%   +45.2% |       5.93%      28,422\n",
      "Scenario 2 - Low Conversion           80%  12%   5%   3% |  +183.6%  +227.5%  +111.6% |       5.22%      41,429\n",
      "Scenario 3 - Conservative             70%  15%  10%   5% |  +301.8%  +376.0%  +180.8% |       4.89%      54,973\n",
      "Scenario 4 - Moderate Low             65%  18%  12%   5% |  +321.3%  +392.3%  +194.8% |       4.90%      57,714\n",
      "Scenario 5 - Moderate                 60%  20%  13%   7% |  +414.1%  +519.3%  +247.2% |       4.73%      67,979\n",
      "Scenario 6 - Moderate High            55%  22%  15%   8% |  +471.7%  +592.2%  +281.1% |       4.67%      74,617\n",
      "Scenario 7 - Optimistic               50%  25%  17%   8% |  +491.3%  +608.6%  +295.1% |       4.68%      77,359\n",
      "Scenario 8 - High Conversion          45%  27%  18%  10% |  +584.0%  +735.6%  +347.5% |       4.58%      87,624\n",
      "Scenario 9 - Very High Conversion     40%  28%  20%  12% |  +679.7%  +865.1%  +401.3% |       4.50%      98,158\n",
      "Scenario 10 - Maximum Conversion      35%  30%  22%  13% |  +737.3%  +938.0%  +435.2% |       4.47%     104,795\n",
      "Scenario 10 - Current Conversion      75%   8%  14%  12% |  +599.9%  +800.2%  +343.5% |       4.44%      86,845\n",
      "\n",
      "========================================================================================================================\n",
      "DETAILED COMPARISON\n",
      "========================================================================================================================\n",
      "\n",
      "              CURRENT STATE (100% Base Price)               \n",
      "------------------------------------------------------------\n",
      "  Total NMV:                279,684.01 EGP\n",
      "  Total Quantity:                1,515 units\n",
      "  Total COGS:               260,104.18 EGP\n",
      "  Total Gross Profit:        19,579.83 EGP\n",
      "  Gross Margin:                   7.00%\n",
      "\n",
      "             CONSERVATIVE SCENARIO (70/15/10/5)             \n",
      "------------------------------------------------------------\n",
      "  Total NMV:              1,123,735.62 EGP  (+301.8%)\n",
      "  Total Quantity:                7,211 units (+376.0%)\n",
      "  Total Gross Profit:        54,972.59 EGP  (+180.8%)\n",
      "  Gross Margin:                   4.89%\n",
      "  GP Change:                +35,392.76 EGP\n",
      "\n",
      "              OPTIMISTIC SCENARIO (50/25/17/8)              \n",
      "------------------------------------------------------------\n",
      "  Total NMV:              1,653,661.25 EGP  (+491.3%)\n",
      "  Total Quantity:               10,735 units (+608.6%)\n",
      "  Total Gross Profit:        77,358.50 EGP  (+295.1%)\n",
      "  Gross Margin:                   4.68%\n",
      "  GP Change:                +57,778.67 EGP\n",
      "\n",
      "========================================================================================================================\n",
      "SCENARIO IMPACT SUMMARY\n",
      "========================================================================================================================\n",
      "\n",
      "Scenario                            |      NMV Change |      Qty Change |       GP Change |  GP Margin\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Scenario 1 - Very Low Conversion    |       +199,492 |         +1,276 |         +8,842 |      5.93%\n",
      "Scenario 2 - Low Conversion         |       +513,618 |         +3,447 |        +21,849 |      5.22%\n",
      "Scenario 3 - Conservative           |       +844,052 |         +5,696 |        +35,393 |      4.89%\n",
      "Scenario 4 - Moderate Low           |       +898,711 |         +5,944 |        +38,134 |      4.90%\n",
      "Scenario 5 - Moderate               |     +1,158,178 |         +7,867 |        +48,400 |      4.73%\n",
      "Scenario 6 - Moderate High          |     +1,319,318 |         +8,972 |        +55,037 |      4.67%\n",
      "Scenario 7 - Optimistic             |     +1,373,977 |         +9,220 |        +57,779 |      4.68%\n",
      "Scenario 8 - High Conversion        |     +1,633,444 |        +11,144 |        +68,044 |      4.58%\n",
      "Scenario 9 - Very High Conversion   |     +1,901,064 |        +13,106 |        +78,578 |      4.50%\n",
      "Scenario 10 - Maximum Conversion    |     +2,062,204 |        +14,211 |        +85,215 |      4.47%\n",
      "Scenario 10 - Current Conversion    |     +1,677,883 |        +12,123 |        +67,265 |      4.44%\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PART 1: HYPOTHETICAL CONVERSION SCENARIOS\n",
    "# =============================================================================\n",
    "# 10 scenarios with different conversion rates:\n",
    "# - Base (no tier): % of orders at packing_unit_price\n",
    "# - Tier 1: % of orders at tier_1_price\n",
    "# - Tier 2: % of orders at tier_2_price\n",
    "# - Wholesale: % of orders at ws_new_price\n",
    "\n",
    "# Define 10 scenarios: (base%, tier1%, tier2%, ws%)\n",
    "# Scenarios range from pessimistic (low conversion) to optimistic (high conversion)\n",
    "scenarios = {\n",
    "    'Scenario 1 - Very Low Conversion':   (90, 7, 2, 1),    # Most orders at base price\n",
    "    'Scenario 2 - Low Conversion':        (80, 12, 5, 3),   # Low tier uptake\n",
    "    'Scenario 3 - Conservative':          (70, 15, 10, 5),  # Conservative estimate\n",
    "    'Scenario 4 - Moderate Low':          (65, 18, 12, 5),  # Slightly better\n",
    "    'Scenario 5 - Moderate':              (60, 20, 13, 7),  # Moderate adoption\n",
    "    'Scenario 6 - Moderate High':         (55, 22, 15, 8),  # Good adoption\n",
    "    'Scenario 7 - Optimistic':            (50, 25, 17, 8),  # Optimistic uptake\n",
    "    'Scenario 8 - High Conversion':       (45, 27, 18, 10), # High tier adoption\n",
    "    'Scenario 9 - Very High Conversion':  (40, 28, 20, 12), # Very high uptake\n",
    "    'Scenario 10 - Maximum Conversion':   (35, 30, 22, 13), # Maximum realistic conversion\n",
    "    'Scenario 10 - Current Conversion':   (75, 8, 14, 12), # Maximum realistic conversion\n",
    "}\n",
    "\n",
    "def calculate_blended_metrics_with_gp(df, base_pct, t1_pct, t2_pct, ws_pct, num_orders=1000):\n",
    "    \"\"\"\n",
    "    Calculate blended price, margin, NMV and Gross Profit for a given conversion scenario.\n",
    "    \n",
    "    Key insight: When retailers convert to tiers, they buy MORE quantity (that's the incentive).\n",
    "    - Base orders: quantity = median_qty (typical order before tier)\n",
    "    - Tier 1 orders: quantity = tier_1_qty (must reach this to get discount)\n",
    "    - Tier 2 orders: quantity = tier_2_qty\n",
    "    - WS orders: quantity = ws_new_qty\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with price and quantity columns\n",
    "        base_pct, t1_pct, t2_pct, ws_pct: % of orders in each tier\n",
    "        num_orders: Assumed number of total orders for simulation\n",
    "    \n",
    "    Returns:\n",
    "        dict with all metrics\n",
    "    \"\"\"\n",
    "    # Convert percentages to decimals\n",
    "    base_pct = base_pct / 100\n",
    "    t1_pct = t1_pct / 100\n",
    "    t2_pct = t2_pct / 100\n",
    "    ws_pct = ws_pct / 100\n",
    "    \n",
    "    df_calc = df.copy()\n",
    "    \n",
    "    # Fill missing values\n",
    "    df_calc['ws_price_filled'] = df_calc['ws_new_price'].fillna(df_calc['tier_2_price'])\n",
    "    df_calc['ws_qty_filled'] = df_calc['ws_new_qty'].fillna(df_calc['tier_2_qty'])\n",
    "    \n",
    "    # Calculate NMV and COGS for each tier (per SKU, per order)\n",
    "    # Base: orders at median_qty * packing_unit_price\n",
    "    df_calc['base_nmv_per_order'] = df_calc['median_qty'] * df_calc['packing_unit_price']\n",
    "    df_calc['base_cogs_per_order'] = df_calc['median_qty'] * df_calc['wac_p']\n",
    "    \n",
    "    # Tier 1: orders at tier_1_qty * tier_1_price\n",
    "    df_calc['t1_nmv_per_order'] = df_calc['tier_1_qty'] * df_calc['tier_1_price']\n",
    "    df_calc['t1_cogs_per_order'] = df_calc['tier_1_qty'] * df_calc['wac_p']\n",
    "    \n",
    "    # Tier 2: orders at tier_2_qty * tier_2_price\n",
    "    df_calc['t2_nmv_per_order'] = df_calc['tier_2_qty'] * df_calc['tier_2_price']\n",
    "    df_calc['t2_cogs_per_order'] = df_calc['tier_2_qty'] * df_calc['wac_p']\n",
    "    \n",
    "    # Wholesale: orders at ws_qty * ws_price\n",
    "    df_calc['ws_nmv_per_order'] = df_calc['ws_qty_filled'] * df_calc['ws_price_filled']\n",
    "    df_calc['ws_cogs_per_order'] = df_calc['ws_qty_filled'] * df_calc['wac_p']\n",
    "    \n",
    "    # Blended NMV per order (weighted by conversion rates)\n",
    "    df_calc['blended_nmv_per_order'] = (\n",
    "        base_pct * df_calc['base_nmv_per_order'] +\n",
    "        t1_pct * df_calc['t1_nmv_per_order'] +\n",
    "        t2_pct * df_calc['t2_nmv_per_order'] +\n",
    "        ws_pct * df_calc['ws_nmv_per_order']\n",
    "    )\n",
    "    \n",
    "    # Blended COGS per order\n",
    "    df_calc['blended_cogs_per_order'] = (\n",
    "        base_pct * df_calc['base_cogs_per_order'] +\n",
    "        t1_pct * df_calc['t1_cogs_per_order'] +\n",
    "        t2_pct * df_calc['t2_cogs_per_order'] +\n",
    "        ws_pct * df_calc['ws_cogs_per_order']\n",
    "    )\n",
    "    \n",
    "    # Blended quantity per order\n",
    "    df_calc['blended_qty_per_order'] = (\n",
    "        base_pct * df_calc['median_qty'] +\n",
    "        t1_pct * df_calc['tier_1_qty'] +\n",
    "        t2_pct * df_calc['tier_2_qty'] +\n",
    "        ws_pct * df_calc['ws_qty_filled']\n",
    "    )\n",
    "    \n",
    "    # Gross Profit per order\n",
    "    df_calc['blended_gp_per_order'] = df_calc['blended_nmv_per_order'] - df_calc['blended_cogs_per_order']\n",
    "    \n",
    "    # Current state (100% base)\n",
    "    df_calc['current_nmv_per_order'] = df_calc['base_nmv_per_order']\n",
    "    df_calc['current_cogs_per_order'] = df_calc['base_cogs_per_order']\n",
    "    df_calc['current_gp_per_order'] = df_calc['current_nmv_per_order'] - df_calc['current_cogs_per_order']\n",
    "    \n",
    "    # Aggregate across all SKUs (simulate num_orders distributed across SKUs)\n",
    "    orders_per_sku = num_orders / len(df_calc)\n",
    "    \n",
    "    total_current_nmv = (df_calc['current_nmv_per_order'] * orders_per_sku).sum()\n",
    "    total_current_cogs = (df_calc['current_cogs_per_order'] * orders_per_sku).sum()\n",
    "    total_current_gp = total_current_nmv - total_current_cogs\n",
    "    total_current_qty = (df_calc['median_qty'] * orders_per_sku).sum()\n",
    "    \n",
    "    total_blended_nmv = (df_calc['blended_nmv_per_order'] * orders_per_sku).sum()\n",
    "    total_blended_cogs = (df_calc['blended_cogs_per_order'] * orders_per_sku).sum()\n",
    "    total_blended_gp = total_blended_nmv - total_blended_cogs\n",
    "    total_blended_qty = (df_calc['blended_qty_per_order'] * orders_per_sku).sum()\n",
    "    \n",
    "    # Calculate changes\n",
    "    nmv_change = total_blended_nmv - total_current_nmv\n",
    "    nmv_change_pct = (nmv_change / total_current_nmv) * 100\n",
    "    \n",
    "    qty_change = total_blended_qty - total_current_qty\n",
    "    qty_change_pct = (qty_change / total_current_qty) * 100\n",
    "    \n",
    "    gp_change = total_blended_gp - total_current_gp\n",
    "    gp_change_pct = (gp_change / total_current_gp) * 100 if total_current_gp != 0 else 0\n",
    "    \n",
    "    # Blended margins\n",
    "    current_margin = total_current_gp / total_current_nmv if total_current_nmv != 0 else 0\n",
    "    blended_margin = total_blended_gp / total_blended_nmv if total_blended_nmv != 0 else 0\n",
    "    \n",
    "    # Average blended price per unit\n",
    "    avg_current_price = total_current_nmv / total_current_qty if total_current_qty != 0 else 0\n",
    "    avg_blended_price = total_blended_nmv / total_blended_qty if total_blended_qty != 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'current_nmv': total_current_nmv,\n",
    "        'blended_nmv': total_blended_nmv,\n",
    "        'nmv_change': nmv_change,\n",
    "        'nmv_change_pct': nmv_change_pct,\n",
    "        'current_qty': total_current_qty,\n",
    "        'blended_qty': total_blended_qty,\n",
    "        'qty_change': qty_change,\n",
    "        'qty_change_pct': qty_change_pct,\n",
    "        'current_gp': total_current_gp,\n",
    "        'blended_gp': total_blended_gp,\n",
    "        'gp_change': gp_change,\n",
    "        'gp_change_pct': gp_change_pct,\n",
    "        'current_margin': current_margin,\n",
    "        'blended_margin': blended_margin,\n",
    "        'avg_current_price': avg_current_price,\n",
    "        'avg_blended_price': avg_blended_price\n",
    "    }\n",
    "\n",
    "# Calculate and display results for each scenario\n",
    "print(\"=\" * 120)\n",
    "print(\"CONVERSION SCENARIOS ANALYSIS - NMV, Quantity & Gross Profit Impact\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "# Assume 1000 orders for simulation\n",
    "NUM_ORDERS = 1000\n",
    "\n",
    "print(f\"\\nSimulation based on {NUM_ORDERS:,} orders distributed across {len(final_data)} SKUs\")\n",
    "print(f\"\\n{'Scenario':<35} {'Base':>5} {'T1':>4} {'T2':>4} {'WS':>4} | {'NMV Δ%':>8} {'Qty Δ%':>8} {'GP Δ%':>8} | {'Blnd Margin':>11} {'GP (EGP)':>12}\")\n",
    "print(\"-\" * 120)\n",
    "\n",
    "scenario_results = []\n",
    "for scenario_name, (base, t1, t2, ws) in scenarios.items():\n",
    "    metrics = calculate_blended_metrics_with_gp(final_data, base, t1, t2, ws, NUM_ORDERS)\n",
    "    \n",
    "    scenario_results.append({\n",
    "        'scenario': scenario_name,\n",
    "        'base_pct': base,\n",
    "        't1_pct': t1,\n",
    "        't2_pct': t2,\n",
    "        'ws_pct': ws,\n",
    "        **metrics\n",
    "    })\n",
    "    \n",
    "    print(f\"{scenario_name:<35} {base:>4}% {t1:>3}% {t2:>3}% {ws:>3}% | \"\n",
    "          f\"{metrics['nmv_change_pct']:>+7.1f}% {metrics['qty_change_pct']:>+7.1f}% {metrics['gp_change_pct']:>+7.1f}% | \"\n",
    "          f\"{metrics['blended_margin']*100:>10.2f}% {metrics['blended_gp']:>11,.0f}\")\n",
    "\n",
    "# Create summary DataFrame\n",
    "scenarios_df = pd.DataFrame(scenario_results)\n",
    "\n",
    "# Current state baseline\n",
    "current_metrics = calculate_blended_metrics_with_gp(final_data, 100, 0, 0, 0, NUM_ORDERS)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"DETAILED COMPARISON\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "print(f\"\\n{'CURRENT STATE (100% Base Price)':^60}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"  Total NMV:           {current_metrics['current_nmv']:>15,.2f} EGP\")\n",
    "print(f\"  Total Quantity:      {current_metrics['current_qty']:>15,.0f} units\")\n",
    "print(f\"  Total COGS:          {current_metrics['current_nmv'] - current_metrics['current_gp']:>15,.2f} EGP\")\n",
    "print(f\"  Total Gross Profit:  {current_metrics['current_gp']:>15,.2f} EGP\")\n",
    "print(f\"  Gross Margin:        {current_metrics['current_margin']*100:>15.2f}%\")\n",
    "\n",
    "# Conservative scenario\n",
    "cons = scenarios_df[scenarios_df['scenario'].str.contains('Conservative')].iloc[0]\n",
    "print(f\"\\n{'CONSERVATIVE SCENARIO (70/15/10/5)':^60}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"  Total NMV:           {cons['blended_nmv']:>15,.2f} EGP  ({cons['nmv_change_pct']:>+.1f}%)\")\n",
    "print(f\"  Total Quantity:      {cons['blended_qty']:>15,.0f} units ({cons['qty_change_pct']:>+.1f}%)\")\n",
    "print(f\"  Total Gross Profit:  {cons['blended_gp']:>15,.2f} EGP  ({cons['gp_change_pct']:>+.1f}%)\")\n",
    "print(f\"  Gross Margin:        {cons['blended_margin']*100:>15.2f}%\")\n",
    "print(f\"  GP Change:           {cons['gp_change']:>+15,.2f} EGP\")\n",
    "\n",
    "# Optimistic scenario\n",
    "opt = scenarios_df[scenarios_df['scenario'].str.contains('Optimistic')].iloc[0]\n",
    "print(f\"\\n{'OPTIMISTIC SCENARIO (50/25/17/8)':^60}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"  Total NMV:           {opt['blended_nmv']:>15,.2f} EGP  ({opt['nmv_change_pct']:>+.1f}%)\")\n",
    "print(f\"  Total Quantity:      {opt['blended_qty']:>15,.0f} units ({opt['qty_change_pct']:>+.1f}%)\")\n",
    "print(f\"  Total Gross Profit:  {opt['blended_gp']:>15,.2f} EGP  ({opt['gp_change_pct']:>+.1f}%)\")\n",
    "print(f\"  Gross Margin:        {opt['blended_margin']*100:>15.2f}%\")\n",
    "print(f\"  GP Change:           {opt['gp_change']:>+15,.2f} EGP\")\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"SCENARIO IMPACT SUMMARY\")\n",
    "print(\"=\" * 120)\n",
    "print(f\"\\n{'Scenario':<35} | {'NMV Change':>15} | {'Qty Change':>15} | {'GP Change':>15} | {'GP Margin':>10}\")\n",
    "print(\"-\" * 100)\n",
    "for _, row in scenarios_df.iterrows():\n",
    "    print(f\"{row['scenario']:<35} | {row['nmv_change']:>+14,.0f} | {row['qty_change']:>+14,.0f} | {row['gp_change']:>+14,.0f} | {row['blended_margin']*100:>9.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b30b8dd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching historical tier conversion data from previous month...\n",
      "✓ Retrieved conversion data for 2040 SKUs\n",
      "  Total orders analyzed: 486,667\n",
      "\n",
      "================================================================================\n",
      "HISTORICAL TIER CONVERSION (Previous Month)\n",
      "================================================================================\n",
      "\n",
      "Overall Conversion Rates (based on 486,667 orders):\n",
      "  Base (no tier):   87.36%  (425,167 orders)\n",
      "  Tier 1:            8.25%  (40,141 orders)\n",
      "  Tier 2:            4.37%  (21,249 orders)\n",
      "  Wholesale:         0.02%  (110 orders)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PART 2: HISTORICAL SIMULATION - Previous Month Tier Conversion\n",
    "# =============================================================================\n",
    "# Query actual order data from previous month to see real tier conversion rates\n",
    "# Then calculate what the blended price and margin would have been\n",
    "\n",
    "# Prepare product list for query\n",
    "selected_df = final_data[['warehouse_id', 'product_id', 'packing_unit_id', \n",
    "                           'tier_1_qty', 'tier_2_qty', 'ws_new_qty',\n",
    "                           'packing_unit_price', 'tier_1_price', 'tier_2_price', \n",
    "                           'ws_new_price', 'wac_p']].copy()\n",
    "\n",
    "# Create tuples string for SQL\n",
    "tuples_for_query = ','.join([\n",
    "    f\"({int(row['warehouse_id'])}, {int(row['product_id'])}, {int(row['packing_unit_id'])}, \"\n",
    "    f\"{int(row['tier_1_qty'])}, {int(row['tier_2_qty'])}, {int(row['ws_new_qty']) if pd.notna(row['ws_new_qty']) else 0})\"\n",
    "    for _, row in selected_df.iterrows()\n",
    "])\n",
    "\n",
    "query = f'''\n",
    "WITH selected_products AS (\n",
    "    SELECT warehouse_id, product_id, packing_unit_id, tier_1_qty, tier_2_qty, ws_qty\n",
    "    FROM (VALUES\n",
    "        {tuples_for_query}\n",
    "    ) AS x(warehouse_id, product_id, packing_unit_id, tier_1_qty, tier_2_qty, ws_qty)\n",
    "),\n",
    "\n",
    "-- Same base filtering as product selection\n",
    "base AS (\n",
    "    SELECT *, ROW_NUMBER() OVER (PARTITION BY retailer_id ORDER BY priority) as rnk \n",
    "    FROM (\n",
    "        SELECT x.*, TAGGABLE_ID as retailer_id \n",
    "        FROM (\n",
    "            SELECT id as cohort_id, name as cohort_name, priority, dynamic_tag_id \n",
    "            FROM cohorts \n",
    "            WHERE is_active = 'true'\n",
    "                AND id IN (700,701,702,703,704,1123,1124,1125,1126)\n",
    "        ) x \n",
    "        JOIN DYNAMIC_TAGgables dt ON x.dynamic_tag_id = dt.dynamic_tag_id\n",
    "        WHERE dt.taggable_id NOT IN (\n",
    "            SELECT taggable_id FROM DYNAMIC_TAGgables \n",
    "            WHERE dynamic_tag_id IN (2807, 2808, 2809, 2810, 2811, 2812)\n",
    "        )\n",
    "    )\n",
    "    QUALIFY rnk = 1 \n",
    "),\n",
    "\n",
    "-- Map regions to warehouses\n",
    "whs AS (\n",
    "    SELECT * FROM (VALUES\n",
    "        ('Cairo', 'El-Marg', 38),\n",
    "        ('Cairo', 'Mostorod', 1),\n",
    "        ('Giza', 'Barageel', 236),\n",
    "        ('Giza', 'Sakkarah', 962),\n",
    "        ('Delta West', 'El-Mahala', 337),\n",
    "        ('Delta West', 'Tanta', 8),\n",
    "        ('Delta East', 'Mansoura FC', 339),\n",
    "        ('Delta East', 'Sharqya', 170),\n",
    "        ('Upper Egypt', 'Assiut FC', 501),\n",
    "        ('Upper Egypt', 'Bani sweif', 401),\n",
    "        ('Upper Egypt', 'Menya Samalot', 703),\n",
    "        ('Upper Egypt', 'Sohag', 632),\n",
    "        ('Alexandria', 'Khorshed Alex', 797)\n",
    "    ) x(region_name, wh, warehouse_id)\n",
    "),\n",
    "\n",
    "-- Get order quantities from previous month\n",
    "previous_month_orders AS (\n",
    "    SELECT \n",
    "        whs.warehouse_id,\n",
    "        pso.product_id,\n",
    "        pso.packing_unit_id,\n",
    "        so.parent_sales_order_id,\n",
    "        so.retailer_id,\n",
    "        SUM(pso.purchased_item_count) as order_qty,\n",
    "        SUM(pso.total_price) as order_value\n",
    "    FROM product_sales_order pso\n",
    "    JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "    JOIN base ON base.retailer_id = so.retailer_id\n",
    "    JOIN materialized_views.retailer_polygon rp ON rp.retailer_id = so.retailer_id\n",
    "    JOIN districts ON districts.id = rp.district_id\n",
    "    JOIN cities ON cities.id = districts.city_id\n",
    "    JOIN states ON states.id = cities.state_id\n",
    "    JOIN regions ON regions.id = states.region_id\n",
    "    JOIN whs ON whs.region_name = CASE WHEN regions.id = 2 THEN states.name_en ELSE regions.name_en END\n",
    "    JOIN selected_products sp \n",
    "        ON sp.warehouse_id = whs.warehouse_id \n",
    "        AND sp.product_id = pso.product_id\n",
    "        AND sp.packing_unit_id = pso.packing_unit_id\n",
    "    WHERE so.created_at::date BETWEEN current_date - 31 and current_Date - 1 \n",
    "                                   \n",
    "        AND so.sales_order_status_id NOT IN (7, 12)\n",
    "        AND so.channel IN ('telesales', 'retailer')\n",
    "        AND pso.purchased_item_count > 0\n",
    "    GROUP BY whs.warehouse_id, pso.product_id, pso.packing_unit_id, \n",
    "             so.parent_sales_order_id, so.retailer_id\n",
    "),\n",
    "\n",
    "-- Classify each order into tiers based on quantity\n",
    "order_tiers AS (\n",
    "    SELECT \n",
    "        pmo.*,\n",
    "        sp.tier_1_qty,\n",
    "        sp.tier_2_qty,\n",
    "        sp.ws_qty,\n",
    "        CASE \n",
    "            WHEN pmo.order_qty >= sp.ws_qty AND sp.ws_qty > 0 THEN 'Wholesale'\n",
    "            WHEN pmo.order_qty >= sp.tier_2_qty THEN 'Tier 2'\n",
    "            WHEN pmo.order_qty >= sp.tier_1_qty THEN 'Tier 1'\n",
    "            ELSE 'Base'\n",
    "        END as tier_reached\n",
    "    FROM previous_month_orders pmo\n",
    "    JOIN selected_products sp \n",
    "        ON sp.warehouse_id = pmo.warehouse_id \n",
    "        AND sp.product_id = pmo.product_id\n",
    "        AND sp.packing_unit_id = pmo.packing_unit_id\n",
    "),\n",
    "\n",
    "-- Aggregate conversion rates per SKU\n",
    "sku_conversion AS (\n",
    "    SELECT \n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        packing_unit_id,\n",
    "        COUNT(*) as total_orders,\n",
    "        SUM(order_value) as total_value,\n",
    "        \n",
    "        -- Order counts by tier\n",
    "        COUNT(CASE WHEN tier_reached = 'Base' THEN 1 END) as base_orders,\n",
    "        COUNT(CASE WHEN tier_reached = 'Tier 1' THEN 1 END) as tier1_orders,\n",
    "        COUNT(CASE WHEN tier_reached = 'Tier 2' THEN 1 END) as tier2_orders,\n",
    "        COUNT(CASE WHEN tier_reached = 'Wholesale' THEN 1 END) as ws_orders,\n",
    "        \n",
    "        -- Conversion percentages\n",
    "        ROUND(100.0 * COUNT(CASE WHEN tier_reached = 'Base' THEN 1 END) / NULLIF(COUNT(*), 0), 2) as base_pct,\n",
    "        ROUND(100.0 * COUNT(CASE WHEN tier_reached = 'Tier 1' THEN 1 END) / NULLIF(COUNT(*), 0), 2) as tier1_pct,\n",
    "        ROUND(100.0 * COUNT(CASE WHEN tier_reached = 'Tier 2' THEN 1 END) / NULLIF(COUNT(*), 0), 2) as tier2_pct,\n",
    "        ROUND(100.0 * COUNT(CASE WHEN tier_reached = 'Wholesale' THEN 1 END) / NULLIF(COUNT(*), 0), 2) as ws_pct\n",
    "        \n",
    "    FROM order_tiers\n",
    "    GROUP BY warehouse_id, product_id, packing_unit_id\n",
    "),\n",
    "\n",
    "-- Overall conversion rates\n",
    "overall_conversion AS (\n",
    "    SELECT \n",
    "        'Overall' as level,\n",
    "        COUNT(*) as total_orders,\n",
    "        SUM(order_value) as total_value,\n",
    "        ROUND(100.0 * COUNT(CASE WHEN tier_reached = 'Base' THEN 1 END) / NULLIF(COUNT(*), 0), 2) as base_pct,\n",
    "        ROUND(100.0 * COUNT(CASE WHEN tier_reached = 'Tier 1' THEN 1 END) / NULLIF(COUNT(*), 0), 2) as tier1_pct,\n",
    "        ROUND(100.0 * COUNT(CASE WHEN tier_reached = 'Tier 2' THEN 1 END) / NULLIF(COUNT(*), 0), 2) as tier2_pct,\n",
    "        ROUND(100.0 * COUNT(CASE WHEN tier_reached = 'Wholesale' THEN 1 END) / NULLIF(COUNT(*), 0), 2) as ws_pct\n",
    "    FROM order_tiers\n",
    ")\n",
    "\n",
    "-- Return both SKU-level and overall results\n",
    "SELECT \n",
    "    sc.warehouse_id,\n",
    "    sc.product_id,\n",
    "    sc.packing_unit_id,\n",
    "    sc.total_orders,\n",
    "    sc.total_value,\n",
    "    sc.base_orders,\n",
    "    sc.tier1_orders,\n",
    "    sc.tier2_orders,\n",
    "    sc.ws_orders,\n",
    "    sc.base_pct,\n",
    "    sc.tier1_pct,\n",
    "    sc.tier2_pct,\n",
    "    sc.ws_pct\n",
    "FROM sku_conversion sc\n",
    "ORDER BY sc.warehouse_id, sc.total_orders DESC\n",
    "'''\n",
    "\n",
    "print(\"Fetching historical tier conversion data from previous month...\")\n",
    "historical_conversion = snowflake_query(\"Egypt\", query)\n",
    "historical_conversion.columns = historical_conversion.columns.str.lower()\n",
    "\n",
    "for col in historical_conversion.columns:\n",
    "    historical_conversion[col] = pd.to_numeric(historical_conversion[col], errors='ignore')\n",
    "\n",
    "print(f\"✓ Retrieved conversion data for {len(historical_conversion)} SKUs\")\n",
    "print(f\"  Total orders analyzed: {historical_conversion['total_orders'].sum():,}\")\n",
    "\n",
    "# Calculate overall conversion rates\n",
    "total_orders = historical_conversion['total_orders'].sum()\n",
    "overall_base_pct = (historical_conversion['base_orders'].sum() / total_orders) * 100\n",
    "overall_t1_pct = (historical_conversion['tier1_orders'].sum() / total_orders) * 100\n",
    "overall_t2_pct = (historical_conversion['tier2_orders'].sum() / total_orders) * 100\n",
    "overall_ws_pct = (historical_conversion['ws_orders'].sum() / total_orders) * 100\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"HISTORICAL TIER CONVERSION (Previous Month)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nOverall Conversion Rates (based on {total_orders:,} orders):\")\n",
    "print(f\"  Base (no tier):  {overall_base_pct:>6.2f}%  ({historical_conversion['base_orders'].sum():,} orders)\")\n",
    "print(f\"  Tier 1:          {overall_t1_pct:>6.2f}%  ({historical_conversion['tier1_orders'].sum():,} orders)\")\n",
    "print(f\"  Tier 2:          {overall_t2_pct:>6.2f}%  ({historical_conversion['tier2_orders'].sum():,} orders)\")\n",
    "print(f\"  Wholesale:       {overall_ws_pct:>6.2f}%  ({historical_conversion['ws_orders'].sum():,} orders)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6b93ee2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================================================\n",
      "HISTORICAL SIMULATION - NMV, QUANTITY & GROSS PROFIT IMPACT\n",
      "========================================================================================================================\n",
      "\n",
      "Based on 486,667 historical orders across 2040 SKUs\n",
      "Conversion: Base 87.4% | T1 8.2% | T2 4.4% | WS 0.0%\n",
      "\n",
      "METRIC                               CURRENT            BLENDED             CHANGE     CHANGE %\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Total NMV (EGP)                 133,914,917       190,088,288       +56,173,370      +41.95%\n",
      "Total Quantity (units)              765,869         1,075,834          +309,965      +40.47%\n",
      "Total COGS (EGP)                124,857,687       178,122,661       +53,264,974      +42.66%\n",
      "Total Gross Profit (EGP)          9,057,230        11,965,627        +2,908,397      +32.11%\n",
      "Gross Margin (%)                       6.76%              6.29%             -0.47 pp\n",
      "\n",
      "========================================================================================================================\n",
      "WAREHOUSE-LEVEL BREAKDOWN\n",
      "========================================================================================================================\n",
      "\n",
      "   WH  Orders  Base%   T1%   T2%   WS% |        NMV Δ   NMV Δ% |         GP Δ    GP Δ% |  Margin\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "    1   91493  90.0%  7.2%  2.8%  0.0% |  +7,592,237   +30.3% |    +415,652   +23.4% |   6.71%\n",
      "    8   34519  89.4%  6.9%  3.7%  0.0% |  +3,189,830   +35.4% |    +170,613   +29.1% |   6.20%\n",
      "  170   29381  87.6%  8.7%  3.6%  0.0% |  +2,790,849   +37.4% |    +137,742   +27.4% |   6.25%\n",
      "  236   57302  88.5%  7.8%  3.7%  0.0% |  +6,017,422   +36.7% |    +317,781   +27.9% |   6.50%\n",
      "  337   35699  88.2%  7.8%  3.9%  0.0% |  +3,458,561   +38.8% |    +185,797   +31.7% |   6.24%\n",
      "  339   29433  88.8%  7.6%  3.6%  0.0% |  +2,672,254   +36.1% |    +135,789   +28.0% |   6.17%\n",
      "  401   35797  83.2% 10.2%  6.6%  0.0% |  +5,550,907   +57.3% |    +272,945   +42.8% |   5.98%\n",
      "  501   35257  84.5%  9.5%  6.1%  0.0% |  +5,355,648   +52.3% |    +259,609   +39.5% |   5.87%\n",
      "  632   33913  82.7% 10.4%  6.8%  0.0% |  +5,903,333   +57.1% |    +288,751   +41.2% |   6.09%\n",
      "  703   40696  84.5%  9.6%  5.8%  0.1% |  +6,527,965   +55.2% |    +306,463   +40.5% |   5.79%\n",
      "  797   10523  85.8%  8.4%  5.7%  0.1% |  +1,569,723   +57.0% |     +94,885   +47.6% |   6.81%\n",
      "  962   52654  89.0%  7.3%  3.7%  0.0% |  +5,544,642   +37.3% |    +322,372   +31.3% |   6.64%\n",
      "\n",
      "========================================================================================================================\n",
      "COMPARISON: Historical vs Hypothetical Scenarios\n",
      "========================================================================================================================\n",
      "\n",
      "Historical Conversion (87.4/8.2/4.4/0.0):\n",
      "  NMV Change:      +48.25%\n",
      "  Quantity Change: +40.89%\n",
      "  GP Change:       +34.89%\n",
      "  Blended Margin:  6.37%\n",
      "\n",
      "Closest Hypothetical Scenario: Scenario 1 - Very Low Conversion\n",
      "\n",
      "========================================================================================================================\n",
      "WHAT-IF SCENARIO: Move 30% of Base Orders to Tiers\n",
      "========================================================================================================================\n",
      "\n",
      "Conversion Rate Comparison:\n",
      "                               Historical  What-If (+30%)       Change\n",
      "----------------------------------------------------------------------\n",
      "Base (no tier)                      87.4%           67.4%       -20.0 pp\n",
      "Tier 1                               8.2%           21.3%       +13.1 pp\n",
      "Tier 2                               4.4%           11.3%        +6.9 pp\n",
      "Wholesale                            0.0%            0.1%        +0.0 pp\n",
      "TOTAL                              100.0%          100.0%\n",
      "\n",
      "METRIC                            CURRENT      HISTORICAL         WHAT-IF   vs Current  vs Historical\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Total NMV (EGP)              133,914,917    190,088,288    303,159,472 +169,244,555  +113,071,185\n",
      "Total Quantity                   765,869      1,075,834      1,598,648    +832,779      +522,814\n",
      "Total Gross Profit             9,057,230     11,965,627     17,239,323  +8,182,093    +5,273,696\n",
      "Gross Margin                       6.76%          6.29%          5.69%      -1.08 pp        -0.61 pp\n",
      "\n",
      "==============================================================================================================\n",
      "                                                IMPACT SUMMARY                                                \n",
      "==============================================================================================================\n",
      "\n",
      "If we shift 20% of base orders to tiers:\n",
      "\n",
      "📈 vs CURRENT STATE (100% base):\n",
      "   • NMV increases by:             +169,244,555 EGP  (+126.38%)\n",
      "   • Quantity increases by:            +832,779 units (+108.74%)\n",
      "   • Gross Profit changes by:        +8,182,093 EGP  (+90.34%)\n",
      "   • Margin changes:                      -1.08 pp\n",
      "\n",
      "📊 vs HISTORICAL CONVERSION (87/8/4/0):\n",
      "   • NMV additional:               +113,071,185 EGP  (+59.48%)\n",
      "   • Quantity additional:              +522,814 units (+48.60%)\n",
      "   • GP additional:                  +5,273,696 EGP  (+44.07%)\n",
      "   • Margin change:                       -0.61 pp\n",
      "\n",
      "========================================================================================================================\n",
      "KEY INSIGHT\n",
      "========================================================================================================================\n",
      "\n",
      "Based on historical conversion rates:\n",
      "• NMV INCREASES by 56,173,370 EGP (+41.9%)\n",
      "  → This is because retailers order MORE quantity to reach tier thresholds\n",
      "\n",
      "• Gross Profit INCREASES by 2,908,397 EGP (+32.1%)\n",
      "  → Higher volume offsets lower price per unit\n",
      "\n",
      "• Gross Margin changes from 6.76% to 6.29%\n",
      "  → Margin compression of 0.47 pp\n",
      "\n",
      "With additional 20% conversion (What-If):\n",
      "• Additional NMV opportunity: +113,071,185 EGP\n",
      "• Additional GP opportunity:  +5,273,696 EGP\n",
      "\n",
      "✓ Simulation results saved to 'QD_simulation_results.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PART 2B: CALCULATE BLENDED NMV, GROSS PROFIT FROM HISTORICAL CONVERSION\n",
    "# =============================================================================\n",
    "\n",
    "# Merge historical conversion data with pricing and quantity data\n",
    "simulation_data = historical_conversion.merge(\n",
    "    final_data[['warehouse_id', 'product_id', 'packing_unit_id',\n",
    "                'packing_unit_price', 'tier_1_price', 'tier_2_price', \n",
    "                'ws_new_price', 'wac_p', 'sku', 'brand',\n",
    "                'median_qty', 'tier_1_qty', 'tier_2_qty', 'ws_new_qty']],\n",
    "    on=['warehouse_id', 'product_id', 'packing_unit_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing values\n",
    "simulation_data['ws_price_filled'] = simulation_data['ws_new_price'].fillna(simulation_data['tier_2_price'])\n",
    "simulation_data['ws_qty_filled'] = simulation_data['ws_new_qty'].fillna(simulation_data['tier_2_qty'])\n",
    "\n",
    "# =============================================================================\n",
    "# Calculate NMV & GP per order for each tier\n",
    "# =============================================================================\n",
    "\n",
    "# Current state: All orders at base price with median quantity\n",
    "simulation_data['current_qty_per_order'] = simulation_data['median_qty']\n",
    "simulation_data['current_nmv_per_order'] = simulation_data['median_qty'] * simulation_data['packing_unit_price']\n",
    "simulation_data['current_cogs_per_order'] = simulation_data['median_qty'] * simulation_data['wac_p']\n",
    "simulation_data['current_gp_per_order'] = simulation_data['current_nmv_per_order'] - simulation_data['current_cogs_per_order']\n",
    "\n",
    "# Blended state: Orders distributed across tiers with corresponding quantities\n",
    "# Base orders: median_qty at packing_unit_price\n",
    "# T1 orders: tier_1_qty at tier_1_price\n",
    "# T2 orders: tier_2_qty at tier_2_price\n",
    "# WS orders: ws_qty at ws_price\n",
    "\n",
    "simulation_data['blended_qty_per_order'] = (\n",
    "    (simulation_data['base_pct'] / 100) * simulation_data['median_qty'] +\n",
    "    (simulation_data['tier1_pct'] / 100) * simulation_data['tier_1_qty'] +\n",
    "    (simulation_data['tier2_pct'] / 100) * simulation_data['tier_2_qty'] +\n",
    "    (simulation_data['ws_pct'] / 100) * simulation_data['ws_qty_filled']\n",
    ")\n",
    "\n",
    "simulation_data['blended_nmv_per_order'] = (\n",
    "    (simulation_data['base_pct'] / 100) * simulation_data['median_qty'] * simulation_data['packing_unit_price'] +\n",
    "    (simulation_data['tier1_pct'] / 100) * simulation_data['tier_1_qty'] * simulation_data['tier_1_price'] +\n",
    "    (simulation_data['tier2_pct'] / 100) * simulation_data['tier_2_qty'] * simulation_data['tier_2_price'] +\n",
    "    (simulation_data['ws_pct'] / 100) * simulation_data['ws_qty_filled'] * simulation_data['ws_price_filled']\n",
    ")\n",
    "\n",
    "simulation_data['blended_cogs_per_order'] = (\n",
    "    (simulation_data['base_pct'] / 100) * simulation_data['median_qty'] * simulation_data['wac_p'] +\n",
    "    (simulation_data['tier1_pct'] / 100) * simulation_data['tier_1_qty'] * simulation_data['wac_p'] +\n",
    "    (simulation_data['tier2_pct'] / 100) * simulation_data['tier_2_qty'] * simulation_data['wac_p'] +\n",
    "    (simulation_data['ws_pct'] / 100) * simulation_data['ws_qty_filled'] * simulation_data['wac_p']\n",
    ")\n",
    "\n",
    "simulation_data['blended_gp_per_order'] = simulation_data['blended_nmv_per_order'] - simulation_data['blended_cogs_per_order']\n",
    "\n",
    "# Calculate totals using actual order counts\n",
    "simulation_data['total_current_nmv'] = simulation_data['current_nmv_per_order'] * simulation_data['total_orders']\n",
    "simulation_data['total_current_cogs'] = simulation_data['current_cogs_per_order'] * simulation_data['total_orders']\n",
    "simulation_data['total_current_gp'] = simulation_data['total_current_nmv'] - simulation_data['total_current_cogs']\n",
    "simulation_data['total_current_qty'] = simulation_data['current_qty_per_order'] * simulation_data['total_orders']\n",
    "\n",
    "simulation_data['total_blended_nmv'] = simulation_data['blended_nmv_per_order'] * simulation_data['total_orders']\n",
    "simulation_data['total_blended_cogs'] = simulation_data['blended_cogs_per_order'] * simulation_data['total_orders']\n",
    "simulation_data['total_blended_gp'] = simulation_data['total_blended_nmv'] - simulation_data['total_blended_cogs']\n",
    "simulation_data['total_blended_qty'] = simulation_data['blended_qty_per_order'] * simulation_data['total_orders']\n",
    "\n",
    "# Calculate changes\n",
    "simulation_data['nmv_change'] = simulation_data['total_blended_nmv'] - simulation_data['total_current_nmv']\n",
    "simulation_data['qty_change'] = simulation_data['total_blended_qty'] - simulation_data['total_current_qty']\n",
    "simulation_data['gp_change'] = simulation_data['total_blended_gp'] - simulation_data['total_current_gp']\n",
    "\n",
    "# Margins\n",
    "simulation_data['current_margin'] = simulation_data['total_current_gp'] / simulation_data['total_current_nmv']\n",
    "simulation_data['blended_margin'] = simulation_data['total_blended_gp'] / simulation_data['total_blended_nmv']\n",
    "\n",
    "# =============================================================================\n",
    "# Summary Statistics\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"HISTORICAL SIMULATION - NMV, QUANTITY & GROSS PROFIT IMPACT\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "# Aggregate totals\n",
    "total_orders = simulation_data['total_orders'].sum()\n",
    "total_current_nmv = simulation_data['total_current_nmv'].sum()\n",
    "total_blended_nmv = simulation_data['total_blended_nmv'].sum()\n",
    "total_current_qty = simulation_data['total_current_qty'].sum()\n",
    "total_blended_qty = simulation_data['total_blended_qty'].sum()\n",
    "total_current_gp = simulation_data['total_current_gp'].sum()\n",
    "total_blended_gp = simulation_data['total_blended_gp'].sum()\n",
    "\n",
    "nmv_change = total_blended_nmv - total_current_nmv\n",
    "qty_change = total_blended_qty - total_current_qty\n",
    "gp_change = total_blended_gp - total_current_gp\n",
    "\n",
    "print(f\"\\nBased on {total_orders:,} historical orders across {len(simulation_data)} SKUs\")\n",
    "print(f\"Conversion: Base {overall_base_pct:.1f}% | T1 {overall_t1_pct:.1f}% | T2 {overall_t2_pct:.1f}% | WS {overall_ws_pct:.1f}%\")\n",
    "\n",
    "print(f\"\\n{'METRIC':<25} {'CURRENT':>18} {'BLENDED':>18} {'CHANGE':>18} {'CHANGE %':>12}\")\n",
    "print(\"-\" * 95)\n",
    "print(f\"{'Total NMV (EGP)':<25} {total_current_nmv:>17,.0f} {total_blended_nmv:>17,.0f} {nmv_change:>+17,.0f} {(nmv_change/total_current_nmv)*100:>+11.2f}%\")\n",
    "print(f\"{'Total Quantity (units)':<25} {total_current_qty:>17,.0f} {total_blended_qty:>17,.0f} {qty_change:>+17,.0f} {(qty_change/total_current_qty)*100:>+11.2f}%\")\n",
    "print(f\"{'Total COGS (EGP)':<25} {total_current_nmv-total_current_gp:>17,.0f} {total_blended_nmv-total_blended_gp:>17,.0f} {(total_blended_nmv-total_blended_gp)-(total_current_nmv-total_current_gp):>+17,.0f} {((total_blended_nmv-total_blended_gp)-(total_current_nmv-total_current_gp))/(total_current_nmv-total_current_gp)*100:>+11.2f}%\")\n",
    "print(f\"{'Total Gross Profit (EGP)':<25} {total_current_gp:>17,.0f} {total_blended_gp:>17,.0f} {gp_change:>+17,.0f} {(gp_change/total_current_gp)*100:>+11.2f}%\")\n",
    "print(f\"{'Gross Margin (%)':<25} {(total_current_gp/total_current_nmv)*100:>17.2f}% {(total_blended_gp/total_blended_nmv)*100:>17.2f}% {((total_blended_gp/total_blended_nmv)-(total_current_gp/total_current_nmv))*100:>+17.2f} pp\")\n",
    "\n",
    "# Warehouse breakdown with GP\n",
    "print(f\"\\n\" + \"=\" * 120)\n",
    "print(\"WAREHOUSE-LEVEL BREAKDOWN\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "warehouse_summary = simulation_data.groupby('warehouse_id').agg({\n",
    "    'total_orders': 'sum',\n",
    "    'base_orders': 'sum',\n",
    "    'tier1_orders': 'sum',\n",
    "    'tier2_orders': 'sum',\n",
    "    'ws_orders': 'sum',\n",
    "    'total_current_nmv': 'sum',\n",
    "    'total_blended_nmv': 'sum',\n",
    "    'total_current_gp': 'sum',\n",
    "    'total_blended_gp': 'sum',\n",
    "    'total_current_qty': 'sum',\n",
    "    'total_blended_qty': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "warehouse_summary['base_pct'] = (warehouse_summary['base_orders'] / warehouse_summary['total_orders'] * 100).round(1)\n",
    "warehouse_summary['tier1_pct'] = (warehouse_summary['tier1_orders'] / warehouse_summary['total_orders'] * 100).round(1)\n",
    "warehouse_summary['tier2_pct'] = (warehouse_summary['tier2_orders'] / warehouse_summary['total_orders'] * 100).round(1)\n",
    "warehouse_summary['ws_pct'] = (warehouse_summary['ws_orders'] / warehouse_summary['total_orders'] * 100).round(1)\n",
    "warehouse_summary['nmv_change'] = warehouse_summary['total_blended_nmv'] - warehouse_summary['total_current_nmv']\n",
    "warehouse_summary['nmv_change_pct'] = (warehouse_summary['nmv_change'] / warehouse_summary['total_current_nmv'] * 100).round(1)\n",
    "warehouse_summary['gp_change'] = warehouse_summary['total_blended_gp'] - warehouse_summary['total_current_gp']\n",
    "warehouse_summary['gp_change_pct'] = (warehouse_summary['gp_change'] / warehouse_summary['total_current_gp'] * 100).round(1)\n",
    "warehouse_summary['current_margin'] = (warehouse_summary['total_current_gp'] / warehouse_summary['total_current_nmv'] * 100).round(2)\n",
    "warehouse_summary['blended_margin'] = (warehouse_summary['total_blended_gp'] / warehouse_summary['total_blended_nmv'] * 100).round(2)\n",
    "\n",
    "print(f\"\\n{'WH':>5} {'Orders':>7} {'Base%':>6} {'T1%':>5} {'T2%':>5} {'WS%':>5} | {'NMV Δ':>12} {'NMV Δ%':>8} | {'GP Δ':>12} {'GP Δ%':>8} | {'Margin':>7}\")\n",
    "print(\"-\" * 110)\n",
    "\n",
    "for _, row in warehouse_summary.iterrows():\n",
    "    print(f\"{int(row['warehouse_id']):>5} {int(row['total_orders']):>7} \"\n",
    "          f\"{row['base_pct']:>5.1f}% {row['tier1_pct']:>4.1f}% {row['tier2_pct']:>4.1f}% {row['ws_pct']:>4.1f}% | \"\n",
    "          f\"{row['nmv_change']:>+11,.0f} {row['nmv_change_pct']:>+7.1f}% | \"\n",
    "          f\"{row['gp_change']:>+11,.0f} {row['gp_change_pct']:>+7.1f}% | \"\n",
    "          f\"{row['blended_margin']:>6.2f}%\")\n",
    "\n",
    "# Compare historical conversion to scenarios\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"COMPARISON: Historical vs Hypothetical Scenarios\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "# Calculate blended metrics using historical rates\n",
    "hist_metrics = calculate_blended_metrics_with_gp(\n",
    "    final_data, overall_base_pct, overall_t1_pct, overall_t2_pct, overall_ws_pct, NUM_ORDERS\n",
    ")\n",
    "\n",
    "print(f\"\\nHistorical Conversion ({overall_base_pct:.1f}/{overall_t1_pct:.1f}/{overall_t2_pct:.1f}/{overall_ws_pct:.1f}):\")\n",
    "print(f\"  NMV Change:      {hist_metrics['nmv_change_pct']:>+.2f}%\")\n",
    "print(f\"  Quantity Change: {hist_metrics['qty_change_pct']:>+.2f}%\")\n",
    "print(f\"  GP Change:       {hist_metrics['gp_change_pct']:>+.2f}%\")\n",
    "print(f\"  Blended Margin:  {hist_metrics['blended_margin']*100:.2f}%\")\n",
    "\n",
    "# Find closest scenario\n",
    "closest_scenario = None\n",
    "min_diff = float('inf')\n",
    "for scenario_name, (base, t1, t2, ws) in scenarios.items():\n",
    "    diff = abs(base - overall_base_pct) + abs(t1 - overall_t1_pct) + abs(t2 - overall_t2_pct) + abs(ws - overall_ws_pct)\n",
    "    if diff < min_diff:\n",
    "        min_diff = diff\n",
    "        closest_scenario = scenario_name\n",
    "\n",
    "print(f\"\\nClosest Hypothetical Scenario: {closest_scenario}\")\n",
    "\n",
    "# =============================================================================\n",
    "# WHAT-IF SCENARIO: Shift 30% from Base to Tiers\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"WHAT-IF SCENARIO: Move 30% of Base Orders to Tiers\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "# Calculate new conversion rates by shifting 30% from base to tiers\n",
    "SHIFT_PCT = 20  # Percentage points to shift from base\n",
    "\n",
    "# New base rate (reduced by 30 pp)\n",
    "new_base_pct = max(overall_base_pct - SHIFT_PCT, 0)\n",
    "\n",
    "# Distribute the shifted percentage to tiers proportionally\n",
    "# Based on existing tier ratios (excluding base)\n",
    "tier_total = overall_t1_pct + overall_t2_pct + overall_ws_pct\n",
    "\n",
    "if tier_total > 0:\n",
    "    # Distribute proportionally to existing tier distribution\n",
    "    t1_share = overall_t1_pct / tier_total\n",
    "    t2_share = overall_t2_pct / tier_total\n",
    "    ws_share = overall_ws_pct / tier_total\n",
    "    \n",
    "    new_t1_pct = overall_t1_pct + (SHIFT_PCT * t1_share)\n",
    "    new_t2_pct = overall_t2_pct + (SHIFT_PCT * t2_share)\n",
    "    new_ws_pct = overall_ws_pct + (SHIFT_PCT * ws_share)\n",
    "else:\n",
    "    # If no tier conversion exists, split evenly\n",
    "    new_t1_pct = overall_t1_pct + (SHIFT_PCT * 0.5)\n",
    "    new_t2_pct = overall_t2_pct + (SHIFT_PCT * 0.3)\n",
    "    new_ws_pct = overall_ws_pct + (SHIFT_PCT * 0.2)\n",
    "\n",
    "print(f\"\\nConversion Rate Comparison:\")\n",
    "print(f\"{'':>25} {'Historical':>15} {'What-If (+30%)':>15} {'Change':>12}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Base (no tier)':<25} {overall_base_pct:>14.1f}% {new_base_pct:>14.1f}% {new_base_pct - overall_base_pct:>+11.1f} pp\")\n",
    "print(f\"{'Tier 1':<25} {overall_t1_pct:>14.1f}% {new_t1_pct:>14.1f}% {new_t1_pct - overall_t1_pct:>+11.1f} pp\")\n",
    "print(f\"{'Tier 2':<25} {overall_t2_pct:>14.1f}% {new_t2_pct:>14.1f}% {new_t2_pct - overall_t2_pct:>+11.1f} pp\")\n",
    "print(f\"{'Wholesale':<25} {overall_ws_pct:>14.1f}% {new_ws_pct:>14.1f}% {new_ws_pct - overall_ws_pct:>+11.1f} pp\")\n",
    "print(f\"{'TOTAL':<25} {overall_base_pct + overall_t1_pct + overall_t2_pct + overall_ws_pct:>14.1f}% {new_base_pct + new_t1_pct + new_t2_pct + new_ws_pct:>14.1f}%\")\n",
    "\n",
    "# Calculate metrics for what-if scenario\n",
    "whatif_metrics = calculate_blended_metrics_with_gp(final_data, new_base_pct, new_t1_pct, new_t2_pct, new_ws_pct, NUM_ORDERS)\n",
    "\n",
    "# Also calculate using actual order count from historical data\n",
    "whatif_simulation = simulation_data.copy()\n",
    "\n",
    "# Recalculate with new conversion rates\n",
    "whatif_simulation['whatif_nmv_per_order'] = (\n",
    "    (new_base_pct / 100) * whatif_simulation['median_qty'] * whatif_simulation['packing_unit_price'] +\n",
    "    (new_t1_pct / 100) * whatif_simulation['tier_1_qty'] * whatif_simulation['tier_1_price'] +\n",
    "    (new_t2_pct / 100) * whatif_simulation['tier_2_qty'] * whatif_simulation['tier_2_price'] +\n",
    "    (new_ws_pct / 100) * whatif_simulation['ws_qty_filled'] * whatif_simulation['ws_price_filled']\n",
    ")\n",
    "\n",
    "whatif_simulation['whatif_cogs_per_order'] = (\n",
    "    (new_base_pct / 100) * whatif_simulation['median_qty'] * whatif_simulation['wac_p'] +\n",
    "    (new_t1_pct / 100) * whatif_simulation['tier_1_qty'] * whatif_simulation['wac_p'] +\n",
    "    (new_t2_pct / 100) * whatif_simulation['tier_2_qty'] * whatif_simulation['wac_p'] +\n",
    "    (new_ws_pct / 100) * whatif_simulation['ws_qty_filled'] * whatif_simulation['wac_p']\n",
    ")\n",
    "\n",
    "whatif_simulation['whatif_qty_per_order'] = (\n",
    "    (new_base_pct / 100) * whatif_simulation['median_qty'] +\n",
    "    (new_t1_pct / 100) * whatif_simulation['tier_1_qty'] +\n",
    "    (new_t2_pct / 100) * whatif_simulation['tier_2_qty'] +\n",
    "    (new_ws_pct / 100) * whatif_simulation['ws_qty_filled']\n",
    ")\n",
    "\n",
    "# Calculate totals with actual historical order counts\n",
    "whatif_simulation['total_whatif_nmv'] = whatif_simulation['whatif_nmv_per_order'] * whatif_simulation['total_orders']\n",
    "whatif_simulation['total_whatif_cogs'] = whatif_simulation['whatif_cogs_per_order'] * whatif_simulation['total_orders']\n",
    "whatif_simulation['total_whatif_gp'] = whatif_simulation['total_whatif_nmv'] - whatif_simulation['total_whatif_cogs']\n",
    "whatif_simulation['total_whatif_qty'] = whatif_simulation['whatif_qty_per_order'] * whatif_simulation['total_orders']\n",
    "\n",
    "# Aggregate totals\n",
    "total_whatif_nmv = whatif_simulation['total_whatif_nmv'].sum()\n",
    "total_whatif_qty = whatif_simulation['total_whatif_qty'].sum()\n",
    "total_whatif_gp = whatif_simulation['total_whatif_gp'].sum()\n",
    "\n",
    "# Calculate changes vs current and vs historical blended\n",
    "print(f\"\\n{'METRIC':<25} {'CURRENT':>15} {'HISTORICAL':>15} {'WHAT-IF':>15} {'vs Current':>12} {'vs Historical':>14}\")\n",
    "print(\"-\" * 110)\n",
    "print(f\"{'Total NMV (EGP)':<25} {total_current_nmv:>14,.0f} {total_blended_nmv:>14,.0f} {total_whatif_nmv:>14,.0f} {(total_whatif_nmv - total_current_nmv):>+11,.0f} {(total_whatif_nmv - total_blended_nmv):>+13,.0f}\")\n",
    "print(f\"{'Total Quantity':<25} {total_current_qty:>14,.0f} {total_blended_qty:>14,.0f} {total_whatif_qty:>14,.0f} {(total_whatif_qty - total_current_qty):>+11,.0f} {(total_whatif_qty - total_blended_qty):>+13,.0f}\")\n",
    "print(f\"{'Total Gross Profit':<25} {total_current_gp:>14,.0f} {total_blended_gp:>14,.0f} {total_whatif_gp:>14,.0f} {(total_whatif_gp - total_current_gp):>+11,.0f} {(total_whatif_gp - total_blended_gp):>+13,.0f}\")\n",
    "\n",
    "current_margin_pct = (total_current_gp / total_current_nmv) * 100\n",
    "historical_margin_pct = (total_blended_gp / total_blended_nmv) * 100\n",
    "whatif_margin_pct = (total_whatif_gp / total_whatif_nmv) * 100\n",
    "\n",
    "print(f\"{'Gross Margin':<25} {current_margin_pct:>13.2f}% {historical_margin_pct:>13.2f}% {whatif_margin_pct:>13.2f}% {(whatif_margin_pct - current_margin_pct):>+10.2f} pp {(whatif_margin_pct - historical_margin_pct):>+12.2f} pp\")\n",
    "\n",
    "# Summary comparison\n",
    "print(f\"\\n{'':=^110}\")\n",
    "print(f\"{'IMPACT SUMMARY':^110}\")\n",
    "print(f\"{'':=^110}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "If we shift {SHIFT_PCT}% of base orders to tiers:\n",
    "\n",
    "📈 vs CURRENT STATE (100% base):\n",
    "   • NMV increases by:          {(total_whatif_nmv - total_current_nmv):>+15,.0f} EGP  ({((total_whatif_nmv - total_current_nmv) / total_current_nmv) * 100:>+6.2f}%)\n",
    "   • Quantity increases by:     {(total_whatif_qty - total_current_qty):>+15,.0f} units ({((total_whatif_qty - total_current_qty) / total_current_qty) * 100:>+6.2f}%)\n",
    "   • Gross Profit changes by:   {(total_whatif_gp - total_current_gp):>+15,.0f} EGP  ({((total_whatif_gp - total_current_gp) / total_current_gp) * 100:>+6.2f}%)\n",
    "   • Margin changes:            {(whatif_margin_pct - current_margin_pct):>+15.2f} pp\n",
    "\n",
    "📊 vs HISTORICAL CONVERSION ({overall_base_pct:.0f}/{overall_t1_pct:.0f}/{overall_t2_pct:.0f}/{overall_ws_pct:.0f}):\n",
    "   • NMV additional:            {(total_whatif_nmv - total_blended_nmv):>+15,.0f} EGP  ({((total_whatif_nmv - total_blended_nmv) / total_blended_nmv) * 100:>+6.2f}%)\n",
    "   • Quantity additional:       {(total_whatif_qty - total_blended_qty):>+15,.0f} units ({((total_whatif_qty - total_blended_qty) / total_blended_qty) * 100:>+6.2f}%)\n",
    "   • GP additional:             {(total_whatif_gp - total_blended_gp):>+15,.0f} EGP  ({((total_whatif_gp - total_blended_gp) / total_blended_gp) * 100:>+6.2f}%)\n",
    "   • Margin change:             {(whatif_margin_pct - historical_margin_pct):>+15.2f} pp\n",
    "\"\"\")\n",
    "\n",
    "# Key insight\n",
    "print(\"=\" * 120)\n",
    "print(\"KEY INSIGHT\")\n",
    "print(\"=\" * 120)\n",
    "print(f\"\"\"\n",
    "Based on historical conversion rates:\n",
    "• NMV {'INCREASES' if nmv_change > 0 else 'DECREASES'} by {abs(nmv_change):,.0f} EGP ({(nmv_change/total_current_nmv)*100:+.1f}%)\n",
    "  → This is because retailers order MORE quantity to reach tier thresholds\n",
    "\n",
    "• Gross Profit {'INCREASES' if gp_change > 0 else 'DECREASES'} by {abs(gp_change):,.0f} EGP ({(gp_change/total_current_gp)*100:+.1f}%)\n",
    "  → {'Higher volume offsets lower price per unit' if gp_change > 0 else 'Lower prices reduce GP despite higher volume'}\n",
    "\n",
    "• Gross Margin changes from {(total_current_gp/total_current_nmv)*100:.2f}% to {(total_blended_gp/total_blended_nmv)*100:.2f}%\n",
    "  → {'Margin compression' if (total_blended_gp/total_blended_nmv) < (total_current_gp/total_current_nmv) else 'Margin improvement'} of {abs((total_blended_gp/total_blended_nmv)-(total_current_gp/total_current_nmv))*100:.2f} pp\n",
    "\n",
    "With additional {SHIFT_PCT}% conversion (What-If):\n",
    "• Additional NMV opportunity: {(total_whatif_nmv - total_blended_nmv):+,.0f} EGP\n",
    "• Additional GP opportunity:  {(total_whatif_gp - total_blended_gp):+,.0f} EGP\n",
    "\"\"\")\n",
    "\n",
    "# Save simulation results\n",
    "simulation_file = 'QD_simulation_results.xlsx'\n",
    "with pd.ExcelWriter(simulation_file, engine='openpyxl') as writer:\n",
    "    scenarios_df.to_excel(writer, sheet_name='Scenarios', index=False)\n",
    "    simulation_data.to_excel(writer, sheet_name='Historical_Simulation', index=False)\n",
    "    warehouse_summary.to_excel(writer, sheet_name='Warehouse_Summary', index=False)\n",
    "\n",
    "print(f\"✓ Simulation results saved to '{simulation_file}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ac640f",
   "metadata": {},
   "source": [
    "## 7. Final Ranking & Export\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b2526c3b-e4f3-4289-86d8-f2a2f3d7c138",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data['check_qty'] = final_data['tier_2_qty']/final_data['tier_1_qty']\n",
    "final_data['target_qty_ratio'] = final_data['discount_ratio']/2\n",
    "final_data['target_tier_2_q'] = np.round(final_data['target_qty_ratio']*final_data['tier_1_qty'])\n",
    "final_data.loc[(final_data['check_qty']<1.3)&(final_data['elasticity_ratio']>3),'tier_2_qty'] = final_data['target_tier_2_q']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "65ee57d5-5719-4d50-a45a-d3a3cbf79803",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload format created: 12 warehouse rows\n",
      "\n",
      "Per warehouse breakdown:\n",
      "  WH 1: Group 1 = 200 items, Group 2 = 198 items\n",
      "  WH 8: Group 1 = 200 items, Group 2 = 198 items\n",
      "  WH 170: Group 1 = 200 items, Group 2 = 199 items\n",
      "  WH 236: Group 1 = 200 items, Group 2 = 200 items\n",
      "  WH 337: Group 1 = 200 items, Group 2 = 200 items\n",
      "  WH 339: Group 1 = 200 items, Group 2 = 199 items\n",
      "  WH 401: Group 1 = 200 items, Group 2 = 198 items\n",
      "  WH 501: Group 1 = 200 items, Group 2 = 198 items\n",
      "  WH 632: Group 1 = 200 items, Group 2 = 198 items\n",
      "  WH 703: Group 1 = 200 items, Group 2 = 200 items\n",
      "  WH 797: Group 1 = 200 items, Group 2 = 200 items\n",
      "  WH 962: Group 1 = 200 items, Group 2 = 200 items\n",
      "\n",
      "=== DETAILED FILE ===\n",
      "Saved 2040 SKUs to 'QD_detailed.xlsx'\n",
      "\n",
      "=== UPLOAD FILE ===\n",
      "Saved 12 rows to 'QD_Data.xlsx'\n",
      "Columns: ['warehouse_id', 'Discounts Group 1', 'Discounts Group 2', 'Description']\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CREATE UPLOAD FORMAT\n",
    "# =============================================================================\n",
    "# Format: ONE row per warehouse_id\n",
    "# - Discounts Group 1: List of [tier 1 items + wholesale items] (max 200, overflow goes to Group 2)\n",
    "# - Discounts Group 2: List of [tier 2 items + overflow from Group 1]\n",
    "# Each item format: [product_id, packing_unit_id, quantity, discount_pct]\n",
    "\n",
    "MAX_GROUP_SIZE = 200\n",
    "MAX_DISCOUNT_CAP_t1 = 4.0\n",
    "MAX_DISCOUNT_CAP_t2 = 5.0\n",
    "MAX_DISCOUNT_CAP_ws = 6.0# Maximum discount capped at 6%\n",
    "\n",
    "final_quantity_discount = pd.DataFrame(columns=['warehouse_id', 'Discounts Group 1', 'Discounts Group 2', 'Description'])\n",
    "\n",
    "for wh_id in final_data.warehouse_id.unique():\n",
    "    warehouse_data = final_data[final_data['warehouse_id'] == wh_id]\n",
    "    warehouse_id = int(wh_id)\n",
    "    \n",
    "    # Collect all tier 1 items\n",
    "    tier_1_items = []\n",
    "    # Collect all tier 2 items\n",
    "    tier_2_items = []\n",
    "    # Collect all wholesale items\n",
    "    ws_items = []\n",
    "    \n",
    "    for i, r in warehouse_data.iterrows():\n",
    "        product_id = int(r['product_id'])\n",
    "        packing_unit_id = int(r['packing_unit_id'])\n",
    "        current_price = r['packing_unit_price']\n",
    "        \n",
    "        # Tier 1 (cap discount at MAX_DISCOUNT_CAP)\n",
    "        q_1 = int(r['tier_1_qty'])\n",
    "        d_1 = min(round(r['discount_1_pct'], 2), MAX_DISCOUNT_CAP_t1)\n",
    "        tier_1_items.append([product_id, packing_unit_id, q_1, d_1])\n",
    "        \n",
    "        # Tier 2 (cap discount at MAX_DISCOUNT_CAP)\n",
    "        q_2 = int(r['tier_2_qty'])\n",
    "        d_2 = min(round(r['discount_2_pct'], 2), MAX_DISCOUNT_CAP_t2)\n",
    "        tier_2_items.append([product_id, packing_unit_id, q_2, d_2])\n",
    "        \n",
    "        # Wholesale (new logic) - cap discount at MAX_DISCOUNT_CAP\n",
    "        ws_qty = r.get('ws_new_qty', None)\n",
    "        ws_price = r.get('ws_new_price', None)\n",
    "        \n",
    "        if pd.notna(ws_qty) and pd.notna(ws_price) and ws_qty > 0 and current_price > 0:\n",
    "            q_ws = int(ws_qty)\n",
    "            d_ws = min(round(((current_price - ws_price) / current_price) * 100, 2), MAX_DISCOUNT_CAP_ws)\n",
    "            ws_items.append([product_id, packing_unit_id, q_ws, d_ws])\n",
    "    \n",
    "    # Group 1: Tier 1 + Wholesale (max 200)\n",
    "    group_1_items = tier_1_items + ws_items\n",
    "    \n",
    "    # Group 2: Tier 2 + overflow from Group 1\n",
    "    if len(group_1_items) > MAX_GROUP_SIZE:\n",
    "        # Overflow goes to Group 2\n",
    "        overflow = group_1_items[MAX_GROUP_SIZE:]\n",
    "        group_1_items = group_1_items[:MAX_GROUP_SIZE]\n",
    "        group_2_items = tier_2_items + overflow\n",
    "    else:\n",
    "        group_2_items = tier_2_items  \n",
    "    \n",
    "    new_row = {\n",
    "        'warehouse_id': warehouse_id,\n",
    "        'Discounts Group 1': group_1_items,\n",
    "        'Discounts Group 2': group_2_items,\n",
    "        'Description': f'{warehouse_id}QD'\n",
    "    }\n",
    "    final_quantity_discount = pd.concat([final_quantity_discount, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "# Summary\n",
    "print(f\"Upload format created: {len(final_quantity_discount)} warehouse rows\")\n",
    "print(f\"\\nPer warehouse breakdown:\")\n",
    "for idx, row in final_quantity_discount.iterrows():\n",
    "    wh = row['warehouse_id']\n",
    "    g1_count = len(row['Discounts Group 1'])\n",
    "    g2_count = len(row['Discounts Group 2'])\n",
    "    print(f\"  WH {wh}: Group 1 = {g1_count} items, Group 2 = {g2_count} items\")\n",
    "\n",
    "# # =============================================================================\n",
    "# # SAVE FILES\n",
    "# # =============================================================================\n",
    "\n",
    "# # Save detailed data\n",
    "detailed_file = 'QD_detailed.xlsx'\n",
    "final_data.to_excel(detailed_file, index=False)\n",
    "print(f\"\\n=== DETAILED FILE ===\")\n",
    "print(f\"Saved {len(final_data)} SKUs to '{detailed_file}'\")\n",
    "\n",
    "# Save upload format\n",
    "upload_file = 'QD_Data.xlsx'\n",
    "final_quantity_discount.to_excel(upload_file, index=False)\n",
    "print(f\"\\n=== UPLOAD FILE ===\")\n",
    "print(f\"Saved {len(final_quantity_discount)} rows to '{upload_file}'\")\n",
    "print(f\"Columns: {list(final_quantity_discount.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "72902879-a6b9-469a-aa6b-bd903e2fd8c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Warehouse to Tag ID mapping for upload\n",
    "df_warehouse_mapping = pd.DataFrame({\n",
    "    'warehouse_name': ['Assiut FC', 'Bani sweif', 'Barageel', 'El-Mahala', 'Khorshed Alex', \n",
    "                       'Mansoura FC', 'Menya Samalot', 'Mostorod', 'Sakkarah', 'Sharqya', \n",
    "                       'Sohag', 'Tanta'],\n",
    "    'warehouse_id':   [501, 401, 236, 337, 797, 339, 703, 1, 962, 170, 632, 8],\n",
    "    'tag_id':         [3301, 3302, 3303, 3304, 3305, 3306, 3307, 3308, 3309, 3310, 3311, 3312]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0b7d74cd-a60a-4778-b642-2d5c5723700e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge upload data with warehouse mapping\n",
    "to_upload = final_quantity_discount.merge(df_warehouse_mapping, on='warehouse_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5fe7d893-03eb-421e-b704-de77e375c062",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/2026 15:12 26/01/2026 12:59\n",
      "✓ Saved upload file: QD_upload.xlsx (12 warehouses)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PREPARE FINAL UPLOAD FILE\n",
    "# =============================================================================\n",
    "\n",
    "# Set description and date/time fields\n",
    "to_upload['Description'] = (\n",
    "    to_upload['warehouse_name'].astype(str)\n",
    "    .str.replace(' ', '')\n",
    "    + \"QD\"\n",
    ")\n",
    "to_upload['Description'] = to_upload['Description'].str.replace(\"-\",\"\") \n",
    "\n",
    "local_tz = pytz.timezone('Africa/Cairo')\n",
    "\n",
    "start_date = datetime.now(local_tz) + timedelta(minutes=10)\n",
    "start_date_str = start_date.strftime('%d/%m/%Y %H:%M')\n",
    "\n",
    "end_date = datetime.now(local_tz) + timedelta(days=1)\n",
    "end_date = end_date.replace(hour=12, minute=59, second=0, microsecond=0)\n",
    "end_date_str = end_date.strftime('%d/%m/%Y %H:%M')\n",
    "print(start_date_str,end_date_str)\n",
    "to_upload['Start Date/Time'] = start_date_str\n",
    "to_upload['End Date/Time'] = end_date_str\n",
    "to_upload = to_upload.rename(columns={'tag_id': 'Tag ID'})\n",
    "\n",
    "to_upload=to_upload[['Tag ID', 'Description', 'Start Date/Time', 'End Date/Time','Discounts Group 1', 'Discounts Group 2']]\n",
    "# Save upload file\n",
    "to_upload.to_excel('QD_upload.xlsx', index=False)\n",
    "print(f\"✓ Saved upload file: QD_upload.xlsx ({len(to_upload)} warehouses)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ac1e89eb-6eba-4c3f-a7de-25433787171d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading QD file to API...\n",
      "✓ Upload succeeded (status: 200)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# UPLOAD TO API\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Uploading QD file to API...\")\n",
    "response = post_QD('QD_upload.xlsx')\n",
    "\n",
    "if response.ok:\n",
    "    print(f\"✓ Upload succeeded (status: {response.status_code})\")\n",
    "else:\n",
    "    print(f\"❌ Upload failed (status: {response.status_code})\")\n",
    "    print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "310698a1-a266-492a-a5cb-5e861571a55c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Cart rules to update: 245 products across 9 cohorts\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PREPARE CART RULES UPDATE\n",
    "# =============================================================================\n",
    "\n",
    "# Merge current cart rules with new tier data\n",
    "cart_rules_update = live_cart_rules.merge(\n",
    "    final_data[['warehouse_id', 'product_id', 'packing_unit_id', 'tier_2_qty', 'ws_new_qty']],\n",
    "    on=['warehouse_id', 'product_id', 'packing_unit_id']\n",
    ")\n",
    "cart_rules_update = cart_rules_update.fillna(0)\n",
    "\n",
    "# New cart rule = max of tier_2_qty and ws_new_qty\n",
    "cart_rules_update['tier_2'] = np.maximum(cart_rules_update['tier_2_qty'], cart_rules_update['ws_new_qty'])\n",
    "\n",
    "# Only update rules that need to increase\n",
    "cart_rules_update = cart_rules_update[cart_rules_update['tier_2'] > cart_rules_update['current_cart_rule']]\n",
    "cart_rules_update = cart_rules_update[['cohort_id', 'product_id', 'packing_unit_id', 'tier_2']]\n",
    "cart_rules_update=cart_rules_update.groupby(['cohort_id', 'product_id', 'packing_unit_id'])['tier_2'].max().reset_index()\n",
    "print(f\"✓ Cart rules to update: {len(cart_rules_update)} products across {cart_rules_update['cohort_id'].nunique()} cohorts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4ba65fe4-ba6c-4d93-9d09-627bd3d3c683",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cohort_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>packing_unit_id</th>\n",
       "      <th>tier_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>700</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>700</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>700</td>\n",
       "      <td>336</td>\n",
       "      <td>15</td>\n",
       "      <td>113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>700</td>\n",
       "      <td>414</td>\n",
       "      <td>1</td>\n",
       "      <td>93.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>700</td>\n",
       "      <td>2878</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>1126</td>\n",
       "      <td>18964</td>\n",
       "      <td>1</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>1126</td>\n",
       "      <td>21712</td>\n",
       "      <td>3</td>\n",
       "      <td>1624.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>1126</td>\n",
       "      <td>21713</td>\n",
       "      <td>3</td>\n",
       "      <td>1312.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>1126</td>\n",
       "      <td>22125</td>\n",
       "      <td>2</td>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>1126</td>\n",
       "      <td>22565</td>\n",
       "      <td>3</td>\n",
       "      <td>685.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>245 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     cohort_id  product_id  packing_unit_id  tier_2\n",
       "0          700           9                1    37.0\n",
       "1          700          38                1     9.0\n",
       "2          700         336               15   113.0\n",
       "3          700         414                1    93.0\n",
       "4          700        2878                1     7.0\n",
       "..         ...         ...              ...     ...\n",
       "240       1126       18964                1    80.0\n",
       "241       1126       21712                3  1624.0\n",
       "242       1126       21713                3  1312.0\n",
       "243       1126       22125                2    73.0\n",
       "244       1126       22565                3   685.0\n",
       "\n",
       "[245 rows x 4 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cart_rules_update=cart_rules_update.drop_duplicates()\n",
    "cart_rules_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ee06fc22-b921-4fad-bcb6-9a7c44e66cf3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading cart rules by cohort...\n",
      "  ✓ Cohort 700: 13 rules uploaded\n",
      "  ✓ Cohort 701: 35 rules uploaded\n",
      "  ✓ Cohort 702: 26 rules uploaded\n",
      "  ✓ Cohort 703: 17 rules uploaded\n",
      "  ✓ Cohort 704: 44 rules uploaded\n",
      "  ✓ Cohort 1123: 26 rules uploaded\n",
      "  ✓ Cohort 1124: 27 rules uploaded\n",
      "  ✓ Cohort 1125: 27 rules uploaded\n",
      "  ✓ Cohort 1126: 30 rules uploaded\n",
      "\n",
      "✓ Cart rules upload complete!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# UPLOAD CART RULES BY COHORT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Uploading cart rules by cohort...\")\n",
    "\n",
    "for cohort in cart_rules_update.cohort_id.unique():\n",
    "    req_data = cart_rules_update[cart_rules_update['cohort_id'] == cohort]\n",
    "    \n",
    "    if len(req_data) > 0:\n",
    "        # Prepare data for upload\n",
    "        req_data = req_data[['product_id', 'packing_unit_id', 'tier_2']]\n",
    "        req_data.columns = ['Product ID', 'Packing Unit ID', 'Cart Rules']\n",
    "        \n",
    "        # Save and upload\n",
    "        filename = f'CartRules_{cohort}.xlsx'\n",
    "        req_data.to_excel(filename, index=False, engine='xlsxwriter')\n",
    "        \n",
    "        time.sleep(5)\n",
    "        response = post_cart_rules(cohort, filename)\n",
    "        \n",
    "        if response.ok:\n",
    "            print(f\"  ✓ Cohort {cohort}: {len(req_data)} rules uploaded\")\n",
    "        else:\n",
    "            print(f\"  ❌ Cohort {cohort}: Upload failed\")\n",
    "            print(response.content)\n",
    "            break\n",
    "\n",
    "print(\"\\n✓ Cart rules upload complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7e231d-6129-4c4f-a46b-72b9ba7c013e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41b6a91-48ec-4da2-bec9-b1913d8d50aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
