{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "353eed4b-fed6-4ddb-930f-0d1c581aaf2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Upgrade pip\n",
    "!pip install --upgrade pip\n",
    "# Connectivity\n",
    "!pip install psycopg2-binary  # PostgreSQL adapter\n",
    "# !pip install snowflake-connector-python  # Snowflake connector\n",
    "!pip install snowflake-connector-python==3.15.0 # Snowflake connector Older Version\n",
    "!pip install snowflake-sqlalchemy  # Snowflake SQLAlchemy connector\n",
    "!pip install warnings # Warnings management\n",
    "# !pip install pyarrow # Serialization\n",
    "!pip install keyring==23.11.0 # Key management\n",
    "!pip install sqlalchemy==1.4.46 # SQLAlchemy\n",
    "!pip install requests # HTTP requests\n",
    "!pip install boto3 # AWS SDK\n",
    "# !pip install slackclient # Slack API\n",
    "!pip install oauth2client # Google Sheets API\n",
    "!pip install gspread==5.9.0 # Google Sheets API\n",
    "!pip install gspread_dataframe # Google Sheets API\n",
    "!pip install google.cloud # Google Cloud\n",
    "# Data manipulation and analysis\n",
    "!pip install polars\n",
    "!pip install pandas==2.2.1\n",
    "!pip install numpy\n",
    "# !pip install fastparquet\n",
    "!pip install openpyxl # Excel file handling\n",
    "!pip install xlsxwriter # Excel file handling\n",
    "# Linear programming\n",
    "!pip install pulp\n",
    "# Date and time handling\n",
    "!pip install --upgrade datetime\n",
    "!pip install python-time\n",
    "!pip install --upgrade pytz\n",
    "# Progress bar\n",
    "!pip install tqdm\n",
    "# Database data types\n",
    "!pip install db-dtypes\n",
    "# Geospatial data handling\n",
    "# !pip install geopandas\n",
    "# !pip install shapely\n",
    "# !pip install fiona\n",
    "# !pip install haversine\n",
    "# Plotting\n",
    "\n",
    "# Modeling\n",
    "!pip install statsmodels\n",
    "!pip install scikit-learn\n",
    "\n",
    "!pip install import-ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64355078-99bc-436e-b5a9-69fbc4f983fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/snowflake/connector/options.py:104: UserWarning: You have an incompatible version of 'pyarrow' installed (21.0.0), please install a version that adheres to: 'pyarrow<19.0.0; extra == \"pandas\"'\n",
      "  warn_incompatible_dep(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "import json\n",
    "from datetime import date, timedelta\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import importlib\n",
    "import import_ipynb\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import pytz  \n",
    "import os\n",
    "import snowflake.connector\n",
    "import boto3\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import base64\n",
    "from botocore.exceptions import ClientError\n",
    "from requests import get\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from time import sleep  # Delays execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba0f4f0-4b39-4c98-8fdc-ff3ee7feb16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to download from S3 bucket\n",
    "class S3Downloader:\n",
    "    def __init__(self, bucket_name):\n",
    "        self.bucket_name = bucket_name\n",
    "\n",
    "    def download_file(self, key, destination):\n",
    "        \"\"\"Download a file from S3 and ensure the local directory exists.\"\"\"\n",
    "        s3 = boto3.resource('s3')\n",
    "\n",
    "        # Create parent directory if it does not exist\n",
    "        destination_dir = os.path.dirname(destination)\n",
    "        if destination_dir:\n",
    "            os.makedirs(destination_dir, exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "        try:\n",
    "            s3.Bucket(self.bucket_name).download_file(key, destination)\n",
    "            print(f\"Downloaded: {key} -> {destination}\")\n",
    "        except ClientError as e:\n",
    "            if e.response['Error']['Code'] == \"404\":\n",
    "                print(f\"Error: {key} does not exist in S3.\")\n",
    "            else:\n",
    "                raise\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"FileNotFoundError: {destination}. Ensure the path is correct.\")\n",
    "            raise e\n",
    "\n",
    "# Initializing downloader class\n",
    "downloader = S3Downloader('io.maxab.sagemaker1')\n",
    "\n",
    "# Class downloads\n",
    "dbdp_s3_path = 'automated-notebooks/amrmaali/DBDP_Revamped'\n",
    "downloader.download_file(f'{dbdp_s3_path}/setup_environment_2.py', 'setup_environment_2.py')\n",
    "\n",
    "# Class imports\n",
    "import setup_environment_2\n",
    "importlib.reload(setup_environment_2)\n",
    "setup_environment_2.initialize_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5cdc8b-89b2-4b8a-a04e-9e70a9a878f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_snowflake(query, columns=[]):\n",
    "    import os\n",
    "    import snowflake.connector\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    con = snowflake.connector.connect(\n",
    "        user =  os.environ[\"SNOWFLAKE_USERNAME\"],\n",
    "        account= os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "        password= os.environ[\"SNOWFLAKE_PASSWORD\"],\n",
    "        database =os.environ[\"SNOWFLAKE_DATABASE\"]\n",
    "    )\n",
    "    try:\n",
    "        cur = con.cursor()\n",
    "        cur.execute(\"USE WAREHOUSE COMPUTE_WH\")\n",
    "        cur.execute(query)\n",
    "        if len(columns) == 0:\n",
    "            out = pd.DataFrame(np.array(cur.fetchall()))\n",
    "        else:\n",
    "            out = pd.DataFrame(np.array(cur.fetchall()),columns=columns)\n",
    "        return out\n",
    "    except Exception as e:\n",
    "        print(\"Error: \", e)\n",
    "    finally:\n",
    "        cur.close()\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9cbaf2-d2e8-4a50-b572-d829fbb3c075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_secret(secret_name):\n",
    "    region_name = \"us-east-1\"\n",
    "\n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "    # In this sample we only handle the specific exceptions for the 'GetSecretValue' API.\n",
    "    # See https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html\n",
    "    # We rethrow the exception by default.\n",
    "\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(SecretId=secret_name)\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'DecryptionFailureException':\n",
    "            # Secrets Manager can't decrypt the protected secret text using the provided KMS key.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InternalServiceErrorException':\n",
    "            # An error occurred on the server side.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidParameterException':\n",
    "            # You provided an invalid value for a parameter.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidRequestException':\n",
    "            # You provided a parameter value that is not valid for the current state of the resource.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'ResourceNotFoundException':\n",
    "            # We can't find the resource that you asked for.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "    else:\n",
    "        # Decrypts secret using the associated KMS CMK.\n",
    "        # Depending on whether the secret is a string or binary, one of these fields will be populated.\n",
    "        if 'SecretString' in get_secret_value_response:\n",
    "            return get_secret_value_response['SecretString']\n",
    "        else:\n",
    "            return base64.b64decode(get_secret_value_response['SecretBinary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bdeb73-5ced-4489-b8cf-16463575c1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pricing_api_secret = json.loads(get_secret(\"prod/pricing/api/\"))\n",
    "username = pricing_api_secret[\"egypt_username\"]\n",
    "password = pricing_api_secret[\"egypt_password\"]\n",
    "secret = pricing_api_secret[\"egypt_secret\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3b34f2-3c22-4369-a435-a0e80c1c2215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_access_token(url, client_id, client_secret):\n",
    "    \"\"\"\n",
    "    get_access_token function takes three parameters and returns a session token\n",
    "    to connect to MaxAB APIs\n",
    "\n",
    "    :param url: production MaxAB token URL\n",
    "    :param client_id: client ID\n",
    "    :param client_secret: client sercret\n",
    "    :return: session token\n",
    "    \"\"\"\n",
    "    response = requests.post(\n",
    "        url,\n",
    "        data={\"grant_type\": \"password\",\n",
    "              \"username\": username,\n",
    "              \"password\": password},\n",
    "        auth=(client_id, client_secret),\n",
    "    )\n",
    "    return response.json()[\"access_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2d2de8-cae4-4464-9d3c-7ca089d1cdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_QD(file_name):\n",
    "    token = get_access_token('https://sso.maxab.info/auth/realms/maxab/protocol/openid-connect/token',\n",
    "                             'main-system-externals',\n",
    "                             secret)\n",
    "    url = \"https://api.maxab.info/commerce/api/admins/v1/quantity-discounts\"\n",
    "    payload={}\n",
    "    files=[\n",
    "      ('file',(file_name,open(file_name,'rb'),'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'))\n",
    "    ]\n",
    "    headers = {\n",
    "      'Authorization': 'bearer {}'.format(token)}\n",
    "\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload, files=files)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6127017-c919-4161-a45f-9034e2c2f412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writer_snowflake_query(query):\n",
    "    import os\n",
    "    import snowflake.connector\n",
    "\n",
    "    config = {\n",
    "        'user': os.environ[\"SNOWFLAKE_SERVICE_USERNAME\"],\n",
    "        'account': os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "        'private_key_file': '/tmp/sagemaker_service.p8',\n",
    "        'database': os.environ[\"SNOWFLAKE_DATABASE\"] ,\n",
    "        'role': os.environ[\"SNOWFLAKE_ROLE\"],\n",
    "        'schema': 'PUBLIC'\n",
    "        }\n",
    "\n",
    "    conn = snowflake.connector.connect(**config)\n",
    "\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(\"USE WAREHOUSE COMPUTE_WH\")\n",
    "        cur.execute(query)\n",
    "        return 1\n",
    "    except Exception as e:\n",
    "        print(\"Error: \", e)\n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "def pandas_dtype_to_snowflake(dtype):\n",
    "    \"\"\"Maps pandas/numpy dtype to Snowflake SQL data type.\"\"\"\n",
    "    if pd.api.types.is_integer_dtype(dtype):\n",
    "        return \"NUMBER,\"\n",
    "    elif pd.api.types.is_float_dtype(dtype):\n",
    "        return \"FLOAT,\"\n",
    "    elif pd.api.types.is_bool_dtype(dtype):\n",
    "        return \"BOOLEAN,\"\n",
    "    elif pd.api.types.is_datetime64_any_dtype(dtype):\n",
    "        return \"TIMESTAMP,\"\n",
    "    elif pd.api.types.is_object_dtype(dtype):\n",
    "        return \"TEXT,\"\n",
    "    elif pd.api.types.is_categorical_dtype(dtype):\n",
    "        return \"TEXT,\"\n",
    "    else:\n",
    "        return \"TEXT,\"  # fallback\n",
    "\n",
    "def dataframe_to_snowflake_columns(df, table_name):\n",
    "    \"\"\"Generates Snowflake-compatible column definitions from a DataFrame.\"\"\"\n",
    "    lines = [f'CREATE TABLE IF NOT EXISTS {table_name} (']\n",
    "    for col in df.columns:\n",
    "        snowflake_type = pandas_dtype_to_snowflake(df[col].dtype)\n",
    "        if col.lower() in ['group', 'section']:\n",
    "            col = f'\"{col}\"'\n",
    "        if col.lower() in ['start_date', 'end_date']:\n",
    "            lines.append(f'{col} TIMESTAMP,')\n",
    "        else:\n",
    "            lines.append(f'{col} {snowflake_type}')\n",
    "    return \"\\n\".join(lines)[:-1] + ');'\n",
    "\n",
    "# create tables if not exist\n",
    "def table_exist_test(df, table_name):\n",
    "    query_string = dataframe_to_snowflake_columns(df, table_name)\n",
    "    writer_snowflake_query(query_string)\n",
    "\n",
    "# Snowflake DB query to write into tables\n",
    "def eg_snowflake_writer(df, table, schema):\n",
    "    import os\n",
    "    import snowflake.connector\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from snowflake.connector.pandas_tools import write_pandas\n",
    "    import os\n",
    "\n",
    "    config = {\n",
    "        'user': os.environ[\"SNOWFLAKE_SERVICE_USERNAME\"],\n",
    "        'account': os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "        'private_key_file': '/tmp/sagemaker_service.p8',\n",
    "        'database': os.environ[\"SNOWFLAKE_DATABASE\"] ,\n",
    "        'role': os.environ[\"SNOWFLAKE_ROLE\"],\n",
    "        'schema': 'PUBLIC'\n",
    "    }\n",
    "    conn = snowflake.connector.connect(**config)\n",
    "    success, _, _, _ = write_pandas(conn=conn, df=df, table_name=table, schema=schema)\n",
    "    return success \n",
    "\n",
    "def DatabaseDump(df, path, erase='False'):\n",
    "    # initialize the standard command\n",
    "    command_string = f\"DELETE FROM {path} WHERE True\"\n",
    "    print_string = f\"Succesfuly Removed & Re-Added Data \\n for {path}\"\n",
    "\n",
    "    # if the flag is 'false', erase latest push (day / time_slot)\n",
    "    if erase.lower() == 'false':\n",
    "        if \"start_date\" in df.columns.str.lower():\n",
    "            date_value = main_df.start_date.values[0]\n",
    "            print(date_value)\n",
    "            command_string += f\" AND start_date = TO_TIMESTAMP('{date_value}')\"\n",
    "            print(command_string)\n",
    "            print_string += f\"\\ndate = {df['start_date'].values[0]}\"\n",
    "\n",
    "        if \"cohort_id\" in df.columns.str.lower():\n",
    "            command_string += f\" AND cohort_id IN {tuple(df.cohort_id.unique())}\"\n",
    "            print_string += f\"\\nCohort IDs IN {tuple(df.cohort_id.unique())}\"\n",
    "\n",
    "    # if the flag is 'month', erase current month\n",
    "    if erase.lower() == 'month':\n",
    "        if \"created_at\" in df.columns.str.lower():\n",
    "            command_string += f\" AND DATE_TRUNC('month', created_at) = DATE_TRUNC('month', SYSDATE())\"\n",
    "            print_string += f\"\\ndate = Current month\"\n",
    "        if \"date\" in df.columns.str.lower():\n",
    "            command_string += f\" AND DATE_TRUNC('month', date) = DATE_TRUNC('month', SYSDATE())\"\n",
    "            print_string += f\"\\ndate = Current month\"\n",
    "\n",
    "    # Remove data of the same day, time_slot, etc...\n",
    "    writer_snowflake_query(command_string)\n",
    "\n",
    "    # Push the new data to the table\n",
    "    df.columns = df.columns.str.upper()\n",
    "    eg_snowflake_writer(df, path.split('.')[1].upper(), path.split('.')[0].upper())\n",
    "    print(print_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582293c4-f7de-4a77-937a-690d4d54bae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_cart_rules(id_,file_name):\n",
    "    token = get_access_token('https://sso.maxab.info/auth/realms/maxab/protocol/openid-connect/token',\n",
    "                             'main-system-externals',\n",
    "                             secret)\n",
    "    url = \"https://api.maxab.info/main-system/api/admin-portal/cohorts/{}/cart-rules\".format(id_)\n",
    "    payload={}\n",
    "    files=[\n",
    "      ('sheet',(file_name,open(file_name,'rb'),'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'))\n",
    "    ]\n",
    "    headers = {\n",
    "      'Authorization': 'bearer {}'.format(token)}\n",
    "\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload, files=files)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aebb18a-ebb3-459a-85ac-e09151ae676a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scope = [\"https://spreadsheets.google.com/feeds\",\n",
    "         'https://www.googleapis.com/auth/spreadsheets',\n",
    "         \"https://www.googleapis.com/auth/drive.file\",\n",
    "         \"https://www.googleapis.com/auth/drive\"]\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_dict(json.loads(setup_environment_2.get_secret(\"prod/maxab-sheets\")), scope)\n",
    "client = gspread.authorize(creds)\n",
    "force_brands = client.open('QD_brands').worksheet('Sheet1')\n",
    "force_brands_df = pd.DataFrame(force_brands.get_all_records())\n",
    "if(force_brands_df.empty):\n",
    "    force_brands_df = pd.DataFrame(columns=['brand'])\n",
    "    brand_filter = \"\"\n",
    "else:\n",
    "    brand_filter = f\"OR brand IN ({','.join([repr(b) for b in list(force_brands_df.brand.unique())])})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a02939-f4ba-445d-bc88-940f233ae3a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if datetime.now(pytz.timezone('Africa/Cairo')).hour >= 13:\n",
    "    query = '''\n",
    "    select cppu.cohort_id,product_id,packing_unit_id,COALESCE(cppu.MAX_PER_SALES_ORDER,cppu2.MAX_PER_SALES_ORDER) as current_cart_rule\n",
    "    from COHORT_PRODUCT_PACKING_UNITS cppu \n",
    "    join PACKING_UNIT_PRODUCTS pup on cppu.PRODUCT_PACKING_UNIT_ID = pup.id \n",
    "    join cohorts c on c.id = cppu.cohort_id\n",
    "    join COHORT_PRODUCT_PACKING_UNITS cppu2 on cppu.PRODUCT_PACKING_UNIT_ID = cppu2.PRODUCT_PACKING_UNIT_ID and cppu2.cohort_id = c.FALLBACK_COHORT_ID \n",
    "    where cppu.cohort_id in (700,701,702,703,704,1123,1124,1125,1126)\n",
    "\n",
    "    '''\n",
    "    live_cart_rules = query_snowflake(query, columns = ['cohort_id','product_id','packing_unit_id','current_cart_rule']) \n",
    "    live_cart_rules.cohort_id=pd.to_numeric(live_cart_rules.cohort_id)\n",
    "    live_cart_rules.product_id=pd.to_numeric(live_cart_rules.product_id)\n",
    "    live_cart_rules.packing_unit_id=pd.to_numeric(live_cart_rules.packing_unit_id)\n",
    "    live_cart_rules.current_cart_rule=pd.to_numeric(live_cart_rules.current_cart_rule)\n",
    "    \n",
    "    command_string =  f'''\n",
    "with rr as (\n",
    "\n",
    "select product_id,warehouse_id,rr\n",
    "from (\n",
    "select * ,max(date)over(partition by product_id,warehouse_id) as max_date\n",
    "from finance.PREDICTED_RUNNING_RATES\n",
    "qualify date = max_date\n",
    "and date::date >= current_date - 14 \n",
    ")\n",
    "\n",
    "),\n",
    "stocks as (\n",
    "WITH whs as (SELECT *\n",
    "             FROM   (values\n",
    "                            ('Cairo', 'El-Marg', 38,700),\n",
    "                            ('Cairo', 'Mostorod', 1,700),\n",
    "                            ('Giza', 'Barageel', 236,701),\n",
    "                            ('Delta West', 'El-Mahala', 337,703),\n",
    "                            ('Delta West', 'Tanta', 8,703),\n",
    "                            ('Delta East', 'Mansoura FC', 339,704),\n",
    "                            ('Delta East', 'Sharqya', 170,704),\n",
    "                            ('Upper Egypt', 'Assiut FC', 501,1124),\n",
    "                            ('Upper Egypt', 'Bani sweif', 401,1126),\n",
    "                            ('Upper Egypt', 'Menya Samalot', 703,1123),\n",
    "                            ('Upper Egypt', 'Sohag', 632,1125),\n",
    "                            ('Alexandria', 'Khorshed Alex', 797,702),\n",
    "\t\t\t\t\t\t\t('Giza', 'Sakkarah', 962,701)\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t)\n",
    "                    x(region, wh, warehouse_id,cohort_id))\n",
    "select cohort_id,product_id,sum(stocks) as stocks ,case when sum(rr) > 0 then SUM(stocks)/sum(rr) else SUM(stocks) end  as doh\n",
    "from (\n",
    "\t\tSELECT DISTINCT whs.region,\n",
    "\t\t\t\tcohort_id,\t\n",
    "                whs.wh,\n",
    "                product_warehouse.product_id,\n",
    "                (product_warehouse.available_stock)::integer as stocks,\n",
    "\t\t\t\tcoalesce(rr.rr,0) as rr \n",
    "        from whs\n",
    "        JOIN product_warehouse ON product_warehouse.warehouse_id = whs.warehouse_id\n",
    "        JOIN products on product_warehouse.product_id = products.id\n",
    "        JOIN product_units ON products.unit_id = product_units.id\n",
    "\t\tleft join rr on rr.product_id= products.id and rr.warehouse_id = whs.warehouse_id\n",
    "\n",
    "        where   product_warehouse.warehouse_id not in (6,9,10)\n",
    "            AND product_warehouse.is_basic_unit = 1\n",
    "\t\t\tand product_warehouse.available_stock > 0 \n",
    "\n",
    ")\n",
    "group by all\n",
    "HAVING doh > 1 \n",
    "),\n",
    "base as (\n",
    "select *, row_number()over(partition by retailer_id order by priority) as rnk \n",
    "from (\n",
    "select x.*,TAGGABLE_ID as retailer_id \n",
    "from (\n",
    "select id as cohort_id,name as cohort_name,priority,dynamic_tag_id \n",
    "from cohorts \n",
    "where is_active = 'true'\n",
    "and id in (700,701,702,703,704,1123,1124,1125,1126)\n",
    ") x \n",
    "join DYNAMIC_TAGgables dt on x.dynamic_tag_id = dt.dynamic_tag_id\n",
    ")\n",
    "qualify rnk = 1 \n",
    "order by cohort_id\n",
    "),\n",
    "selected_skus as (\n",
    "select *\n",
    "from (\n",
    "select cohort_id,cohort_name,product_id,cat,brand,row_number()over(partition by cohort_id,cat order by cntrb) as num_skus\n",
    "from (\n",
    "select *,min(case when cumulative_sum > 0.4 then cumulative_sum end) over(partition by cat, cohort_id) as thres\n",
    "from (\n",
    "select *,SUM(cntrb) OVER (partition by cat, cohort_id ORDER BY cntrb desc ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum\n",
    "from (\n",
    "select *, num_order/sum(num_order)over(partition by cat,cohort_id) as cntrb\n",
    "from (\n",
    "SELECT  DISTINCT\n",
    "\t\tbase.cohort_id,\n",
    "\t\tbase.cohort_name,\n",
    "\t\tpso.product_id,\n",
    "\t\tCONCAT(products.name_ar,' ',products.size,' ',product_units.name_ar) as sku,\n",
    "\t\tbrands.name_ar as brand, \n",
    "\t\tcategories.name_ar as cat,\n",
    "\t\tcount(distinct parent_sales_order_id) as num_order ,\n",
    "        sum(pso.total_price) as nmv,\n",
    "       sum(COALESCE(f.wac_p,0) * pso.purchased_item_count * pso.basic_unit_count) as cogs,\n",
    "\t   (nmv-cogs)/nmv as bm \n",
    "\t\t\n",
    "\n",
    "FROM product_sales_order pso\n",
    "JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "join base on base.retailer_id = so.retailer_id\n",
    "JOIN products on products.id=pso.product_id\n",
    "JOIN brands on products.brand_id = brands.id\n",
    "JOIN categories ON products.category_id = categories.id and categories.name_ar not like '%سايب%'\n",
    "JOIN finance.all_cogs f  ON f.product_id = pso.product_id\n",
    "                        AND f.from_date::date <= so.created_at::date\n",
    "                        AND f.to_date::date > so.created_at::date\n",
    "JOIN product_units ON product_units.id = products.unit_id \n",
    "join stocks s on s.product_id = pso.product_id and s.cohort_id = base.cohort_id\n",
    "\n",
    "WHERE   True\n",
    "    AND so.created_at::date between date_trunc('month',current_date - interval '2 months') and CURRENT_date-1\n",
    "    AND so.sales_order_status_id not in (7,12)\n",
    "    AND so.channel IN ('telesales','retailer')\n",
    "    AND pso.purchased_item_count <> 0\n",
    "\n",
    "GROUP BY ALL\n",
    ")\n",
    ")\n",
    ")\n",
    ")\n",
    "where cumulative_sum <= thres \n",
    "{brand_filter}\n",
    ")\n",
    "where  num_skus <= 10 \n",
    "{brand_filter}\n",
    "),\n",
    "\n",
    "main as (\n",
    "select * ,max(rets) over(partition by cohort_id) as max_rets \n",
    "from(\n",
    "select *,count(distinct retailer_id) over(partition by region,cohort_id) as rets \n",
    "from (\n",
    "SELECT  DISTINCT\n",
    "\t\tso.created_at::date as date,\n",
    "\t\tparent_sales_order_id,\n",
    "\t\tso.retailer_id,\n",
    "\t\tbase.cohort_id,\n",
    "\t\tbase.cohort_name,\n",
    "\t\tcase when regions.id = 2 then states.name_en else regions.name_en end as region,\n",
    "\t\tpso.product_id,\n",
    "\t\tCONCAT(products.name_ar,' ',products.size,' ',product_units.name_ar) as sku,\n",
    "\t\tpacking_unit_id,\n",
    "\t\tbrands.name_ar as brand, \n",
    "\t\tcategories.name_ar as cat,\n",
    "\t\tsum(pso.purchased_item_count) as qty,\n",
    "        sum(pso.total_price) as nmv,\n",
    "       sum(COALESCE(f.wac_p,0) * pso.purchased_item_count * pso.basic_unit_count) as cogs,\n",
    "\t   (nmv-cogs)/nmv as bm \n",
    "\t\t\n",
    "\n",
    "FROM product_sales_order pso\n",
    "JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "join base on base.retailer_id = so.retailer_id\n",
    "JOIN products on products.id=pso.product_id\n",
    "JOIN brands on products.brand_id = brands.id\n",
    "JOIN categories ON products.category_id = categories.id\n",
    "JOIN finance.all_cogs f  ON f.product_id = pso.product_id\n",
    "                        AND f.from_date::date <= so.created_at::date\n",
    "                        AND f.to_date::date > so.created_at::date\n",
    "JOIN product_units ON product_units.id = products.unit_id \n",
    "JOIN materialized_views.retailer_polygon on materialized_views.retailer_polygon.retailer_id=so.retailer_id\n",
    "JOIN districts on districts.id=materialized_views.retailer_polygon.district_id\n",
    "JOIN cities on cities.id=districts.city_id\n",
    "join states on states.id=cities.state_id\n",
    "join regions on regions.id=states.region_id\n",
    "join selected_skus ss on ss.product_id = pso.product_id and ss.cohort_id = base.cohort_id\n",
    "\n",
    "WHERE   True\n",
    "    AND so.created_at::date between date_trunc('month',current_date - interval '2 months') and CURRENT_date-1\n",
    "    AND so.sales_order_status_id not in (7,12)\n",
    "    AND so.channel IN ('telesales','retailer')\n",
    "    AND pso.purchased_item_count <> 0\n",
    "\n",
    "GROUP BY ALL\n",
    ")\n",
    ")\n",
    "qualify rets = max_rets\n",
    "),\n",
    "cohort_data as (\n",
    "select region,cohort_id,cohort_name,product_id,sku,brand,cat,packing_unit_id,\n",
    "PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY qty) AS region_q1,\n",
    "MEDIAN(qty) as region_median,\n",
    "PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY qty) AS region_q3,\n",
    "PERCENTILE_CONT(0.85) WITHIN GROUP (ORDER BY qty) AS region_85,\n",
    "STDDEV_POP(qty) as std\n",
    "from main\n",
    "group by all \n",
    "),\n",
    "recent_cohort_data as (\n",
    "select cohort_id,cohort_name,product_id,sku,brand,cat,packing_unit_id,\n",
    "MEDIAN(qty) as recent_region_median,\n",
    "PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY qty) AS recent_region_q3,\n",
    "STDDEV_POP(qty) as recent_std\n",
    "from main\n",
    "where date between current_date - 8 and current_date - 1 \n",
    "group by all \n",
    "),\n",
    " freq_table AS (\n",
    "  SELECT\n",
    "  \t cohort_id,cohort_name,\n",
    "    PRODUCT_ID,sku,brand,cat,\n",
    "\tpacking_unit_id,\n",
    "    qty,\n",
    "    COUNT(distinct parent_sales_order_id) AS freq\n",
    "  FROM main\n",
    "  GROUP BY all\n",
    "),\n",
    "lag_lead AS (\n",
    "  SELECT\n",
    "   cohort_id,cohort_name,\n",
    "    PRODUCT_ID,sku,brand,cat,\n",
    "\tpacking_unit_id,\n",
    "    qty,\n",
    "    freq,\n",
    "    LAG(freq) OVER (PARTITION BY cohort_id,PRODUCT_ID,packing_unit_id ORDER BY qty) AS prev_freq,\n",
    "    LEAD(freq) OVER (PARTITION BY cohort_id,PRODUCT_ID,packing_unit_id ORDER BY qty) AS next_freq\n",
    "  FROM freq_table\n",
    "),\n",
    "most_freq as (\n",
    "select * \n",
    "from (\n",
    "select *,max(cntrb)over(partition by product_id,packing_unit_id,cohort_id) as max_cntrb\n",
    "from (\n",
    "SELECT *, freq/sum(freq) over(partition by product_id,packing_unit_id,cohort_id) as cntrb\n",
    "FROM lag_lead ll \n",
    "WHERE (freq > COALESCE(prev_freq, -1))\n",
    "  AND (freq > COALESCE(next_freq, -1))\n",
    "  )\n",
    "  )\n",
    "  where cntrb >= max_cntrb- 0.05\n",
    "  order by product_id\n",
    "),\n",
    "most_qty as (\n",
    "select cohort_id,cohort_name,product_id,sku,cat,brand,packing_unit_id,ceil(sum(freq_cntrb*qty)) as final_qty \n",
    "from (\n",
    "select *,freq/sum(freq)over(partition by  product_id,packing_unit_id,cohort_id) as freq_cntrb\n",
    "from most_freq \n",
    ")\n",
    "group by all \n",
    "),\n",
    "final_data as (\n",
    "select *,\n",
    "ceil(\n",
    "least(\n",
    "GREATEST(\n",
    "      recent_region_median + 0.75 * recent_std,\n",
    "      final_qty,\n",
    "\t  region_median+0.75*std,\n",
    "\t  region_median+2,\n",
    "\t  2\n",
    "    ),\n",
    "\tGREATEST(region_median+2,region_median*1.5)\n",
    "\t)\n",
    "\t\n",
    "\t) as tier_1,\n",
    " ceil(\n",
    " least(\n",
    " GREATEST(\n",
    "      final_qty + 1 * std,\n",
    "      region_q3 + 1 * std,\n",
    "\t  region_85 + 0.5 * std,\n",
    "      recent_region_q3 + 1 * recent_std,\n",
    "\t  tier_1*1.4\n",
    "    ),\n",
    "\ttier_1*3\n",
    "\t)\n",
    "\t) as tier_2\n",
    "from (\n",
    "select  rd.region,mq.*,region_q1,\n",
    "region_median,\n",
    "region_q3,\n",
    "region_85,\n",
    "std,\n",
    "COALESCE(recent_region_median,0) as recent_region_median,\n",
    "COALESCE(recent_region_q3,0) as recent_region_q3,\n",
    "COALESCE(recent_std,0) as recent_std\n",
    "from cohort_data rd \n",
    "join most_qty mq on rd.cohort_id =mq.cohort_id\n",
    "and rd.product_id =  mq.product_id\n",
    "and rd.packing_unit_id = mq.packing_unit_id \n",
    "left join recent_cohort_data rrd on rrd.cohort_id =mq.cohort_id\n",
    "and rrd.product_id =  mq.product_id\n",
    "and rrd.packing_unit_id = mq.packing_unit_id \n",
    ")\n",
    "),\n",
    "local_prices as (\n",
    "SELECT  case when cpu.cohort_id in (700,695) then 'Cairo'\n",
    "             when cpu.cohort_id in (701) then 'Giza'\n",
    "             when cpu.cohort_id in (704,698) then 'Delta East'\n",
    "             when cpu.cohort_id in (703,697) then 'Delta West'\n",
    "             when cpu.cohort_id in (696,1123,1124,1125,1126) then 'Upper Egypt'\n",
    "             when cpu.cohort_id in (702,699) then 'Alexandria'\n",
    "        end as region,\n",
    "\t\tcohort_id,\n",
    "        pu.product_id,\n",
    "\t\tpu.packing_unit_id as packing_unit_id,\n",
    "\t\tpu.basic_unit_count,\n",
    "        avg(cpu.price) as price\n",
    "FROM    cohort_product_packing_units cpu\n",
    "join    PACKING_UNIT_PRODUCTS pu on pu.id = cpu.product_packing_unit_id\n",
    "WHERE   cpu.cohort_id in (700,701,702,703,704,696,695,698,697,699,1123,1124,1125,1126)\n",
    "    and cpu.created_at::date<>'2023-07-31'\n",
    "    and cpu.is_customized = true\n",
    "\tgroup by all \n",
    "),\n",
    "live_prices as (\n",
    "select region,cohort_id,product_id,pu_id as packing_unit_id,buc as basic_unit_count,NEW_PRICE as price\n",
    "from materialized_views.DBDP_PRICES\n",
    "where created_at = current_date\n",
    "and DATE_PART('hour',CURRENT_TIME) BETWEEN SPLIT_PART(time_slot, '-', 1)::int AND SPLIT_PART(time_slot, '-', 2)::int\n",
    "and cohort_id in (700,701,702,703,704,696,695,698,697,699,1123,1124,1125,1126)\n",
    "),\n",
    "prices as (\n",
    "select *\n",
    "from (\n",
    "    SELECT *, 1 AS priority FROM live_prices\n",
    "    UNION ALL\n",
    "    SELECT *, 2 AS priority FROM local_prices\n",
    ")\n",
    "QUALIFY ROW_NUMBER() OVER (PARTITION BY region,cohort_id,product_id,packing_unit_id ORDER BY priority) = 1\n",
    "),\n",
    "region_prices as (\n",
    "select region,product_id,packing_unit_id,basic_unit_count,avg(price) as region_price\n",
    "from prices \n",
    "where price is not null \n",
    "group by all\n",
    "\n",
    "),\n",
    "finalized as (\n",
    "select fd.*,COALESCE(p.basic_unit_count,rp.basic_unit_count) as basic_unit_count,COALESCE(p.price,rp.region_price) as price\n",
    "from final_data fd \n",
    "left join prices p on fd.cohort_id = p.cohort_id and p.product_id = fd.product_id and p.packing_unit_id = fd.packing_unit_id\n",
    "left join region_prices rp on case when fd.region = 'Giza' then 'Cairo' else fd.region end = rp.region and rp.product_id = fd.product_id and rp.packing_unit_id = fd.packing_unit_id\n",
    "),\n",
    "cntrbs as (\n",
    "select main.product_id,main.cohort_id,main.packing_unit_id,\n",
    "count(distinct case when qty < tier_1 then retailer_id end ) as ret_below_t1,\n",
    "count(distinct case when qty >= tier_1  and qty <tier_2 then retailer_id end ) as ret_t1,\n",
    "count(distinct case when qty >= tier_2  then retailer_id end )as ret_t2 \n",
    "from main\n",
    "join finalized f on main.product_id = f.product_id and main.cohort_id = f.cohort_id and f.packing_unit_id = main.packing_unit_id \n",
    "group by all \n",
    ")\n",
    "select f.region,f.cohort_id,f.cohort_name,f.product_id,f.sku,f.cat,f.brand,f.packing_unit_id,TIER_1,tier_2,price, c.ret_below_t1,ret_t1,ret_t2,wac_p*BASIC_UNIT_COUNT as wac,f.region_median,f.RECENT_REGION_MEDIAN\n",
    "from finalized f\n",
    "join cntrbs c on c.product_id = f.product_id and c.cohort_id = f.cohort_id and f.packing_unit_id = c.packing_unit_id\n",
    "join finance.all_cogs cogs on cogs.product_id = f.product_id and CURRENT_TIMESTAMP between cogs.from_date and cogs.to_date\n",
    "where price is not null\n",
    "'''\n",
    "    quantity_disc_data = query_snowflake(command_string, columns = ['REGION','COHORT_ID','COHORT_NAME','PRODUCT_ID','SKU','CAT','BRAND','PACKING_UNIT_ID','TIER_1','TIER_2','PRICE','RET_BELOW_T1','RET_T1','RET_T2','WAC_P','region_median','RECENT_REGION_MEDIAN'])\n",
    "    quantity_disc_data.columns = quantity_disc_data.columns.str.lower()\n",
    "    quantity_disc_data.product_id = pd.to_numeric(quantity_disc_data.product_id)\n",
    "    quantity_disc_data.packing_unit_id = pd.to_numeric(quantity_disc_data.packing_unit_id)\n",
    "    quantity_disc_data.tier_1 = pd.to_numeric(quantity_disc_data.tier_1)\n",
    "    quantity_disc_data.tier_2 = pd.to_numeric(quantity_disc_data.tier_2)\n",
    "\n",
    "    quantity_disc_data.price = pd.to_numeric(quantity_disc_data.price)\n",
    "    quantity_disc_data.wac_p = pd.to_numeric(quantity_disc_data.wac_p)\n",
    "    quantity_disc_data.ret_below_t1 = pd.to_numeric(quantity_disc_data.ret_below_t1)\n",
    "\n",
    "    quantity_disc_data.ret_t1 = pd.to_numeric(quantity_disc_data.ret_t1)\n",
    "    quantity_disc_data.ret_t2 = pd.to_numeric(quantity_disc_data.ret_t2)\n",
    "\n",
    "    quantity_disc_data.region_median = pd.to_numeric(quantity_disc_data.region_median)\n",
    "    quantity_disc_data.recent_region_median = pd.to_numeric(quantity_disc_data.recent_region_median)\n",
    "\n",
    "\n",
    "    quantity_disc_data = quantity_disc_data[~quantity_disc_data['cat'].isin(['كروت شحن','مياه معدنيه','مقرمشات','شيبسي'])]\n",
    "    quantity_disc_data = quantity_disc_data[~quantity_disc_data['brand'].isin(['فيوري'])]\n",
    "    quantity_disc_data['bm'] = (quantity_disc_data['price']-quantity_disc_data['wac_p']) / quantity_disc_data['price']\n",
    "\n",
    "\n",
    "\n",
    "    query = '''\n",
    "\n",
    "    with base as (\n",
    "    select *, row_number()over(partition by retailer_id order by priority) as rnk \n",
    "    from (\n",
    "    select x.*,TAGGABLE_ID as retailer_id \n",
    "    from (\n",
    "    select id as cohort_id,name as cohort_name,priority,dynamic_tag_id \n",
    "    from cohorts \n",
    "    where is_active = 'true'\n",
    "    and id in (700,701,702,703,704,1123,1124,1125,1126)\n",
    "    ) x \n",
    "    join DYNAMIC_TAGgables dt on x.dynamic_tag_id = dt.dynamic_tag_id\n",
    "    )\n",
    "    qualify rnk = 1 \n",
    "    order by cohort_id\n",
    "    )\n",
    "\n",
    "    SELECT  DISTINCT\n",
    "            base.cohort_id,\n",
    "            pso.product_id,\n",
    "            pso.packing_unit_id,\n",
    "            sum(pso.total_price) as nmv\n",
    "\n",
    "    FROM product_sales_order pso\n",
    "    JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "    JOIN products on products.id=pso.product_id\n",
    "    JOIN brands on products.brand_id = brands.id\n",
    "    JOIN categories ON products.category_id = categories.id and categories.name_ar not like '%سايب%'\n",
    "    JOIN product_units ON product_units.id = products.unit_id \n",
    "    JOIN materialized_views.retailer_polygon on materialized_views.retailer_polygon.retailer_id=so.retailer_id\n",
    "    JOIN districts on districts.id=materialized_views.retailer_polygon.district_id\n",
    "    JOIN cities on cities.id=districts.city_id\n",
    "    join states on states.id=cities.state_id\n",
    "    join regions on regions.id=states.region_id\n",
    "    join base on base.retailer_id = so.retailer_id\n",
    "\n",
    "    WHERE   so.created_at ::date between date_trunc('month',current_date - interval '2 months') and current_date -1 \n",
    "        AND so.sales_order_status_id not in (7,12)\n",
    "        AND so.channel IN ('telesales','retailer')\n",
    "        AND pso.purchased_item_count <> 0\n",
    "\n",
    "    GROUP BY ALL\n",
    "    '''\n",
    "    sales  = query_snowflake(query, columns = ['cohort_id','product_id','packing_unit_id','total_sales'])\n",
    "    sales.product_id = pd.to_numeric(sales.product_id)\n",
    "    sales.cohort_id = pd.to_numeric(sales.cohort_id)\n",
    "    sales.packing_unit_id = pd.to_numeric(sales.packing_unit_id)\n",
    "    sales.total_sales = pd.to_numeric(sales.total_sales)\n",
    "\n",
    "\n",
    "\n",
    "    query = '''\n",
    "    with base as (\n",
    "    select *, row_number()over(partition by retailer_id order by priority) as rnk \n",
    "    from (\n",
    "    select x.*,TAGGABLE_ID as retailer_id \n",
    "    from (\n",
    "    select id as cohort_id,name as cohort_name,priority,dynamic_tag_id \n",
    "    from cohorts \n",
    "    where is_active = 'true'\n",
    "    and id in (700,701,702,703,704,1123,1124,1125,1126)\n",
    "    ) x \n",
    "    join DYNAMIC_TAGgables dt on x.dynamic_tag_id = dt.dynamic_tag_id\n",
    "    )\n",
    "    qualify rnk = 1 \n",
    "    order by cohort_id\n",
    "    )\n",
    "    select region,cohort_id,product_id,packing_unit_id,\n",
    "    avg(num_retailers) as daily_avg_retailers,\n",
    "    STDDEV(num_retailers)  as std \n",
    "    from (\n",
    "    select * \n",
    "    from (\n",
    "    select *, \n",
    "    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY num_retailers) over(partition by product_id,cohort_id,packing_unit_id)AS q1,\n",
    "    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY num_retailers) over(partition by product_id,cohort_id,packing_unit_id)AS median,\n",
    "    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY num_retailers) over(partition by product_id,cohort_id,packing_unit_id)AS q3,\n",
    "    STDDEV(num_retailers) over(partition by product_id,cohort_id,packing_unit_id) as std,\n",
    "    q3-q1 as iqr\n",
    "    from (\n",
    "    select * , dense_rank() over(partition by date,product_id,cohort_id,packing_unit_id order by num_retailers desc ) as rnk \n",
    "    from (\n",
    "    select date,region,cohort_id,product_id,packing_unit_id,count(distinct retailer_id) as num_retailers\n",
    "    from (\n",
    "    SELECT  DISTINCT\n",
    "            so.created_at::date as date,\n",
    "            parent_sales_order_id,\n",
    "            so.retailer_id,\n",
    "            base.cohort_id,\n",
    "            base.cohort_name,\n",
    "            case when regions.id = 2 then states.name_en else regions.name_en end as region,\n",
    "            pso.product_id,\n",
    "            CONCAT(products.name_ar,' ',products.size,' ',product_units.name_ar) as sku,\n",
    "            packing_unit_id,\n",
    "            brands.name_ar as brand, \n",
    "            categories.name_ar as cat,\n",
    "            sum(pso.purchased_item_count) as qty,\n",
    "            sum(pso.total_price) as nmv,\n",
    "           sum(COALESCE(f.wac_p,0) * pso.purchased_item_count * pso.basic_unit_count) as cogs,\n",
    "           (nmv-cogs)/nmv as bm \n",
    "\n",
    "\n",
    "    FROM product_sales_order pso\n",
    "    JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "    join base on base.retailer_id = so.retailer_id\n",
    "    JOIN products on products.id=pso.product_id\n",
    "    JOIN brands on products.brand_id = brands.id\n",
    "    JOIN categories ON products.category_id = categories.id\n",
    "    JOIN finance.all_cogs f  ON f.product_id = pso.product_id\n",
    "                            AND f.from_date::date <= so.created_at::date\n",
    "                            AND f.to_date::date > so.created_at::date\n",
    "    JOIN product_units ON product_units.id = products.unit_id \n",
    "    JOIN materialized_views.retailer_polygon on materialized_views.retailer_polygon.retailer_id=so.retailer_id\n",
    "    JOIN districts on districts.id=materialized_views.retailer_polygon.district_id\n",
    "    JOIN cities on cities.id=districts.city_id\n",
    "    join states on states.id=cities.state_id\n",
    "    join regions on regions.id=states.region_id\n",
    "\n",
    "    WHERE   True\n",
    "        AND so.created_at::date between current_date -30 and  CURRENT_date-1\n",
    "        AND so.sales_order_status_id not in (7,12)\n",
    "        AND so.channel IN ('telesales','retailer')\n",
    "        AND pso.purchased_item_count <> 0\n",
    "    GROUP BY ALL\n",
    "    )\n",
    "    group by all \n",
    "    )\n",
    "    qualify rnk = 1 \n",
    "    )\n",
    "    )\n",
    "    WHERE \n",
    "    num_retailers >= q1-(1.2*iqr)\n",
    "    and num_retailers <= q3+(1.2*iqr)\n",
    "    and num_retailers between median and median+std\n",
    "    )\n",
    "    group by all\n",
    "    order by daily_avg_retailers desc\n",
    "    '''\n",
    "    avg_daily  = query_snowflake(query, columns = ['region','cohort_id','product_id','packing_unit_id','daily_avg_retailers','daily_std'])\n",
    "    avg_daily.product_id = pd.to_numeric(avg_daily.product_id)\n",
    "    avg_daily.cohort_id = pd.to_numeric(avg_daily.cohort_id)\n",
    "    avg_daily.packing_unit_id = pd.to_numeric(avg_daily.packing_unit_id)\n",
    "    avg_daily.daily_avg_retailers = pd.to_numeric(avg_daily.daily_avg_retailers)\n",
    "    avg_daily['daily_std'] = pd.to_numeric(avg_daily['daily_std'])\n",
    "\n",
    "\n",
    "\n",
    "    query = '''\n",
    "    SELECT DISTINCT cat, brand, margin as target_bm\n",
    "    FROM    performance.commercial_targets cplan\n",
    "    QUALIFY CASE WHEN DATE_TRUNC('month', MAX(DATE)OVER()) = DATE_TRUNC('month', CURRENT_DATE) THEN DATE_TRUNC('month', CURRENT_DATE)\n",
    "    ELSE DATE_TRUNC('month', CURRENT_DATE - INTERVAL '1 month') END = DATE_TRUNC('month', date)\n",
    "    '''\n",
    "    target_margin = query_snowflake(query, columns = ['cat','brand','target_margin']) \n",
    "    target_margin.target_margin=pd.to_numeric(target_margin.target_margin)\n",
    "\n",
    "\n",
    "\n",
    "    #changed\n",
    "    quantity_disc_data=quantity_disc_data[~quantity_disc_data['wac_p'].isna()]\n",
    "    quantity_disc_data=quantity_disc_data[quantity_disc_data['bm']>0]\n",
    "    quantity_disc_data = quantity_disc_data.merge(target_margin,on=['cat','brand'],how='left')\n",
    "    quantity_disc_data['min'] = quantity_disc_data['target_margin'] * 0.85 \n",
    "    quantity_disc_data['min'] =quantity_disc_data['min'].fillna(0.02)\n",
    "    quantity_disc_data=quantity_disc_data[((quantity_disc_data['bm'] >= quantity_disc_data['min'])&(quantity_disc_data['cat']!= 'حاجه ساقعه'))|((quantity_disc_data['bm'] > 0)&(quantity_disc_data['cat']== 'حاجه ساقعه')) ] \n",
    "\n",
    "   #changed\n",
    "    main_df = quantity_disc_data.copy()\n",
    "    main_df['t0_perc'] = main_df['ret_below_t1']/(main_df['ret_below_t1']+main_df['ret_t1']+main_df['ret_t2'])\n",
    "    main_df['t1_perc'] = main_df['ret_t1']/(main_df['ret_below_t1']+main_df['ret_t1']+main_df['ret_t2'])\n",
    "    main_df['t2_perc'] = main_df['ret_t2']/(main_df['ret_below_t1']+main_df['ret_t1']+main_df['ret_t2'])\n",
    "\n",
    "    main_df['current_median'] =quantity_disc_data.region_median\n",
    "\n",
    "    main_df['t0_to_others'] = round(0.25*main_df['ret_below_t1'])\n",
    "    main_df['t0_to_t1'] = round(0.4*main_df['t0_to_others'])\n",
    "    main_df['t0_to_t2'] = round(0.6*main_df['t0_to_others'])\n",
    "\n",
    "    main_df['t0_new_rets'] = main_df['ret_below_t1']-main_df['t0_to_others']\n",
    "    main_df['t1_new_rets'] = main_df['ret_t1']+main_df['t0_to_t1']\n",
    "    main_df['t2_new_rets'] = main_df['ret_t2']+main_df['t0_to_t2']\n",
    "\n",
    "    main_df['t0_new_perc'] = main_df['t0_new_rets']/(main_df['t0_new_rets']+main_df['t1_new_rets']+main_df['t2_new_rets'])\n",
    "    main_df['t1_new_perc'] = main_df['t1_new_rets']/(main_df['t0_new_rets']+main_df['t1_new_rets']+main_df['t2_new_rets'])\n",
    "    main_df['t2_new_perc'] = main_df['t2_new_rets']/(main_df['t0_new_rets']+main_df['t1_new_rets']+main_df['t2_new_rets'])\n",
    "\n",
    "    main_df['new_median'] = (main_df['t0_new_perc']*quantity_disc_data.region_median)+(main_df['t1_new_perc']*main_df['tier_1'] )+(main_df['t2_new_perc']*main_df['tier_2'])\n",
    "\n",
    "    main_df['t1_nmv'] = main_df['price']*(main_df['t0_new_rets']+main_df['t1_new_rets']+main_df['t2_new_rets'])*main_df['tier_1']*main_df['t1_new_perc']\n",
    "    main_df['t2_nmv'] = main_df['price']*(main_df['t0_new_rets']+main_df['t1_new_rets']+main_df['t2_new_rets'])*main_df['tier_2']*main_df['t2_new_perc']\n",
    "\n",
    "    main_df['median_diff'] = main_df['new_median']-main_df['current_median']\n",
    "    main_df['OA_increase'] = main_df['median_diff']*main_df['price']*(main_df['t0_new_rets']+main_df['t1_new_rets']+main_df['t2_new_rets'])*main_df['bm']\n",
    "    main_df['OA_burn'] = main_df['OA_increase']/(main_df['t1_nmv']+main_df['t2_nmv'])\n",
    "    ########new######\n",
    "    main_df['burn_2'] = 0.02*(main_df['t1_nmv']+main_df['t2_nmv'])\n",
    "    main_df['burn_40'] = 0.4*main_df['OA_increase']\n",
    "    main_df['Burn_perc_margin'] = (0.25*main_df['bm'])*(main_df['t1_nmv']+main_df['t2_nmv'])\n",
    "    main_df['Burn_use']=np.minimum(np.minimum(main_df['burn_2'],main_df['burn_40']),main_df['Burn_perc_margin'])\n",
    "    ##################\n",
    "    main_df['t1_nmv_cntrb'] = main_df['t1_nmv']/(main_df['t1_nmv'] +main_df['t2_nmv']) \n",
    "    main_df['t2_nmv_cntrb'] = main_df['t2_nmv']/(main_df['t1_nmv'] +main_df['t2_nmv']) \n",
    "\n",
    "    main_df['Tiers_diff'] =  (main_df['tier_2'] - main_df['tier_1'] )/ main_df['tier_1']\n",
    "    main_df['Discount_t1'] = ((main_df['Burn_use']/(1+main_df['Tiers_diff']))*main_df['t1_nmv_cntrb'])/main_df['t1_nmv'] \n",
    "    main_df['Discount_t2'] = (main_df['Burn_use'] - ((main_df['Burn_use']/(1+main_df['Tiers_diff']))*main_df['t1_nmv_cntrb']))/main_df['t2_nmv'] \n",
    "\n",
    "    main_df = main_df[(~main_df['Discount_t1'].isna()) & (~main_df['Discount_t2'].isna())]\n",
    "    main_df = main_df[(main_df['t0_new_rets']>0) &(main_df['t1_new_rets']>0) & (main_df['t2_new_rets']>0)]\n",
    "    main_df = main_df[(main_df['bm']>0)]\n",
    "    main_df = main_df[(main_df['Discount_t1']>0) &(main_df['Discount_t2']>0)]\n",
    "\n",
    "\n",
    "    main_df = main_df.merge(sales,on = ['cohort_id','product_id','packing_unit_id'])\n",
    "    main_df = main_df.merge(avg_daily,on = ['region','cohort_id','product_id','packing_unit_id'])\n",
    "    main_df= main_df.sort_values(['cohort_id', 'total_sales'], ascending=[True, False])\n",
    "    main_df['row_number'] = main_df.groupby('cohort_id').cumcount() + 1\n",
    "    main_df = main_df[main_df['row_number']<=100]\n",
    "    main_df = main_df[main_df['cohort_id'].isin([700,701,702,703,704,1123])]\n",
    "\n",
    "\n",
    "    final_quantity_discount = pd.DataFrame(columns =['region','Discounts Group 1','Discounts Group 2','Description'])\n",
    "    for reg in main_df.region.unique():\n",
    "        region_data = main_df[main_df['region']== reg]\n",
    "        for i,r in region_data.iterrows():\n",
    "            region = r['region']\n",
    "            product_id = r['product_id']\n",
    "            packing_unit_id = r['packing_unit_id']\n",
    "            q_1 = int(r['tier_1'])\n",
    "            q_2 = int(r['tier_2'])\n",
    "            d_1 = round(r['Discount_t1']*100,2)\n",
    "            d_2 = round(r['Discount_t2']*100,2)\n",
    "            a_1 = [product_id]+[packing_unit_id]+[q_1]+[d_1]\n",
    "            a_2 = [product_id]+[packing_unit_id]+[q_2]+[d_2]\n",
    "            new_row = {'region':region ,'Discounts Group 1':a_1,'Discounts Group 2':a_2,'Description':f'{reg}QD'}\n",
    "            new_row_df = pd.DataFrame([new_row]) \n",
    "            final_quantity_discount = pd.concat([final_quantity_discount, new_row_df], ignore_index=True)\n",
    "\n",
    "\n",
    "    Tag_def = {\n",
    "        'region': ['Cairo', 'Giza', 'Alexandria', 'Upper Egypt', 'Delta East', 'Delta West'],\n",
    "        'Tag ID': [2807, 2808, 2809, 2810, 2811, 2812]\n",
    "    }\n",
    "\n",
    "    Tag_map = pd.DataFrame(Tag_def)\n",
    "    slots = ['0-12','13-17','18-23']\n",
    "    local_tz = pytz.timezone('Africa/Cairo')\n",
    "    current_hour = datetime.now(local_tz).hour\n",
    "    chosen_slot = [np.nan,np.nan]\n",
    "\n",
    "    for slot in slots:\n",
    "        parts = slot.split(\"-\")\n",
    "        if(current_hour >= int(parts[0]) and current_hour < int(parts[1])):\n",
    "            chosen_slot[0] = int(parts[0]) \n",
    "            chosen_slot[1] = int(parts[1]) \n",
    "            break\n",
    "        else:\n",
    "            chosen_slot[0] = 0\n",
    "            chosen_slot[1] = 0 \n",
    "\n",
    "    today = datetime.now(local_tz) \n",
    "    start_hour = np.maximum(current_hour,chosen_slot[0])\n",
    "    if(start_hour==current_hour):\n",
    "        start_mins =  (datetime.now(local_tz).minute) +10\n",
    "    else:\n",
    "        start_mins = 30 \n",
    "\n",
    "\n",
    "    start_date = (today.replace(hour=start_hour, minute=0, second=0, microsecond=0)+ timedelta(minutes=start_mins)).strftime('%d/%m/%Y %H:%M')\n",
    "    end_date = (today.replace(hour=chosen_slot[1], minute=59, second=0, microsecond=0)).strftime('%d/%m/%Y %H:%M')\n",
    "    final_quantity_discount = final_quantity_discount.merge(Tag_map,on='region')\n",
    "    final_quantity_discount['Start Date/Time']= start_date\n",
    "    final_quantity_discount['End Date/Time']= end_date\n",
    "    main_df['start_date'] = start_date\n",
    "    main_df['end_date'] = end_date\n",
    "    main_df = main_df.merge(Tag_map,on='region')\n",
    "    \n",
    "    cart_rules_data = main_df[['region','product_id','packing_unit_id','tier_2']].copy()\n",
    "    cohort_def = {\n",
    "        'region': ['Cairo', 'Giza', 'Alexandria', 'Delta East', 'Delta West','Upper Egypt','Upper Egypt','Upper Egypt','Upper Egypt'],\n",
    "        'cohort_id': [700, 701, 702, 704, 703, 1123,1124,1125,1126]\n",
    "    }\n",
    "    region_cohort_map = pd.DataFrame(cohort_def)\n",
    "    cart_rules_data = cart_rules_data.merge(region_cohort_map,on='region')\n",
    "    cart_rules_data = cart_rules_data.merge(live_cart_rules,on=['cohort_id','product_id','packing_unit_id'])\n",
    "    cart_rules_data = cart_rules_data[cart_rules_data['tier_2']>cart_rules_data['current_cart_rule']]\n",
    "    cart_rules_data=cart_rules_data[['cohort_id','product_id','packing_unit_id','tier_2']]\n",
    "    \n",
    "    final_data = final_quantity_discount.groupby(['Tag ID','Description', 'Start Date/Time', 'End Date/Time'], as_index=False).agg({\n",
    "        'Discounts Group 1': list ,\n",
    "        'Discounts Group 2' : list\n",
    "    })\n",
    "    \n",
    "    for cohort in cart_rules_data.cohort_id.unique():\n",
    "        req_data = cart_rules_data[cart_rules_data['cohort_id']==cohort]\n",
    "        if len(req_data) > 0 :\n",
    "            req_data = req_data[['product_id','packing_unit_id','tier_2']]\n",
    "            req_data.columns = ['Product ID','Packing Unit ID','Cart Rules']\n",
    "            req_data.to_excel(f'CartRules_{cohort}.xlsx', index=False, engine='xlsxwriter')\n",
    "            sleep(5)\n",
    "            x =  post_cart_rules(cohort,f'CartRules_{cohort}.xlsx')\n",
    "            if x.ok:\n",
    "                print(f\"success_{cohort}\")\n",
    "            else:\n",
    "                print(f\"ERROR_{cohort}\")\n",
    "                print(x.content)\n",
    "                break\n",
    "            \n",
    "    final_data.to_excel('QD_upload.xlsx', index=False)\n",
    "    response = post_QD('QD_upload.xlsx')\n",
    "    if response.ok:\n",
    "        main_df.start_date = pd.to_datetime(main_df.start_date, format=\"%d/%m/%Y %H:%M\").dt.strftime('%Y-%m-%d %H:%M')\n",
    "        main_df.end_date = pd.to_datetime(main_df.end_date, format=\"%d/%m/%Y %H:%M\").dt.strftime('%Y-%m-%d %H:%M')\n",
    "        main_df['cohort_id'] = pd.to_numeric(main_df['cohort_id'])\n",
    "        table_exist_test(main_df.rename(columns={'Tag ID': 'tag_id'}), \"materialized_views.qd_targets\")\n",
    "        main_df = main_df.drop(columns=[\"Burn_perc_margin\"])\n",
    "        DatabaseDump(main_df.rename(columns={'Tag ID': 'tag_id'}).reset_index(drop=True), \"materialized_views.qd_targets\")\n",
    "\n",
    "    else:\n",
    "        print(\"Failed with status:\", response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f2f922-864d-417c-8431-67813928a243",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
