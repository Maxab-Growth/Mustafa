{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99f8db73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.Renviron\n",
      "/home/ec2-user/service_account_key.json\n",
      "✓ Environment initialized\n",
      "✓ Snowflake query function loaded\n"
     ]
    }
   ],
   "source": [
    "# Pricing Status Analysis Script\n",
    "# Converted from SQL query to Python for easier editing and maintenance\n",
    "\n",
    "# =============================================================================\n",
    "# STANDARD LIBRARY IMPORTS\n",
    "# =============================================================================\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "# =============================================================================\n",
    "# THIRD-PARTY IMPORTS\n",
    "# =============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import snowflake.connector\n",
    "\n",
    "# =============================================================================\n",
    "# LOCAL IMPORTS & ENVIRONMENT SETUP\n",
    "# =============================================================================\n",
    "import setup_environment_2\n",
    "import importlib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "importlib.reload(setup_environment_2)\n",
    "setup_environment_2.initialize_env()\n",
    "\n",
    "print(\"✓ Environment initialized\")\n",
    "\n",
    "# =============================================================================\n",
    "# SNOWFLAKE QUERY FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def snowflake_query(country, query, warehouse=None, columns=[], conn=None):\n",
    "    \"\"\"\n",
    "    Execute a query against Snowflake and return results as DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        country: Country identifier (e.g., \"Egypt\")\n",
    "        query: SQL query string to execute\n",
    "        warehouse: Snowflake warehouse (optional)\n",
    "        columns: Custom column names (optional)\n",
    "        conn: Existing connection (optional)\n",
    "        \n",
    "    Returns:\n",
    "        pandas DataFrame with query results\n",
    "    \"\"\"\n",
    "    con = snowflake.connector.connect(\n",
    "        user     = os.environ[\"SNOWFLAKE_USERNAME\"],\n",
    "        account  = os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "        password = os.environ[\"SNOWFLAKE_PASSWORD\"],\n",
    "        database = os.environ[\"SNOWFLAKE_DATABASE\"]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        cur = con.cursor()\n",
    "        cur.execute(\"USE WAREHOUSE COMPUTE_WH\")\n",
    "        cur.execute(query)\n",
    "        \n",
    "        column_names = [col[0] for col in cur.description]\n",
    "        results = cur.fetchall()\n",
    "        \n",
    "        if not results:\n",
    "            out = pd.DataFrame(columns=[name.lower() for name in column_names])\n",
    "        else:\n",
    "            if len(columns) == 0:\n",
    "                out = pd.DataFrame(np.array(results), columns=column_names)\n",
    "                out.columns = out.columns.str.lower()\n",
    "            else:\n",
    "                out = pd.DataFrame(np.array(results), columns=columns)\n",
    "                out.columns = out.columns.str.lower()\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Query error: {e}\")\n",
    "        raise\n",
    "        \n",
    "    finally:\n",
    "        cur.close()\n",
    "        con.close()\n",
    "\n",
    "print(\"✓ Snowflake query function loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8bb08940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warehouse Mapping:\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PART 1: STATIC DATA - Warehouse Mapping\n",
    "# =============================================================================\n",
    "\n",
    "def get_warehouse_mapping():\n",
    "    \"\"\"Define warehouse to region/cohort mapping.\"\"\"\n",
    "    whs_data = [\n",
    "        ('Cairo', 'Mostorod', 1, 700),\n",
    "        ('Giza', 'Barageel', 236, 701),\n",
    "        ('Delta West', 'El-Mahala', 337, 703),\n",
    "        ('Delta West', 'Tanta', 8, 703),\n",
    "        ('Delta East', 'Mansoura FC', 339, 704),\n",
    "        ('Delta East', 'Sharqya', 170, 704),\n",
    "        ('Upper Egypt', 'Assiut FC', 501, 1124),\n",
    "        ('Upper Egypt', 'Bani sweif', 401, 1126),\n",
    "        ('Upper Egypt', 'Menya Samalot', 703, 1123),\n",
    "        ('Upper Egypt', 'Sohag', 632, 1125),\n",
    "        ('Alexandria', 'Khorshed Alex', 797, 702),\n",
    "        ('Giza', 'Sakkarah', 962, 701)\n",
    "    ]\n",
    "    \n",
    "    df_whs = pd.DataFrame(whs_data, columns=['region', 'wh', 'warehouse_id', 'cohort_id'])\n",
    "    return df_whs\n",
    "\n",
    "# Get warehouse mapping\n",
    "df_whs = get_warehouse_mapping()\n",
    "print(\"Warehouse Mapping:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1db2ac3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COGS records: 8080\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PART 2: FETCH COGS DATA\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_current_cogs():\n",
    "    \"\"\"Fetch current cost of goods sold data.\"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT product_id, wac_p\n",
    "    FROM finance.all_cogs\n",
    "    WHERE CURRENT_TIMESTAMP BETWEEN from_date AND to_date\n",
    "    \"\"\"\n",
    "    df = snowflake_query(\"Egypt\", query)\n",
    "    df['product_id'] = pd.to_numeric(df['product_id'])\n",
    "    df['wac_p'] = pd.to_numeric(df['wac_p'])\n",
    "    return df\n",
    "\n",
    "# Run:\n",
    "df_cogs = fetch_current_cogs()\n",
    "print(f\"COGS records: {len(df_cogs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ecfca4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running rates records: 25470\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PART 3: FETCH RUNNING RATES DATA\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_running_rates():\n",
    "    \"\"\"Fetch predicted running rates - latest per product/warehouse within 14 days.\"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT product_id, warehouse_id, rr\n",
    "    FROM finance.PREDICTED_RUNNING_RATES\n",
    "    QUALIFY MAX(date) OVER (PARTITION BY product_id, warehouse_id) = date\n",
    "        AND date::DATE >= CURRENT_DATE - 14\n",
    "    \"\"\"\n",
    "    df = snowflake_query(\"Egypt\", query)\n",
    "    for col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='ignore')\n",
    "    return df\n",
    "\n",
    "# Run:\n",
    "df_rr = fetch_running_rates()\n",
    "print(f\"Running rates records: {len(df_rr)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "482ee20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock records: 1802852\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PART 4: FETCH STOCKS DATA\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_stocks():\n",
    "    \"\"\"Fetch stock data with running rates and DOH calculation.\"\"\"\n",
    "    query = \"\"\"\n",
    "    WITH rr AS (\n",
    "        SELECT product_id, warehouse_id, rr\n",
    "        FROM finance.PREDICTED_RUNNING_RATES\n",
    "        QUALIFY MAX(date) OVER (PARTITION BY product_id, warehouse_id) = date\n",
    "            AND date::DATE >= CURRENT_DATE - 14\n",
    "    )\n",
    "    SELECT \n",
    "        pw.warehouse_id,\n",
    "        pw.product_id,\n",
    "        pw.available_stock::INTEGER AS stocks,\n",
    "        COALESCE(rr.rr, 0) AS rr,\n",
    "        CASE WHEN COALESCE(rr.rr, 0) = 0 THEN pw.available_stock::INTEGER \n",
    "             ELSE pw.available_stock::INTEGER / rr.rr \n",
    "        END AS doh\n",
    "    FROM product_warehouse pw\n",
    "    LEFT JOIN rr ON rr.product_id = pw.product_id AND rr.warehouse_id = pw.warehouse_id\n",
    "    WHERE pw.warehouse_id NOT IN (6, 9, 10)\n",
    "        AND pw.is_basic_unit = 1\n",
    "    \"\"\"\n",
    "    df = snowflake_query(\"Egypt\", query)\n",
    "    for col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='ignore')\n",
    "    return df\n",
    "\n",
    "# Run:\n",
    "df_stocks = fetch_stocks()\n",
    "print(f\"Stock records: {len(df_stocks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9728c536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales records: 33170\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PART 5: FETCH SALES DATA\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_sales():\n",
    "    \"\"\"Fetch sales data with aggregations for RR and retailer metrics.\"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        warehouse_id, \n",
    "        product_id,\n",
    "        sku,\n",
    "        brand,\n",
    "        cat,\n",
    "        PERCENTILE_CONT(0.8) WITHIN GROUP (ORDER BY CASE WHEN date < CURRENT_DATE - 3 THEN qty END) AS high_rr,\n",
    "        PERCENTILE_CONT(0.8) WITHIN GROUP (ORDER BY CASE WHEN date < CURRENT_DATE - 3 THEN num_rets END) AS high_rets,\n",
    "        COALESCE(STDDEV(CASE WHEN date < CURRENT_DATE - 3 THEN qty END), 0) AS qty_std,\n",
    "        COALESCE(STDDEV(CASE WHEN date < CURRENT_DATE - 3 THEN num_rets END), 0) AS rets_std,\n",
    "        COALESCE(SUM(CASE WHEN date = CURRENT_DATE - 1 THEN qty END), 0) AS cu_rr,\n",
    "        COALESCE(SUM(CASE WHEN date = CURRENT_DATE THEN qty END), 0) AS today_rr,\n",
    "        COALESCE(SUM(CASE WHEN date = CURRENT_DATE - 1 THEN num_rets END), 0) AS cu_rets\n",
    "    FROM (\n",
    "        SELECT\n",
    "            so.created_at::DATE AS date,\n",
    "            pso.warehouse_id,\n",
    "            pso.product_id,\n",
    "            CONCAT(p.name_ar, ' ', p.size, ' ', pu.name_ar) AS sku,\n",
    "            b.name_ar AS brand, \n",
    "            c.name_ar AS cat,\n",
    "            SUM(pso.purchased_item_count * pso.basic_unit_count) AS qty,\n",
    "            COUNT(DISTINCT so.retailer_id) AS num_rets\n",
    "        FROM product_sales_order pso\n",
    "        JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "        JOIN products p ON p.id = pso.product_id\n",
    "        JOIN brands b ON p.brand_id = b.id \n",
    "        JOIN categories c ON p.category_id = c.id\n",
    "        JOIN product_units pu ON pu.id = p.unit_id\n",
    "        WHERE so.created_at::DATE BETWEEN CURRENT_DATE - 150 AND CURRENT_DATE \n",
    "            AND so.sales_order_status_id NOT IN (7, 12)\n",
    "            AND so.channel IN ('telesales', 'retailer')\n",
    "            AND pso.purchased_item_count <> 0\n",
    "            AND DAYNAME(so.created_at::DATE) <> 'Fri'\n",
    "        GROUP BY 1, 2, 3, 4, 5, 6\n",
    "    )\n",
    "    GROUP BY 1, 2, 3, 4, 5\n",
    "    \"\"\"\n",
    "    df = snowflake_query(\"Egypt\", query)\n",
    "    # Convert numeric columns\n",
    "    numeric_cols = ['warehouse_id', 'product_id', 'high_rr', 'high_rets', 'qty_std', 'rets_std', 'cu_rr', 'today_rr', 'cu_rets']\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='ignore')\n",
    "    return df\n",
    "\n",
    "# Run:\n",
    "df_sales = fetch_sales()\n",
    "print(f\"Sales records: {len(df_sales)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f9713f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price records: 192546\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PART 6: FETCH PRICES DATA\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_prices():\n",
    "    \"\"\"Fetch latest prices per product/cohort.\"\"\"\n",
    "    cohort_ids = [700, 701, 702, 703, 704, 696, 695, 698, 697, 699, 1123, 1124, 1125, 1126]\n",
    "    cohort_str = ', '.join(map(str, cohort_ids))\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    SELECT cohort_id, product_id, price\n",
    "    FROM (\n",
    "        SELECT \n",
    "            cpc.cohort_id,\n",
    "            pu.product_id,\n",
    "            cpc.price,\n",
    "            ROW_NUMBER() OVER (PARTITION BY pu.product_id, cpc.cohort_id ORDER BY cpc.created_at DESC) AS rn\n",
    "        FROM cohort_pricing_changes cpc \n",
    "        JOIN PACKING_UNIT_PRODUCTS pu ON pu.id = cpc.product_packing_unit_id\n",
    "        WHERE cpc.cohort_id IN ({cohort_str})\n",
    "            AND pu.is_basic_unit = 1 \n",
    "    )\n",
    "    WHERE rn = 1\n",
    "    \"\"\"\n",
    "    df = snowflake_query(\"Egypt\", query)\n",
    "    df['cohort_id'] = pd.to_numeric(df['cohort_id'])\n",
    "    df['product_id'] = pd.to_numeric(df['product_id'])\n",
    "    df['price'] = pd.to_numeric(df['price'])\n",
    "    return df\n",
    "\n",
    "# Run:\n",
    "df_prices = fetch_prices()\n",
    "print(f\"Price records: {len(df_prices)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "16388890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marketplace price records: 7875\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PART 7: FETCH MARKETPLACE PRICES DATA\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_marketplace_prices():\n",
    "    \"\"\"Fetch marketplace price data (min, mod, max).\"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        mp.region,\n",
    "        mp.product_id,\n",
    "        AVG(mp.min_price / pup.basic_unit_count) AS min_price,\n",
    "        AVG(mp.mod_price / pup.basic_unit_count) AS mod_price,\n",
    "        AVG(mp.max_price / pup.basic_unit_count) AS max_price\n",
    "    FROM materialized_views.marketplace_prices mp\n",
    "    JOIN PACKING_UNIT_PRODUCTS pup ON pup.product_id = mp.product_id AND mp.pu_id = pup.packing_unit_id\n",
    "    GROUP BY 1, 2\n",
    "    \"\"\"\n",
    "    df = snowflake_query(\"Egypt\", query)\n",
    "    df['product_id'] = pd.to_numeric(df['product_id'])\n",
    "    df['min_price'] = pd.to_numeric(df['min_price'])\n",
    "    df['mod_price'] = pd.to_numeric(df['mod_price'])\n",
    "    df['max_price'] = pd.to_numeric(df['max_price'])\n",
    "    return df\n",
    "\n",
    "# Run:\n",
    "df_mp = fetch_marketplace_prices()\n",
    "print(f\"Marketplace price records: {len(df_mp)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5bff7a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ben Soliman price records: 1371\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PART 8: FETCH BEN SOLIMAN PRICES\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_ben_soliman_prices():\n",
    "    \"\"\"Fetch Ben Soliman competitor prices with validation.\"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT z.* \n",
    "    FROM (\n",
    "        SELECT maxab_product_id AS product_id, AVG(bs_final_price) AS ben_soliman_price\n",
    "        FROM (\n",
    "            SELECT *, ROW_NUMBER() OVER(PARTITION BY maxab_product_id ORDER BY diff) AS rnk_2\n",
    "            FROM (\n",
    "                SELECT *, (bs_final_price - wac_p) / wac_p AS diff_2\n",
    "                FROM (\n",
    "                    SELECT *, bs_price / maxab_basic_unit_count AS bs_final_price\n",
    "                    FROM (\n",
    "                        SELECT *, ROW_NUMBER() OVER(PARTITION BY maxab_product_id, maxab_pu ORDER BY diff) AS rnk \n",
    "                        FROM (\n",
    "                            SELECT *, MAX(INJECTION_DATE::DATE) OVER(PARTITION BY maxab_product_id, maxab_pu) AS max_date\n",
    "                            FROM (\n",
    "                                SELECT sm.*, wac1, wac_p, \n",
    "                                    ABS(bs_price - (wac_p * maxab_basic_unit_count)) / (wac_p * maxab_basic_unit_count) AS diff \n",
    "                                FROM materialized_views.savvy_mapping sm \n",
    "                                JOIN finance.all_cogs f ON f.product_id = sm.maxab_product_id \n",
    "                                    AND CURRENT_TIMESTAMP BETWEEN f.from_date AND f.to_date\n",
    "                                WHERE bs_price IS NOT NULL \n",
    "                                    AND INJECTION_DATE::DATE >= CURRENT_TIMESTAMP::DATE - 5 \n",
    "                                    AND ABS(bs_price - (wac_p * maxab_basic_unit_count)) / (wac_p * maxab_basic_unit_count) < 0.3\n",
    "                            )\n",
    "                            QUALIFY max_date = INJECTION_DATE\n",
    "                        )\n",
    "                        QUALIFY rnk = 1 \n",
    "                    )\n",
    "                )\n",
    "                WHERE diff_2 BETWEEN -0.5 AND 0.5 \n",
    "            )\n",
    "            QUALIFY rnk_2 = 1 \n",
    "        )\n",
    "        GROUP BY ALL\n",
    "    ) z \n",
    "    JOIN finance.all_cogs f ON f.product_id = z.product_id \n",
    "        AND CURRENT_TIMESTAMP BETWEEN f.from_date AND f.to_date\n",
    "    WHERE ben_soliman_price BETWEEN f.wac_p * 0.9 AND f.wac_p * 1.3\n",
    "    \"\"\"\n",
    "    df = snowflake_query(\"Egypt\", query)\n",
    "    df['product_id'] = pd.to_numeric(df['product_id'])\n",
    "    df['ben_soliman_price'] = pd.to_numeric(df['ben_soliman_price'])\n",
    "    return df\n",
    "\n",
    "# Run:\n",
    "df_bsp = fetch_ben_soliman_prices()\n",
    "print(f\"Ben Soliman price records: {len(df_bsp)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9cddba5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapped price records: 5583\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PART 9: FETCH SCRAPPED/CLEANED MARKET PRICES\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_scrapped_prices():\n",
    "    \"\"\"Fetch scraped market prices with min/max/median aggregations.\"\"\"\n",
    "    query = \"\"\"\n",
    "    WITH current_cogs AS (\n",
    "        SELECT product_id, wac_p\n",
    "        FROM finance.all_cogs\n",
    "        WHERE CURRENT_TIMESTAMP BETWEEN from_date AND to_date\n",
    "    )\n",
    "    SELECT \n",
    "        product_id,\n",
    "        region,\n",
    "        MIN(market_price) AS min_scrapped,\n",
    "        MAX(market_price) AS max_scrapped,\n",
    "        MEDIAN(market_price) AS median_scrapped\n",
    "    FROM (\n",
    "        SELECT \n",
    "            cmp.product_id,\n",
    "            cmp.region,\n",
    "            cmp.market_price\n",
    "        FROM materialized_views.cleaned_market_prices cmp\n",
    "        JOIN current_cogs f ON f.product_id = cmp.product_id\n",
    "        WHERE cmp.date >= CURRENT_DATE - 5\n",
    "            AND cmp.market_price BETWEEN f.wac_p * 0.9 AND f.wac_p * 1.3\n",
    "        QUALIFY MAX(cmp.date) OVER (PARTITION BY cmp.region, cmp.product_id, cmp.competitor) = cmp.date\n",
    "    )\n",
    "    GROUP BY 1, 2\n",
    "    \"\"\"\n",
    "    df = snowflake_query(\"Egypt\", query)\n",
    "    df['product_id'] = pd.to_numeric(df['product_id'])\n",
    "    df['min_scrapped'] = pd.to_numeric(df['min_scrapped'])\n",
    "    df['max_scrapped'] = pd.to_numeric(df['max_scrapped'])\n",
    "    df['median_scrapped'] = pd.to_numeric(df['median_scrapped'])\n",
    "    return df\n",
    "\n",
    "# Run:\n",
    "df_scrapped = fetch_scrapped_prices()\n",
    "print(f\"Scrapped price records: {len(df_scrapped)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8eec751c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat/Brand target records: 482\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PART 10: FETCH TARGETS DATA\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_cat_brand_targets():\n",
    "    \"\"\"Fetch category/brand targets from commercial plan.\"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        cat, \n",
    "        brand, \n",
    "        SUM(nmv) AS target_nmv, \n",
    "        AVG(margin) AS target_bm,\n",
    "        DATE_TRUNC('month', DATE) AS month_date\n",
    "    FROM performance.commercial_targets\n",
    "    WHERE cat IS NOT NULL AND brand IS NOT NULL \n",
    "        AND date >= DATE_TRUNC('month', CURRENT_DATE)\n",
    "    GROUP BY ALL\n",
    "    \"\"\"\n",
    "    df = snowflake_query(\"Egypt\", query)\n",
    "    df['target_nmv'] = pd.to_numeric(df['target_nmv'])\n",
    "    df['target_bm'] = pd.to_numeric(df['target_bm'])\n",
    "    return df\n",
    "\n",
    "def fetch_cat_targets(df_cat_brand_targets):\n",
    "    \"\"\"Calculate category-level targets from brand targets.\"\"\"\n",
    "    df = df_cat_brand_targets.copy()\n",
    "    df['weighted_margin'] = df['target_bm'] * df['target_nmv']\n",
    "    cat_targets = df.groupby('cat').apply(\n",
    "        lambda x: x['weighted_margin'].sum() / x['target_nmv'].sum() if x['target_nmv'].sum() > 0 else 0\n",
    "    ).reset_index()\n",
    "    cat_targets.columns = ['cat', 'cat_target_margin']\n",
    "    return cat_targets\n",
    "\n",
    "# Run:\n",
    "df_cat_brand_targets = fetch_cat_brand_targets()\n",
    "df_cat_targets = fetch_cat_targets(df_cat_brand_targets)\n",
    "print(f\"Cat/Brand target records: {len(df_cat_brand_targets)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0479501c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discounted sales records: 10146\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PART 11: FETCH DISCOUNTED SALES DATA\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_discounted_sales():\n",
    "    \"\"\"Fetch yesterday's discounted sales breakdown.\"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT  \n",
    "        pso.warehouse_id,\n",
    "        pso.product_id,\n",
    "        SUM(pso.total_price) AS total_nmv,\n",
    "        SUM(CASE WHEN pso.dynamic_bundle_sales_order_id IS NOT NULL THEN pso.total_price END) AS bundle_nmv,\n",
    "        SUM(CASE WHEN pso.sku_discount_id IS NOT NULL THEN pso.total_price END) AS sku_discount_nmv,\n",
    "        SUM(CASE WHEN pso.quantity_discount_id IS NOT NULL THEN pso.total_price END) AS quantity_nmv\n",
    "    FROM product_sales_order pso \n",
    "    JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "    WHERE so.created_at::DATE = CURRENT_DATE - 1 \n",
    "        AND so.sales_order_status_id NOT IN (7, 12)\n",
    "        AND so.channel IN ('telesales', 'retailer')\n",
    "        AND pso.purchased_item_count <> 0\n",
    "    GROUP BY 1, 2\n",
    "    \"\"\"\n",
    "    df = snowflake_query(\"Egypt\", query)\n",
    "    for col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='ignore')\n",
    "    return df\n",
    "\n",
    "# Run:\n",
    "df_discounted = fetch_discounted_sales()\n",
    "print(f\"Discounted sales records: {len(df_discounted)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "46a953cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product activation records: 27753\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PART 13B: FETCH PRODUCT WAREHOUSE ACTIVATION\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_product_activation():\n",
    "    \"\"\"\n",
    "    Fetch product warehouse activation status.\n",
    "    Uses the top selling packing unit per product in the last 3 months \n",
    "    as the representative packing unit to get activation status.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    WITH top_selling_pu AS (\n",
    "        -- Get the top selling packing unit per product/warehouse in last 3 months\n",
    "        SELECT \n",
    "            pso.product_id,\n",
    "            pso.warehouse_id,\n",
    "            pso.packing_unit_id,\n",
    "            SUM(pso.total_price) AS total_nmv\n",
    "        FROM product_sales_order pso\n",
    "        JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "        WHERE so.created_at::DATE >= CURRENT_DATE - 90\n",
    "            AND so.sales_order_status_id NOT IN (7, 12)\n",
    "            AND so.channel IN ('telesales', 'retailer')\n",
    "            AND pso.purchased_item_count <> 0\n",
    "        GROUP BY 1, 2, 3\n",
    "        QUALIFY ROW_NUMBER() OVER (\n",
    "            PARTITION BY pso.product_id, pso.warehouse_id \n",
    "            ORDER BY SUM(pso.total_price) DESC\n",
    "        ) = 1\n",
    "    )\n",
    "    SELECT \n",
    "        tspu.product_id,\n",
    "        tspu.warehouse_id,\n",
    "        tspu.packing_unit_id AS top_selling_pu,\n",
    "        pw.activation AS activation\n",
    "    FROM top_selling_pu tspu\n",
    "    JOIN product_warehouse pw \n",
    "        ON pw.product_id = tspu.product_id \n",
    "        AND pw.warehouse_id = tspu.warehouse_id\n",
    "        AND pw.packing_unit_id = tspu.packing_unit_id\n",
    "    \"\"\"\n",
    "    df = snowflake_query(\"Egypt\", query)\n",
    "    df['product_id'] = pd.to_numeric(df['product_id'])\n",
    "    df['warehouse_id'] = pd.to_numeric(df['warehouse_id'])\n",
    "    df['top_selling_pu'] = pd.to_numeric(df['top_selling_pu'])\n",
    "    return df\n",
    "\n",
    "# Run:\n",
    "df_activation = fetch_product_activation()\n",
    "print(f\"Product activation records: {len(df_activation)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f6c46e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOS yesterday records: 1871950\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PART 13C: FETCH OOS YESTERDAY DATA\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_oos_yesterday():\n",
    "    \"\"\"\n",
    "    Fetch whether product was out of stock yesterday.\n",
    "    Returns oos_yesterday = 1 if product had 0 opening AND 0 closing stock,\n",
    "    meaning it was OOS the entire day.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT distinct product_id, warehouse_id,\n",
    "        CASE WHEN opening_stocks = 0 AND closing_stocks = 0 THEN 1\n",
    "             ELSE 0 \n",
    "        END AS oos_yesterday\n",
    "    FROM (\n",
    "        SELECT \n",
    "            timestamp,\n",
    "            product_id,\n",
    "            warehouse_id, \n",
    "            AVAILABLE_STOCK AS closing_stocks,\n",
    "            LAG(AVAILABLE_STOCK) OVER (PARTITION BY product_id, warehouse_id ORDER BY TIMESTAMP) AS opening_stocks\n",
    "        FROM materialized_views.stock_day_close\n",
    "        WHERE timestamp::DATE >= CURRENT_DATE - 2\n",
    "        QUALIFY opening_stocks IS NOT NULL\n",
    "    )\n",
    "    \"\"\"\n",
    "    df = snowflake_query(\"Egypt\", query)\n",
    "    df['product_id'] = pd.to_numeric(df['product_id'])\n",
    "    df['warehouse_id'] = pd.to_numeric(df['warehouse_id'])\n",
    "    df['oos_yesterday'] = pd.to_numeric(df['oos_yesterday'])\n",
    "    return df\n",
    "\n",
    "# Run:\n",
    "df_oos_yesterday = fetch_oos_yesterday()\n",
    "print(f\"OOS yesterday records: {len(df_oos_yesterday)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8aab9d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PO data records: 18708\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PART 13D: FETCH PURCHASE ORDER DATA\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_po_data():\n",
    "    \"\"\"\n",
    "    Fetch purchase order data from last 15 days.\n",
    "    Returns last PO info and count of supplier rejections.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    WITH last_data AS (\n",
    "        SELECT product_id, warehouse_id, confirmation_status, PO_date::DATE AS last_po_date, ordered_qty\n",
    "        FROM (\n",
    "            SELECT \n",
    "                product_id,\n",
    "                DROPOFF_WAREHOUSE_ID AS warehouse_id,\n",
    "                confirmation_status,\n",
    "                created_at AS PO_date,\n",
    "                MIN_QUANTITY AS ordered_qty,\n",
    "                reason,\n",
    "                MAX(PO_date) OVER (PARTITION BY product_id, warehouse_id) AS last_po\n",
    "            FROM retool.PO_INITIAL_PLAN\n",
    "            WHERE created_at::DATE >= CURRENT_DATE - 15 \n",
    "            QUALIFY last_po = PO_date\n",
    "        )\n",
    "    ),\n",
    "    last_15_data AS (\n",
    "        SELECT \n",
    "            product_id,\n",
    "            DROPOFF_WAREHOUSE_ID AS warehouse_id,\n",
    "            COUNT(DISTINCT CASE WHEN confirmation_status <> 'yes' THEN created_at END) AS no_last_15\n",
    "        FROM retool.PO_INITIAL_PLAN\n",
    "        WHERE created_at::DATE >= CURRENT_DATE - 15 \n",
    "        GROUP BY ALL\n",
    "    )\n",
    "    SELECT \n",
    "        ld.product_id,\n",
    "        ld.warehouse_id,\n",
    "        ld.confirmation_status,\n",
    "        ld.last_po_date,\n",
    "        ld.ordered_qty,\n",
    "        COALESCE(lfd.no_last_15, 0) AS no_last_15\n",
    "    FROM last_data ld \n",
    "    LEFT JOIN last_15_data lfd \n",
    "        ON lfd.product_id = ld.product_id \n",
    "        AND lfd.warehouse_id = ld.warehouse_id\n",
    "    \"\"\"\n",
    "    df = snowflake_query(\"Egypt\", query)\n",
    "    df['product_id'] = pd.to_numeric(df['product_id'])\n",
    "    df['warehouse_id'] = pd.to_numeric(df['warehouse_id'])\n",
    "    df['ordered_qty'] = pd.to_numeric(df['ordered_qty'])\n",
    "    df['no_last_15'] = pd.to_numeric(df['no_last_15'])\n",
    "    df['last_po_date'] = pd.to_datetime(df['last_po_date'])\n",
    "    return df\n",
    "\n",
    "# Run:\n",
    "df_po_data = fetch_po_data()\n",
    "print(f\"PO data records: {len(df_po_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e930f03f-1492-40d4-8dd6-7c0b43072989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>warehouse_id</th>\n",
       "      <th>top_selling_pu</th>\n",
       "      <th>activation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1311</th>\n",
       "      <td>5152</td>\n",
       "      <td>236</td>\n",
       "      <td>2</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1671</th>\n",
       "      <td>5152</td>\n",
       "      <td>703</td>\n",
       "      <td>2</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11308</th>\n",
       "      <td>5152</td>\n",
       "      <td>797</td>\n",
       "      <td>2</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11839</th>\n",
       "      <td>5152</td>\n",
       "      <td>632</td>\n",
       "      <td>2</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15896</th>\n",
       "      <td>5152</td>\n",
       "      <td>337</td>\n",
       "      <td>2</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17132</th>\n",
       "      <td>5152</td>\n",
       "      <td>339</td>\n",
       "      <td>2</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17891</th>\n",
       "      <td>5152</td>\n",
       "      <td>962</td>\n",
       "      <td>2</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19924</th>\n",
       "      <td>5152</td>\n",
       "      <td>170</td>\n",
       "      <td>2</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20938</th>\n",
       "      <td>5152</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24976</th>\n",
       "      <td>5152</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27482</th>\n",
       "      <td>5152</td>\n",
       "      <td>401</td>\n",
       "      <td>2</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27658</th>\n",
       "      <td>5152</td>\n",
       "      <td>501</td>\n",
       "      <td>2</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       product_id  warehouse_id  top_selling_pu activation\n",
       "1311         5152           236               2       true\n",
       "1671         5152           703               2       true\n",
       "11308        5152           797               2       true\n",
       "11839        5152           632               2       true\n",
       "15896        5152           337               2       true\n",
       "17132        5152           339               2       true\n",
       "17891        5152           962               2       true\n",
       "19924        5152           170               2       true\n",
       "20938        5152             8               2       true\n",
       "24976        5152             1               2       true\n",
       "27482        5152           401               2       true\n",
       "27658        5152           501               2       true"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_activation[df_activation['product_id']==5152]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4c41ec38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commercial constraint records: 709\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PART 12: FETCH COMMERCIAL CONSTRAINTS (MIN PRICES)\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_commercial_constraints():\n",
    "    \"\"\"Fetch commercial minimum price constraints.\"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT product_id, region, min_price\n",
    "    FROM (\n",
    "        SELECT \n",
    "            product_id, \n",
    "            region, \n",
    "            min_price,\n",
    "            created_at,\n",
    "            MAX(created_at) OVER (PARTITION BY product_id, region) AS max_created\n",
    "        FROM finance.minimum_prices\n",
    "        WHERE is_deleted = 'false'\n",
    "            AND created_at BETWEEN \n",
    "                CASE WHEN DATE_PART('day', CURRENT_DATE) < 7 \n",
    "                     THEN DATE_TRUNC('month', CURRENT_DATE - INTERVAL '1 month') \n",
    "                     ELSE DATE_TRUNC('month', CURRENT_DATE)\n",
    "                END\n",
    "                AND DATE_TRUNC('month', CURRENT_DATE) + INTERVAL '1 month' + INTERVAL '6 days'\n",
    "    )\n",
    "    WHERE created_at = max_created\n",
    "    \"\"\"\n",
    "    df = snowflake_query(\"Egypt\", query)\n",
    "    df['product_id'] = pd.to_numeric(df['product_id'])\n",
    "    df['min_price'] = pd.to_numeric(df['min_price'])\n",
    "    return df\n",
    "\n",
    "# Run:\n",
    "df_commercial = fetch_commercial_constraints()\n",
    "print(f\"Commercial constraint records: {len(df_commercial)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f30adb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets data records: 27268\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PART 13: FETCH TARGETS DATA (COMPLEX - WAREHOUSE SKU TARGETS)\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_targets_data(df_whs):\n",
    "    \"\"\"Fetch complex targets data with warehouse-level SKU targets.\"\"\"\n",
    "    # Build warehouse IDs list for the query\n",
    "    warehouse_ids = df_whs['warehouse_id'].tolist()\n",
    "    wh_str = ', '.join(map(str, warehouse_ids))\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    WITH whs AS (\n",
    "        SELECT *\n",
    "        FROM (VALUES\n",
    "            ('Cairo', 'Mostorod', 1, 700),\n",
    "            ('Giza', 'Barageel', 236, 701),\n",
    "            ('Delta West', 'El-Mahala', 337, 703),\n",
    "            ('Delta West', 'Tanta', 8, 703),\n",
    "            ('Delta East', 'Mansoura FC', 339, 704),\n",
    "            ('Delta East', 'Sharqya', 170, 704),\n",
    "            ('Upper Egypt', 'Assiut FC', 501, 1124),\n",
    "            ('Upper Egypt', 'Bani sweif', 401, 1126),\n",
    "            ('Upper Egypt', 'Menya Samalot', 703, 1123),\n",
    "            ('Upper Egypt', 'Sohag', 632, 1125),\n",
    "            ('Alexandria', 'Khorshed Alex', 797, 702),\n",
    "            ('Giza', 'Sakkarah', 962, 701)\n",
    "        ) x(region, wh, warehouse_id, cohort_id)\n",
    "    ),\n",
    "    base_sales AS (\n",
    "        SELECT\n",
    "            CASE WHEN whs.region LIKE '%Delta%' THEN 'Delta' \n",
    "                 WHEN whs.region = 'Cairo' OR whs.region = 'Giza' THEN 'Greater Cairo' \n",
    "                 ELSE whs.region \n",
    "            END AS region,\n",
    "            pso.warehouse_id,\n",
    "            pso.product_id,\n",
    "            c.name_ar AS cat,\n",
    "            b.name_ar AS brand,\n",
    "            SUM(pso.total_price) AS nmv,\n",
    "            so.created_at::DATE AS sale_date\n",
    "        FROM product_sales_order pso\n",
    "        JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "        JOIN products p ON p.id = pso.product_id\n",
    "        JOIN categories c ON c.id = p.category_id\n",
    "        JOIN brands b ON b.id = p.brand_id\n",
    "        JOIN whs ON whs.warehouse_id = pso.warehouse_id\n",
    "        WHERE so.sales_order_status_id NOT IN (7, 12)\n",
    "            AND pso.purchased_item_count <> 0\n",
    "            AND so.channel IN ('retailer', 'telesales')\n",
    "            AND so.created_at::DATE BETWEEN DATE_TRUNC('month', CURRENT_DATE - INTERVAL '3 month') AND CURRENT_DATE - 1\n",
    "        GROUP BY 1, 2, 3, 4, 5, 7\n",
    "    ),\n",
    "    region_product_nmv AS (\n",
    "        SELECT region, product_id, cat, brand, SUM(nmv) AS region_product_nmv\n",
    "        FROM base_sales\n",
    "        GROUP BY 1, 2, 3, 4\n",
    "    ),\n",
    "    warehouse_contribution AS (\n",
    "        SELECT \n",
    "            bs.region,\n",
    "            bs.warehouse_id,\n",
    "            bs.product_id,\n",
    "            bs.cat,\n",
    "            bs.brand,\n",
    "            SUM(bs.nmv) AS warehouse_nmv,\n",
    "            SUM(bs.nmv) / NULLIF(rpn.region_product_nmv, 0) AS wh_cntrb_in_region\n",
    "        FROM base_sales bs\n",
    "        JOIN region_product_nmv rpn ON rpn.region = bs.region \n",
    "            AND rpn.product_id = bs.product_id\n",
    "        GROUP BY 1, 2, 3, 4, 5, rpn.region_product_nmv\n",
    "    ),\n",
    "    region_sku_cntrb AS (\n",
    "        SELECT region, product_id, cat, brand,\n",
    "            SUM(region_product_nmv) / SUM(SUM(region_product_nmv)) OVER (PARTITION BY region, cat, brand) AS sku_cntrb\n",
    "        FROM region_product_nmv\n",
    "        GROUP BY 1, 2, 3, 4\n",
    "    ),\n",
    "    comm_plan AS (\n",
    "        SELECT\n",
    "            CASE WHEN city = 'Alex' THEN 'Alexandria' ELSE city END AS region,\n",
    "            cat, brand,\n",
    "            SUM(nmv) AS target\n",
    "        FROM performance.commercial_targets\n",
    "        WHERE date BETWEEN DATE_TRUNC('month', CURRENT_DATE) AND CURRENT_DATE - 1\n",
    "        GROUP BY 1, 2, 3\n",
    "    ),\n",
    "    current_month_sales AS (\n",
    "        SELECT region, warehouse_id, product_id, SUM(nmv) AS nmv\n",
    "        FROM base_sales\n",
    "        WHERE sale_date >= DATE_TRUNC('month', CURRENT_DATE)\n",
    "        GROUP BY 1, 2, 3\n",
    "    )\n",
    "    SELECT \n",
    "        wc.region,\n",
    "        wc.warehouse_id,\n",
    "        wc.product_id,\n",
    "        wc.cat,\n",
    "        wc.brand,\n",
    "        cp.target * rsc.sku_cntrb AS region_sku_target,\n",
    "        cp.target * rsc.sku_cntrb * wc.wh_cntrb_in_region AS wh_sku_target,\n",
    "        COALESCE(cms.nmv, 0) AS sales,\n",
    "        cp.target * rsc.sku_cntrb * wc.wh_cntrb_in_region - COALESCE(cms.nmv, 0) AS rem_nmv\n",
    "    FROM warehouse_contribution wc\n",
    "    JOIN region_sku_cntrb rsc ON rsc.region = wc.region \n",
    "        AND rsc.product_id = wc.product_id\n",
    "    JOIN comm_plan cp ON cp.region = wc.region \n",
    "        AND cp.cat = wc.cat \n",
    "        AND cp.brand = wc.brand\n",
    "    LEFT JOIN current_month_sales cms ON cms.product_id = wc.product_id \n",
    "        AND cms.warehouse_id = wc.warehouse_id\n",
    "        AND cms.region = wc.region\n",
    "    \"\"\"\n",
    "    df = snowflake_query(\"Egypt\", query)\n",
    "    for col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='ignore')\n",
    "    return df\n",
    "\n",
    "# Run:\n",
    "df_targets = fetch_targets_data(df_whs)\n",
    "print(f\"Targets data records: {len(df_targets)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "33888c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product metrics records: 28439\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PART 14: BUILD PRODUCT METRICS (MERGE ALL DATA)\n",
    "# =============================================================================\n",
    "\n",
    "def build_product_metrics(df_stocks, df_sales, df_whs, df_prices, df_cogs, \n",
    "                          df_mp, df_bsp, df_scrapped, df_cat_brand_targets, \n",
    "                          df_cat_targets, df_discounted, df_activation, df_oos_yesterday, df_po_data):\n",
    "    \"\"\"\n",
    "    Merge all data sources to build product metrics.\n",
    "    This replicates the 'product_metrics' CTE from the SQL query.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start with stocks and sales join\n",
    "    df = df_stocks.merge(\n",
    "        df_sales, \n",
    "        on=['product_id', 'warehouse_id'], \n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    # Join warehouse mapping\n",
    "    df = df.merge(df_whs, on='warehouse_id', how='inner')\n",
    "    \n",
    "    # Join prices (using cohort_id from warehouse mapping)\n",
    "    df = df.merge(\n",
    "        df_prices, \n",
    "        on=['product_id', 'cohort_id'], \n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    # Join COGS\n",
    "    df = df.merge(df_cogs, on='product_id', how='inner')\n",
    "    \n",
    "    # Calculate BM (basic margin)\n",
    "    df['bm'] = (df['price'] - df['wac_p']) / df['price'].replace(0, np.nan)\n",
    "    \n",
    "    # Calculate in_stock_perc\n",
    "    df['in_stock_perc'] = (df['stocks'] > 0).astype(int)\n",
    "    \n",
    "    # Join marketplace prices\n",
    "    df = df.merge(\n",
    "        df_mp.rename(columns={\n",
    "            'min_price': 'mp_min_price',\n",
    "            'mod_price': 'mp_mod_price', \n",
    "            'max_price': 'mp_max_price'\n",
    "        }), \n",
    "        on=['product_id', 'region'], \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Join Ben Soliman prices\n",
    "    df = df.merge(df_bsp[['product_id', 'ben_soliman_price']], on='product_id', how='left')\n",
    "    \n",
    "    # Join scrapped prices\n",
    "    df = df.merge(\n",
    "        df_scrapped.rename(columns={\n",
    "            'min_scrapped': 'min_scrapped',\n",
    "            'max_scrapped': 'max_scrapped',\n",
    "            'median_scrapped': 'median_scrapped'\n",
    "        }), \n",
    "        on=['product_id', 'region'], \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Join category/brand targets\n",
    "    df = df.merge(\n",
    "        df_cat_brand_targets[['cat', 'brand', 'target_bm']].drop_duplicates(),\n",
    "        on=['cat', 'brand'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Join category targets (fallback)\n",
    "    df = df.merge(df_cat_targets, on='cat', how='left')\n",
    "    \n",
    "    # Set target_margin (use cat_brand target, fall back to cat target)\n",
    "    df['target_margin'] = df['target_bm'].fillna(df['cat_target_margin'])\n",
    "    \n",
    "    # Join discounted sales\n",
    "    df = df.merge(\n",
    "        df_discounted,\n",
    "        on=['warehouse_id', 'product_id'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Join product activation status\n",
    "    df = df.merge(\n",
    "        df_activation[['product_id', 'warehouse_id', 'activation']],\n",
    "        on=['product_id', 'warehouse_id'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Join OOS yesterday status\n",
    "    df = df.merge(\n",
    "        df_oos_yesterday[['product_id', 'warehouse_id', 'oos_yesterday']],\n",
    "        on=['product_id', 'warehouse_id'],\n",
    "        how='left'\n",
    "    )\n",
    "    # Fill missing oos_yesterday with 0 (assume was in stock if no data)\n",
    "    df['oos_yesterday'] = df['oos_yesterday'].fillna(0)\n",
    "    \n",
    "    # Join PO data\n",
    "    df = df.merge(\n",
    "        df_po_data[['product_id', 'warehouse_id', 'confirmation_status', 'last_po_date', 'ordered_qty', 'no_last_15']],\n",
    "        on=['product_id', 'warehouse_id'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Filter to positive prices and high_rr\n",
    "    df = df[(df['price'] > 0) & (df['high_rr'] > 0)]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Run after fetching all data:\n",
    "df_metrics = build_product_metrics(\n",
    "    df_stocks, df_sales, df_whs, df_prices, df_cogs,\n",
    "    df_mp, df_bsp, df_scrapped, df_cat_brand_targets,\n",
    "    df_cat_targets, df_discounted, df_activation, df_oos_yesterday, df_po_data\n",
    ")\n",
    "print(f\"Product metrics records: {len(df_metrics)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a01f9225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 15: SCORING AND CLASSIFICATION\n",
    "# =============================================================================\n",
    "\n",
    "def add_scoring_classification(df, df_commercial):\n",
    "    \"\"\"\n",
    "    Add scoring and classification columns.\n",
    "    Replicates 'scored_classified' and 'final_scored' CTEs.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Map region for commercial constraints\n",
    "    df['region_mapped'] = df['region'].apply(\n",
    "        lambda x: 'Greater Cairo' if x in ['Cairo', 'Giza'] else x\n",
    "    )\n",
    "    \n",
    "    # Join commercial constraints\n",
    "    df = df.merge(\n",
    "        df_commercial.rename(columns={'min_price': 'commercial_min'}),\n",
    "        left_on=['product_id', 'region_mapped'],\n",
    "        right_on=['product_id', 'region'],\n",
    "        how='left',\n",
    "        suffixes=('', '_comm')\n",
    "    )\n",
    "    \n",
    "    # Calculate offers percentage\n",
    "    df['offers_perc'] = (\n",
    "        df['bundle_nmv'].fillna(0) + \n",
    "        df['sku_discount_nmv'].fillna(0) + \n",
    "        df['quantity_nmv'].fillna(0)\n",
    "    ) / df['total_nmv'].replace(0, np.nan)\n",
    "    \n",
    "    # Calculate combined market prices\n",
    "    df['combined_min_market'] = df[['mp_min_price','mp_mod_price', 'ben_soliman_price', 'min_scrapped']].min(axis=1)\n",
    "    df['combined_max_market'] = df[['mp_max_price', 'ben_soliman_price', 'max_scrapped']].max(axis=1)\n",
    "    \n",
    "    # Calculate combined median (average of available medians)\n",
    "    median_cols = ['mp_mod_price', 'ben_soliman_price', 'median_scrapped']\n",
    "    df['combined_median_market'] = df[median_cols].mean(axis=1, skipna=True)\n",
    "    \n",
    "    # Calculate mp_price_score\n",
    "    df['mp_price_score'] = (df['price'] - df['mp_min_price']) / (\n",
    "        df['mp_max_price'] - df['mp_min_price']\n",
    "    ).replace(0, np.nan)\n",
    "    \n",
    "    # Stock comment\n",
    "    def get_stock_comment(row):\n",
    "        if row['in_stock_perc'] == 0:\n",
    "            return 'OOS'\n",
    "        elif row['doh'] > 30:\n",
    "            return 'Over Stocked'\n",
    "        elif row['doh'] < 1:\n",
    "            return 'low stock'\n",
    "        else:\n",
    "            return 'Good stocks'\n",
    "    \n",
    "    df['stock_comment'] = df.apply(get_stock_comment, axis=1)\n",
    "    \n",
    "    # RR comment\n",
    "    def get_rr_comment(row):\n",
    "        cu_rr = row['cu_rr']\n",
    "        high_rr = row['high_rr']\n",
    "        std = row['qty_std']\n",
    "        \n",
    "        if cu_rr >= high_rr - 0.5 * std and cu_rr <= high_rr + 0.5 * std:\n",
    "            return 'Normal rr'\n",
    "        elif cu_rr < high_rr - 0.5 * std:\n",
    "            return 'low rr'\n",
    "        elif cu_rr >= high_rr + 0.5 * std and cu_rr <= high_rr + 1.5 * std:\n",
    "            return 'High rr'\n",
    "        elif cu_rr > high_rr + 1.5 * std:\n",
    "            return 'Very High rr'\n",
    "        else:\n",
    "            return ''\n",
    "    \n",
    "    df['rr_comment'] = df.apply(get_rr_comment, axis=1)\n",
    "    \n",
    "    # Rets comment\n",
    "    def get_rets_comment(row):\n",
    "        cu_rets = row['cu_rets']\n",
    "        high_rets = row['high_rets']\n",
    "        rets_std = row['rets_std']\n",
    "        \n",
    "        if cu_rets >= high_rets - 0.5 * rets_std and cu_rets <= high_rets + 0.5 * rets_std:\n",
    "            return 'Normal rets'\n",
    "        elif cu_rets < high_rets - 0.5 * rets_std:\n",
    "            return 'low rets'\n",
    "        elif cu_rets >= high_rets + 0.5 * rets_std and cu_rets <= high_rets + 1.5 * rets_std:\n",
    "            return 'High rets'\n",
    "        elif cu_rets > high_rets + 1.5 * rets_std:\n",
    "            return 'Very High rets'\n",
    "        else:\n",
    "            return ''\n",
    "    \n",
    "    df['rets_comment'] = df.apply(get_rets_comment, axis=1)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "66b9320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 16: FINAL SCORING - MARKET POSITION & PRICE COMMENTS\n",
    "# =============================================================================\n",
    "\n",
    "def add_final_scoring(df):\n",
    "    \"\"\"\n",
    "    Add final scoring columns: combined_price_score, market_position_status, price_comment.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Combined price score\n",
    "    def calc_combined_price_score(row):\n",
    "        combined_max = row['combined_max_market']\n",
    "        combined_min = row['combined_min_market']\n",
    "        price = row['price']\n",
    "        mp_score = row['mp_price_score']\n",
    "        \n",
    "        if pd.notna(combined_max) and combined_max > 0 and pd.notna(combined_min) and combined_min < 1e9:\n",
    "            if combined_max != combined_min:\n",
    "                return (price - combined_min) / (combined_max - combined_min)\n",
    "        return mp_score\n",
    "    \n",
    "    df['combined_price_score'] = df.apply(calc_combined_price_score, axis=1)\n",
    "    \n",
    "    # Market position status\n",
    "    def get_market_position(row):\n",
    "        price = row['price']\n",
    "        combined_min = row['combined_min_market']\n",
    "        combined_median = row['combined_median_market']\n",
    "        combined_max = row['combined_max_market']\n",
    "        mp_min = row['mp_min_price']\n",
    "        bsp = row['ben_soliman_price']\n",
    "        median_scr = row['median_scrapped']\n",
    "        \n",
    "        # Check if no market data\n",
    "        if (pd.isna(combined_median) and pd.isna(mp_min) and \n",
    "            pd.isna(bsp) and pd.isna(median_scr)):\n",
    "            return 'No Market Data'\n",
    "        \n",
    "        # Adjust for edge cases\n",
    "        min_val = combined_min if pd.notna(combined_min) and combined_min < 1e9 else None\n",
    "        max_val = combined_max if pd.notna(combined_max) and combined_max > 0 else None\n",
    "        \n",
    "        if min_val is not None:\n",
    "            if price < min_val:\n",
    "                return 'Below Market'\n",
    "            elif price <= min_val * 1.005:\n",
    "                return 'At Market Min'\n",
    "        \n",
    "        if pd.notna(combined_median):\n",
    "            if price < combined_median * 0.995:\n",
    "                return 'Below Median'\n",
    "            elif price <= combined_median * 1.005:\n",
    "                return 'At Median'\n",
    "        \n",
    "        if max_val is not None:\n",
    "            if price < max_val * 0.995:\n",
    "                return 'Above Median'\n",
    "            elif price <= max_val * 1.005:\n",
    "                return 'At Market Max'\n",
    "            elif price > max_val * 1.005:\n",
    "                return 'Above Market'\n",
    "        \n",
    "        return 'At Median'\n",
    "    \n",
    "    df['market_position_status'] = df.apply(get_market_position, axis=1)\n",
    "    \n",
    "    # Price comment\n",
    "    def get_price_comment(row):\n",
    "        combined_min = row['combined_min_market']\n",
    "        combined_max = row['combined_max_market']\n",
    "        price = row['price']\n",
    "        bm = row['bm']\n",
    "        target = row['target_margin']\n",
    "        mp_score = row['mp_price_score']\n",
    "        \n",
    "        # Calculate price score\n",
    "        if pd.notna(combined_max) and pd.notna(combined_min) and combined_max != combined_min:\n",
    "            price_score = (price - combined_min) / (combined_max - combined_min)\n",
    "        else:\n",
    "            price_score = mp_score\n",
    "        \n",
    "        if pd.isna(price_score):\n",
    "            if pd.notna(bm) and pd.notna(target):\n",
    "                return 'below target' if bm < target else 'above target'\n",
    "            return 'above target'\n",
    "        \n",
    "        # price_score >= 0: at or above market minimum\n",
    "        if price_score >= 0 and bm > target:\n",
    "            return 'High price'\n",
    "        elif price_score >= 0 and bm < target:\n",
    "            return 'Credit note'\n",
    "        # price_score < 0: below market minimum\n",
    "        elif price_score < 0 and bm < target:\n",
    "            return 'Low Price'\n",
    "        elif price_score < 0 and bm > target:\n",
    "            return 'room to reduce'\n",
    "        elif bm < target:\n",
    "            return 'below target'\n",
    "        else:\n",
    "            return 'above target'\n",
    "    \n",
    "    df['price_comment'] = df.apply(get_price_comment, axis=1)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "931d1969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 17: ACTION CLASSIFICATION LOGIC\n",
    "# =============================================================================\n",
    "\n",
    "def determine_action(row):\n",
    "    \"\"\"\n",
    "    Determine recommended action based on stock, price, and RR status.\n",
    "    This replicates the complex CASE statement in the final SELECT.\n",
    "    \"\"\"\n",
    "    stock_comment = row['stock_comment']\n",
    "    price_comment = row['price_comment']\n",
    "    rr_comment = row['rr_comment']\n",
    "    offers_perc = row.get('offers_perc', 0) or 0\n",
    "    commercial_min = row.get('commercial_min')\n",
    "    bm = row['bm']\n",
    "    target = row['target_margin']\n",
    "    cu_rr = row['cu_rr']\n",
    "    today_rr = row['today_rr']\n",
    "    stocks = row['stocks']\n",
    "    activation = row.get('activation', True)\n",
    "    oos_yesterday = row.get('oos_yesterday', 0)\n",
    "    \n",
    "    # OOS - always needs purchase regardless of other conditions\n",
    "    if stock_comment == 'OOS':\n",
    "        return 'Purchase'\n",
    "    \n",
    "    # If product was OOS yesterday and has low rr, no action needed\n",
    "    # (low rr is expected when product was out of stock)\n",
    "    if rr_comment == 'low rr' and oos_yesterday == 1:\n",
    "        return 'No action'\n",
    "    \n",
    "    # Good stocks scenarios\n",
    "    if stock_comment == 'Good stocks':\n",
    "        if price_comment in ['Low Price', 'below target'] and rr_comment == 'low rr':\n",
    "            if offers_perc < 0.1:\n",
    "                return 'Offers & Credit Note'\n",
    "            return 'Credit Note'\n",
    "        if price_comment in ['Low Price', 'below target'] and rr_comment != 'low rr':\n",
    "            return 'Increase price'\n",
    "        if price_comment == 'Credit note' and rr_comment == 'low rr':\n",
    "            return 'Credit Note'\n",
    "        # With market data: price position known - reduce price\n",
    "        if price_comment in ['High price', 'room to reduce'] and rr_comment == 'low rr':\n",
    "            if pd.isna(commercial_min):\n",
    "                return 'Reduce price'\n",
    "            return 'Remove commercial min'\n",
    "        # No market data: only margin > target - check offers first\n",
    "        if price_comment == 'above target' and rr_comment == 'low rr':\n",
    "            if offers_perc < 0.1:\n",
    "                return 'Offers'\n",
    "            if pd.isna(commercial_min):\n",
    "                return 'Reduce price'\n",
    "            return 'Remove commercial min'\n",
    "        if rr_comment == 'Normal rr':\n",
    "            return 'No action'\n",
    "        if rr_comment == 'Very High rr' and bm < target:\n",
    "            return 'Increase price'\n",
    "        if rr_comment in ['Very High rr', 'High rr'] and bm >= target:\n",
    "            return 'No action'\n",
    "        if rr_comment == 'High rr' and bm < target:\n",
    "            return 'Increase price a bit'\n",
    "    \n",
    "    # Low stock scenarios\n",
    "    if stock_comment == 'low stock':\n",
    "        if price_comment == 'Credit note' and rr_comment == 'low rr':\n",
    "            return 'Purchase & Credit Note'\n",
    "        # No market data: margin < target - need purchase + credit note\n",
    "        if price_comment == 'below target' and rr_comment == 'low rr':\n",
    "            return 'Purchase & Credit Note'\n",
    "        # With market data: price < min, margin < target\n",
    "        if price_comment == 'Low Price' and rr_comment == 'low rr':\n",
    "            if offers_perc < 0.1:\n",
    "                return 'Purchase & Offers & Credit Note'\n",
    "            return 'Purchase & Credit Note'\n",
    "        # With market data: price position known - purchase + reduce price\n",
    "        if price_comment in ['High price', 'room to reduce'] and rr_comment == 'low rr':\n",
    "            if pd.isna(commercial_min):\n",
    "                return 'Purchase & Reduce price'\n",
    "            return 'Purchase & Remove commercial min'\n",
    "        # No market data: margin > target - check offers first\n",
    "        if price_comment == 'above target' and rr_comment == 'low rr':\n",
    "            if offers_perc < 0.1:\n",
    "                return 'Purchase & Offers'\n",
    "            if pd.isna(commercial_min):\n",
    "                return 'Purchase & Reduce price'\n",
    "            return 'Purchase & Remove commercial min'\n",
    "        if rr_comment in ['High rr', 'Normal rr']:\n",
    "            return 'Purchase'\n",
    "        if rr_comment == 'Very High rr':\n",
    "            return 'Purchase & Increase price'\n",
    "    \n",
    "    # Over stocked scenarios\n",
    "    if stock_comment == 'Over Stocked':\n",
    "        if price_comment in ['below target', 'Low Price', 'Credit note'] and rr_comment == 'low rr':\n",
    "            return 'Credit Note'\n",
    "        # With market data: price position known\n",
    "        if price_comment in ['High price', 'room to reduce'] and rr_comment == 'low rr':\n",
    "            if cu_rr > 0:\n",
    "                if pd.isna(commercial_min):\n",
    "                    return 'Reduce price'\n",
    "                return 'Remove commercial min'\n",
    "            elif today_rr == 0:\n",
    "                if activation == False:\n",
    "                    return 'Reactivate'\n",
    "                else:  # activation == True\n",
    "                    if pd.isna(commercial_min):\n",
    "                        return 'Reduce price'\n",
    "                    return 'Remove commercial min'\n",
    "            else:\n",
    "                # cu_rr <= 0 but today_rr > 0: sales recovering, no action needed\n",
    "                return 'No action'\n",
    "        # No market data: margin > target - check offers first\n",
    "        if price_comment == 'above target' and rr_comment == 'low rr':\n",
    "            if cu_rr > 0:\n",
    "                if offers_perc < 0.1:\n",
    "                    return 'Offers'\n",
    "                if pd.isna(commercial_min):\n",
    "                    return 'Reduce price'\n",
    "                return 'Remove commercial min'\n",
    "            elif today_rr == 0:\n",
    "                if activation == False:\n",
    "                    return 'Reactivate'\n",
    "                else:  # activation == True\n",
    "                    if offers_perc < 0.1:\n",
    "                        return 'Offers'\n",
    "                    if pd.isna(commercial_min):\n",
    "                        return 'Reduce price'\n",
    "                    return 'Remove commercial min'\n",
    "            else:\n",
    "                # cu_rr <= 0 but today_rr > 0: sales recovering, no action needed\n",
    "                return 'No action'\n",
    "        if price_comment in ['below target', 'Low Price', 'Credit note'] and rr_comment in ['Very High rr', 'High rr', 'Normal rr']:\n",
    "            if stocks / (cu_rr if cu_rr > 0 else 1) < 30:\n",
    "                return 'No Action'\n",
    "            return 'Credit Note'\n",
    "        if price_comment in ['High price', 'above target'] and rr_comment in ['Very High rr', 'High rr', 'Normal rr']:\n",
    "            if stocks / (cu_rr if cu_rr > 0 else 1) < 30:\n",
    "                return 'No Action'\n",
    "            return 'Reduce Price'\n",
    "    \n",
    "    # Additional edge cases\n",
    "    if price_comment in ['below target', 'Low Price'] and rr_comment == 'low rr':\n",
    "        if cu_rr == 0 and today_rr > 0:\n",
    "            return 'No action'\n",
    "        elif cu_rr == 0:\n",
    "            if activation == False:\n",
    "                return 'Reactivate'\n",
    "            else:  # activation == True\n",
    "                return 'Credit Note'\n",
    "    \n",
    "    # Edge case for above target with no running rate\n",
    "    if price_comment == 'above target' and rr_comment == 'low rr':\n",
    "        if cu_rr == 0 and today_rr > 0:\n",
    "            return 'No action'\n",
    "        elif cu_rr == 0:\n",
    "            if activation == False:\n",
    "                return 'Reactivate'\n",
    "            else:  # activation == True\n",
    "                if offers_perc < 0.1:\n",
    "                    return 'Offers'\n",
    "                return 'Reduce price'\n",
    "    \n",
    "    return None\n",
    "\n",
    "def add_actions(df):\n",
    "    \"\"\"Add action and team assignment columns.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Determine action\n",
    "    df['action'] = df.apply(determine_action, axis=1)\n",
    "    \n",
    "    # Assign to teams based on action\n",
    "    df['pricing_team'] = df['action'].apply(\n",
    "        lambda x: 1 if pd.notna(x) and ('price' in str(x).lower() or 'offers' in str(x).lower()) else None\n",
    "    )\n",
    "    df['purchase_team'] = df['action'].apply(\n",
    "        lambda x: 1 if pd.notna(x) and 'purchase' in str(x).lower() else None\n",
    "    )\n",
    "    df['commercial_team'] = df['action'].apply(\n",
    "        lambda x: 1 if pd.notna(x) and ('credit note' in str(x).lower() or \n",
    "                                         'commercial min' in str(x).lower() or \n",
    "                                         'reactivate' in str(x).lower() or\n",
    "                                         'supplier' in str(x).lower()) else None\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_stock_issue_owner(df):\n",
    "    \"\"\"\n",
    "    Determine who is responsible for stock issues (OOS/Low stock).\n",
    "    Only applies to Low stock and OOS products.\n",
    "    \n",
    "    Logic:\n",
    "    - If ordered_qty is low (< 3*cu_rr or < 0.9*high_rr if cu_rr=0) → Purchase team\n",
    "    - If in top 60% of positive NMV gap AND no_last_15 > 0 AND ordered in last 2 days → Commercial team\n",
    "    - If not ordered in last 2 days → Purchase team\n",
    "    \n",
    "    Also updates the action based on the issue ownership.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Calculate minimum required order qty\n",
    "    df['min_required_qty'] = df.apply(\n",
    "        lambda row: 3 * row['cu_rr'] if row['cu_rr'] > 0 else 0.9 * row['high_rr'],\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Check if ordered qty is low\n",
    "    df['ordered_qty_low'] = df.apply(\n",
    "        lambda row: (pd.notna(row['ordered_qty']) and row['ordered_qty'] < row['min_required_qty']),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Calculate days since last PO\n",
    "    today = pd.Timestamp.now().normalize()\n",
    "    df['days_since_po'] = df['last_po_date'].apply(\n",
    "        lambda x: (today - x).days if pd.notna(x) else None\n",
    "    )\n",
    "    \n",
    "    # Calculate nmv_gap: (high_rr * price) - (cu_rr * price)\n",
    "    df['nmv_gap'] = (df['high_rr'] * df['price']) - (df['cu_rr'] * df['price'])\n",
    "    print(df.columns)\n",
    "    # Calculate cumulative contribution of positive NMV gap\n",
    "    # Only consider positive gaps (behind target)\n",
    "    df['positive_nmv_gap'] = df['nmv_gap'].apply(lambda x: max(0, x) if pd.notna(x) else 0)\n",
    "    \n",
    "    # Sort by positive gap descending and calculate cumulative contribution\n",
    "    df_sorted = df.sort_values('positive_nmv_gap', ascending=False).copy()\n",
    "    total_positive_gap = df_sorted['positive_nmv_gap'].sum()\n",
    "    \n",
    "    if total_positive_gap > 0:\n",
    "        df_sorted['cumulative_gap'] = df_sorted['positive_nmv_gap'].cumsum()\n",
    "        df_sorted['cumulative_gap_pct'] = df_sorted['cumulative_gap'] / total_positive_gap\n",
    "        # Mark products in top 60% of gap contribution\n",
    "        df_sorted['in_top_60_gap'] = df_sorted['cumulative_gap_pct'] <= 0.6\n",
    "    else:\n",
    "        df_sorted['in_top_60_gap'] = False\n",
    "    \n",
    "    # Merge back the in_top_60_gap flag\n",
    "    df = df.merge(\n",
    "        df_sorted[['product_id', 'warehouse_id', 'in_top_60_gap']],\n",
    "        on=['product_id', 'warehouse_id'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Determine stock issue owner and update action (only for OOS and low stock)\n",
    "    def get_stock_issue_info(row):\n",
    "        stock_comment = row['stock_comment']\n",
    "        \n",
    "        # Only applies to OOS and low stock\n",
    "        if stock_comment not in ['OOS', 'low stock']:\n",
    "            return None, row['action']\n",
    "        \n",
    "        ordered_qty = row.get('ordered_qty')\n",
    "        days_since_po = row.get('days_since_po')\n",
    "        no_last_15 = row.get('no_last_15', 0) or 0\n",
    "        in_top_60_gap = row.get('in_top_60_gap', False)\n",
    "        ordered_qty_low = row.get('ordered_qty_low', False)\n",
    "        last_po_date = row.get('last_po_date')\n",
    "        min_required_qty = row.get('min_required_qty', 0)\n",
    "        \n",
    "        # Format last_po_date for display\n",
    "        last_po_str = last_po_date.strftime('%Y-%m-%d') if pd.notna(last_po_date) else 'Never'\n",
    "        \n",
    "        # If not ordered in last 2 days → Purchase team - need to place order\n",
    "        if pd.isna(days_since_po) or days_since_po > 2:\n",
    "            owner = 'Purchase team'\n",
    "            action = f'Purchase (last order: {last_po_str})'\n",
    "            return owner, action\n",
    "        \n",
    "        # If ordered qty is low → Purchase team - ordered but not enough\n",
    "        if ordered_qty_low:\n",
    "            owner = 'Purchase team'\n",
    "            action = f'Purchase (ordered qty {int(ordered_qty)} is low, need {int(min_required_qty)})'\n",
    "            return owner, action\n",
    "        \n",
    "        # If in top 60% gap AND multiple no confirmations AND ordered recently → Commercial team\n",
    "        if in_top_60_gap and no_last_15 > 0 and days_since_po <= 2:\n",
    "            owner = 'Commercial team'\n",
    "            action = f'Supplier issue ({int(no_last_15)} rejections) - negotiate with supplier'\n",
    "            return owner, action\n",
    "        \n",
    "        # Default: Purchase team (ordered but other issues)\n",
    "        owner = 'Purchase team'\n",
    "        action = f'Purchase (last order: {last_po_str})'\n",
    "        return owner, action\n",
    "    \n",
    "    # Apply the function to get stock issue info\n",
    "    df['_stock_issue_info'] = df.apply(get_stock_issue_info, axis=1)\n",
    "    \n",
    "    # Extract owner and action from the tuple\n",
    "    df['stock_issue_owner'] = df['_stock_issue_info'].apply(lambda x: x[0] if x else None)\n",
    "    df['_new_action'] = df['_stock_issue_info'].apply(lambda x: x[1] if x else None)\n",
    "    \n",
    "    # Update action only for OOS/low stock products\n",
    "    mask = df['stock_comment'].isin(['OOS', 'low stock'])\n",
    "    df.loc[mask, 'action'] = df.loc[mask, '_new_action']\n",
    "    \n",
    "    # Clean up temporary columns\n",
    "    df = df.drop(columns=['min_required_qty', 'ordered_qty_low', 'positive_nmv_gap', 'in_top_60_gap', '_stock_issue_info', '_new_action'], errors='ignore')\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "015fd88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 18: FINALIZE OUTPUT & ADD CALCULATED COLUMNS\n",
    "# =============================================================================\n",
    "\n",
    "def finalize_output(df, df_targets):\n",
    "    \"\"\"\n",
    "    Finalize the output DataFrame with all calculated columns.\n",
    "    Add stock value, stock contribution, and join targets data.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Calculate stock value\n",
    "    df['stock_value'] = df['stocks'] * df['price']\n",
    "    \n",
    "    # Calculate stock contribution per warehouse\n",
    "    df['stock_cntrb'] = df.groupby('warehouse_id')['stock_value'].transform(\n",
    "        lambda x: x / x.sum() if x.sum() > 0 else 0\n",
    "    )\n",
    "    \n",
    "    # Join targets data\n",
    "    df = df.merge(\n",
    "        df_targets[['warehouse_id', 'product_id', 'wh_sku_target', 'rem_nmv']],\n",
    "        on=['warehouse_id', 'product_id'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Clean up combined_min_market (replace inf with None)\n",
    "    df['combined_min_market'] = df['combined_min_market'].replace([np.inf, -np.inf, 1e9], np.nan)\n",
    "    df['combined_max_market'] = df['combined_max_market'].replace([0, np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Select and order final columns\n",
    "    final_columns = [\n",
    "        'region', 'wh', 'warehouse_id', 'product_id', 'sku', 'cat', 'brand',\n",
    "        'stocks', 'doh', 'stock_comment','wac_p', 'price', 'bm', 'target_margin', 'price_comment',\n",
    "        'mp_min_price', 'mp_mod_price', 'mp_max_price', 'ben_soliman_price',\n",
    "        'min_scrapped', 'median_scrapped', 'max_scrapped',\n",
    "        'combined_min_market', 'combined_median_market', 'combined_max_market',\n",
    "        'mp_price_score', 'combined_price_score', 'market_position_status',\n",
    "        'high_rr', 'cu_rr', 'today_rr', 'rr_comment',\n",
    "        'high_rets', 'cu_rets', 'rets_comment', 'offers_perc', 'commercial_min',\n",
    "        'action', 'pricing_team', 'purchase_team', 'commercial_team', 'activation', 'oos_yesterday',\n",
    "        'last_po_date', 'ordered_qty', 'confirmation_status', 'no_last_15', 'days_since_po', 'stock_issue_owner',\n",
    "        'stock_value', 'stock_cntrb', 'wh_sku_target', 'rem_nmv'\n",
    "    ]\n",
    "    \n",
    "    # Keep only columns that exist\n",
    "    existing_cols = [c for c in final_columns if c in df.columns]\n",
    "    df = df[existing_cols]\n",
    "    \n",
    "    # Sort by high_rr * price descending\n",
    "    df['_sort_key'] = df['high_rr'] * df['price']\n",
    "    df = df.sort_values('_sort_key', ascending=False).drop('_sort_key', axis=1)\n",
    "    \n",
    "    # Rename 'wh' to 'warehouse_name' for clarity\n",
    "    df = df.rename(columns={'wh': 'warehouse_name', 'cu_rr': 'current_rr'})\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "abdb29c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PRICING STATUS ANALYSIS\n",
      "============================================================\n",
      "\n",
      "[1/15] Loading warehouse mapping...\n",
      "       ✓ 12 warehouses loaded\n",
      "\n",
      "[2/15] Fetching COGS data...\n",
      "       ✓ 8080 COGS records fetched\n",
      "\n",
      "[3/15] Fetching stock data...\n",
      "       ✓ 1802102 stock records fetched\n",
      "\n",
      "[4/15] Fetching sales data...\n",
      "       ✓ 33168 sales records fetched\n",
      "\n",
      "[5/15] Fetching price data...\n",
      "       ✓ 192496 price records fetched\n",
      "\n",
      "[6/15] Fetching marketplace prices...\n",
      "       ✓ 7875 marketplace price records fetched\n",
      "\n",
      "[7/15] Fetching Ben Soliman prices...\n",
      "       ✓ 1371 Ben Soliman price records fetched\n",
      "\n",
      "[8/15] Fetching scrapped market prices...\n",
      "       ✓ 5583 scrapped price records fetched\n",
      "\n",
      "[9/15] Fetching category/brand targets...\n",
      "       ✓ 482 category/brand targets fetched\n",
      "\n",
      "[10/15] Fetching discounted sales data...\n",
      "       ✓ 10156 discounted sales records fetched\n",
      "\n",
      "[11/15] Fetching commercial constraints...\n",
      "       ✓ 709 commercial constraint records fetched\n",
      "\n",
      "[12/15] Fetching warehouse SKU targets...\n",
      "       ✓ 27267 target records fetched\n",
      "\n",
      "[13/15] Fetching product activation status...\n",
      "       ✓ 27751 activation records fetched\n",
      "\n",
      "[14/15] Fetching OOS yesterday status...\n",
      "       ✓ 1871950 OOS yesterday records fetched\n",
      "\n",
      "[15/15] Fetching purchase order data...\n",
      "       ✓ 18708 PO records fetched\n",
      "\n",
      "------------------------------------------------------------\n",
      "PROCESSING DATA...\n",
      "------------------------------------------------------------\n",
      "\n",
      "[A] Building product metrics...\n",
      "    ✓ 28439 product-warehouse combinations\n",
      "\n",
      "[B] Adding scoring and classification...\n",
      "    ✓ Scoring added\n",
      "\n",
      "[C] Adding final scoring (market position, price comments)...\n",
      "    ✓ Final scoring added\n",
      "\n",
      "[D] Determining recommended actions...\n",
      "    ✓ Actions determined\n",
      "\n",
      "[E] Determining stock issue ownership...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'nmv_gap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'nmv_gap'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11035/704412592.py\u001b[0m in \u001b[0;36m<cell line: 138>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;31m# Run the full analysis:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m \u001b[0mdf_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_pricing_status_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0mdf_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_11035/704412592.py\u001b[0m in \u001b[0;36mrun_pricing_status_analysis\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;31m# Add stock issue owner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n[E] Determining stock issue ownership...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0mdf_with_stock_owner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_stock_issue_owner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_with_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"    ✓ Stock issue ownership determined\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_11035/2609030935.py\u001b[0m in \u001b[0;36madd_stock_issue_owner\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;31m# Calculate cumulative contribution of positive NMV gap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;31m# Only consider positive gaps (behind target)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'positive_nmv_gap'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nmv_gap'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;31m# Sort by positive gap descending and calculate cumulative contribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4088\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4089\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4090\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4091\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4092\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'nmv_gap'"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PART 19: MAIN EXECUTION - RUN THE COMPLETE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def run_pricing_status_analysis():\n",
    "    \"\"\"\n",
    "    Main function to run the complete pricing status analysis.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with all pricing status metrics and recommended actions.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"PRICING STATUS ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Get warehouse mapping\n",
    "    print(\"\\n[1/15] Loading warehouse mapping...\")\n",
    "    df_whs = get_warehouse_mapping()\n",
    "    print(f\"       ✓ {len(df_whs)} warehouses loaded\")\n",
    "    \n",
    "    # Step 2: Fetch COGS data\n",
    "    print(\"\\n[2/15] Fetching COGS data...\")\n",
    "    df_cogs = fetch_current_cogs()\n",
    "    print(f\"       ✓ {len(df_cogs)} COGS records fetched\")\n",
    "    \n",
    "    # Step 3: Fetch stocks data\n",
    "    print(\"\\n[3/15] Fetching stock data...\")\n",
    "    df_stocks = fetch_stocks()\n",
    "    print(f\"       ✓ {len(df_stocks)} stock records fetched\")\n",
    "    \n",
    "    # Step 4: Fetch sales data\n",
    "    print(\"\\n[4/15] Fetching sales data...\")\n",
    "    df_sales = fetch_sales()\n",
    "    print(f\"       ✓ {len(df_sales)} sales records fetched\")\n",
    "    \n",
    "    # Step 5: Fetch prices\n",
    "    print(\"\\n[5/15] Fetching price data...\")\n",
    "    df_prices = fetch_prices()\n",
    "    print(f\"       ✓ {len(df_prices)} price records fetched\")\n",
    "    \n",
    "    # Step 6: Fetch marketplace prices\n",
    "    print(\"\\n[6/15] Fetching marketplace prices...\")\n",
    "    df_mp = fetch_marketplace_prices()\n",
    "    print(f\"       ✓ {len(df_mp)} marketplace price records fetched\")\n",
    "    \n",
    "    # Step 7: Fetch Ben Soliman prices\n",
    "    print(\"\\n[7/15] Fetching Ben Soliman prices...\")\n",
    "    df_bsp = fetch_ben_soliman_prices()\n",
    "    print(f\"       ✓ {len(df_bsp)} Ben Soliman price records fetched\")\n",
    "    \n",
    "    # Step 8: Fetch scrapped prices\n",
    "    print(\"\\n[8/15] Fetching scrapped market prices...\")\n",
    "    df_scrapped = fetch_scrapped_prices()\n",
    "    print(f\"       ✓ {len(df_scrapped)} scrapped price records fetched\")\n",
    "    \n",
    "    # Step 9: Fetch targets\n",
    "    print(\"\\n[9/15] Fetching category/brand targets...\")\n",
    "    df_cat_brand_targets = fetch_cat_brand_targets()\n",
    "    df_cat_targets = fetch_cat_targets(df_cat_brand_targets)\n",
    "    print(f\"       ✓ {len(df_cat_brand_targets)} category/brand targets fetched\")\n",
    "    \n",
    "    # Step 10: Fetch discounted sales\n",
    "    print(\"\\n[10/15] Fetching discounted sales data...\")\n",
    "    df_discounted = fetch_discounted_sales()\n",
    "    print(f\"       ✓ {len(df_discounted)} discounted sales records fetched\")\n",
    "    \n",
    "    # Step 11: Fetch commercial constraints\n",
    "    print(\"\\n[11/15] Fetching commercial constraints...\")\n",
    "    df_commercial = fetch_commercial_constraints()\n",
    "    print(f\"       ✓ {len(df_commercial)} commercial constraint records fetched\")\n",
    "    \n",
    "    # Step 12: Fetch targets data\n",
    "    print(\"\\n[12/15] Fetching warehouse SKU targets...\")\n",
    "    df_targets = fetch_targets_data(df_whs)\n",
    "    print(f\"       ✓ {len(df_targets)} target records fetched\")\n",
    "    \n",
    "    # Step 13: Fetch product activation status\n",
    "    print(\"\\n[13/15] Fetching product activation status...\")\n",
    "    df_activation = fetch_product_activation()\n",
    "    print(f\"       ✓ {len(df_activation)} activation records fetched\")\n",
    "    \n",
    "    # Step 14: Fetch OOS yesterday status\n",
    "    print(\"\\n[14/15] Fetching OOS yesterday status...\")\n",
    "    df_oos_yesterday = fetch_oos_yesterday()\n",
    "    print(f\"       ✓ {len(df_oos_yesterday)} OOS yesterday records fetched\")\n",
    "    \n",
    "    # Step 15: Fetch PO data\n",
    "    print(\"\\n[15/15] Fetching purchase order data...\")\n",
    "    df_po_data = fetch_po_data()\n",
    "    print(f\"       ✓ {len(df_po_data)} PO records fetched\")\n",
    "    \n",
    "    # Process and merge data\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"PROCESSING DATA...\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Build product metrics\n",
    "    print(\"\\n[A] Building product metrics...\")\n",
    "    df_metrics = build_product_metrics(\n",
    "        df_stocks, df_sales, df_whs, df_prices, df_cogs,\n",
    "        df_mp, df_bsp, df_scrapped, df_cat_brand_targets,\n",
    "        df_cat_targets, df_discounted, df_activation, df_oos_yesterday, df_po_data\n",
    "    )\n",
    "    print(f\"    ✓ {len(df_metrics)} product-warehouse combinations\")\n",
    "    \n",
    "    # Add scoring and classification\n",
    "    print(\"\\n[B] Adding scoring and classification...\")\n",
    "    df_scored = add_scoring_classification(df_metrics, df_commercial)\n",
    "    print(f\"    ✓ Scoring added\")\n",
    "    \n",
    "    # Add final scoring\n",
    "    print(\"\\n[C] Adding final scoring (market position, price comments)...\")\n",
    "    df_final_scored = add_final_scoring(df_scored)\n",
    "    print(f\"    ✓ Final scoring added\")\n",
    "    \n",
    "    # Add actions\n",
    "    print(\"\\n[D] Determining recommended actions...\")\n",
    "    df_with_actions = add_actions(df_final_scored)\n",
    "    print(f\"    ✓ Actions determined\")\n",
    "    \n",
    "    # Add stock issue owner\n",
    "    print(\"\\n[E] Determining stock issue ownership...\")\n",
    "    df_with_stock_owner = add_stock_issue_owner(df_with_actions)\n",
    "    print(f\"    ✓ Stock issue ownership determined\")\n",
    "    \n",
    "    # Finalize output\n",
    "    print(\"\\n[F] Finalizing output...\")\n",
    "    df_final = finalize_output(df_with_stock_owner, df_targets)\n",
    "    print(f\"    ✓ Final output ready with {len(df_final)} records\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ANALYSIS COMPLETE!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "# Run the full analysis:\n",
    "df_result = run_pricing_status_analysis()\n",
    "df_result.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c646d27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 20: UTILITY FUNCTIONS - EXPORT & SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "def export_results(df, filename='pricing_status_output.xlsx'):\n",
    "    \"\"\"Export results to Excel file.\"\"\"\n",
    "    df.to_excel(filename, index=False)\n",
    "    print(f\"Results exported to {filename}\")\n",
    "    return filename\n",
    "\n",
    "def get_summary_stats(df):\n",
    "    \"\"\"Generate summary statistics from the analysis results.\"\"\"\n",
    "    summary = {\n",
    "        'Total SKU-Warehouse combinations': len(df),\n",
    "        'Unique Products': df['product_id'].nunique() if 'product_id' in df.columns else 0,\n",
    "        'Unique Warehouses': df['warehouse_id'].nunique() if 'warehouse_id' in df.columns else 0,\n",
    "    }\n",
    "    \n",
    "    # Stock status breakdown\n",
    "    if 'stock_comment' in df.columns:\n",
    "        stock_status = df['stock_comment'].value_counts().to_dict()\n",
    "        summary['Stock Status'] = stock_status\n",
    "    \n",
    "    # Action breakdown\n",
    "    if 'action' in df.columns:\n",
    "        action_counts = df['action'].value_counts().to_dict()\n",
    "        summary['Actions'] = action_counts\n",
    "    \n",
    "    # Team assignments\n",
    "    if 'pricing_team' in df.columns:\n",
    "        summary['Pricing Team Items'] = df['pricing_team'].notna().sum()\n",
    "    if 'purchase_team' in df.columns:\n",
    "        summary['Purchase Team Items'] = df['purchase_team'].notna().sum()\n",
    "    if 'commercial_team' in df.columns:\n",
    "        summary['Commercial Team Items'] = df['commercial_team'].notna().sum()\n",
    "    \n",
    "    # Market position breakdown\n",
    "    if 'market_position_status' in df.columns:\n",
    "        market_pos = df['market_position_status'].value_counts().to_dict()\n",
    "        summary['Market Position'] = market_pos\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def print_summary(summary):\n",
    "    \"\"\"Pretty print the summary statistics.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SUMMARY STATISTICS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for key, value in summary.items():\n",
    "        if isinstance(value, dict):\n",
    "            print(f\"\\n{key}:\")\n",
    "            for k, v in value.items():\n",
    "                print(f\"    {k}: {v}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# Usage:\n",
    "summary = get_summary_stats(df_result)\n",
    "print_summary(summary)\n",
    "export_results(df_result, 'pricing_status_output.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988918c0",
   "metadata": {},
   "source": [
    "# Pricing Status Analysis - Quick Reference\n",
    "\n",
    "## Data Flow Overview:\n",
    "\n",
    "1. **Static Data**: Warehouse mappings (region, cohort_id)\n",
    "2. **COGS**: Current cost of goods (wac_p)\n",
    "3. **Running Rates**: Predicted running rates from past 14 days\n",
    "4. **Stocks**: Available stock with DOH calculations\n",
    "5. **Sales**: 150-day sales history with percentile metrics\n",
    "6. **Prices**: Latest cohort pricing\n",
    "7. **Market Prices**: Min/Mod/Max from marketplace, Ben Soliman, and scraped data\n",
    "8. **Targets**: Category/brand margin targets\n",
    "9. **Discounts**: Bundle, SKU discount, quantity discount percentages\n",
    "10. **Commercial Constraints**: Minimum price restrictions\n",
    "\n",
    "## Key Metrics:\n",
    "\n",
    "| Metric | Description |\n",
    "|--------|-------------|\n",
    "| `doh` | Days on Hand (stocks / running_rate) |\n",
    "| `bm` | Basic Margin ((price - cost) / price) |\n",
    "| `high_rr` | 80th percentile of historical running rate |\n",
    "| `combined_price_score` | Position within market price range (0-1) |\n",
    "\n",
    "## Action Matrix:\n",
    "\n",
    "| Stock Status | Price Status | RR Status | Recommended Action |\n",
    "|--------------|--------------|-----------|-------------------|\n",
    "| OOS | - | - | Purchase |\n",
    "| Good stocks | Low/Below target | Low RR | Offers & Credit Note |\n",
    "| Good stocks | High | Low RR | Reduce price / Remove commercial min |\n",
    "| Low stock | - | Very High RR | Increase price |\n",
    "| Over Stocked | High | Low RR, cu_rr=0 | Check activation |\n",
    "\n",
    "## Configuration:\n",
    "\n",
    "To customize the analysis, modify:\n",
    "- `get_warehouse_mapping()` - Add/remove warehouses\n",
    "- `fetch_prices()` - Modify cohort_ids\n",
    "- `determine_action()` - Adjust action logic thresholds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a538dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RUN ANALYSIS - UNCOMMENT AND EXECUTE\n",
    "# =============================================================================\n",
    "\n",
    "# Run the full analysis:\n",
    "df_result = run_pricing_status_analysis()\n",
    "\n",
    "# View summary:\n",
    "summary = get_summary_stats(df_result)\n",
    "print_summary(summary)\n",
    "\n",
    "# Preview the data:\n",
    "df_result.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556310e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPORT RESULTS TO EXCEL (Optional)\n",
    "# =============================================================================\n",
    "\n",
    "# Export:\n",
    "export_results(df_result, 'pricing_status_output.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2595bbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# AGGREGATE ANALYSIS VIEW\n",
    "# =============================================================================\n",
    "\n",
    "def create_aggregate_analysis(df):\n",
    "    \"\"\"\n",
    "    Create aggregate analysis showing:\n",
    "    - Total target NMV (high_rr * price)\n",
    "    - Top dropping brands based on RR performance\n",
    "    - Market status breakdown by brand\n",
    "    - Required actions summary\n",
    "    \"\"\"\n",
    "    df_analysis = df.copy()\n",
    "    \n",
    "    # Calculate target NMV per row (high_rr * price)\n",
    "    df_analysis['target_nmv'] = df_analysis['high_rr'] * df_analysis['price']\n",
    "    \n",
    "    # Calculate RR drop percentage: (high_rr - current_rr) / high_rr\n",
    "    df_analysis['rr_drop_pct'] = (df_analysis['high_rr'] - df_analysis['current_rr']) / df_analysis['high_rr'].replace(0, np.nan)\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # 1. TOTAL TARGET NMV SUMMARY\n",
    "    # ==========================================================================\n",
    "    total_target_nmv = df_analysis['target_nmv'].sum()\n",
    "    total_current_nmv = (df_analysis['current_rr'] * df_analysis['price']).sum()\n",
    "    nmv_gap = total_target_nmv - total_current_nmv\n",
    "    nmv_gap_pct = nmv_gap / total_target_nmv * 100 if total_target_nmv > 0 else 0\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"📊 AGGREGATE ANALYSIS - PRICING STATUS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\n\" + \"─\" * 80)\n",
    "    print(\"💰 TOTAL NMV SUMMARY\")\n",
    "    print(\"─\" * 80)\n",
    "    print(f\"  Target NMV (High RR × Price):    {total_target_nmv:>15,.0f} EGP\")\n",
    "    print(f\"  Current NMV (Current RR × Price): {total_current_nmv:>15,.0f} EGP\")\n",
    "    print(f\"  NMV Gap:                          {nmv_gap:>15,.0f} EGP ({nmv_gap_pct:.1f}%)\")\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # 2. TOP DROPPING BRANDS ANALYSIS\n",
    "    # ==========================================================================\n",
    "    brand_agg = df_analysis.groupby('brand').agg({\n",
    "        'target_nmv': 'sum',\n",
    "        'high_rr': 'sum',\n",
    "        'current_rr': 'sum',\n",
    "        'price': 'mean',\n",
    "        'product_id': 'nunique',\n",
    "        'warehouse_id': 'nunique'\n",
    "    }).reset_index()\n",
    "    \n",
    "    brand_agg.columns = ['brand', 'target_nmv', 'total_high_rr', 'total_current_rr', \n",
    "                         'avg_price', 'num_products', 'num_warehouses']\n",
    "    \n",
    "    # Calculate current NMV and drop metrics\n",
    "    brand_agg['current_nmv'] = brand_agg['total_current_rr'] * brand_agg['avg_price']\n",
    "    brand_agg['nmv_drop'] = brand_agg['target_nmv'] - brand_agg['current_nmv']\n",
    "    brand_agg['rr_drop_pct'] = ((brand_agg['total_high_rr'] - brand_agg['total_current_rr']) / \n",
    "                                 brand_agg['total_high_rr'].replace(0, np.nan) * 100)\n",
    "    \n",
    "    # Sort by NMV drop (biggest drops first)\n",
    "    brand_agg_sorted = brand_agg.sort_values('nmv_drop', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + \"─\" * 80)\n",
    "    print(\"📉 TOP 15 DROPPING BRANDS (by NMV Gap)\")\n",
    "    print(\"─\" * 80)\n",
    "    \n",
    "    top_dropping = brand_agg_sorted.head(15)\n",
    "    print(f\"{'Brand':<30} {'Target NMV':>15} {'Current NMV':>15} {'NMV Drop':>15} {'RR Drop%':>10}\")\n",
    "    print(\"─\" * 85)\n",
    "    for _, row in top_dropping.iterrows():\n",
    "        print(f\"{str(row['brand'])[:30]:<30} {row['target_nmv']:>15,.0f} {row['current_nmv']:>15,.0f} \"\n",
    "              f\"{row['nmv_drop']:>15,.0f} {row['rr_drop_pct']:>9.1f}%\")\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # 3. MARKET STATUS BY DROPPING BRANDS\n",
    "    # ==========================================================================\n",
    "    # Get top 15 dropping brand names\n",
    "    top_dropping_brands = top_dropping['brand'].tolist()\n",
    "    \n",
    "    # Filter data to only include top dropping brands\n",
    "    df_top_brands = df_analysis[df_analysis['brand'].isin(top_dropping_brands)]\n",
    "    \n",
    "    # Market status breakdown for top dropping brands\n",
    "    market_status_by_brand = df_top_brands.groupby(['brand', 'market_position_status']).agg({\n",
    "        'target_nmv': 'sum',\n",
    "        'product_id': 'nunique'\n",
    "    }).reset_index()\n",
    "    \n",
    "    market_status_by_brand.columns = ['brand', 'market_position', 'target_nmv', 'num_skus']\n",
    "    \n",
    "    # Pivot for better view\n",
    "    market_pivot = market_status_by_brand.pivot_table(\n",
    "        index='brand', \n",
    "        columns='market_position', \n",
    "        values='num_skus', \n",
    "        fill_value=0\n",
    "    ).reset_index()\n",
    "    \n",
    "    print(\"\\n\" + \"─\" * 80)\n",
    "    print(\"🏪 MARKET POSITION STATUS (Top Dropping Brands - SKU Count)\")\n",
    "    print(\"─\" * 80)\n",
    "    print(market_pivot.to_string(index=False))\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # 4. REQUIRED ACTIONS BY BRAND\n",
    "    # ==========================================================================\n",
    "    actions_by_brand = df_top_brands.groupby(['brand', 'action']).agg({\n",
    "        'target_nmv': 'sum',\n",
    "        'product_id': 'nunique'\n",
    "    }).reset_index()\n",
    "    \n",
    "    actions_by_brand.columns = ['brand', 'action', 'target_nmv', 'num_skus']\n",
    "    \n",
    "    # Pivot actions\n",
    "    action_pivot = actions_by_brand.pivot_table(\n",
    "        index='brand',\n",
    "        columns='action',\n",
    "        values='num_skus',\n",
    "        fill_value=0\n",
    "    ).reset_index()\n",
    "    \n",
    "    print(\"\\n\" + \"─\" * 80)\n",
    "    print(\"⚡ REQUIRED ACTIONS (Top Dropping Brands - SKU Count)\")\n",
    "    print(\"─\" * 80)\n",
    "    print(action_pivot.to_string(index=False))\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # 5. ACTION SUMMARY FOR TOP DROPPING BRANDS\n",
    "    # ==========================================================================\n",
    "    action_summary = df_top_brands.groupby('action').agg({\n",
    "        'target_nmv': 'sum',\n",
    "        'product_id': 'nunique',\n",
    "        'brand': 'nunique'\n",
    "    }).reset_index()\n",
    "    \n",
    "    action_summary.columns = ['action', 'target_nmv_at_risk', 'num_skus', 'num_brands']\n",
    "    action_summary = action_summary.sort_values('target_nmv_at_risk', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + \"─\" * 80)\n",
    "    print(\"📋 ACTION PRIORITY SUMMARY (Top Dropping Brands)\")\n",
    "    print(\"─\" * 80)\n",
    "    print(f\"{'Action':<35} {'Target NMV at Risk':>18} {'# SKUs':>10} {'# Brands':>10}\")\n",
    "    print(\"─\" * 73)\n",
    "    for _, row in action_summary.iterrows():\n",
    "        action_name = str(row['action']) if pd.notna(row['action']) else 'No Action'\n",
    "        print(f\"{action_name[:35]:<35} {row['target_nmv_at_risk']:>18,.0f} {row['num_skus']:>10} {row['num_brands']:>10}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    \n",
    "    # Return dataframes for further analysis\n",
    "    return {\n",
    "        'total_metrics': {\n",
    "            'target_nmv': total_target_nmv,\n",
    "            'current_nmv': total_current_nmv,\n",
    "            'nmv_gap': nmv_gap,\n",
    "            'nmv_gap_pct': nmv_gap_pct\n",
    "        },\n",
    "        'brand_analysis': brand_agg_sorted,\n",
    "        'market_status_pivot': market_pivot,\n",
    "        'action_pivot': action_pivot,\n",
    "        'action_summary': action_summary\n",
    "    }\n",
    "\n",
    "# Run the aggregate analysis\n",
    "aggregate_results = create_aggregate_analysis(df_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b31069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DETAILED BRAND DRILLDOWN VIEW\n",
    "# =============================================================================\n",
    "\n",
    "def get_brand_drilldown(df, brand_name):\n",
    "    \"\"\"\n",
    "    Get detailed drilldown for a specific brand showing:\n",
    "    - All SKUs for the brand\n",
    "    - Their market status, RR status, and recommended actions\n",
    "    \"\"\"\n",
    "    df_brand = df[df['brand'] == brand_name].copy()\n",
    "    \n",
    "    if len(df_brand) == 0:\n",
    "        print(f\"No data found for brand: {brand_name}\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate target NMV\n",
    "    df_brand['target_nmv'] = df_brand['high_rr'] * df_brand['price']\n",
    "    df_brand['current_nmv'] = df_brand['current_rr'] * df_brand['price']\n",
    "    df_brand['nmv_gap'] = df_brand['target_nmv'] - df_brand['current_nmv']\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"🔍 BRAND DRILLDOWN: {brand_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Summary stats\n",
    "    print(f\"\\n📊 Summary:\")\n",
    "    print(f\"   Total SKUs: {df_brand['product_id'].nunique()}\")\n",
    "    print(f\"   Warehouses: {df_brand['warehouse_id'].nunique()}\")\n",
    "    print(f\"   Target NMV: {df_brand['target_nmv'].sum():,.0f} EGP\")\n",
    "    print(f\"   Current NMV: {df_brand['current_nmv'].sum():,.0f} EGP\")\n",
    "    print(f\"   NMV Gap: {df_brand['nmv_gap'].sum():,.0f} EGP\")\n",
    "    \n",
    "    # Show detailed SKU breakdown\n",
    "    columns_to_show = ['warehouse_name', 'sku', 'price', 'high_rr', 'current_rr', \n",
    "                       'stock_comment', 'market_position_status', 'price_comment', \n",
    "                       'rr_comment', 'action', 'nmv_gap']\n",
    "    \n",
    "    existing_cols = [c for c in columns_to_show if c in df_brand.columns]\n",
    "    \n",
    "    df_display = df_brand[existing_cols].sort_values('nmv_gap', ascending=False)\n",
    "    \n",
    "    print(f\"\\n📋 SKU Details (sorted by NMV Gap):\")\n",
    "    print(df_display.to_string(index=False))\n",
    "    \n",
    "    return df_brand\n",
    "\n",
    "# View top dropping brands DataFrame\n",
    "print(\"📈 TOP DROPPING BRANDS (Full DataFrame):\")\n",
    "aggregate_results['brand_analysis'].head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44476a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXAMPLE: DRILLDOWN INTO TOP DROPPING BRAND\n",
    "# =============================================================================\n",
    "\n",
    "# Get the top dropping brand name\n",
    "top_brand = aggregate_results['brand_analysis'].iloc[0]['brand']\n",
    "\n",
    "# Drilldown into the top dropping brand\n",
    "brand_detail = get_brand_drilldown(df_result, top_brand)\n",
    "\n",
    "# Or specify a brand manually:\n",
    "# brand_detail = get_brand_drilldown(df_result, \"Your Brand Name Here\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931c29cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPORT AGGREGATE ANALYSIS TO EXCEL\n",
    "# =============================================================================\n",
    "\n",
    "def get_team_sheet(df, team_flag_column, team_name):\n",
    "    \"\"\"\n",
    "    Get SKUs assigned to a specific team based on the team flag.\n",
    "    Sorted by NMV gap descending.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with all SKU data\n",
    "        team_flag_column: Column name for the team flag (e.g., 'pricing_team')\n",
    "        team_name: Name of the team for display\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame filtered and sorted for the team\n",
    "    \"\"\"\n",
    "    df_team = df.copy()\n",
    "    \n",
    "    # Calculate NMV metrics\n",
    "    df_team['target_nmv'] = df_team['high_rr'] * df_team['price']\n",
    "    df_team['current_nmv'] = df_team['current_rr'] * df_team['price']\n",
    "    df_team['nmv_gap'] = df_team['target_nmv'] - df_team['current_nmv']\n",
    "    df_team['margin_gap'] = df_team['bm'] - df_team['target_margin']\n",
    "    \n",
    "    # Filter by team flag = 1\n",
    "    df_team = df_team[df_team[team_flag_column] == 1].copy()\n",
    "    \n",
    "    # Sort by NMV gap descending\n",
    "    df_team = df_team.sort_values('nmv_gap', ascending=False)\n",
    "    \n",
    "    # Select relevant columns\n",
    "    columns_to_export = [\n",
    "        'region', 'warehouse_name', 'product_id', 'sku', 'cat', 'brand',\n",
    "        'stocks', 'doh', 'stock_comment', 'wac_p',\n",
    "        'price', 'bm', 'target_margin', 'margin_gap', 'price_comment',\n",
    "        'combined_min_market', 'combined_median_market', 'combined_max_market',\n",
    "        'market_position_status',\n",
    "        'high_rr', 'current_rr', 'rr_comment',\n",
    "        'action', 'offers_perc', 'commercial_min', 'activation',\n",
    "        'target_nmv', 'current_nmv', 'nmv_gap'\n",
    "    ]\n",
    "    \n",
    "    existing_cols = [c for c in columns_to_export if c in df_team.columns]\n",
    "    \n",
    "    return df_team[existing_cols]\n",
    "\n",
    "\n",
    "def get_team_summary(df):\n",
    "    \"\"\"\n",
    "    Create a summary showing each team's total assignments and NMV gap responsibility.\n",
    "    \"\"\"\n",
    "    df_summary = df.copy()\n",
    "    \n",
    "    # Calculate NMV metrics\n",
    "    df_summary['target_nmv'] = df_summary['high_rr'] * df_summary['price']\n",
    "    df_summary['current_nmv'] = df_summary['current_rr'] * df_summary['price']\n",
    "    df_summary['nmv_gap'] = df_summary['target_nmv'] - df_summary['current_nmv']\n",
    "    \n",
    "    # Calculate team metrics\n",
    "    teams_data = []\n",
    "    \n",
    "    # Pricing Team\n",
    "    pricing_df = df_summary[df_summary['pricing_team'] == 1]\n",
    "    teams_data.append({\n",
    "        'Team': 'Pricing Team',\n",
    "        'Total SKUs Assigned': len(pricing_df),\n",
    "        'Unique Products': pricing_df['product_id'].nunique(),\n",
    "        'Total Target NMV': pricing_df['target_nmv'].sum(),\n",
    "        'Total Current NMV': pricing_df['current_nmv'].sum(),\n",
    "        'Total NMV Gap': pricing_df['nmv_gap'].sum(),\n",
    "        'NMV Gap %': (pricing_df['nmv_gap'].sum() / pricing_df['target_nmv'].sum() * 100) if pricing_df['target_nmv'].sum() > 0 else 0\n",
    "    })\n",
    "    \n",
    "    # Purchase Team\n",
    "    purchase_df = df_summary[df_summary['purchase_team'] == 1]\n",
    "    teams_data.append({\n",
    "        'Team': 'Purchase Team',\n",
    "        'Total SKUs Assigned': len(purchase_df),\n",
    "        'Unique Products': purchase_df['product_id'].nunique(),\n",
    "        'Total Target NMV': purchase_df['target_nmv'].sum(),\n",
    "        'Total Current NMV': purchase_df['current_nmv'].sum(),\n",
    "        'Total NMV Gap': purchase_df['nmv_gap'].sum(),\n",
    "        'NMV Gap %': (purchase_df['nmv_gap'].sum() / purchase_df['target_nmv'].sum() * 100) if purchase_df['target_nmv'].sum() > 0 else 0\n",
    "    })\n",
    "    \n",
    "    # Commercial Team\n",
    "    commercial_df = df_summary[df_summary['commercial_team'] == 1]\n",
    "    teams_data.append({\n",
    "        'Team': 'Commercial Team',\n",
    "        'Total SKUs Assigned': len(commercial_df),\n",
    "        'Unique Products': commercial_df['product_id'].nunique(),\n",
    "        'Total Target NMV': commercial_df['target_nmv'].sum(),\n",
    "        'Total Current NMV': commercial_df['current_nmv'].sum(),\n",
    "        'Total NMV Gap': commercial_df['nmv_gap'].sum(),\n",
    "        'NMV Gap %': (commercial_df['nmv_gap'].sum() / commercial_df['target_nmv'].sum() * 100) if commercial_df['target_nmv'].sum() > 0 else 0\n",
    "    })\n",
    "    \n",
    "    # Total (all teams combined - note: some SKUs may be assigned to multiple teams)\n",
    "    any_team = df_summary[(df_summary['pricing_team'] == 1) | \n",
    "                          (df_summary['purchase_team'] == 1) | \n",
    "                          (df_summary['commercial_team'] == 1)]\n",
    "    teams_data.append({\n",
    "        'Team': 'TOTAL (All Teams)',\n",
    "        'Total SKUs Assigned': len(any_team),\n",
    "        'Unique Products': any_team['product_id'].nunique(),\n",
    "        'Total Target NMV': any_team['target_nmv'].sum(),\n",
    "        'Total Current NMV': any_team['current_nmv'].sum(),\n",
    "        'Total NMV Gap': any_team['nmv_gap'].sum(),\n",
    "        'NMV Gap %': (any_team['nmv_gap'].sum() / any_team['target_nmv'].sum() * 100) if any_team['target_nmv'].sum() > 0 else 0\n",
    "    })\n",
    "    \n",
    "    df_teams = pd.DataFrame(teams_data)\n",
    "    \n",
    "    # Sort by NMV Gap descending\n",
    "    df_teams = df_teams.sort_values('Total NMV Gap', ascending=False)\n",
    "    \n",
    "    return df_teams\n",
    "\n",
    "\n",
    "def export_aggregate_analysis(df, aggregate_results, filename='pricing_aggregate_analysis.xlsx'):\n",
    "    \"\"\"\n",
    "    Export the aggregate analysis to an Excel file with multiple sheets:\n",
    "    - Team Summary: Aggregate view of each team's assignments and NMV gap\n",
    "    - Pricing Team: SKUs assigned to pricing team (sorted by NMV gap desc)\n",
    "    - Purchase Team: SKUs assigned to purchase team (sorted by NMV gap desc)\n",
    "    - Commercial Team: SKUs assigned to commercial team (sorted by NMV gap desc)\n",
    "    - Brand Analysis: Top dropping brands\n",
    "    - Market Status: Market position by brand\n",
    "    - Actions by Brand: Required actions by brand\n",
    "    - Action Summary: Summary of actions\n",
    "    - Raw Data: Full detail data\n",
    "    \"\"\"\n",
    "    df_analysis = df.copy()\n",
    "    df_analysis['target_nmv'] = df_analysis['high_rr'] * df_analysis['price']\n",
    "    df_analysis['current_nmv'] = df_analysis['current_rr'] * df_analysis['price']\n",
    "    df_analysis['nmv_gap'] = df_analysis['target_nmv'] - df_analysis['current_nmv']\n",
    "    \n",
    "    # Get team sheets\n",
    "    df_pricing_team = get_team_sheet(df, 'pricing_team', 'Pricing Team')\n",
    "    df_purchase_team = get_team_sheet(df, 'purchase_team', 'Purchase Team')\n",
    "    df_commercial_team = get_team_sheet(df, 'commercial_team', 'Commercial Team')\n",
    "    df_team_summary = get_team_summary(df)\n",
    "    \n",
    "    with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
    "        # Sheet 1: Team Summary - Aggregate view of each team\n",
    "        df_team_summary.to_excel(writer, sheet_name='Team Summary', index=False)\n",
    "        \n",
    "        # Sheet 2: Pricing Team - sorted by NMV gap descending\n",
    "        df_pricing_team.to_excel(writer, sheet_name='Pricing Team', index=False)\n",
    "        \n",
    "        # Sheet 3: Purchase Team - sorted by NMV gap descending\n",
    "        df_purchase_team.to_excel(writer, sheet_name='Purchase Team', index=False)\n",
    "        \n",
    "        # Sheet 4: Commercial Team - sorted by NMV gap descending\n",
    "        df_commercial_team.to_excel(writer, sheet_name='Commercial Team', index=False)\n",
    "        \n",
    "        # Sheet 5: Brand Analysis (all brands)\n",
    "        aggregate_results['brand_analysis'].to_excel(writer, sheet_name='Brand Analysis', index=False)\n",
    "        \n",
    "        # Sheet 6: Market Status Pivot\n",
    "        aggregate_results['market_status_pivot'].to_excel(writer, sheet_name='Market Status', index=False)\n",
    "        \n",
    "        # Sheet 7: Action Pivot\n",
    "        aggregate_results['action_pivot'].to_excel(writer, sheet_name='Actions by Brand', index=False)\n",
    "        \n",
    "        # Sheet 8: Action Summary\n",
    "        aggregate_results['action_summary'].to_excel(writer, sheet_name='Action Summary', index=False)\n",
    "        \n",
    "        # Sheet 9: Raw Data with NMV calculations\n",
    "        df_analysis.to_excel(writer, sheet_name='Raw Data', index=False)\n",
    "    \n",
    "    print(f\"✓ Aggregate analysis exported to: {filename}\")\n",
    "    print(f\"\\n📊 TEAM ASSIGNMENTS SUMMARY:\")\n",
    "    print(f\"  - Pricing Team:    {len(df_pricing_team):>6} SKUs | NMV Gap: {df_pricing_team['nmv_gap'].sum():>15,.0f} EGP\")\n",
    "    print(f\"  - Purchase Team:   {len(df_purchase_team):>6} SKUs | NMV Gap: {df_purchase_team['nmv_gap'].sum():>15,.0f} EGP\")\n",
    "    print(f\"  - Commercial Team: {len(df_commercial_team):>6} SKUs | NMV Gap: {df_commercial_team['nmv_gap'].sum():>15,.0f} EGP\")\n",
    "    return filename\n",
    "\n",
    "# Export:\n",
    "export_aggregate_analysis(df_result, aggregate_results, 'pricing_aggregate_analysis.xlsx')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
