{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99f8db73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/snowflake/connector/options.py:104: UserWarning: You have an incompatible version of 'pyarrow' installed (22.0.0), please install a version that adheres to: 'pyarrow<19.0.0; extra == \"pandas\"'\n",
      "  warn_incompatible_dep(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.Renviron\n",
      "/home/ec2-user/service_account_key.json\n",
      "✓ Environment initialized\n",
      "✓ Snowflake query function loaded\n"
     ]
    }
   ],
   "source": [
    "# Pricing Status Analysis Script\n",
    "# Converted from SQL query to Python for easier editing and maintenance\n",
    "\n",
    "# =============================================================================\n",
    "# STANDARD LIBRARY IMPORTS\n",
    "# =============================================================================\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "# =============================================================================\n",
    "# THIRD-PARTY IMPORTS\n",
    "# =============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import snowflake.connector\n",
    "\n",
    "# =============================================================================\n",
    "# LOCAL IMPORTS & ENVIRONMENT SETUP\n",
    "# =============================================================================\n",
    "import setup_environment_2\n",
    "import importlib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "importlib.reload(setup_environment_2)\n",
    "setup_environment_2.initialize_env()\n",
    "\n",
    "print(\"✓ Environment initialized\")\n",
    "\n",
    "# =============================================================================\n",
    "# SNOWFLAKE QUERY FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def snowflake_query(country, query, warehouse=None, columns=[], conn=None):\n",
    "    \"\"\"\n",
    "    Execute a query against Snowflake and return results as DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        country: Country identifier (e.g., \"Egypt\")\n",
    "        query: SQL query string to execute\n",
    "        warehouse: Snowflake warehouse (optional)\n",
    "        columns: Custom column names (optional)\n",
    "        conn: Existing connection (optional)\n",
    "        \n",
    "    Returns:\n",
    "        pandas DataFrame with query results\n",
    "    \"\"\"\n",
    "    con = snowflake.connector.connect(\n",
    "        user     = os.environ[\"SNOWFLAKE_USERNAME\"],\n",
    "        account  = os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "        password = os.environ[\"SNOWFLAKE_PASSWORD\"],\n",
    "        database = os.environ[\"SNOWFLAKE_DATABASE\"]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        cur = con.cursor()\n",
    "        cur.execute(\"USE WAREHOUSE COMPUTE_WH\")\n",
    "        cur.execute(query)\n",
    "        \n",
    "        column_names = [col[0] for col in cur.description]\n",
    "        results = cur.fetchall()\n",
    "        \n",
    "        if not results:\n",
    "            out = pd.DataFrame(columns=[name.lower() for name in column_names])\n",
    "        else:\n",
    "            if len(columns) == 0:\n",
    "                out = pd.DataFrame(np.array(results), columns=column_names)\n",
    "                out.columns = out.columns.str.lower()\n",
    "            else:\n",
    "                out = pd.DataFrame(np.array(results), columns=columns)\n",
    "                out.columns = out.columns.str.lower()\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Query error: {e}\")\n",
    "        raise\n",
    "        \n",
    "    finally:\n",
    "        cur.close()\n",
    "        con.close()\n",
    "\n",
    "print(\"✓ Snowflake query function loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17b80854-a11d-4399-a8a0-5b838b94c355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using timezone: America/Los_Angeles\n"
     ]
    }
   ],
   "source": [
    "query = \"SHOW PARAMETERS LIKE 'TIMEZONE'\"\n",
    "timezone_result = snowflake_query(\"Egypt\", query)\n",
    "zone_to_use = timezone_result['value'].values[0]\n",
    "print(f\"✓ Using timezone: {zone_to_use}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bb08940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warehouse Mapping:\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PART 1: STATIC DATA - Warehouse Mapping\n",
    "# =============================================================================\n",
    "\n",
    "def get_warehouse_mapping():\n",
    "    \"\"\"Define warehouse to region/cohort mapping.\"\"\"\n",
    "    whs_data = [\n",
    "        ('Cairo', 'Mostorod', 1, 700),\n",
    "        ('Giza', 'Barageel', 236, 701),\n",
    "        ('Delta West', 'El-Mahala', 337, 703),\n",
    "        ('Delta West', 'Tanta', 8, 703),\n",
    "        ('Delta East', 'Mansoura FC', 339, 704),\n",
    "        ('Delta East', 'Sharqya', 170, 704),\n",
    "        ('Upper Egypt', 'Assiut FC', 501, 1124),\n",
    "        ('Upper Egypt', 'Bani sweif', 401, 1126),\n",
    "        ('Upper Egypt', 'Menya Samalot', 703, 1123),\n",
    "        ('Upper Egypt', 'Sohag', 632, 1125),\n",
    "        ('Alexandria', 'Khorshed Alex', 797, 702),\n",
    "        ('Giza', 'Sakkarah', 962, 701)\n",
    "    ]\n",
    "    \n",
    "    df_whs = pd.DataFrame(whs_data, columns=['region', 'wh', 'warehouse_id', 'cohort_id'])\n",
    "    return df_whs\n",
    "\n",
    "# Get warehouse mapping\n",
    "df_whs = get_warehouse_mapping()\n",
    "print(\"Warehouse Mapping:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1db2ac3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COGS records: 8102\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PART 2: FETCH COGS DATA\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_current_cogs():\n",
    "    \"\"\"Fetch current cost of goods sold data.\"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT product_id, wac_p\n",
    "    FROM finance.all_cogs\n",
    "    WHERE CURRENT_TIMESTAMP BETWEEN from_date AND to_date\n",
    "    \"\"\"\n",
    "    df = snowflake_query(\"Egypt\", query)\n",
    "    df['product_id'] = pd.to_numeric(df['product_id'])\n",
    "    df['wac_p'] = pd.to_numeric(df['wac_p'])\n",
    "    return df\n",
    "\n",
    "# Run:\n",
    "df_cogs = fetch_current_cogs()\n",
    "print(f\"COGS records: {len(df_cogs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecfca4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 3: FETCH RUNNING RATES DATA\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_running_rates():\n",
    "    \"\"\"Fetch predicted running rates - latest per product/warehouse within 14 days.\"\"\"\n",
    "    query = \"\"\"\n",
    "--------------------------------------------------------------------------------\n",
    "-- Snowflake: SKU × Warehouse daily forecast (1-day forecast)\n",
    "-- Enhanced: excludes zero-sales last 4 days (with stock) SKUs\n",
    "-- (fixed aliasing / casting error in zero_sales_excluded)\n",
    "--------------------------------------------------------------------------------\n",
    "WITH params AS (\n",
    "  SELECT\n",
    "    CURRENT_DATE() AS run_date,\n",
    "    DATEADD(month, -3, CURRENT_DATE()) AS history_start,\n",
    "    21 AS recency_half_life_days,\n",
    "    4  AS zero_rule_days\n",
    "),\n",
    "\n",
    "/* 1) Daily sales aggregation */\n",
    "sales_base AS (\n",
    "  SELECT\n",
    "    pso.product_id            AS PRODUCT_ID,\n",
    "    pso.warehouse_id,\n",
    "    CAST(DATE_TRUNC('day', pso.created_at) AS DATE) AS date,\n",
    "    SUM(pso.purchased_item_count * pso.basic_unit_count) AS sold_units,\n",
    "    SUM(pso.purchased_item_count * pso.basic_unit_count * pso.item_price)\n",
    "      / NULLIF(SUM(pso.purchased_item_count * pso.basic_unit_count),0) AS avg_selling_price,\n",
    "    COUNT(DISTINCT so.retailer_id) AS retailer_count\n",
    "  FROM product_sales_order pso\n",
    "  JOIN sales_orders so ON pso.sales_order_id = so.id\n",
    "  WHERE CAST(DATE_TRUNC('day', pso.created_at) AS DATE) >= (SELECT history_start FROM params)\n",
    "  GROUP BY 1,2,3\n",
    "),\n",
    "\n",
    "/* 2) Stock snapshots -> daily metrics */\n",
    "stock_snapshots_hourly AS (\n",
    "  SELECT\n",
    "    ss.product_id AS product_id,\n",
    "    ss.warehouse_id,\n",
    "    CAST(DATE_TRUNC('day', ss.TIMESTAMP) AS DATE) AS date,\n",
    "    ss.available_stock,\n",
    "    ss.activation,\n",
    "    ss.TIMESTAMP AS snapshot_time\n",
    "  FROM materialized_views.STOCK_SNAP_SHOTS_RECENT ss\n",
    "  WHERE ss.product_id IS NOT NULL\n",
    "),\n",
    "\n",
    "stock_daily AS (\n",
    "  SELECT\n",
    "    product_id,\n",
    "    warehouse_id,\n",
    "    date,\n",
    "    MAX_BY(available_stock, snapshot_time) AS stock_closing,\n",
    "    24 * (\n",
    "      SUM(CASE WHEN activation = FALSE OR available_stock = 0 THEN 1 ELSE 0 END)::FLOAT\n",
    "      / NULLIF(COUNT(*),0)\n",
    "    ) AS oos_hours,\n",
    "    CASE WHEN MAX(CASE WHEN activation = TRUE AND available_stock > 0 THEN 1 ELSE 0 END) = 1 THEN 1 ELSE 0 END AS in_stock_flag\n",
    "  FROM stock_snapshots_hourly\n",
    "  GROUP BY product_id, warehouse_id, date\n",
    "),\n",
    "\n",
    "/* 3) Join sales + stock + WAC */\n",
    "base_data AS (\n",
    "  SELECT\n",
    "    sb.product_id,\n",
    "    sb.warehouse_id,\n",
    "    sb.date,\n",
    "    sb.sold_units,\n",
    "    sb.avg_selling_price,\n",
    "    sb.retailer_count,\n",
    "    sd.stock_closing,\n",
    "    sd.oos_hours,\n",
    "    sd.in_stock_flag,\n",
    "    ac.wac_p AS wac,\n",
    "    CASE WHEN DAYOFWEEKISO(sb.date) IN (5,6) THEN 1 ELSE 0 END AS is_weekend\n",
    "  FROM sales_base sb\n",
    "  LEFT JOIN stock_daily sd\n",
    "    ON sb.product_id = sd.product_id\n",
    "   AND sb.warehouse_id = sd.warehouse_id\n",
    "   AND sb.date = sd.date\n",
    "  LEFT JOIN finance.ALL_COGS ac\n",
    "    ON sb.product_id = ac.product_id\n",
    "   AND sb.date BETWEEN ac.from_date AND ac.to_date\n",
    "  WHERE sd.in_stock_flag = 1\n",
    "),\n",
    "\n",
    "/* 4) Stats per SKU × WH */\n",
    "sku_wh_stats AS (\n",
    "  SELECT\n",
    "    product_id,\n",
    "    warehouse_id,\n",
    "    AVG(sold_units) AS avg_units,\n",
    "    STDDEV_SAMP(sold_units) AS SIGMA_D,\n",
    "    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY sold_units) AS med_units,\n",
    "    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY sold_units) AS pct95_units,\n",
    "    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY retailer_count) AS med_retailers,\n",
    "    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY CASE \n",
    "       WHEN avg_selling_price IS NULL OR avg_selling_price = 0 THEN 0 \n",
    "       ELSE (avg_selling_price - COALESCE(wac,0))/NULLIF(avg_selling_price,0) END) AS med_margin\n",
    "  FROM base_data\n",
    "  GROUP BY product_id, warehouse_id\n",
    "),\n",
    "\n",
    "\n",
    "/* 5) Remove outliers */\n",
    "cleaned AS (\n",
    "  SELECT\n",
    "    b.*,\n",
    "    s.med_units,\n",
    "    s.pct95_units,\n",
    "    s.med_retailers,\n",
    "    s.med_margin,\n",
    "    CASE WHEN b.sold_units > s.pct95_units THEN s.pct95_units ELSE b.sold_units END AS units_capped,\n",
    "    CASE WHEN b.retailer_count > GREATEST(2, s.med_retailers * 2) THEN 1 ELSE 0 END AS retailer_spike\n",
    "  FROM base_data b\n",
    "  LEFT JOIN sku_wh_stats s\n",
    "    ON b.product_id = s.product_id AND b.warehouse_id = s.warehouse_id\n",
    "),\n",
    "\n",
    "/* 6) Scale down retailer spikes */\n",
    "adjusted AS (\n",
    "  SELECT\n",
    "    *,\n",
    "    CASE\n",
    "      WHEN retailer_spike = 1 AND retailer_count > 0 AND med_retailers IS NOT NULL\n",
    "        THEN ROUND(units_capped * (med_retailers::FLOAT / NULLIF(retailer_count::FLOAT,0)),0)\n",
    "      ELSE units_capped\n",
    "    END AS units_adjusted\n",
    "  FROM cleaned\n",
    "),\n",
    "\n",
    "/* 7) Add weighting */\n",
    "weighted AS (\n",
    "  SELECT\n",
    "    a.*,\n",
    "    DATEDIFF('day', a.date, (SELECT run_date FROM params)) AS days_ago,\n",
    "    CASE\n",
    "      WHEN a.date >= DATEADD(day, -21, (SELECT run_date FROM params)) THEN 1.5\n",
    "      WHEN a.date >= DATEADD(day, -90, (SELECT run_date FROM params)) THEN 1.0\n",
    "      ELSE 0.5\n",
    "    END AS w_recency,\n",
    "    CASE\n",
    "      WHEN COALESCE(a.in_stock_flag,0) = 1 AND COALESCE(a.oos_hours,0) < 12 THEN 1.4\n",
    "      WHEN COALESCE(a.in_stock_flag,0) = 1 AND COALESCE(a.oos_hours,0) >= 12 THEN 0.9\n",
    "      ELSE 0.6\n",
    "    END AS w_instock,\n",
    "    CASE WHEN a.is_weekend = 1 THEN 0.7 ELSE 1.0 END AS w_weekend,\n",
    "    CASE\n",
    "      WHEN a.avg_selling_price IS NULL OR a.avg_selling_price = 0 THEN 1.0\n",
    "      WHEN a.med_margin IS NULL THEN 1.0\n",
    "      ELSE\n",
    "        CASE\n",
    "          WHEN ((a.avg_selling_price - COALESCE(a.wac,0)) / NULLIF(a.avg_selling_price,0)) < a.med_margin\n",
    "            THEN 1.0 + LEAST((a.med_margin - ((a.avg_selling_price - COALESCE(a.wac,0))/NULLIF(a.avg_selling_price,0))) * 2.0, 0.6)\n",
    "          WHEN ((a.avg_selling_price - COALESCE(a.wac,0)) / NULLIF(a.avg_selling_price,0)) > a.med_margin\n",
    "            THEN 1.0 - LEAST((((a.avg_selling_price - COALESCE(a.wac,0))/NULLIF(a.avg_selling_price,0)) - a.med_margin) * 2.0, 0.4)\n",
    "          ELSE 1.0\n",
    "        END\n",
    "    END AS w_margin\n",
    "  FROM adjusted a\n",
    "),\n",
    "\n",
    "/* 8) Weighted final rows */\n",
    "weighted_final AS (\n",
    "  SELECT\n",
    "    product_id,\n",
    "    warehouse_id,\n",
    "    date,\n",
    "    units_adjusted,\n",
    "    w_recency,\n",
    "    w_instock,\n",
    "    w_weekend,\n",
    "    w_margin,\n",
    "    (w_recency * w_instock * w_weekend * w_margin) AS final_weight,\n",
    "    in_stock_flag\n",
    "  FROM weighted\n",
    "  WHERE units_adjusted IS NOT NULL\n",
    "    AND CAST(date AS DATE) >= (SELECT history_start FROM params)\n",
    "),\n",
    "\n",
    "/* 9) Forecast base */\n",
    "forecast_base AS (\n",
    "  SELECT\n",
    "    product_id,\n",
    "    warehouse_id,\n",
    "    SUM(units_adjusted * final_weight) / NULLIF(SUM(final_weight),0) AS weighted_avg_units,\n",
    "    COUNT(*) AS N_Days_Used\n",
    "  FROM weighted_final\n",
    "  GROUP BY product_id, warehouse_id\n",
    "),\n",
    "\n",
    "/* 10) Zero-sales last 4 days detection (standard logic) */\n",
    "last_4_days AS (\n",
    "  SELECT\n",
    "    hb.product_id,\n",
    "    hb.warehouse_id,\n",
    "    hb.date,\n",
    "    hb.sold_units,\n",
    "    hb.in_stock_flag\n",
    "  FROM base_data hb\n",
    "  WHERE hb.date >= DATEADD(day, -4, (SELECT run_date FROM params))\n",
    "    AND hb.date < (SELECT run_date FROM params)\n",
    "),\n",
    "\n",
    "last4_flag AS (\n",
    "  SELECT\n",
    "    product_id,\n",
    "    warehouse_id,\n",
    "    CASE WHEN COUNT(*) = 4\n",
    "              AND SUM(CASE WHEN COALESCE(sold_units,0) = 0 AND COALESCE(in_stock_flag,0) = 1 THEN 1 ELSE 0 END) = 4\n",
    "         THEN 1 ELSE 0 END AS last4_all_instock_zero\n",
    "  FROM last_4_days\n",
    "  GROUP BY product_id, warehouse_id\n",
    "),\n",
    "\n",
    "/* 10.5) Exclude SKUs with stock > 0, zero sales 4 days, and low receipts\n",
    "   (fixed aliasing, casting and NULL-safe arithmetic) */\n",
    "zero_sales_excluded AS (\n",
    "  -- base: sku×warehouse with positive available stock\n",
    "  SELECT DISTINCT s.warehouse_id, s.product_id AS product_id\n",
    "  FROM (\n",
    "    SELECT \n",
    "      pw.warehouse_id,\n",
    "      pw.product_id,\n",
    "      CAST(SUM(pw.available_stock) AS INT) AS stocks\n",
    "    FROM product_warehouse pw\n",
    "    WHERE pw.warehouse_id NOT IN (6,9,10)\n",
    "      AND pw.is_basic_unit = 1\n",
    "      AND pw.available_stock > 0\n",
    "    GROUP BY pw.warehouse_id, pw.product_id\n",
    "  ) s\n",
    "  LEFT JOIN (\n",
    "    SELECT \n",
    "      pso.product_id,\n",
    "      pso.warehouse_id,\n",
    "      SUM(pso.total_price) AS nmv\n",
    "    FROM product_sales_order pso\n",
    "    JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "    WHERE so.created_at::date BETWEEN CURRENT_DATE - 5 AND CURRENT_DATE - 1\n",
    "      AND so.sales_order_status_id NOT IN (7,12)\n",
    "      AND so.channel IN ('telesales','retailer')\n",
    "      AND pso.purchased_item_count <> 0\n",
    "    GROUP BY pso.product_id, pso.warehouse_id\n",
    "  ) md ON md.product_id = s.product_id AND md.warehouse_id = s.warehouse_id\n",
    "  LEFT JOIN finance.all_cogs f  \n",
    "    ON f.product_id = s.product_id\n",
    "   AND f.from_date::date <= CURRENT_DATE\n",
    "   AND f.to_date::date > CURRENT_DATE\n",
    "  LEFT JOIN (\n",
    "    SELECT pr.warehouse_id, ppr.product_id, SUM(ppr.final_price) AS total_prs\n",
    "    FROM product_purchased_receipts ppr\n",
    "    JOIN purchased_receipts pr ON pr.id = ppr.purchased_receipt_id\n",
    "    JOIN products p ON p.id = ppr.product_id\n",
    "    WHERE pr.date::date >= CURRENT_DATE - 4\n",
    "      AND pr.is_actual = 'true'\n",
    "      AND pr.purchased_receipt_status_id IN (4,5,7)\n",
    "      AND ppr.purchased_item_count <> 0\n",
    "    GROUP BY pr.warehouse_id, ppr.product_id\n",
    "  ) prs_data ON prs_data.product_id = s.product_id AND prs_data.warehouse_id = s.warehouse_id\n",
    "  WHERE s.stocks > 0\n",
    "    AND COALESCE(md.nmv,0) = 0\n",
    "    AND COALESCE(prs_data.total_prs,0) < 0.7 * (COALESCE(f.wac_p,0) * s.stocks)\n",
    "),\n",
    "\n",
    "/* 11) First sale detection */\n",
    "first_sale AS (\n",
    "  SELECT product_id, warehouse_id, MIN(date) AS first_sale_date\n",
    "  FROM base_data\n",
    "  WHERE sold_units > 0\n",
    "  GROUP BY product_id, warehouse_id\n",
    "),\n",
    "\n",
    "/* 12) Final forecast */\n",
    "final_forecast AS (\n",
    "  SELECT\n",
    "    fb.product_id,\n",
    "    fb.warehouse_id,\n",
    "    fb.weighted_avg_units,\n",
    "    fb.N_Days_Used,\n",
    "    CASE\n",
    "      WHEN l4.last4_all_instock_zero = 1 THEN 0\n",
    "      WHEN fs.first_sale_date IS NOT NULL \n",
    "           AND fs.first_sale_date >= DATEADD(day, -2, (SELECT run_date FROM params))\n",
    "         THEN GREATEST(CEIL(fb.weighted_avg_units), 1)\n",
    "      ELSE CEIL(fb.weighted_avg_units)\n",
    "    END AS AVG_RUN_RATE\n",
    "  FROM forecast_base fb\n",
    "  LEFT JOIN last4_flag l4 ON fb.product_id = l4.product_id AND fb.warehouse_id = l4.warehouse_id\n",
    "  LEFT JOIN first_sale fs ON fb.product_id = fs.product_id AND fb.warehouse_id = fs.warehouse_id\n",
    "  LEFT JOIN zero_sales_excluded zse ON fb.product_id = zse.product_id AND fb.warehouse_id = zse.warehouse_id\n",
    "  WHERE zse.product_id IS NULL\n",
    ")\n",
    "\n",
    "SELECT\n",
    "    ff.product_id,\n",
    "    ff.warehouse_id,\n",
    "    ff.AVG_RUN_RATE as rr\n",
    "FROM final_forecast ff\n",
    "LEFT JOIN sku_wh_stats s\n",
    "    ON ff.product_id = s.product_id\n",
    "   AND ff.warehouse_id = s.warehouse_id\n",
    "ORDER BY ff.warehouse_id, ff.product_id;\n",
    "    \"\"\"\n",
    "    df = snowflake_query(\"Egypt\", query)\n",
    "    for col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='ignore')\n",
    "    return df\n",
    "\n",
    "# Run:\n",
    "df_rr = fetch_running_rates()\n",
    "print(f\"Running rates records: {len(df_rr)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482ee20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 4: FETCH STOCKS DATA\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_stocks():\n",
    "    \"\"\"Fetch stock data with running rates and DOH calculation.\"\"\"\n",
    "    query = \"\"\"\n",
    "    WITH rr AS (\n",
    "        SELECT product_id, warehouse_id, rr\n",
    "        FROM finance.PREDICTED_RUNNING_RATES\n",
    "        QUALIFY MAX(date) OVER (PARTITION BY product_id, warehouse_id) = date\n",
    "            AND date::DATE >= CURRENT_DATE - 14\n",
    "    )\n",
    "    SELECT \n",
    "        pw.warehouse_id,\n",
    "        pw.product_id,\n",
    "        pw.available_stock::INTEGER AS stocks,\n",
    "        COALESCE(rr.rr, 0) AS rr,\n",
    "        CASE WHEN COALESCE(rr.rr, 0) = 0 THEN pw.available_stock::INTEGER \n",
    "             ELSE pw.available_stock::INTEGER / rr.rr \n",
    "        END AS doh\n",
    "    FROM product_warehouse pw\n",
    "    LEFT JOIN rr ON rr.product_id = pw.product_id AND rr.warehouse_id = pw.warehouse_id\n",
    "    WHERE pw.warehouse_id NOT IN (6, 9, 10)\n",
    "        AND pw.is_basic_unit = 1\n",
    "    \"\"\"\n",
    "    df = snowflake_query(\"Egypt\", query)\n",
    "    for col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='ignore')\n",
    "    return df\n",
    "\n",
    "# Run:\n",
    "df_stocks = fetch_stocks()\n",
    "print(f\"Stock records: {len(df_stocks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9728c536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 5: FETCH SALES DATA\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_sales():\n",
    "    \"\"\"Fetch sales data with aggregations for RR and retailer metrics.\"\"\"\n",
    "    query = \"\"\"\n",
    "        WITH raw_orders AS (\n",
    "    SELECT\n",
    "        so.created_at::DATE AS date,\n",
    "        pso.warehouse_id,\n",
    "        pso.product_id,\n",
    "        CONCAT(p.name_ar, ' ', p.size, ' ', pu.name_ar) AS sku,\n",
    "        b.name_ar AS brand, \n",
    "        c.name_ar AS cat,\n",
    "        so.retailer_id,\n",
    "        pso.purchased_item_count * pso.basic_unit_count AS qty\n",
    "    FROM product_sales_order pso\n",
    "    JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "    JOIN products p ON p.id = pso.product_id\n",
    "    JOIN brands b ON p.brand_id = b.id \n",
    "    JOIN categories c ON p.category_id = c.id\n",
    "    JOIN product_units pu ON pu.id = p.unit_id\n",
    "    WHERE so.created_at::DATE BETWEEN date_trunc('month',CURRENT_DATE - interval '8 months') AND CURRENT_DATE\n",
    "        AND so.sales_order_status_id NOT IN (7, 12)\n",
    "        AND so.channel IN ('telesales', 'retailer')\n",
    "        AND pso.purchased_item_count <> 0\n",
    "),\n",
    "daily_agg AS (\n",
    "    -- Aggregate to daily level for daily metrics\n",
    "    SELECT\n",
    "        date,\n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        sku,\n",
    "        brand,\n",
    "        cat,\n",
    "        SUM(qty) AS qty,\n",
    "        COUNT(DISTINCT retailer_id) AS num_rets\n",
    "    FROM raw_orders\n",
    "    GROUP BY 1, 2, 3, 4, 5, 6\n",
    "),\n",
    "daily_metrics AS (\n",
    "    -- Original daily metrics\n",
    "    SELECT \n",
    "        warehouse_id, \n",
    "        product_id,\n",
    "        sku,\n",
    "        brand,\n",
    "        cat,\n",
    "        PERCENTILE_CONT(0.8) WITHIN GROUP (ORDER BY CASE WHEN date < CURRENT_DATE - 3 THEN qty END) AS high_rr,\n",
    "        PERCENTILE_CONT(0.8) WITHIN GROUP (ORDER BY CASE WHEN date < CURRENT_DATE - 3 THEN num_rets END) AS high_rets,\n",
    "        COALESCE(STDDEV(CASE WHEN date < CURRENT_DATE - 3 THEN qty END), 0) AS qty_std,\n",
    "        COALESCE(STDDEV(CASE WHEN date < CURRENT_DATE - 3 THEN num_rets END), 0) AS rets_std,\n",
    "        COALESCE(SUM(CASE WHEN date = CURRENT_DATE - 1 THEN qty END), 0) AS cu_rr,\n",
    "        COALESCE(SUM(CASE WHEN date = CURRENT_DATE THEN qty END), 0) AS today_rr,\n",
    "        COALESCE(SUM(CASE WHEN date = CURRENT_DATE - 1 THEN num_rets END), 0) AS cu_rets\n",
    "    FROM daily_agg\n",
    "    GROUP BY 1, 2, 3, 4, 5\n",
    "),\n",
    "\n",
    "/* ---------- CURRENT MTD ---------- */\n",
    "current_mtd AS (\n",
    "    SELECT\n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        sku,\n",
    "        brand,\n",
    "        cat,\n",
    "        SUM(qty) AS cu_mtd_rr,\n",
    "        COUNT(DISTINCT retailer_id) AS cu_mtd_rets\n",
    "    FROM raw_orders\n",
    "    WHERE (\n",
    "        EXTRACT(DAY FROM CURRENT_DATE) = 1\n",
    "        AND date = CURRENT_DATE - 1\n",
    "    ) OR (\n",
    "        EXTRACT(DAY FROM CURRENT_DATE) > 1\n",
    "        AND date >= DATE_TRUNC('month', CURRENT_DATE)\n",
    "        AND date < CURRENT_DATE\n",
    "    )\n",
    "    GROUP BY 1,2,3,4,5\n",
    "),\n",
    "\n",
    "/* ---------- HISTORICAL MTD (aligned day-of-month) ---------- */\n",
    "historical_mtd AS (\n",
    "    SELECT\n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        sku,\n",
    "        brand,\n",
    "        cat,\n",
    "        DATE_TRUNC('month', date) AS month_start,\n",
    "        SUM(qty) AS mtd_qty,\n",
    "        COUNT(DISTINCT retailer_id) AS mtd_rets\n",
    "    FROM raw_orders\n",
    "    WHERE DATE_TRUNC('month', date) < DATE_TRUNC('month', CURRENT_DATE)\n",
    "      AND date < DATEADD(\n",
    "            day,\n",
    "            EXTRACT(day FROM CURRENT_DATE),\n",
    "            DATE_TRUNC('month', date)\n",
    "          )\n",
    "    GROUP BY 1,2,3,4,5,6\n",
    "),\n",
    "\n",
    "historical_full AS (\n",
    "    SELECT\n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        sku,\n",
    "        brand,\n",
    "        cat,\n",
    "        DATE_TRUNC('month', date) AS month_start,\n",
    "        SUM(qty) AS full_qty,\n",
    "        COUNT(DISTINCT retailer_id) AS full_rets\n",
    "    FROM raw_orders\n",
    "    WHERE DATE_TRUNC('month', date) < DATE_TRUNC('month', CURRENT_DATE)\n",
    "    GROUP BY 1,2,3,4,5,6\n",
    "),\n",
    "\n",
    "/* ---------- P80 / MEDIANS ---------- */\n",
    "mtd_p80_base AS (\n",
    "    SELECT\n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        sku,\n",
    "        brand,\n",
    "        cat,\n",
    "        PERCENTILE_CONT(0.8) WITHIN GROUP (ORDER BY mtd_qty)  AS high_mtd_rr,\n",
    "        PERCENTILE_CONT(0.8) WITHIN GROUP (ORDER BY mtd_rets) AS high_mtd_rets,\n",
    "        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY mtd_qty)  AS median_mtd_rr,\n",
    "        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY mtd_rets) AS median_mtd_rets\n",
    "    FROM historical_mtd\n",
    "    GROUP BY 1,2,3,4,5\n",
    "),\n",
    "\n",
    "full_p80_base AS (\n",
    "    SELECT\n",
    "        warehouse_id,\n",
    "        product_id,\n",
    "        sku,\n",
    "        brand,\n",
    "        cat,\n",
    "        PERCENTILE_CONT(0.8) WITHIN GROUP (ORDER BY full_qty)  AS high_full_rr,\n",
    "        PERCENTILE_CONT(0.8) WITHIN GROUP (ORDER BY full_rets) AS high_full_rets,\n",
    "        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY full_qty)  AS median_full_rr,\n",
    "        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY full_rets) AS median_full_rets\n",
    "    FROM historical_full\n",
    "    GROUP BY 1,2,3,4,5\n",
    "),\n",
    "\n",
    "mtd_p80 AS (\n",
    "    SELECT\n",
    "        p.warehouse_id,\n",
    "        p.product_id,\n",
    "        p.sku,\n",
    "        p.brand,\n",
    "        p.cat,\n",
    "        p.high_mtd_rr,\n",
    "        p.high_mtd_rets,\n",
    "        COALESCE(STDDEV(CASE WHEN h.mtd_qty  >= p.median_mtd_rr  THEN h.mtd_qty  END),0) AS mtd_qty_std,\n",
    "        COALESCE(STDDEV(CASE WHEN h.mtd_rets >= p.median_mtd_rets THEN h.mtd_rets END),0) AS mtd_rets_std\n",
    "    FROM mtd_p80_base p\n",
    "    LEFT JOIN historical_mtd h\n",
    "        ON p.warehouse_id = h.warehouse_id\n",
    "       AND p.product_id  = h.product_id\n",
    "    GROUP BY 1,2,3,4,5,6,7\n",
    "),\n",
    "\n",
    "full_p80 AS (\n",
    "    SELECT\n",
    "        p.warehouse_id,\n",
    "        p.product_id,\n",
    "        p.sku,\n",
    "        p.brand,\n",
    "        p.cat,\n",
    "        p.high_full_rr,\n",
    "        p.high_full_rets,\n",
    "        COALESCE(STDDEV(CASE WHEN h.full_qty  >= p.median_full_rr  THEN h.full_qty  END),0) AS full_qty_std,\n",
    "        COALESCE(STDDEV(CASE WHEN h.full_rets >= p.median_full_rets THEN h.full_rets END),0) AS full_rets_std\n",
    "    FROM full_p80_base p\n",
    "    LEFT JOIN historical_full h\n",
    "        ON p.warehouse_id = h.warehouse_id\n",
    "       AND p.product_id  = h.product_id\n",
    "    GROUP BY 1,2,3,4,5,6,7\n",
    ")\n",
    "\n",
    "SELECT\n",
    "    d.*,\n",
    "    COALESCE(p.high_mtd_rr,0)     AS high_mtd_rr,\n",
    "    COALESCE(p.high_mtd_rets,0)  AS high_mtd_rets,\n",
    "    COALESCE(p.mtd_qty_std,0)    AS mtd_qty_std,\n",
    "    COALESCE(p.mtd_rets_std,0)   AS mtd_rets_std,\n",
    "    COALESCE(f.high_full_rr,0)   AS high_full_rr,\n",
    "    COALESCE(f.high_full_rets,0) AS high_full_rets,\n",
    "    COALESCE(f.full_qty_std,0)   AS full_qty_std,\n",
    "    COALESCE(f.full_rets_std,0)  AS full_rets_std,\n",
    "    COALESCE(c.cu_mtd_rr,0)      AS cu_mtd_rr,\n",
    "    COALESCE(c.cu_mtd_rets,0)    AS cu_mtd_rets\n",
    "FROM daily_metrics d\n",
    "LEFT JOIN mtd_p80  p ON d.warehouse_id = p.warehouse_id AND d.product_id = p.product_id\n",
    "LEFT JOIN full_p80 f ON d.warehouse_id = f.warehouse_id AND d.product_id = f.product_id\n",
    "LEFT JOIN current_mtd c ON d.warehouse_id = c.warehouse_id AND d.product_id = c.product_id\n",
    "ORDER BY high_mtd_rr DESC;\n",
    "\n",
    "    \"\"\"\n",
    "    df = snowflake_query(\"Egypt\", query)\n",
    "    # Convert numeric columns\n",
    "    for col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='ignore')\n",
    "    return df\n",
    "\n",
    "# Run:\n",
    "df_sales = fetch_sales()\n",
    "print(f\"Sales records: {len(df_sales)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9713f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 6: FETCH PRICES DATA\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_prices():\n",
    "    \"\"\"Fetch latest prices per product/cohort.\"\"\"\n",
    "    cohort_ids = [700, 701, 702, 703, 704, 696, 695, 698, 697, 699, 1123, 1124, 1125, 1126]\n",
    "    cohort_str = ', '.join(map(str, cohort_ids))\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    SELECT cohort_id, product_id, price\n",
    "    FROM (\n",
    "        SELECT \n",
    "            cpc.cohort_id,\n",
    "            pu.product_id,\n",
    "            cpc.price,\n",
    "            ROW_NUMBER() OVER (PARTITION BY pu.product_id, cpc.cohort_id ORDER BY cpc.created_at DESC) AS rn\n",
    "        FROM cohort_pricing_changes cpc \n",
    "        JOIN PACKING_UNIT_PRODUCTS pu ON pu.id = cpc.product_packing_unit_id\n",
    "        WHERE cpc.cohort_id IN ({cohort_str})\n",
    "            AND pu.is_basic_unit = 1 \n",
    "    )\n",
    "    WHERE rn = 1\n",
    "    \"\"\"\n",
    "    df = snowflake_query(\"Egypt\", query)\n",
    "    df['cohort_id'] = pd.to_numeric(df['cohort_id'])\n",
    "    df['product_id'] = pd.to_numeric(df['product_id'])\n",
    "    df['price'] = pd.to_numeric(df['price'])\n",
    "    return df\n",
    "\n",
    "# Run:\n",
    "df_prices = fetch_prices()\n",
    "print(f\"Price records: {len(df_prices)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16388890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 7: FETCH MARKETPLACE PRICES DATA\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_marketplace_prices():\n",
    "    \"\"\"Fetch marketplace price data (min, mod, max).\"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        mp.region,\n",
    "        mp.product_id,\n",
    "        AVG(mp.min_price / pup.basic_unit_count) AS min_price,\n",
    "        AVG(mp.mod_price / pup.basic_unit_count) AS mod_price,\n",
    "        AVG(mp.max_price / pup.basic_unit_count) AS max_price\n",
    "    FROM materialized_views.marketplace_prices mp\n",
    "    JOIN PACKING_UNIT_PRODUCTS pup ON pup.product_id = mp.product_id AND mp.pu_id = pup.packing_unit_id\n",
    "    GROUP BY 1, 2\n",
    "    \"\"\"\n",
    "    df = snowflake_query(\"Egypt\", query)\n",
    "    df['product_id'] = pd.to_numeric(df['product_id'])\n",
    "    df['min_price'] = pd.to_numeric(df['min_price'])\n",
    "    df['mod_price'] = pd.to_numeric(df['mod_price'])\n",
    "    df['max_price'] = pd.to_numeric(df['max_price'])\n",
    "    return df\n",
    "\n",
    "# Run:\n",
    "df_mp = fetch_marketplace_prices()\n",
    "print(f\"Marketplace price records: {len(df_mp)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bff7a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 8: FETCH BEN SOLIMAN PRICES\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_ben_soliman_prices():\n",
    "    \"\"\"Fetch Ben Soliman competitor prices with validation.\"\"\"\n",
    "    query = f\"\"\"\n",
    " with lower as (\n",
    "select distinct product_id,sku,new_d*bs_price as ben_soliman_price,INJECTION_DATE\n",
    "from (\n",
    "select maxab_product_id as product_id,maxab_sku as sku,INJECTION_DATE,wac1,wac_p,(bs_price/bs_unit_count) as bs_price,diff,cu_price,case when p1 > 1 then child_quantity else 0 end as scheck,round(p1/2)*2 as p1,p2,case when (ROUND(p1 / scheck) * scheck) = 0 then p1 else (ROUND(p1 / scheck) * scheck) end as new_d\n",
    "from (\n",
    "select sm.*,wac1, wac_p, abs((bs_price/bs_unit_count)-(wac_p*maxab_basic_unit_count))/(wac_p*maxab_basic_unit_count) as diff,cpc.price as cu_price,pup.child_quantity , round((cu_price/(bs_price/bs_unit_count))) as p1, round(((bs_price/bs_unit_count)/cu_price)) as p2\n",
    "from materialized_views.savvy_mapping sm \n",
    "join finance.all_cogs f on f.product_id = sm.maxab_product_id and CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMESTAMP()) between f.from_Date and f.to_date\n",
    "join   PACKING_UNIT_PRODUCTS pu on pu.product_id = sm.maxab_product_id and pu.IS_BASIC_UNIT = 1 \n",
    "join cohort_product_packing_units cpc on cpc.PRODUCT_PACKING_UNIT_ID = pu.id and cohort_id = 700 \n",
    "join packing_unit_products pup on pup.product_id = sm.maxab_product_id and pup.is_basic_unit = 1  \n",
    "where bs_price is not null and INJECTION_DATE::date >= CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - 5 \n",
    "and diff > 0.3\n",
    "and p1 > 1\n",
    ")\n",
    ")\n",
    "qualify max(INJECTION_DATE)over(partition by product_id)  = INJECTION_DATE\n",
    "),\n",
    "m_bs as (\n",
    "select z.* from (\n",
    "\tselect maxab_product_id as product_id, maxab_sku as sku, avg(bs_final_price) as ben_soliman_price,INJECTION_DATE\n",
    "\tfrom (\n",
    "\t\tselect *, row_number() over(partition by maxab_product_id order by diff) as rnk_2 from (\n",
    "\t\t\tselect *, (bs_final_price-wac_p)/wac_p as diff_2 from (\n",
    "\t\t\t\tselect *, bs_price/maxab_basic_unit_count as bs_final_price from (\n",
    "\t\t\t\t\tselect *, row_number() over(partition by maxab_product_id, maxab_pu order by diff) as rnk from (\n",
    "\t\t\t\t\t\tselect * ,max(INJECTION_DATE::date) over(partition by maxab_product_id, maxab_pu) as max_date,\n",
    "\t\t\t\t\t\tfrom (\n",
    "\t\t\t\t\t\t\tselect sm.*,wac1, wac_p, abs(bs_price-(wac_p*maxab_basic_unit_count))/(wac_p*maxab_basic_unit_count) as diff \n",
    "\t\t\t\t\tfrom materialized_views.savvy_mapping sm \n",
    "\t\t\t\t\tjoin finance.all_cogs f on f.product_id = sm.maxab_product_id and CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMESTAMP()) between f.from_Date and f.to_date\n",
    "\t\t\t\t\twhere bs_price is not null and INJECTION_DATE::date >= CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMESTAMP())::date - 5 \n",
    "\t\t\t\t\tand diff < 0.3\n",
    "\t\t\t\t\t)\n",
    "\t\t\t\t\tqualify max_date = INJECTION_DATE\n",
    "\t\t\t\t\t) qualify rnk = 1 \n",
    "\t\t\t\t)\n",
    "\t\t\t) where diff_2 between -0.5 and 0.5 \n",
    "\t\t) qualify rnk_2 = 1 \n",
    "\t) group by all\n",
    ") z \n",
    "join finance.all_cogs f on f.product_id = z.product_id and CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMESTAMP()) between f.from_Date and f.to_date\n",
    "where ben_soliman_price between f.wac_p*0.7 and f.wac_p*1.3\n",
    ")\n",
    "\n",
    "select product_id,sku,avg(ben_soliman_price) as ben_soliman_price\n",
    "from (\n",
    "select *\n",
    "from (\n",
    "select * \n",
    "from m_bs \n",
    "\n",
    "union all\n",
    "\n",
    " select *\n",
    " from lower\n",
    " )\n",
    " qualify max(INJECTION_DATE) over(partition by product_id) = INJECTION_DATE\n",
    " )\n",
    " group by all\n",
    "    \"\"\"\n",
    "    df = snowflake_query(\"Egypt\", query)\n",
    "    df['product_id'] = pd.to_numeric(df['product_id'])\n",
    "    df['ben_soliman_price'] = pd.to_numeric(df['ben_soliman_price'])\n",
    "    return df\n",
    "\n",
    "# Run:\n",
    "df_bsp = fetch_ben_soliman_prices()\n",
    "print(f\"Ben Soliman price records: {len(df_bsp)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cddba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 9: FETCH SCRAPPED/CLEANED MARKET PRICES\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_scrapped_prices():\n",
    "    \"\"\"Fetch scraped market prices with min/max/median aggregations.\"\"\"\n",
    "    query = \"\"\"\n",
    "    WITH current_cogs AS (\n",
    "        SELECT product_id, wac_p\n",
    "        FROM finance.all_cogs\n",
    "        WHERE CURRENT_TIMESTAMP BETWEEN from_date AND to_date\n",
    "    )\n",
    "    SELECT \n",
    "        product_id,\n",
    "        region,\n",
    "        MIN(market_price) AS min_scrapped,\n",
    "        MAX(market_price) AS max_scrapped,\n",
    "        MEDIAN(market_price) AS median_scrapped\n",
    "    FROM (\n",
    "        SELECT \n",
    "            cmp.product_id,\n",
    "            cmp.region,\n",
    "            cmp.market_price\n",
    "        FROM materialized_views.cleaned_market_prices cmp\n",
    "        JOIN current_cogs f ON f.product_id = cmp.product_id\n",
    "        WHERE cmp.date >= CURRENT_DATE - 5\n",
    "            AND cmp.market_price BETWEEN f.wac_p * 0.9 AND f.wac_p * 1.3\n",
    "        QUALIFY MAX(cmp.date) OVER (PARTITION BY cmp.region, cmp.product_id, cmp.competitor) = cmp.date\n",
    "    )\n",
    "    GROUP BY 1, 2\n",
    "    \"\"\"\n",
    "    df = snowflake_query(\"Egypt\", query)\n",
    "    df['product_id'] = pd.to_numeric(df['product_id'])\n",
    "    df['min_scrapped'] = pd.to_numeric(df['min_scrapped'])\n",
    "    df['max_scrapped'] = pd.to_numeric(df['max_scrapped'])\n",
    "    df['median_scrapped'] = pd.to_numeric(df['median_scrapped'])\n",
    "    return df\n",
    "\n",
    "# Run:\n",
    "df_scrapped = fetch_scrapped_prices()\n",
    "print(f\"Scrapped price records: {len(df_scrapped)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eec751c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 10: FETCH TARGETS DATA\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_cat_brand_targets():\n",
    "    \"\"\"Fetch category/brand targets from commercial plan.\"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        cat, \n",
    "        brand, \n",
    "        SUM(nmv) AS target_nmv, \n",
    "        AVG(margin) AS target_bm,\n",
    "        DATE_TRUNC('month', DATE) AS month_date\n",
    "    FROM performance.commercial_targets\n",
    "    WHERE cat IS NOT NULL AND brand IS NOT NULL \n",
    "        AND date >= DATE_TRUNC('month', CURRENT_DATE)\n",
    "    GROUP BY ALL\n",
    "    \"\"\"\n",
    "    df = snowflake_query(\"Egypt\", query)\n",
    "    df['target_nmv'] = pd.to_numeric(df['target_nmv'])\n",
    "    df['target_bm'] = pd.to_numeric(df['target_bm'])\n",
    "    return df\n",
    "\n",
    "def fetch_cat_targets(df_cat_brand_targets):\n",
    "    \"\"\"Calculate category-level targets from brand targets.\"\"\"\n",
    "    df = df_cat_brand_targets.copy()\n",
    "    df['weighted_margin'] = df['target_bm'] * df['target_nmv']\n",
    "    cat_targets = df.groupby('cat').apply(\n",
    "        lambda x: x['weighted_margin'].sum() / x['target_nmv'].sum() if x['target_nmv'].sum() > 0 else 0\n",
    "    ).reset_index()\n",
    "    cat_targets.columns = ['cat', 'cat_target_margin']\n",
    "    return cat_targets\n",
    "\n",
    "# Run:\n",
    "df_cat_brand_targets = fetch_cat_brand_targets()\n",
    "df_cat_targets = fetch_cat_targets(df_cat_brand_targets)\n",
    "print(f\"Cat/Brand target records: {len(df_cat_brand_targets)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0479501c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 11: FETCH DISCOUNTED SALES DATA\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_discounted_sales():\n",
    "    \"\"\"Fetch yesterday's discounted sales breakdown.\"\"\"\n",
    "    query = \"\"\"\n",
    "select warehouse_id,product_id,total_nmv,bundle_nmv,sku_discount_nmv,quantity_nmv,blended_price\n",
    "from (\n",
    "select warehouse_id,product_id,total_nmv,bundle_nmv,sku_discount_nmv,quantity_nmv,cogs/min_qty as b_wac , (total_nmv-(cogs+total_discount))/total_nmv as b_margin,b_wac/(1-b_margin) as blended_price\n",
    "from (\n",
    "  SELECT  \n",
    "        pso.warehouse_id,\n",
    "        pso.product_id,\n",
    "        SUM(pso.total_price) AS total_nmv,\n",
    "        SUM(CASE WHEN pso.dynamic_bundle_sales_order_id IS NOT NULL THEN pso.total_price END) AS bundle_nmv,\n",
    "        SUM(CASE WHEN pso.sku_discount_id IS NOT NULL THEN pso.total_price END) AS sku_discount_nmv,\n",
    "        SUM(CASE WHEN pso.quantity_discount_id IS NOT NULL THEN pso.total_price END) AS quantity_nmv,\n",
    "\t\tsum(f.wac_p*pso.purchased_item_count*pso.basic_unit_count) as cogs,\n",
    "\t\tsum(pso.purchased_item_count*pso.basic_unit_count) as min_qty,\n",
    "\t\tsum((ITEM_QUANTITY_DISCOUNT_VALUE*pso.purchased_item_count) + (ITEM_DISCOUNT_VALUE*pso.purchased_item_count)) as total_discount\n",
    "    FROM product_sales_order pso \n",
    "    JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "\tjoin finance.all_cogs f on f.product_id = pso.product_id and so.created_at between from_date and to_date \n",
    "    WHERE so.created_at::DATE = CURRENT_DATE - 1 \n",
    "        AND so.sales_order_status_id NOT IN (7, 12)\n",
    "        AND so.channel IN ('telesales', 'retailer')\n",
    "        AND pso.purchased_item_count <> 0\n",
    "    GROUP BY 1, 2)\n",
    ") \n",
    "    \"\"\"\n",
    "    df = snowflake_query(\"Egypt\", query)\n",
    "    for col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='ignore')\n",
    "    return df\n",
    "\n",
    "# Run:\n",
    "df_discounted = fetch_discounted_sales()\n",
    "print(f\"Discounted sales records: {len(df_discounted)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a953cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 13B: FETCH PRODUCT WAREHOUSE ACTIVATION\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_product_activation():\n",
    "    \"\"\"\n",
    "    Fetch product warehouse activation status.\n",
    "    Uses the top selling packing unit per product in the last 3 months \n",
    "    as the representative packing unit to get activation status.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    WITH top_selling_pu AS (\n",
    "        -- Get the top selling packing unit per product/warehouse in last 3 months\n",
    "        SELECT \n",
    "            pso.product_id,\n",
    "            pso.warehouse_id,\n",
    "            pso.packing_unit_id,\n",
    "            SUM(pso.total_price) AS total_nmv\n",
    "        FROM product_sales_order pso\n",
    "        JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "        WHERE so.created_at::DATE >= CURRENT_DATE - 90\n",
    "            AND so.sales_order_status_id NOT IN (7, 12)\n",
    "            AND so.channel IN ('telesales', 'retailer')\n",
    "            AND pso.purchased_item_count <> 0\n",
    "        GROUP BY 1, 2, 3\n",
    "        QUALIFY ROW_NUMBER() OVER (\n",
    "            PARTITION BY pso.product_id, pso.warehouse_id \n",
    "            ORDER BY SUM(pso.total_price) DESC\n",
    "        ) = 1\n",
    "    )\n",
    "    SELECT \n",
    "        tspu.product_id,\n",
    "        tspu.warehouse_id,\n",
    "        tspu.packing_unit_id AS top_selling_pu,\n",
    "        pw.activation AS activation\n",
    "    FROM top_selling_pu tspu\n",
    "    JOIN product_warehouse pw \n",
    "        ON pw.product_id = tspu.product_id \n",
    "        AND pw.warehouse_id = tspu.warehouse_id\n",
    "        AND pw.packing_unit_id = tspu.packing_unit_id\n",
    "    \"\"\"\n",
    "    df = snowflake_query(\"Egypt\", query)\n",
    "    df['product_id'] = pd.to_numeric(df['product_id'])\n",
    "    df['warehouse_id'] = pd.to_numeric(df['warehouse_id'])\n",
    "    df['top_selling_pu'] = pd.to_numeric(df['top_selling_pu'])\n",
    "    return df\n",
    "\n",
    "# Run:\n",
    "df_activation = fetch_product_activation()\n",
    "print(f\"Product activation records: {len(df_activation)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c46e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 13C: FETCH OOS YESTERDAY DATA\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_oos_yesterday():\n",
    "    \"\"\"\n",
    "    Fetch whether product was out of stock yesterday.\n",
    "    Returns oos_yesterday = 1 if product had 0 opening AND 0 closing stock,\n",
    "    meaning it was OOS the entire day.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT distinct product_id, warehouse_id,\n",
    "        CASE WHEN opening_stocks = 0 AND closing_stocks = 0 THEN 1\n",
    "             ELSE 0 \n",
    "        END AS oos_yesterday\n",
    "    FROM (\n",
    "        SELECT \n",
    "            timestamp,\n",
    "            product_id,\n",
    "            warehouse_id, \n",
    "            AVAILABLE_STOCK AS closing_stocks,\n",
    "            LAG(AVAILABLE_STOCK) OVER (PARTITION BY product_id, warehouse_id ORDER BY TIMESTAMP) AS opening_stocks\n",
    "        FROM materialized_views.stock_day_close\n",
    "        WHERE timestamp::DATE >= CURRENT_DATE - 2\n",
    "        QUALIFY opening_stocks IS NOT NULL\n",
    "    )\n",
    "    \"\"\"\n",
    "    df = snowflake_query(\"Egypt\", query)\n",
    "    df['product_id'] = pd.to_numeric(df['product_id'])\n",
    "    df['warehouse_id'] = pd.to_numeric(df['warehouse_id'])\n",
    "    df['oos_yesterday'] = pd.to_numeric(df['oos_yesterday'])\n",
    "    return df\n",
    "\n",
    "# Run:\n",
    "df_oos_yesterday = fetch_oos_yesterday()\n",
    "print(f\"OOS yesterday records: {len(df_oos_yesterday)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aab9d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 13D: FETCH PURCHASE ORDER DATA\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_po_data():\n",
    "    \"\"\"\n",
    "    Fetch purchase order data from last 15 days.\n",
    "    Returns last PO info and count of supplier rejections.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    WITH last_data AS (\n",
    "        SELECT product_id, warehouse_id, confirmation_status, PO_date::DATE AS last_po_date, ordered_qty\n",
    "        FROM (\n",
    "            SELECT \n",
    "                product_id,\n",
    "                Target_WAREHOUSE_ID AS warehouse_id,\n",
    "                confirmation_status,\n",
    "                created_at AS PO_date,\n",
    "                MIN_QUANTITY AS ordered_qty,\n",
    "                reason,\n",
    "                MAX(PO_date) OVER (PARTITION BY product_id, warehouse_id) AS last_po\n",
    "            FROM retool.PO_INITIAL_PLAN\n",
    "            WHERE created_at::DATE >= CURRENT_DATE - 15 \n",
    "            QUALIFY last_po = PO_date\n",
    "        )\n",
    "    ),\n",
    "    last_15_data AS (\n",
    "        SELECT \n",
    "            product_id,\n",
    "            target_WAREHOUSE_ID AS warehouse_id,\n",
    "            COUNT(DISTINCT CASE WHEN confirmation_status <> 'yes' THEN created_at END) AS no_last_15\n",
    "        FROM retool.PO_INITIAL_PLAN\n",
    "        WHERE created_at::DATE >= CURRENT_DATE - 15 \n",
    "        GROUP BY ALL\n",
    "    )\n",
    "    SELECT \n",
    "        ld.product_id,\n",
    "        ld.warehouse_id,\n",
    "        ld.confirmation_status,\n",
    "        ld.last_po_date,\n",
    "        ld.ordered_qty,\n",
    "        COALESCE(lfd.no_last_15, 0) AS no_last_15\n",
    "    FROM last_data ld \n",
    "    LEFT JOIN last_15_data lfd \n",
    "        ON lfd.product_id = ld.product_id \n",
    "        AND lfd.warehouse_id = ld.warehouse_id\n",
    "    \"\"\"\n",
    "    df = snowflake_query(\"Egypt\", query)\n",
    "    df['product_id'] = pd.to_numeric(df['product_id'])\n",
    "    df['warehouse_id'] = pd.to_numeric(df['warehouse_id'])\n",
    "    df['ordered_qty'] = pd.to_numeric(df['ordered_qty'])\n",
    "    df['no_last_15'] = pd.to_numeric(df['no_last_15'])\n",
    "    df['last_po_date'] = pd.to_datetime(df['last_po_date'])\n",
    "    return df\n",
    "\n",
    "# Run:\n",
    "df_po_data = fetch_po_data()\n",
    "print(f\"PO data records: {len(df_po_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c41ec38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 12: FETCH COMMERCIAL CONSTRAINTS (MIN PRICES)\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_commercial_constraints():\n",
    "    \"\"\"Fetch commercial minimum price constraints.\"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT product_id, region, min_price\n",
    "    FROM (\n",
    "        SELECT \n",
    "            product_id, \n",
    "            region, \n",
    "            min_price,\n",
    "            created_at,\n",
    "            MAX(created_at) OVER (PARTITION BY product_id, region) AS max_created\n",
    "        FROM finance.minimum_prices\n",
    "        WHERE is_deleted = 'false'\n",
    "            AND created_at BETWEEN \n",
    "                CASE WHEN DATE_PART('day', CURRENT_DATE) < 7 \n",
    "                     THEN DATE_TRUNC('month', CURRENT_DATE - INTERVAL '1 month') \n",
    "                     ELSE DATE_TRUNC('month', CURRENT_DATE)\n",
    "                END\n",
    "                AND DATE_TRUNC('month', CURRENT_DATE) + INTERVAL '1 month' + INTERVAL '6 days'\n",
    "    )\n",
    "    WHERE created_at = max_created\n",
    "    \"\"\"\n",
    "    df = snowflake_query(\"Egypt\", query)\n",
    "    df['product_id'] = pd.to_numeric(df['product_id'])\n",
    "    df['min_price'] = pd.to_numeric(df['min_price'])\n",
    "    return df\n",
    "\n",
    "# Run:\n",
    "df_commercial = fetch_commercial_constraints()\n",
    "print(f\"Commercial constraint records: {len(df_commercial)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30adb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 13: FETCH TARGETS DATA (COMPLEX - WAREHOUSE SKU TARGETS)\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_targets_data(df_whs):\n",
    "    \"\"\"Fetch complex targets data with warehouse-level SKU targets.\"\"\"\n",
    "    # Build warehouse IDs list for the query\n",
    "    warehouse_ids = df_whs['warehouse_id'].tolist()\n",
    "    wh_str = ', '.join(map(str, warehouse_ids))\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    WITH whs AS (\n",
    "        SELECT *\n",
    "        FROM (VALUES\n",
    "            ('Cairo', 'Mostorod', 1, 700),\n",
    "            ('Giza', 'Barageel', 236, 701),\n",
    "            ('Delta West', 'El-Mahala', 337, 703),\n",
    "            ('Delta West', 'Tanta', 8, 703),\n",
    "            ('Delta East', 'Mansoura FC', 339, 704),\n",
    "            ('Delta East', 'Sharqya', 170, 704),\n",
    "            ('Upper Egypt', 'Assiut FC', 501, 1124),\n",
    "            ('Upper Egypt', 'Bani sweif', 401, 1126),\n",
    "            ('Upper Egypt', 'Menya Samalot', 703, 1123),\n",
    "            ('Upper Egypt', 'Sohag', 632, 1125),\n",
    "            ('Alexandria', 'Khorshed Alex', 797, 702),\n",
    "            ('Giza', 'Sakkarah', 962, 701)\n",
    "        ) x(region, wh, warehouse_id, cohort_id)\n",
    "    ),\n",
    "    base_sales AS (\n",
    "        SELECT\n",
    "            CASE WHEN whs.region LIKE '%Delta%' THEN 'Delta' \n",
    "                 WHEN whs.region = 'Cairo' OR whs.region = 'Giza' THEN 'Greater Cairo' \n",
    "                 ELSE whs.region \n",
    "            END AS region,\n",
    "            pso.warehouse_id,\n",
    "            pso.product_id,\n",
    "            c.name_ar AS cat,\n",
    "            b.name_ar AS brand,\n",
    "            SUM(pso.total_price) AS nmv,\n",
    "            so.created_at::DATE AS sale_date\n",
    "        FROM product_sales_order pso\n",
    "        JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "        JOIN products p ON p.id = pso.product_id\n",
    "        JOIN categories c ON c.id = p.category_id\n",
    "        JOIN brands b ON b.id = p.brand_id\n",
    "        JOIN whs ON whs.warehouse_id = pso.warehouse_id\n",
    "        WHERE so.sales_order_status_id NOT IN (7, 12)\n",
    "            AND pso.purchased_item_count <> 0\n",
    "            AND so.channel IN ('retailer', 'telesales')\n",
    "            AND so.created_at::DATE BETWEEN DATE_TRUNC('month', CURRENT_DATE - INTERVAL '3 month') AND CURRENT_DATE - 1\n",
    "        GROUP BY 1, 2, 3, 4, 5, 7\n",
    "    ),\n",
    "    region_product_nmv AS (\n",
    "        SELECT region, product_id, cat, brand, SUM(nmv) AS region_product_nmv\n",
    "        FROM base_sales\n",
    "        GROUP BY 1, 2, 3, 4\n",
    "    ),\n",
    "    warehouse_contribution AS (\n",
    "        SELECT \n",
    "            bs.region,\n",
    "            bs.warehouse_id,\n",
    "            bs.product_id,\n",
    "            bs.cat,\n",
    "            bs.brand,\n",
    "            SUM(bs.nmv) AS warehouse_nmv,\n",
    "            SUM(bs.nmv) / NULLIF(rpn.region_product_nmv, 0) AS wh_cntrb_in_region\n",
    "        FROM base_sales bs\n",
    "        JOIN region_product_nmv rpn ON rpn.region = bs.region \n",
    "            AND rpn.product_id = bs.product_id\n",
    "        GROUP BY 1, 2, 3, 4, 5, rpn.region_product_nmv\n",
    "    ),\n",
    "    region_sku_cntrb AS (\n",
    "        SELECT region, product_id, cat, brand,\n",
    "            SUM(region_product_nmv) / SUM(SUM(region_product_nmv)) OVER (PARTITION BY region, cat, brand) AS sku_cntrb\n",
    "        FROM region_product_nmv\n",
    "        GROUP BY 1, 2, 3, 4\n",
    "    ),\n",
    "    comm_plan AS (\n",
    "        SELECT\n",
    "            CASE WHEN city = 'Alex' THEN 'Alexandria' ELSE city END AS region,\n",
    "            cat, brand,\n",
    "            SUM(nmv) AS target\n",
    "        FROM performance.commercial_targets\n",
    "        WHERE date BETWEEN DATE_TRUNC('month', CURRENT_DATE) AND CURRENT_DATE - 1\n",
    "        GROUP BY 1, 2, 3\n",
    "    ),\n",
    "    current_month_sales AS (\n",
    "        SELECT region, warehouse_id, product_id, SUM(nmv) AS nmv\n",
    "        FROM base_sales\n",
    "        WHERE sale_date >= DATE_TRUNC('month', CURRENT_DATE)\n",
    "        GROUP BY 1, 2, 3\n",
    "    )\n",
    "    SELECT \n",
    "        wc.region,\n",
    "        wc.warehouse_id,\n",
    "        wc.product_id,\n",
    "        wc.cat,\n",
    "        wc.brand,\n",
    "        cp.target * rsc.sku_cntrb AS region_sku_target,\n",
    "        cp.target * rsc.sku_cntrb * wc.wh_cntrb_in_region AS wh_sku_target,\n",
    "        COALESCE(cms.nmv, 0) AS sales,\n",
    "        cp.target * rsc.sku_cntrb * wc.wh_cntrb_in_region - COALESCE(cms.nmv, 0) AS rem_nmv\n",
    "    FROM warehouse_contribution wc\n",
    "    JOIN region_sku_cntrb rsc ON rsc.region = wc.region \n",
    "        AND rsc.product_id = wc.product_id\n",
    "    JOIN comm_plan cp ON cp.region = wc.region \n",
    "        AND cp.cat = wc.cat \n",
    "        AND cp.brand = wc.brand\n",
    "    LEFT JOIN current_month_sales cms ON cms.product_id = wc.product_id \n",
    "        AND cms.warehouse_id = wc.warehouse_id\n",
    "        AND cms.region = wc.region\n",
    "    \"\"\"\n",
    "    df = snowflake_query(\"Egypt\", query)\n",
    "    for col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='ignore')\n",
    "    return df\n",
    "\n",
    "# Run:\n",
    "df_targets = fetch_targets_data(df_whs)\n",
    "print(f\"Targets data records: {len(df_targets)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33888c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 14: BUILD PRODUCT METRICS (MERGE ALL DATA)\n",
    "# =============================================================================\n",
    "\n",
    "def build_product_metrics(df_stocks, df_sales, df_whs, df_prices, df_cogs, \n",
    "                          df_mp, df_bsp, df_scrapped, df_cat_brand_targets, \n",
    "                          df_cat_targets, df_discounted, df_activation, df_oos_yesterday, df_po_data):\n",
    "    \"\"\"\n",
    "    Merge all data sources to build product metrics.\n",
    "    This replicates the 'product_metrics' CTE from the SQL query.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start with stocks and sales join\n",
    "    df = df_stocks.merge(\n",
    "        df_sales, \n",
    "        on=['product_id', 'warehouse_id'], \n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    # Join warehouse mapping\n",
    "    df = df.merge(df_whs, on='warehouse_id', how='inner')\n",
    "    \n",
    "    # Join prices (using cohort_id from warehouse mapping)\n",
    "    df = df.merge(\n",
    "        df_prices, \n",
    "        on=['product_id', 'cohort_id'], \n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    # Join COGS\n",
    "    df = df.merge(df_cogs, on='product_id', how='inner')\n",
    "    \n",
    "    # Calculate BM (basic margin)\n",
    "    df['bm'] = (df['price'] - df['wac_p']) / df['price'].replace(0, np.nan)\n",
    "    \n",
    "    # Calculate in_stock_perc\n",
    "    df['in_stock_perc'] = (df['stocks'] > 0).astype(int)\n",
    "    \n",
    "    # Join marketplace prices\n",
    "    df = df.merge(\n",
    "        df_mp.rename(columns={\n",
    "            'min_price': 'mp_min_price',\n",
    "            'mod_price': 'mp_mod_price', \n",
    "            'max_price': 'mp_max_price'\n",
    "        }), \n",
    "        on=['product_id', 'region'], \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Join Ben Soliman prices\n",
    "    df = df.merge(df_bsp[['product_id', 'ben_soliman_price']], on='product_id', how='left')\n",
    "    \n",
    "    # Join scrapped prices\n",
    "    df = df.merge(\n",
    "        df_scrapped.rename(columns={\n",
    "            'min_scrapped': 'min_scrapped',\n",
    "            'max_scrapped': 'max_scrapped',\n",
    "            'median_scrapped': 'median_scrapped'\n",
    "        }), \n",
    "        on=['product_id', 'region'], \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Join category/brand targets\n",
    "    df = df.merge(\n",
    "        df_cat_brand_targets[['cat', 'brand', 'target_bm']].drop_duplicates(),\n",
    "        on=['cat', 'brand'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Join category targets (fallback)\n",
    "    df = df.merge(df_cat_targets, on='cat', how='left')\n",
    "    \n",
    "    # Set target_margin (use cat_brand target, fall back to cat target)\n",
    "    df['target_margin'] = df['target_bm'].fillna(df['cat_target_margin'])\n",
    "    \n",
    "    # Join discounted sales\n",
    "    df = df.merge(\n",
    "        df_discounted,\n",
    "        on=['warehouse_id', 'product_id'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Join product activation status\n",
    "    df = df.merge(\n",
    "        df_activation[['product_id', 'warehouse_id', 'activation']],\n",
    "        on=['product_id', 'warehouse_id'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Join OOS yesterday status\n",
    "    df = df.merge(\n",
    "        df_oos_yesterday[['product_id', 'warehouse_id', 'oos_yesterday']],\n",
    "        on=['product_id', 'warehouse_id'],\n",
    "        how='left'\n",
    "    )\n",
    "    # Fill missing oos_yesterday with 0 (assume was in stock if no data)\n",
    "    df['oos_yesterday'] = df['oos_yesterday'].fillna(0)\n",
    "    \n",
    "    # Join PO data\n",
    "    df = df.merge(\n",
    "        df_po_data[['product_id', 'warehouse_id', 'confirmation_status', 'last_po_date', 'ordered_qty', 'no_last_15']],\n",
    "        on=['product_id', 'warehouse_id'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Filter to positive prices and high_rr\n",
    "    df = df[(df['price'] > 0) & (df['high_rr'] > 0)]\n",
    "    \n",
    "    # Remove duplicates - keep first occurrence per product/warehouse combination\n",
    "    df = df.drop_duplicates(subset=['product_id', 'warehouse_id'], keep='first')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Run after fetching all data:\n",
    "df_metrics = build_product_metrics(\n",
    "    df_stocks, df_sales, df_whs, df_prices, df_cogs,\n",
    "    df_mp, df_bsp, df_scrapped, df_cat_brand_targets,\n",
    "    df_cat_targets, df_discounted, df_activation, df_oos_yesterday, df_po_data\n",
    ")\n",
    "print(f\"Product metrics records: {len(df_metrics)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01f9225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 15: SCORING AND CLASSIFICATION\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_predicted_closing_rr(row):\n",
    "    \"\"\"\n",
    "    Calculate predicted closing RR using time-weighted blending (Option 3).\n",
    "    \n",
    "    Logic:\n",
    "    - As month progresses, trust MTD average more (more data available)\n",
    "    - Early in month, weight recent daily RR more (MTD average is noisy)\n",
    "    \n",
    "    Formula:\n",
    "    - month_progress = days_passed / days_in_month\n",
    "    - blended_rate = (month_progress × mtd_avg) + ((1 - month_progress) × cu_rr)\n",
    "    - predicted_closing = cu_mtd_rr + (blended_rate × days_remaining)\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    import calendar\n",
    "    \n",
    "    # Get current date info\n",
    "    today = datetime.now()\n",
    "    days_passed = today.day - 1  # Days completed (excluding today)\n",
    "    days_in_month = calendar.monthrange(today.year, today.month)[1]\n",
    "    days_remaining = days_in_month - days_passed\n",
    "    \n",
    "    # Get values from row\n",
    "    cu_rr = row.get('cu_rr', 0) or 0\n",
    "    cu_mtd_rr = row.get('cu_mtd_rr', 0) or 0\n",
    "    \n",
    "    # Handle edge cases\n",
    "    if days_passed == 0:\n",
    "        # Day 1 of month: no MTD data, use cu_rr as daily estimate\n",
    "        return cu_rr * days_in_month\n",
    "    \n",
    "    if cu_mtd_rr == 0 and cu_rr == 0:\n",
    "        # No sales at all\n",
    "        return 0\n",
    "    \n",
    "    # Calculate MTD average daily rate\n",
    "    mtd_avg = cu_mtd_rr / days_passed\n",
    "    \n",
    "    # Time-weighted blending\n",
    "    # month_progress: 0.0 (start of month) to 1.0 (end of month)\n",
    "    month_progress = days_passed / days_in_month\n",
    "    \n",
    "    # Blend: more weight on MTD as month progresses\n",
    "    mtd_weight = month_progress\n",
    "    recent_weight = 1 - month_progress\n",
    "    \n",
    "    blended_daily_rate = (mtd_weight * mtd_avg) + (recent_weight * cu_rr)\n",
    "    \n",
    "    # Predicted closing = what we have + projected remaining\n",
    "    predicted_closing_rr = cu_mtd_rr + (blended_daily_rate * days_remaining)\n",
    "    \n",
    "    return round(predicted_closing_rr, 0)\n",
    "\n",
    "\n",
    "def add_scoring_classification(df, df_commercial):\n",
    "    \"\"\"\n",
    "    Add scoring and classification columns.\n",
    "    Replicates 'scored_classified' and 'final_scored' CTEs.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Calculate predicted closing RR (Option 3: Time-Weighted Blending)\n",
    "    df['predicted_closing_rr'] = df.apply(calculate_predicted_closing_rr, axis=1)\n",
    "    \n",
    "    # Map region for commercial constraints\n",
    "    df['region_mapped'] = df['region'].apply(\n",
    "        lambda x: 'Greater Cairo' if x in ['Cairo', 'Giza'] else x\n",
    "    )\n",
    "    \n",
    "    # Join commercial constraints\n",
    "    df = df.merge(\n",
    "        df_commercial.rename(columns={'min_price': 'commercial_min'}),\n",
    "        left_on=['product_id', 'region_mapped'],\n",
    "        right_on=['product_id', 'region'],\n",
    "        how='left',\n",
    "        suffixes=('', '_comm')\n",
    "    )\n",
    "    \n",
    "    # Calculate individual discount percentages\n",
    "    df['sku_discount_perc'] = df['sku_discount_nmv'].fillna(0) / df['total_nmv'].replace(0, np.nan)\n",
    "    df['quantity_discount_perc'] = df['quantity_nmv'].fillna(0) / df['total_nmv'].replace(0, np.nan)\n",
    "    \n",
    "    # Calculate total offers percentage (excluding bundle_nmv)\n",
    "    df['offers_perc'] = df['sku_discount_perc'].fillna(0) + df['quantity_discount_perc'].fillna(0)\n",
    "    \n",
    "    # Calculate blended margin (margin using net price after all discounts)\n",
    "    df['blended_margin'] = (df['blended_price'] - df['wac_p']) / df['blended_price'].replace(0, np.nan)\n",
    "    \n",
    "    # Calculate combined market prices\n",
    "    df['combined_min_market'] = df[['mp_min_price','mp_mod_price', 'ben_soliman_price', 'min_scrapped']].min(axis=1)\n",
    "    df['combined_max_market'] = df[['mp_max_price', 'ben_soliman_price', 'max_scrapped']].max(axis=1)\n",
    "    \n",
    "    # Calculate combined median (average of available medians)\n",
    "    median_cols = ['mp_mod_price', 'ben_soliman_price', 'median_scrapped']\n",
    "    df['combined_median_market'] = df[median_cols].mean(axis=1, skipna=True)\n",
    "    \n",
    "    # Calculate mp_price_score\n",
    "    df['mp_price_score'] = (df['price'] - df['mp_min_price']) / (\n",
    "        df['mp_max_price'] - df['mp_min_price']\n",
    "    ).replace(0, np.nan)\n",
    "    \n",
    "    # Stock comment\n",
    "    def get_stock_comment(row):\n",
    "        if row['in_stock_perc'] == 0:\n",
    "            return 'OOS'\n",
    "        elif row['doh'] > 30:\n",
    "            return 'Over Stocked'\n",
    "        elif row['doh'] < 4:\n",
    "            return 'low stock'\n",
    "        else:\n",
    "            return 'Good stocks'\n",
    "    \n",
    "    df['stock_comment'] = df.apply(get_stock_comment, axis=1)\n",
    "    \n",
    "    # RR comment - Weighted MTD and CU logic\n",
    "    # Weights: MTD = 0.6 (higher importance), CU = 0.4\n",
    "    MTD_WEIGHT = 0.7\n",
    "    CU_WEIGHT = 0.3\n",
    "    \n",
    "    # Status to numeric level mapping\n",
    "    STATUS_LEVELS = {'Low': 1, 'Normal': 2, 'High': 3, 'Very High': 4}\n",
    "    \n",
    "    def get_rr_comment(row):\n",
    "        # CU RR variables\n",
    "        cu_rr = row['cu_rr']\n",
    "        high_rr = row['high_rr']\n",
    "        std = row['qty_std']\n",
    "        \n",
    "        # MTD RR variables\n",
    "        cu_mtd_rr = row.get('cu_mtd_rr', 0) or 0\n",
    "        high_mtd_rr = row.get('high_mtd_rr', 0) or 0\n",
    "        mtd_std = row.get('mtd_qty_std', 0) or 0\n",
    "        \n",
    "        # Step 1: Determine MTD Status\n",
    "        if cu_mtd_rr >= high_mtd_rr - 0.5 * mtd_std and cu_mtd_rr <= high_mtd_rr + 0.5 * mtd_std:\n",
    "            mtd_status = 'Normal'\n",
    "        elif cu_mtd_rr < high_mtd_rr - 0.5 * mtd_std:\n",
    "            mtd_status = 'Low'\n",
    "        elif cu_mtd_rr > high_mtd_rr + 0.5 * mtd_std and cu_mtd_rr <= high_mtd_rr + 1.5 * mtd_std:\n",
    "            mtd_status = 'High'\n",
    "        elif cu_mtd_rr > high_mtd_rr + 1.5 * mtd_std:\n",
    "            mtd_status = 'Very High'\n",
    "        else:\n",
    "            mtd_status = 'Normal'\n",
    "        \n",
    "        # Step 2: Determine CU RR Status\n",
    "        if cu_rr >= high_rr - 0.5 * std and cu_rr <= high_rr + 0.5 * std:\n",
    "            cu_status = 'Normal'\n",
    "        elif cu_rr < high_rr - 0.5 * std:\n",
    "            cu_status = 'Low'\n",
    "        elif cu_rr >= high_rr + 0.5 * std and cu_rr <= high_rr + 1.5 * std:\n",
    "            cu_status = 'High'\n",
    "        elif cu_rr > high_rr + 1.5 * std:\n",
    "            cu_status = 'Very High'\n",
    "        else:\n",
    "            cu_status = 'Normal'\n",
    "        \n",
    "        # Step 3: Calculate weighted score\n",
    "        mtd_level = STATUS_LEVELS[mtd_status]\n",
    "        cu_level = STATUS_LEVELS[cu_status]\n",
    "        weighted_score = (mtd_level * MTD_WEIGHT) + (cu_level * CU_WEIGHT)\n",
    "        \n",
    "        # Step 4: Map weighted score to final status\n",
    "        # Score range: 1.0 (both Low) to 4.0 (both Very High)\n",
    "        if weighted_score < 1.5:\n",
    "            return 'low rr'\n",
    "        elif weighted_score < 2.5:\n",
    "            return 'Normal rr'\n",
    "        elif weighted_score < 3.5:\n",
    "            return 'High rr'\n",
    "        else:\n",
    "            return 'Very High rr'\n",
    "    \n",
    "    df['rr_comment'] = df.apply(get_rr_comment, axis=1)\n",
    "    \n",
    "    # Rets comment\n",
    "    def get_rets_comment(row):\n",
    "        cu_rets = row['cu_rets']\n",
    "        high_rets = row['high_rets']\n",
    "        rets_std = row['rets_std']\n",
    "        \n",
    "        if cu_rets >= high_rets - 0.5 * rets_std and cu_rets <= high_rets + 0.5 * rets_std:\n",
    "            return 'Normal rets'\n",
    "        elif cu_rets < high_rets - 0.5 * rets_std:\n",
    "            return 'low rets'\n",
    "        elif cu_rets >= high_rets + 0.5 * rets_std and cu_rets <= high_rets + 1.5 * rets_std:\n",
    "            return 'High rets'\n",
    "        elif cu_rets > high_rets + 1.5 * rets_std:\n",
    "            return 'Very High rets'\n",
    "        else:\n",
    "            return ''\n",
    "    \n",
    "    df['rets_comment'] = df.apply(get_rets_comment, axis=1)\n",
    "    \n",
    "    # Closing RR comment - compare predicted closing with high_full_rr using full_qty_std\n",
    "    def get_closing_rr_comment(row):\n",
    "        predicted_closing = row.get('predicted_closing_rr', 0) or 0\n",
    "        high_full_rr = row.get('high_full_rr', 0) or 0\n",
    "        full_qty_std = row.get('full_qty_std', 0) or 0\n",
    "        \n",
    "        if high_full_rr == 0:\n",
    "            return ''\n",
    "        \n",
    "        if predicted_closing >= high_full_rr - 0.5 * full_qty_std and predicted_closing <= high_full_rr + 0.5 * full_qty_std:\n",
    "            return 'Normal closing'\n",
    "        elif predicted_closing < high_full_rr - 0.5 * full_qty_std:\n",
    "            return 'Low closing'\n",
    "        elif predicted_closing > high_full_rr + 0.5 * full_qty_std and predicted_closing <= high_full_rr + 1.5 * full_qty_std:\n",
    "            return 'High closing'\n",
    "        elif predicted_closing > high_full_rr + 1.5 * full_qty_std:\n",
    "            return 'Very High closing'\n",
    "        else:\n",
    "            return 'Normal closing'\n",
    "    \n",
    "    df['closing_rr_comment'] = df.apply(get_closing_rr_comment, axis=1)\n",
    "    \n",
    "    # Calculate remaining NMV = (high_full_rr - predicted_closing_rr) * price\n",
    "    df['closing_remaining_nmv'] = (df['high_full_rr'].fillna(0) - df['predicted_closing_rr'].fillna(0)) * df['price'].fillna(0)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b9320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 16: FINAL SCORING - MARKET POSITION & PRICE COMMENTS\n",
    "# =============================================================================\n",
    "\n",
    "def add_final_scoring(df):\n",
    "    \"\"\"\n",
    "    Add final scoring columns: combined_price_score, market_position_status, price_comment.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Combined price score\n",
    "    def calc_combined_price_score(row):\n",
    "        combined_max = row['combined_max_market']\n",
    "        combined_min = row['combined_min_market']\n",
    "        price = row['price']\n",
    "        mp_score = row['mp_price_score']\n",
    "        \n",
    "        if pd.notna(combined_max) and combined_max > 0 and pd.notna(combined_min) and combined_min < 1e9:\n",
    "            if combined_max != combined_min:\n",
    "                return (price - combined_min) / (combined_max - combined_min)\n",
    "        return mp_score\n",
    "    \n",
    "    df['combined_price_score'] = df.apply(calc_combined_price_score, axis=1)\n",
    "    \n",
    "    # Market position status\n",
    "    def get_market_position(row):\n",
    "        price = row['price']\n",
    "        combined_min = row['combined_min_market']\n",
    "        combined_median = row['combined_median_market']\n",
    "        combined_max = row['combined_max_market']\n",
    "        mp_min = row['mp_min_price']\n",
    "        bsp = row['ben_soliman_price']\n",
    "        median_scr = row['median_scrapped']\n",
    "        \n",
    "        # Check if no market data\n",
    "        if (pd.isna(combined_median) and pd.isna(mp_min) and \n",
    "            pd.isna(bsp) and pd.isna(median_scr)):\n",
    "            return 'No Market Data'\n",
    "        \n",
    "        # Adjust for edge cases\n",
    "        min_val = combined_min if pd.notna(combined_min) and combined_min < 1e9 else None\n",
    "        max_val = combined_max if pd.notna(combined_max) and combined_max > 0 else None\n",
    "        \n",
    "        if min_val is not None:\n",
    "            if price < min_val:\n",
    "                return 'Below Market'\n",
    "            elif price <= min_val * 1.005:\n",
    "                return 'At Market Min'\n",
    "        \n",
    "        if pd.notna(combined_median):\n",
    "            if price < combined_median * 0.995:\n",
    "                return 'Below Median'\n",
    "            elif price <= combined_median * 1.005:\n",
    "                return 'At Median'\n",
    "        \n",
    "        if max_val is not None:\n",
    "            if price < max_val * 0.995:\n",
    "                return 'Above Median'\n",
    "            elif price <= max_val * 1.005:\n",
    "                return 'At Market Max'\n",
    "            elif price > max_val * 1.005:\n",
    "                return 'Above Market'\n",
    "        \n",
    "        return 'At Median'\n",
    "    \n",
    "    df['market_position_status'] = df.apply(get_market_position, axis=1)\n",
    "    \n",
    "    # Price comment\n",
    "    def get_price_comment(row):\n",
    "        combined_min = row['combined_min_market']\n",
    "        combined_max = row['combined_max_market']\n",
    "        price = row['price']\n",
    "        bm = row['bm']\n",
    "        target = row['target_margin']\n",
    "        mp_score = row['mp_price_score']\n",
    "        \n",
    "        # Calculate price score\n",
    "        if pd.notna(combined_max) and pd.notna(combined_min) and combined_max != combined_min:\n",
    "            price_score = (price - combined_min) / (combined_max - combined_min)\n",
    "        else:\n",
    "            price_score = mp_score\n",
    "        \n",
    "        if pd.isna(price_score):\n",
    "            if pd.notna(bm) and pd.notna(target):\n",
    "                return 'below target' if bm < target else 'above target'\n",
    "            return 'above target'\n",
    "        \n",
    "        # price_score >= 0: at or above market minimum\n",
    "        if price_score >= 0 and bm > target:\n",
    "            return 'High price'\n",
    "        elif price_score >= 0 and bm < target:\n",
    "            return 'Credit note'\n",
    "        # price_score < 0: below market minimum\n",
    "        elif price_score < 0 and bm < target:\n",
    "            return 'Low Price'\n",
    "        elif price_score < 0 and bm > target:\n",
    "            return 'room to reduce'\n",
    "        elif bm < target:\n",
    "            return 'below target'\n",
    "        else:\n",
    "            return 'above target'\n",
    "    \n",
    "    df['price_comment'] = df.apply(get_price_comment, axis=1)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931d1969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 17: ACTION CLASSIFICATION LOGIC\n",
    "# =============================================================================\n",
    "\n",
    "def determine_action(row):\n",
    "    \"\"\"\n",
    "    Determine recommended action based on stock, price, and RR status.\n",
    "    This replicates the complex CASE statement in the final SELECT.\n",
    "    \"\"\"\n",
    "    stock_comment = row['stock_comment']\n",
    "    price_comment = row['price_comment']\n",
    "    rr_comment = row['rr_comment']\n",
    "    offers_perc = row.get('offers_perc', 0) or 0\n",
    "    commercial_min = row.get('commercial_min')\n",
    "    bm = row['bm']\n",
    "    target = row['target_margin']\n",
    "    cu_rr = row['cu_rr']\n",
    "    today_rr = row['today_rr']\n",
    "    stocks = row['stocks']\n",
    "    activation = row.get('activation', True)\n",
    "    oos_yesterday = row.get('oos_yesterday', 0)\n",
    "    price = row['price']\n",
    "    blended_price = row.get('blended_price')\n",
    "    blended_margin = row.get('blended_margin')\n",
    "    combined_min_market = row.get('combined_min_market')\n",
    "    \n",
    "    # OOS - always needs purchase regardless of other conditions\n",
    "    if stock_comment == 'OOS':\n",
    "        return 'Purchase'\n",
    "    \n",
    "    # If product was OOS yesterday and has low rr, no action needed\n",
    "    # (low rr is expected when product was out of stock)\n",
    "    if rr_comment == 'low rr' and oos_yesterday == 1 and today_rr > 0 :\n",
    "        return 'No action'\n",
    "    \n",
    "    # Check if High RR / Very High RR products need offer revision\n",
    "    # Only applies if SKU has active offers (offers_perc > 0)\n",
    "    # If blended price is 1% below min market price OR (no market data AND blended margin is 15% below target)\n",
    "    if rr_comment in ['High rr', 'Very High rr'] and pd.notna(blended_price) and offers_perc > 0:\n",
    "        has_market_data = pd.notna(combined_min_market)\n",
    "        if has_market_data:\n",
    "            # Blended price is 1% or more below minimum market price\n",
    "            if blended_price < combined_min_market * 0.99:\n",
    "                return 'Revisit the offer'\n",
    "        else:\n",
    "            # No market data - check if blended margin is 15% below target\n",
    "            if pd.notna(blended_margin) and pd.notna(target) and blended_margin < (target*0.9):\n",
    "                return 'Revisit the offer'\n",
    "    \n",
    "    # Good stocks scenarios\n",
    "    if stock_comment == 'Good stocks':\n",
    "        if price_comment in ['Low Price', 'below target'] and rr_comment == 'low rr':\n",
    "            if offers_perc < 0.1:\n",
    "                return 'Offers & Credit Note'\n",
    "            return 'Credit Note'\n",
    "        if price_comment in ['Low Price', 'below target'] and rr_comment != 'low rr':\n",
    "            return 'Increase price'\n",
    "        if price_comment == 'Credit note' and rr_comment == 'low rr':\n",
    "            return 'Credit Note'\n",
    "        # With market data: price position known - reduce price\n",
    "        if price_comment in ['High price', 'room to reduce'] and rr_comment == 'low rr':\n",
    "            if pd.isna(commercial_min) or commercial_min < price*0.99:\n",
    "                return 'Reduce price'\n",
    "            return 'Remove commercial min'\n",
    "        # No market data: only margin > target - check offers first\n",
    "        if price_comment == 'above target' and rr_comment == 'low rr':\n",
    "            if offers_perc < 0.1:\n",
    "                return 'Offers'\n",
    "            if pd.isna(commercial_min) or commercial_min < price*0.99:\n",
    "                return 'Reduce price'\n",
    "            return 'Remove commercial min'\n",
    "        if rr_comment == 'Normal rr':\n",
    "            return 'No action'\n",
    "        if rr_comment == 'Very High rr' and bm < target:\n",
    "            return 'Increase price'\n",
    "        if rr_comment in ['Very High rr', 'High rr'] and bm >= target:\n",
    "            return 'No action'\n",
    "        if rr_comment == 'High rr' and bm < target:\n",
    "            return 'Increase price a bit'\n",
    "    \n",
    "    # Low stock scenarios\n",
    "    if stock_comment == 'low stock':\n",
    "        if price_comment == 'Credit note' and rr_comment == 'low rr':\n",
    "            return 'Purchase & Credit Note'\n",
    "        # No market data: margin < target - need purchase + credit note\n",
    "        if price_comment == 'below target' and rr_comment == 'low rr':\n",
    "            return 'Purchase & Credit Note'\n",
    "        # With market data: price < min, margin < target\n",
    "        if price_comment == 'Low Price' and rr_comment == 'low rr':\n",
    "            if offers_perc < 0.1:\n",
    "                return 'Purchase & Offers & Credit Note'\n",
    "            return 'Purchase & Credit Note'\n",
    "        # With market data: price position known - purchase + reduce price\n",
    "        if price_comment in ['High price', 'room to reduce'] and rr_comment == 'low rr':\n",
    "            if pd.isna(commercial_min) or commercial_min < price*0.99:\n",
    "                return 'Purchase & Reduce price'\n",
    "            return 'Purchase & Remove commercial min'\n",
    "        # No market data: margin > target - check offers first\n",
    "        if price_comment == 'above target' and rr_comment == 'low rr':\n",
    "            if offers_perc < 0.1:\n",
    "                return 'Purchase & Offers'\n",
    "            if pd.isna(commercial_min) or commercial_min < price*0.99:\n",
    "                return 'Purchase & Reduce price'\n",
    "            return 'Purchase & Remove commercial min'\n",
    "        if rr_comment in ['High rr', 'Normal rr']:\n",
    "            return 'Purchase'\n",
    "        if rr_comment == 'Very High rr':\n",
    "            return 'Purchase & Increase price'\n",
    "    \n",
    "    # Over stocked scenarios\n",
    "    if stock_comment == 'Over Stocked':\n",
    "        if price_comment in ['below target', 'Low Price', 'Credit note'] and rr_comment == 'low rr':\n",
    "            return 'Credit Note'\n",
    "        # With market data: price position known\n",
    "        if price_comment in ['High price', 'room to reduce'] and rr_comment == 'low rr':\n",
    "            if cu_rr > 0:\n",
    "                if pd.isna(commercial_min) or commercial_min < price*0.99:\n",
    "                    return 'Reduce price'\n",
    "                return 'Remove commercial min'\n",
    "            elif today_rr == 0:\n",
    "                if activation == False:\n",
    "                    return 'Reactivate'\n",
    "                else:  # activation == True\n",
    "                    if pd.isna(commercial_min) or commercial_min < price*0.99:\n",
    "                        return 'Reduce price'\n",
    "                    return 'Remove commercial min'\n",
    "            else:\n",
    "                # cu_rr <= 0 but today_rr > 0: sales recovering, no action needed\n",
    "                return 'No action'\n",
    "        # No market data: margin > target - check offers first\n",
    "        if price_comment == 'above target' and rr_comment == 'low rr':\n",
    "            if cu_rr > 0:\n",
    "                if offers_perc < 0.1:\n",
    "                    return 'Offers'\n",
    "                if pd.isna(commercial_min) or commercial_min < price*0.99:\n",
    "                    return 'Reduce price'\n",
    "                return 'Remove commercial min'\n",
    "            elif today_rr == 0:\n",
    "                if activation == False:\n",
    "                    return 'Reactivate'\n",
    "                else:  # activation == True\n",
    "                    if offers_perc < 0.1:\n",
    "                        return 'Offers'\n",
    "                    if pd.isna(commercial_min) or commercial_min < price*0.99:\n",
    "                        return 'Reduce price'\n",
    "                    return 'Remove commercial min'\n",
    "            else:\n",
    "                # cu_rr <= 0 but today_rr > 0: sales recovering, no action needed\n",
    "                return 'No action'\n",
    "        if price_comment in ['below target', 'Low Price', 'Credit note'] and rr_comment in ['Very High rr', 'High rr', 'Normal rr']:\n",
    "            if stocks / (cu_rr if cu_rr > 0 else 1) < 30:\n",
    "                return 'No Action'\n",
    "            return 'Credit Note'\n",
    "        if price_comment in ['High price', 'above target'] and rr_comment in ['Very High rr', 'High rr', 'Normal rr']:\n",
    "            if stocks / (cu_rr if cu_rr > 0 else 1) < 30:\n",
    "                return 'No Action'\n",
    "            return 'Reduce Price'\n",
    "    \n",
    "    # Additional edge cases\n",
    "    if price_comment in ['below target', 'Low Price'] and rr_comment == 'low rr':\n",
    "        if cu_rr == 0 and today_rr > 0:\n",
    "            return 'No action'\n",
    "        elif cu_rr == 0:\n",
    "            if activation == False:\n",
    "                return 'Reactivate'\n",
    "            else:  # activation == True\n",
    "                return 'Credit Note'\n",
    "    \n",
    "    # Edge case for above target with no running rate\n",
    "    if price_comment == 'above target' and rr_comment == 'low rr':\n",
    "        if cu_rr == 0 and today_rr > 0:\n",
    "            return 'No action'\n",
    "        elif cu_rr == 0:\n",
    "            if activation == False:\n",
    "                return 'Reactivate'\n",
    "            else:  # activation == True\n",
    "                if offers_perc < 0.1:\n",
    "                    return 'Offers'\n",
    "                return 'Reduce price'\n",
    "    \n",
    "    return None\n",
    "\n",
    "def add_actions(df):\n",
    "    \"\"\"Add action column (team assignment done separately after all action modifications).\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Determine action\n",
    "    df['action'] = df.apply(determine_action, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def assign_teams(df):\n",
    "    \"\"\"\n",
    "    Assign teams based on final action.\n",
    "    This should be called AFTER all action modifications (including add_stock_issue_owner).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Assign to teams based on action\n",
    "    df['pricing_team'] = df['action'].apply(\n",
    "        lambda x: 1 if pd.notna(x) and ('price' in str(x).lower() or 'offers' in str(x).lower() or 'offer' in str(x).lower()) else None\n",
    "    )\n",
    "    df['purchase_team'] = df['action'].apply(\n",
    "        lambda x: 1 if pd.notna(x) and 'purchase' in str(x).lower() else None\n",
    "    )\n",
    "    df['commercial_team'] = df['action'].apply(\n",
    "        lambda x: 1 if pd.notna(x) and ('credit note' in str(x).lower() or \n",
    "                                         'commercial min' in str(x).lower() or \n",
    "                                         'reactivate' in str(x).lower() or\n",
    "                                         'supplier' in str(x).lower()) else None\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def add_stock_issue_owner(df):\n",
    "    \"\"\"\n",
    "    Determine who is responsible for stock issues (OOS/Low stock).\n",
    "    Only applies to Low stock and OOS products.\n",
    "    \n",
    "    Logic:\n",
    "    - If ordered_qty is low (< 3*cu_rr or < 0.9*high_rr if cu_rr=0) → Purchase team\n",
    "    - If in top 60% of positive NMV gap AND no_last_15 > 0 AND ordered in last 2 days → Commercial team\n",
    "    - If not ordered in last 2 days → Purchase team\n",
    "    \n",
    "    Also updates the action based on the issue ownership.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Calculate minimum required order qty\n",
    "    df['min_required_qty'] = df.apply(\n",
    "        lambda row: 3 * (\n",
    "    0.85*row['high_rr'] if row['cu_rr'] == 0\n",
    "    else row['cu_rr'] if row['high_rr'] == 0\n",
    "    else min(row['high_rr'], row['cu_rr'])\n",
    "),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Check if ordered qty is low (ordered_qty < min_required_qty)\n",
    "    df['ordered_qty_low'] = df['ordered_qty'].fillna(0) < df['min_required_qty']\n",
    "    \n",
    "    # Calculate days since last PO\n",
    "    today = pd.Timestamp.now().normalize()\n",
    "    df['days_since_po'] = df['last_po_date'].apply(\n",
    "        lambda x: (today - x).days if pd.notna(x) else None\n",
    "    )\n",
    "    \n",
    "    # Calculate nmv_gap: (high_rr * price) - (cu_rr * price)\n",
    "    df['nmv_gap'] = (df['high_rr'] * df['price']) - (df['cu_rr'] * df['price'])\n",
    "    \n",
    "    # Calculate cumulative contribution of positive NMV gap\n",
    "    # Only consider positive gaps (behind target)\n",
    "    df['positive_nmv_gap'] = df['nmv_gap'].apply(lambda x: max(0, x) if pd.notna(x) else 0)\n",
    "    \n",
    "    # IMPORTANT: Deduplicate BEFORE calculating cumulative contribution\n",
    "    # to ensure correct gap percentages\n",
    "    df_for_gap = df[['product_id', 'warehouse_id', 'positive_nmv_gap']].drop_duplicates(\n",
    "        subset=['product_id', 'warehouse_id'], keep='first'\n",
    "    )\n",
    "    \n",
    "    # Sort by positive gap descending and calculate cumulative contribution\n",
    "    df_sorted = df_for_gap.sort_values('positive_nmv_gap', ascending=False).copy()\n",
    "    total_positive_gap = df_sorted['positive_nmv_gap'].sum()\n",
    "    \n",
    "    if total_positive_gap > 0:\n",
    "        df_sorted['cumulative_gap'] = df_sorted['positive_nmv_gap'].cumsum()\n",
    "        df_sorted['cumulative_gap_pct'] = df_sorted['cumulative_gap'] / total_positive_gap\n",
    "        # Mark products in top 60% of gap contribution\n",
    "        df_sorted['in_top_60_gap'] = df_sorted['cumulative_gap_pct'] <= 0.6\n",
    "    else:\n",
    "        df_sorted['in_top_60_gap'] = False\n",
    "    \n",
    "    # Merge back the in_top_60_gap flag\n",
    "    df = df.merge(\n",
    "        df_sorted[['product_id', 'warehouse_id', 'in_top_60_gap']],\n",
    "        on=['product_id', 'warehouse_id'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Determine stock issue owner and update action (only for OOS and low stock)\n",
    "    def get_stock_issue_info(row):\n",
    "        stock_comment = row['stock_comment']\n",
    "        \n",
    "        # Only applies to OOS and low stock\n",
    "        if stock_comment not in ['OOS', 'low stock']:\n",
    "            return None, row['action']\n",
    "        \n",
    "        ordered_qty = row.get('ordered_qty')\n",
    "        days_since_po = row.get('days_since_po')\n",
    "        no_last_15 = row.get('no_last_15', 0) or 0\n",
    "        in_top_60_gap = row.get('in_top_60_gap', False)\n",
    "        ordered_qty_low = row.get('ordered_qty_low', False)\n",
    "        last_po_date = row.get('last_po_date')\n",
    "        min_required_qty = row.get('min_required_qty', 0)\n",
    "        \n",
    "        # Format last_po_date for display\n",
    "        last_po_str = last_po_date.strftime('%Y-%m-%d') if pd.notna(last_po_date) else 'Never'\n",
    "        \n",
    "        # If not ordered in last 2 days → Purchase team - need to place order\n",
    "        if pd.isna(days_since_po) or days_since_po > 2:\n",
    "            owner = 'Purchase team'\n",
    "            action = f'Purchase (last order: {last_po_str})'\n",
    "            return owner, action\n",
    "        \n",
    "        # If ordered qty is low → Purchase team - ordered but not enough\n",
    "        if ordered_qty_low:\n",
    "            owner = 'Purchase team'\n",
    "            action = f'Purchase (ordered qty {int(ordered_qty)} is low, need {int(min_required_qty)})'\n",
    "            return owner, action\n",
    "        \n",
    "        # If in top 60% gap AND multiple no confirmations AND ordered recently → Commercial team\n",
    "        if in_top_60_gap and no_last_15 > 0 and days_since_po <= 2:\n",
    "            owner = 'Commercial team'\n",
    "            action = f'Supplier issue ({int(no_last_15)} rejections) - negotiate with supplier'\n",
    "            return owner, action\n",
    "        \n",
    "        # Default: Purchase team (ordered but other issues)\n",
    "        # owner = 'Purchase team'\n",
    "        # action = f'Purchase (last order: {last_po_str})'\n",
    "        owner = None\n",
    "        action = 'No action'\n",
    "        return owner, action\n",
    "    \n",
    "    # Apply the function to get stock issue info\n",
    "    df['_stock_issue_info'] = df.apply(get_stock_issue_info, axis=1)\n",
    "    \n",
    "    # Extract owner and action from the tuple\n",
    "    df['stock_issue_owner'] = df['_stock_issue_info'].apply(lambda x: x[0] if x else None)\n",
    "    df['_new_action'] = df['_stock_issue_info'].apply(lambda x: x[1] if x else None)\n",
    "    \n",
    "    # Update action only for OOS/low stock products\n",
    "    mask = df['stock_comment'].isin(['OOS', 'low stock'])\n",
    "    df.loc[mask, 'action'] = df.loc[mask, '_new_action']\n",
    "    \n",
    "    # Clean up temporary columns\n",
    "    df = df.drop(columns=['min_required_qty', 'ordered_qty_low', 'positive_nmv_gap', 'in_top_60_gap', '_stock_issue_info', '_new_action'], errors='ignore')\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015fd88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 18: FINALIZE OUTPUT & ADD CALCULATED COLUMNS\n",
    "# =============================================================================\n",
    "\n",
    "def finalize_output(df, df_targets):\n",
    "    \"\"\"\n",
    "    Finalize the output DataFrame with all calculated columns.\n",
    "    Add stock value, stock contribution, and join targets data.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Calculate stock value\n",
    "    df['stock_value'] = df['stocks'] * df['price']\n",
    "    \n",
    "    # Calculate stock contribution per warehouse\n",
    "    df['stock_cntrb'] = df.groupby('warehouse_id')['stock_value'].transform(\n",
    "        lambda x: x / x.sum() if x.sum() > 0 else 0\n",
    "    )\n",
    "    \n",
    "    # Join targets data (deduplicate targets first to avoid row multiplication)\n",
    "    df_targets_dedup = df_targets[['warehouse_id', 'product_id', 'wh_sku_target', 'rem_nmv']].drop_duplicates(\n",
    "        subset=['warehouse_id', 'product_id'], keep='first'\n",
    "    )\n",
    "    df = df.merge(\n",
    "        df_targets_dedup,\n",
    "        on=['warehouse_id', 'product_id'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Clean up combined_min_market (replace inf with None)\n",
    "    df['combined_min_market'] = df['combined_min_market'].replace([np.inf, -np.inf, 1e9], np.nan)\n",
    "    df['combined_max_market'] = df['combined_max_market'].replace([0, np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Select and order final columns\n",
    "    final_columns = [\n",
    "        'region', 'wh', 'warehouse_id', 'product_id', 'sku', 'cat', 'brand',\n",
    "        'stocks', 'doh', 'stock_comment','wac_p', 'price', 'blended_price', 'bm', 'blended_margin', 'target_margin', 'price_comment',\n",
    "        'mp_min_price', 'mp_mod_price', 'mp_max_price', 'ben_soliman_price',\n",
    "        'min_scrapped', 'median_scrapped', 'max_scrapped',\n",
    "        'combined_min_market', 'combined_median_market', 'combined_max_market',\n",
    "        'mp_price_score', 'combined_price_score', 'market_position_status',\n",
    "        'high_rr', 'cu_rr', 'today_rr', 'high_mtd_rr', 'cu_mtd_rr', 'predicted_closing_rr', 'high_full_rr', 'closing_rr_comment', 'closing_remaining_nmv', 'rr_comment',\n",
    "        'high_rets', 'cu_rets', 'rets_comment', 'sku_discount_perc', 'quantity_discount_perc', 'offers_perc', 'commercial_min',\n",
    "        'action', 'pricing_team', 'purchase_team', 'commercial_team', 'activation', 'oos_yesterday',\n",
    "        'last_po_date', 'ordered_qty', 'confirmation_status', 'no_last_15', 'days_since_po', 'stock_issue_owner',\n",
    "        'stock_value', 'stock_cntrb', 'wh_sku_target', 'rem_nmv'\n",
    "    ]\n",
    "    \n",
    "    # Keep only columns that exist\n",
    "    existing_cols = [c for c in final_columns if c in df.columns]\n",
    "    df = df[existing_cols]\n",
    "    \n",
    "    # Sort by high_rr * price descending\n",
    "    df['_sort_key'] = df['high_rr'] * df['price']\n",
    "    df = df.sort_values('_sort_key', ascending=False).drop('_sort_key', axis=1)\n",
    "    \n",
    "    # Rename 'wh' to 'warehouse_name' for clarity\n",
    "    df = df.rename(columns={'wh': 'warehouse_name', 'cu_rr': 'current_rr'})\n",
    "    \n",
    "    # Final deduplication - ensure no duplicate product/warehouse rows\n",
    "    df = df.drop_duplicates(subset=['product_id', 'warehouse_id'], keep='first')\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdb29c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 19: MAIN EXECUTION - RUN THE COMPLETE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def run_pricing_status_analysis():\n",
    "    \"\"\"\n",
    "    Main function to run the complete pricing status analysis.\n",
    "    Uses global dataframes that were already fetched by running the cells above.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with all pricing status metrics and recommended actions.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"PRICING STATUS ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Use global dataframes (already fetched by running cells above)\n",
    "    print(\"\\nUsing pre-fetched data:\")\n",
    "    print(f\"    ✓ df_whs: {len(df_whs)} warehouses\")\n",
    "    print(f\"    ✓ df_cogs: {len(df_cogs)} COGS records\")\n",
    "    print(f\"    ✓ df_stocks: {len(df_stocks)} stock records\")\n",
    "    print(f\"    ✓ df_sales: {len(df_sales)} sales records\")\n",
    "    print(f\"    ✓ df_prices: {len(df_prices)} price records\")\n",
    "    print(f\"    ✓ df_mp: {len(df_mp)} marketplace price records\")\n",
    "    print(f\"    ✓ df_bsp: {len(df_bsp)} Ben Soliman price records\")\n",
    "    print(f\"    ✓ df_scrapped: {len(df_scrapped)} scrapped price records\")\n",
    "    print(f\"    ✓ df_cat_brand_targets: {len(df_cat_brand_targets)} category/brand targets\")\n",
    "    print(f\"    ✓ df_cat_targets: {len(df_cat_targets)} category targets\")\n",
    "    print(f\"    ✓ df_discounted: {len(df_discounted)} discounted sales records\")\n",
    "    print(f\"    ✓ df_commercial: {len(df_commercial)} commercial constraint records\")\n",
    "    print(f\"    ✓ df_targets: {len(df_targets)} target records\")\n",
    "    print(f\"    ✓ df_activation: {len(df_activation)} activation records\")\n",
    "    print(f\"    ✓ df_oos_yesterday: {len(df_oos_yesterday)} OOS yesterday records\")\n",
    "    print(f\"    ✓ df_po_data: {len(df_po_data)} PO records\")\n",
    "    \n",
    "    # Process and merge data\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"PROCESSING DATA...\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Build product metrics\n",
    "    print(\"\\n[A] Building product metrics...\")\n",
    "    df_metrics = build_product_metrics(\n",
    "        df_stocks, df_sales, df_whs, df_prices, df_cogs,\n",
    "        df_mp, df_bsp, df_scrapped, df_cat_brand_targets,\n",
    "        df_cat_targets, df_discounted, df_activation, df_oos_yesterday, df_po_data\n",
    "    )\n",
    "    print(f\"    ✓ {len(df_metrics)} product-warehouse combinations\")\n",
    "    \n",
    "    # Add scoring and classification\n",
    "    print(\"\\n[B] Adding scoring and classification...\")\n",
    "    df_scored = add_scoring_classification(df_metrics, df_commercial)\n",
    "    print(f\"    ✓ Scoring added\")\n",
    "    \n",
    "    # Add final scoring\n",
    "    print(\"\\n[C] Adding final scoring (market position, price comments)...\")\n",
    "    df_final_scored = add_final_scoring(df_scored)\n",
    "    print(f\"    ✓ Final scoring added\")\n",
    "    \n",
    "    # Add actions\n",
    "    print(\"\\n[D] Determining recommended actions...\")\n",
    "    df_with_actions = add_actions(df_final_scored)\n",
    "    print(f\"    ✓ Actions determined\")\n",
    "    \n",
    "    # Add stock issue owner\n",
    "    print(\"\\n[E] Determining stock issue ownership...\")\n",
    "    df_with_stock_owner = add_stock_issue_owner(df_with_actions)\n",
    "    print(f\"    ✓ Stock issue ownership determined\")\n",
    "    \n",
    "    # Assign teams based on final actions\n",
    "    print(\"\\n[F] Assigning teams...\")\n",
    "    df_with_teams = assign_teams(df_with_stock_owner)\n",
    "    print(f\"    ✓ Teams assigned\")\n",
    "    \n",
    "    # Finalize output\n",
    "    print(\"\\n[G] Finalizing output...\")\n",
    "    df_final = finalize_output(df_with_teams, df_targets)\n",
    "    print(f\"    ✓ Final output ready with {len(df_final)} records\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ANALYSIS COMPLETE!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "# Run the full analysis:\n",
    "df_result = run_pricing_status_analysis()\n",
    "df_result.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c646d27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 20: UTILITY FUNCTIONS - EXPORT & SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "def export_results(df, filename='pricing_status_output.xlsx'):\n",
    "    \"\"\"Export results to Excel file.\"\"\"\n",
    "    df.to_excel(filename, index=False)\n",
    "    print(f\"Results exported to {filename}\")\n",
    "    return filename\n",
    "\n",
    "def get_summary_stats(df):\n",
    "    \"\"\"Generate summary statistics from the analysis results.\"\"\"\n",
    "    summary = {\n",
    "        'Total SKU-Warehouse combinations': len(df),\n",
    "        'Unique Products': df['product_id'].nunique() if 'product_id' in df.columns else 0,\n",
    "        'Unique Warehouses': df['warehouse_id'].nunique() if 'warehouse_id' in df.columns else 0,\n",
    "    }\n",
    "    \n",
    "    # Stock status breakdown\n",
    "    if 'stock_comment' in df.columns:\n",
    "        stock_status = df['stock_comment'].value_counts().to_dict()\n",
    "        summary['Stock Status'] = stock_status\n",
    "    \n",
    "    # Action breakdown\n",
    "    if 'action' in df.columns:\n",
    "        action_counts = df['action'].value_counts().to_dict()\n",
    "        summary['Actions'] = action_counts\n",
    "    \n",
    "    # Team assignments\n",
    "    if 'pricing_team' in df.columns:\n",
    "        summary['Pricing Team Items'] = df['pricing_team'].notna().sum()\n",
    "    if 'purchase_team' in df.columns:\n",
    "        summary['Purchase Team Items'] = df['purchase_team'].notna().sum()\n",
    "    if 'commercial_team' in df.columns:\n",
    "        summary['Commercial Team Items'] = df['commercial_team'].notna().sum()\n",
    "    \n",
    "    # Market position breakdown\n",
    "    if 'market_position_status' in df.columns:\n",
    "        market_pos = df['market_position_status'].value_counts().to_dict()\n",
    "        summary['Market Position'] = market_pos\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def print_summary(summary):\n",
    "    \"\"\"Pretty print the summary statistics.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SUMMARY STATISTICS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for key, value in summary.items():\n",
    "        if isinstance(value, dict):\n",
    "            print(f\"\\n{key}:\")\n",
    "            for k, v in value.items():\n",
    "                print(f\"    {k}: {v}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# Usage:\n",
    "summary = get_summary_stats(df_result)\n",
    "print_summary(summary)\n",
    "export_results(df_result, 'pricing_status_output.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988918c0",
   "metadata": {},
   "source": [
    "# Pricing Status Analysis - Quick Reference\n",
    "\n",
    "## Data Flow Overview:\n",
    "\n",
    "1. **Static Data**: Warehouse mappings (region, cohort_id)\n",
    "2. **COGS**: Current cost of goods (wac_p)\n",
    "3. **Running Rates**: Predicted running rates from past 14 days\n",
    "4. **Stocks**: Available stock with DOH calculations\n",
    "5. **Sales**: 150-day sales history with percentile metrics\n",
    "6. **Prices**: Latest cohort pricing\n",
    "7. **Market Prices**: Min/Mod/Max from marketplace, Ben Soliman, and scraped data\n",
    "8. **Targets**: Category/brand margin targets\n",
    "9. **Discounts**: Bundle, SKU discount, quantity discount percentages\n",
    "10. **Commercial Constraints**: Minimum price restrictions\n",
    "\n",
    "## Key Metrics:\n",
    "\n",
    "| Metric | Description |\n",
    "|--------|-------------|\n",
    "| `doh` | Days on Hand (stocks / running_rate) |\n",
    "| `bm` | Basic Margin ((price - cost) / price) |\n",
    "| `high_rr` | 80th percentile of historical running rate |\n",
    "| `combined_price_score` | Position within market price range (0-1) |\n",
    "\n",
    "## Action Matrix:\n",
    "\n",
    "| Stock Status | Price Status | RR Status | Recommended Action |\n",
    "|--------------|--------------|-----------|-------------------|\n",
    "| OOS | - | - | Purchase |\n",
    "| Good stocks | Low/Below target | Low RR | Offers & Credit Note |\n",
    "| Good stocks | High | Low RR | Reduce price / Remove commercial min |\n",
    "| Low stock | - | Very High RR | Increase price |\n",
    "| Over Stocked | High | Low RR, cu_rr=0 | Check activation |\n",
    "\n",
    "## Configuration:\n",
    "\n",
    "To customize the analysis, modify:\n",
    "- `get_warehouse_mapping()` - Add/remove warehouses\n",
    "- `fetch_prices()` - Modify cohort_ids\n",
    "- `determine_action()` - Adjust action logic thresholds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a538dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RUN ANALYSIS - UNCOMMENT AND EXECUTE\n",
    "# =============================================================================\n",
    "\n",
    "# Run the full analysis:\n",
    "df_result = run_pricing_status_analysis()\n",
    "\n",
    "# View summary:\n",
    "summary = get_summary_stats(df_result)\n",
    "print_summary(summary)\n",
    "\n",
    "# Preview the data:\n",
    "df_result.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556310e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPORT RESULTS TO EXCEL (Optional)\n",
    "# =============================================================================\n",
    "\n",
    "# Export:\n",
    "export_results(df_result, 'pricing_status_output.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2595bbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# AGGREGATE ANALYSIS VIEW\n",
    "# =============================================================================\n",
    "\n",
    "def create_aggregate_analysis(df):\n",
    "    \"\"\"\n",
    "    Create aggregate analysis showing:\n",
    "    - Total target NMV (high_rr * price)\n",
    "    - Top dropping brands based on RR performance\n",
    "    - Market status breakdown by brand\n",
    "    - Required actions summary\n",
    "    \"\"\"\n",
    "    df_analysis = df.copy()\n",
    "    \n",
    "    # Calculate target NMV per row (high_rr * price)\n",
    "    df_analysis['target_nmv'] = df_analysis['high_rr'] * df_analysis['price']\n",
    "    \n",
    "    # Calculate RR drop percentage: (high_rr - current_rr) / high_rr\n",
    "    df_analysis['rr_drop_pct'] = (df_analysis['high_rr'] - df_analysis['current_rr']) / df_analysis['high_rr'].replace(0, np.nan)\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # 1. TOTAL TARGET NMV SUMMARY\n",
    "    # ==========================================================================\n",
    "    total_target_nmv = df_analysis['target_nmv'].sum()\n",
    "    total_current_nmv = (df_analysis['current_rr'] * df_analysis['price']).sum()\n",
    "    nmv_gap = total_target_nmv - total_current_nmv\n",
    "    nmv_gap_pct = nmv_gap / total_target_nmv * 100 if total_target_nmv > 0 else 0\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"📊 AGGREGATE ANALYSIS - PRICING STATUS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\n\" + \"─\" * 80)\n",
    "    print(\"💰 TOTAL NMV SUMMARY\")\n",
    "    print(\"─\" * 80)\n",
    "    print(f\"  Target NMV (High RR × Price):    {total_target_nmv:>15,.0f} EGP\")\n",
    "    print(f\"  Current NMV (Current RR × Price): {total_current_nmv:>15,.0f} EGP\")\n",
    "    print(f\"  NMV Gap:                          {nmv_gap:>15,.0f} EGP ({nmv_gap_pct:.1f}%)\")\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # 2. TOP DROPPING BRANDS ANALYSIS\n",
    "    # ==========================================================================\n",
    "    brand_agg = df_analysis.groupby('brand').agg({\n",
    "        'target_nmv': 'sum',\n",
    "        'high_rr': 'sum',\n",
    "        'current_rr': 'sum',\n",
    "        'price': 'mean',\n",
    "        'product_id': 'nunique',\n",
    "        'warehouse_id': 'nunique'\n",
    "    }).reset_index()\n",
    "    \n",
    "    brand_agg.columns = ['brand', 'target_nmv', 'total_high_rr', 'total_current_rr', \n",
    "                         'avg_price', 'num_products', 'num_warehouses']\n",
    "    \n",
    "    # Calculate current NMV and drop metrics\n",
    "    brand_agg['current_nmv'] = brand_agg['total_current_rr'] * brand_agg['avg_price']\n",
    "    brand_agg['nmv_drop'] = brand_agg['target_nmv'] - brand_agg['current_nmv']\n",
    "    brand_agg['rr_drop_pct'] = ((brand_agg['total_high_rr'] - brand_agg['total_current_rr']) / \n",
    "                                 brand_agg['total_high_rr'].replace(0, np.nan) * 100)\n",
    "    \n",
    "    # Sort by NMV drop (biggest drops first)\n",
    "    brand_agg_sorted = brand_agg.sort_values('nmv_drop', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + \"─\" * 80)\n",
    "    print(\"📉 TOP 15 DROPPING BRANDS (by NMV Gap)\")\n",
    "    print(\"─\" * 80)\n",
    "    \n",
    "    top_dropping = brand_agg_sorted.head(15)\n",
    "    print(f\"{'Brand':<30} {'Target NMV':>15} {'Current NMV':>15} {'NMV Drop':>15} {'RR Drop%':>10}\")\n",
    "    print(\"─\" * 85)\n",
    "    for _, row in top_dropping.iterrows():\n",
    "        print(f\"{str(row['brand'])[:30]:<30} {row['target_nmv']:>15,.0f} {row['current_nmv']:>15,.0f} \"\n",
    "              f\"{row['nmv_drop']:>15,.0f} {row['rr_drop_pct']:>9.1f}%\")\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # 3. MARKET STATUS BY DROPPING BRANDS\n",
    "    # ==========================================================================\n",
    "    # Get top 15 dropping brand names\n",
    "    top_dropping_brands = top_dropping['brand'].tolist()\n",
    "    \n",
    "    # Filter data to only include top dropping brands\n",
    "    df_top_brands = df_analysis[df_analysis['brand'].isin(top_dropping_brands)]\n",
    "    \n",
    "    # Market status breakdown for top dropping brands\n",
    "    market_status_by_brand = df_top_brands.groupby(['brand', 'market_position_status']).agg({\n",
    "        'target_nmv': 'sum',\n",
    "        'product_id': 'nunique'\n",
    "    }).reset_index()\n",
    "    \n",
    "    market_status_by_brand.columns = ['brand', 'market_position', 'target_nmv', 'num_skus']\n",
    "    \n",
    "    # Pivot for better view\n",
    "    market_pivot = market_status_by_brand.pivot_table(\n",
    "        index='brand', \n",
    "        columns='market_position', \n",
    "        values='num_skus', \n",
    "        fill_value=0\n",
    "    ).reset_index()\n",
    "    \n",
    "    print(\"\\n\" + \"─\" * 80)\n",
    "    print(\"🏪 MARKET POSITION STATUS (Top Dropping Brands - SKU Count)\")\n",
    "    print(\"─\" * 80)\n",
    "    print(market_pivot.to_string(index=False))\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # 4. REQUIRED ACTIONS BY BRAND\n",
    "    # ==========================================================================\n",
    "    actions_by_brand = df_top_brands.groupby(['brand', 'action']).agg({\n",
    "        'target_nmv': 'sum',\n",
    "        'product_id': 'nunique'\n",
    "    }).reset_index()\n",
    "    \n",
    "    actions_by_brand.columns = ['brand', 'action', 'target_nmv', 'num_skus']\n",
    "    \n",
    "    # Pivot actions\n",
    "    action_pivot = actions_by_brand.pivot_table(\n",
    "        index='brand',\n",
    "        columns='action',\n",
    "        values='num_skus',\n",
    "        fill_value=0\n",
    "    ).reset_index()\n",
    "    \n",
    "    print(\"\\n\" + \"─\" * 80)\n",
    "    print(\"⚡ REQUIRED ACTIONS (Top Dropping Brands - SKU Count)\")\n",
    "    print(\"─\" * 80)\n",
    "    print(action_pivot.to_string(index=False))\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # 5. ACTION SUMMARY FOR TOP DROPPING BRANDS\n",
    "    # ==========================================================================\n",
    "    action_summary = df_top_brands.groupby('action').agg({\n",
    "        'target_nmv': 'sum',\n",
    "        'product_id': 'nunique',\n",
    "        'brand': 'nunique'\n",
    "    }).reset_index()\n",
    "    \n",
    "    action_summary.columns = ['action', 'target_nmv_at_risk', 'num_skus', 'num_brands']\n",
    "    action_summary = action_summary.sort_values('target_nmv_at_risk', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + \"─\" * 80)\n",
    "    print(\"📋 ACTION PRIORITY SUMMARY (Top Dropping Brands)\")\n",
    "    print(\"─\" * 80)\n",
    "    print(f\"{'Action':<35} {'Target NMV at Risk':>18} {'# SKUs':>10} {'# Brands':>10}\")\n",
    "    print(\"─\" * 73)\n",
    "    for _, row in action_summary.iterrows():\n",
    "        action_name = str(row['action']) if pd.notna(row['action']) else 'No Action'\n",
    "        print(f\"{action_name[:35]:<35} {row['target_nmv_at_risk']:>18,.0f} {row['num_skus']:>10} {row['num_brands']:>10}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    \n",
    "    # Return dataframes for further analysis\n",
    "    return {\n",
    "        'total_metrics': {\n",
    "            'target_nmv': total_target_nmv,\n",
    "            'current_nmv': total_current_nmv,\n",
    "            'nmv_gap': nmv_gap,\n",
    "            'nmv_gap_pct': nmv_gap_pct\n",
    "        },\n",
    "        'brand_analysis': brand_agg_sorted,\n",
    "        'market_status_pivot': market_pivot,\n",
    "        'action_pivot': action_pivot,\n",
    "        'action_summary': action_summary\n",
    "    }\n",
    "\n",
    "# Run the aggregate analysis\n",
    "aggregate_results = create_aggregate_analysis(df_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b31069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DETAILED BRAND DRILLDOWN VIEW\n",
    "# =============================================================================\n",
    "\n",
    "def get_brand_drilldown(df, brand_name):\n",
    "    \"\"\"\n",
    "    Get detailed drilldown for a specific brand showing:\n",
    "    - All SKUs for the brand\n",
    "    - Their market status, RR status, and recommended actions\n",
    "    \"\"\"\n",
    "    df_brand = df[df['brand'] == brand_name].copy()\n",
    "    \n",
    "    if len(df_brand) == 0:\n",
    "        print(f\"No data found for brand: {brand_name}\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate target NMV\n",
    "    df_brand['target_nmv'] = df_brand['high_rr'] * df_brand['price']\n",
    "    df_brand['current_nmv'] = df_brand['current_rr'] * df_brand['price']\n",
    "    df_brand['nmv_gap'] = df_brand['target_nmv'] - df_brand['current_nmv']\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"🔍 BRAND DRILLDOWN: {brand_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Summary stats\n",
    "    print(f\"\\n📊 Summary:\")\n",
    "    print(f\"   Total SKUs: {df_brand['product_id'].nunique()}\")\n",
    "    print(f\"   Warehouses: {df_brand['warehouse_id'].nunique()}\")\n",
    "    print(f\"   Target NMV: {df_brand['target_nmv'].sum():,.0f} EGP\")\n",
    "    print(f\"   Current NMV: {df_brand['current_nmv'].sum():,.0f} EGP\")\n",
    "    print(f\"   NMV Gap: {df_brand['nmv_gap'].sum():,.0f} EGP\")\n",
    "    \n",
    "    # Show detailed SKU breakdown\n",
    "    columns_to_show = ['warehouse_name', 'sku', 'price', 'high_rr', 'current_rr', \n",
    "                       'stock_comment', 'market_position_status', 'price_comment', \n",
    "                       'rr_comment', 'action', 'nmv_gap']\n",
    "    \n",
    "    existing_cols = [c for c in columns_to_show if c in df_brand.columns]\n",
    "    \n",
    "    df_display = df_brand[existing_cols].sort_values('nmv_gap', ascending=False)\n",
    "    \n",
    "    print(f\"\\n📋 SKU Details (sorted by NMV Gap):\")\n",
    "    print(df_display.to_string(index=False))\n",
    "    \n",
    "    return df_brand\n",
    "\n",
    "# View top dropping brands DataFrame\n",
    "print(\"📈 TOP DROPPING BRANDS (Full DataFrame):\")\n",
    "aggregate_results['brand_analysis'].head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44476a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXAMPLE: DRILLDOWN INTO TOP DROPPING BRAND\n",
    "# =============================================================================\n",
    "\n",
    "# Get the top dropping brand name\n",
    "top_brand = aggregate_results['brand_analysis'].iloc[0]['brand']\n",
    "\n",
    "# Drilldown into the top dropping brand\n",
    "brand_detail = get_brand_drilldown(df_result, top_brand)\n",
    "\n",
    "# Or specify a brand manually:\n",
    "# brand_detail = get_brand_drilldown(df_result, \"Your Brand Name Here\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931c29cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPORT AGGREGATE ANALYSIS TO EXCEL\n",
    "# =============================================================================\n",
    "\n",
    "def get_team_sheet(df, team_flag_column, team_name):\n",
    "    \"\"\"\n",
    "    Get SKUs assigned to a specific team based on the team flag.\n",
    "    Sorted by NMV gap descending.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with all SKU data\n",
    "        team_flag_column: Column name for the team flag (e.g., 'pricing_team')\n",
    "        team_name: Name of the team for display\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame filtered and sorted for the team\n",
    "    \"\"\"\n",
    "    df_team = df.copy()\n",
    "    \n",
    "    # Calculate NMV metrics\n",
    "    df_team['target_nmv'] = df_team['high_rr'] * df_team['price']\n",
    "    df_team['current_nmv'] = df_team['current_rr'] * df_team['price']\n",
    "    df_team['nmv_gap'] = df_team['target_nmv'] - df_team['current_nmv']\n",
    "    df_team['margin_gap'] = df_team['bm'] - df_team['target_margin']\n",
    "    \n",
    "    # Filter by team flag = 1\n",
    "    df_team = df_team[df_team[team_flag_column] == 1].copy()\n",
    "    \n",
    "    # Sort by NMV gap descending\n",
    "    df_team = df_team.sort_values('nmv_gap', ascending=False)\n",
    "    \n",
    "    # Select relevant columns\n",
    "    columns_to_export = [\n",
    "        'region', 'warehouse_name', 'product_id', 'sku', 'cat', 'brand',\n",
    "        'stocks', 'doh', 'stock_comment', 'wac_p',\n",
    "        'price', 'blended_price', 'bm', 'blended_margin', 'target_margin', 'margin_gap', 'price_comment',\n",
    "        'combined_min_market', 'combined_median_market', 'combined_max_market',\n",
    "        'market_position_status',\n",
    "        'high_rr', 'current_rr', 'today_rr', 'high_mtd_rr', 'cu_mtd_rr', 'rr_comment',\n",
    "        'action', 'sku_discount_perc', 'quantity_discount_perc', 'offers_perc', 'commercial_min', 'activation',\n",
    "        'target_nmv', 'current_nmv', 'nmv_gap'\n",
    "    ]\n",
    "    \n",
    "    existing_cols = [c for c in columns_to_export if c in df_team.columns]\n",
    "    \n",
    "    return df_team[existing_cols]\n",
    "\n",
    "\n",
    "def get_team_summary(df):\n",
    "    \"\"\"\n",
    "    Create a summary showing each team's total assignments and NMV gap responsibility.\n",
    "    \"\"\"\n",
    "    df_summary = df.copy()\n",
    "    \n",
    "    # Calculate NMV metrics\n",
    "    df_summary['target_nmv'] = df_summary['high_rr'] * df_summary['price']\n",
    "    df_summary['current_nmv'] = df_summary['current_rr'] * df_summary['price']\n",
    "    df_summary['nmv_gap'] = df_summary['target_nmv'] - df_summary['current_nmv']\n",
    "    \n",
    "    # Calculate team metrics\n",
    "    teams_data = []\n",
    "    \n",
    "    # Pricing Team\n",
    "    pricing_df = df_summary[df_summary['pricing_team'] == 1]\n",
    "    teams_data.append({\n",
    "        'Team': 'Pricing Team',\n",
    "        'Total SKUs Assigned': len(pricing_df),\n",
    "        'Unique Products': pricing_df['product_id'].nunique(),\n",
    "        'Total Target NMV': pricing_df['target_nmv'].sum(),\n",
    "        'Total Current NMV': pricing_df['current_nmv'].sum(),\n",
    "        'Total NMV Gap': pricing_df['nmv_gap'].sum(),\n",
    "        'NMV Gap %': (pricing_df['nmv_gap'].sum() / pricing_df['target_nmv'].sum() * 100) if pricing_df['target_nmv'].sum() > 0 else 0\n",
    "    })\n",
    "    \n",
    "    # Purchase Team\n",
    "    purchase_df = df_summary[df_summary['purchase_team'] == 1]\n",
    "    teams_data.append({\n",
    "        'Team': 'Purchase Team',\n",
    "        'Total SKUs Assigned': len(purchase_df),\n",
    "        'Unique Products': purchase_df['product_id'].nunique(),\n",
    "        'Total Target NMV': purchase_df['target_nmv'].sum(),\n",
    "        'Total Current NMV': purchase_df['current_nmv'].sum(),\n",
    "        'Total NMV Gap': purchase_df['nmv_gap'].sum(),\n",
    "        'NMV Gap %': (purchase_df['nmv_gap'].sum() / purchase_df['target_nmv'].sum() * 100) if purchase_df['target_nmv'].sum() > 0 else 0\n",
    "    })\n",
    "    \n",
    "    # Commercial Team\n",
    "    commercial_df = df_summary[df_summary['commercial_team'] == 1]\n",
    "    teams_data.append({\n",
    "        'Team': 'Commercial Team',\n",
    "        'Total SKUs Assigned': len(commercial_df),\n",
    "        'Unique Products': commercial_df['product_id'].nunique(),\n",
    "        'Total Target NMV': commercial_df['target_nmv'].sum(),\n",
    "        'Total Current NMV': commercial_df['current_nmv'].sum(),\n",
    "        'Total NMV Gap': commercial_df['nmv_gap'].sum(),\n",
    "        'NMV Gap %': (commercial_df['nmv_gap'].sum() / commercial_df['target_nmv'].sum() * 100) if commercial_df['target_nmv'].sum() > 0 else 0\n",
    "    })\n",
    "    \n",
    "    # Total (all teams combined - note: some SKUs may be assigned to multiple teams)\n",
    "    any_team = df_summary[(df_summary['pricing_team'] == 1) | \n",
    "                          (df_summary['purchase_team'] == 1) | \n",
    "                          (df_summary['commercial_team'] == 1)]\n",
    "    teams_data.append({\n",
    "        'Team': 'TOTAL (All Teams)',\n",
    "        'Total SKUs Assigned': len(any_team),\n",
    "        'Unique Products': any_team['product_id'].nunique(),\n",
    "        'Total Target NMV': any_team['target_nmv'].sum(),\n",
    "        'Total Current NMV': any_team['current_nmv'].sum(),\n",
    "        'Total NMV Gap': any_team['nmv_gap'].sum(),\n",
    "        'NMV Gap %': (any_team['nmv_gap'].sum() / any_team['target_nmv'].sum() * 100) if any_team['target_nmv'].sum() > 0 else 0\n",
    "    })\n",
    "    \n",
    "    df_teams = pd.DataFrame(teams_data)\n",
    "    \n",
    "    # Sort by NMV Gap descending\n",
    "    df_teams = df_teams.sort_values('Total NMV Gap', ascending=False)\n",
    "    \n",
    "    return df_teams\n",
    "\n",
    "\n",
    "def export_aggregate_analysis(df, aggregate_results, filename='pricing_aggregate_analysis.xlsx'):\n",
    "    \"\"\"\n",
    "    Export the aggregate analysis to an Excel file with multiple sheets:\n",
    "    - Team Summary: Aggregate view of each team's assignments and NMV gap\n",
    "    - Pricing Team: SKUs assigned to pricing team (sorted by NMV gap desc)\n",
    "    - Purchase Team: SKUs assigned to purchase team (sorted by NMV gap desc)\n",
    "    - Commercial Team: SKUs assigned to commercial team (sorted by NMV gap desc)\n",
    "    - Brand Analysis: Top dropping brands\n",
    "    - Market Status: Market position by brand\n",
    "    - Actions by Brand: Required actions by brand\n",
    "    - Action Summary: Summary of actions\n",
    "    - Raw Data: Full detail data\n",
    "    \"\"\"\n",
    "    df_analysis = df.copy()\n",
    "    df_analysis['target_nmv'] = df_analysis['high_rr'] * df_analysis['price']\n",
    "    df_analysis['current_nmv'] = df_analysis['current_rr'] * df_analysis['price']\n",
    "    df_analysis['nmv_gap'] = df_analysis['target_nmv'] - df_analysis['current_nmv']\n",
    "    \n",
    "    # Get team sheets\n",
    "    df_pricing_team = get_team_sheet(df, 'pricing_team', 'Pricing Team')\n",
    "    df_purchase_team = get_team_sheet(df, 'purchase_team', 'Purchase Team')\n",
    "    df_commercial_team = get_team_sheet(df, 'commercial_team', 'Commercial Team')\n",
    "    df_team_summary = get_team_summary(df)\n",
    "    \n",
    "    with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
    "        # Sheet 1: Team Summary - Aggregate view of each team\n",
    "        df_team_summary.to_excel(writer, sheet_name='Team Summary', index=False)\n",
    "        \n",
    "        # Sheet 2: Pricing Team - sorted by NMV gap descending\n",
    "        df_pricing_team.to_excel(writer, sheet_name='Pricing Team', index=False)\n",
    "        \n",
    "        # Sheet 3: Purchase Team - sorted by NMV gap descending\n",
    "        df_purchase_team.to_excel(writer, sheet_name='Purchase Team', index=False)\n",
    "        \n",
    "        # Sheet 4: Commercial Team - sorted by NMV gap descending\n",
    "        df_commercial_team.to_excel(writer, sheet_name='Commercial Team', index=False)\n",
    "        \n",
    "        # Sheet 5: Brand Analysis (all brands)\n",
    "        aggregate_results['brand_analysis'].to_excel(writer, sheet_name='Brand Analysis', index=False)\n",
    "        \n",
    "        # Sheet 6: Market Status Pivot\n",
    "        aggregate_results['market_status_pivot'].to_excel(writer, sheet_name='Market Status', index=False)\n",
    "        \n",
    "        # Sheet 7: Action Pivot\n",
    "        aggregate_results['action_pivot'].to_excel(writer, sheet_name='Actions by Brand', index=False)\n",
    "        \n",
    "        # Sheet 8: Action Summary\n",
    "        aggregate_results['action_summary'].to_excel(writer, sheet_name='Action Summary', index=False)\n",
    "        \n",
    "        # Sheet 9: Raw Data with NMV calculations\n",
    "        df_analysis.to_excel(writer, sheet_name='Raw Data', index=False)\n",
    "    \n",
    "    print(f\"✓ Aggregate analysis exported to: {filename}\")\n",
    "    print(f\"\\n📊 TEAM ASSIGNMENTS SUMMARY:\")\n",
    "    print(f\"  - Pricing Team:    {len(df_pricing_team):>6} SKUs | NMV Gap: {df_pricing_team['nmv_gap'].sum():>15,.0f} EGP\")\n",
    "    print(f\"  - Purchase Team:   {len(df_purchase_team):>6} SKUs | NMV Gap: {df_purchase_team['nmv_gap'].sum():>15,.0f} EGP\")\n",
    "    print(f\"  - Commercial Team: {len(df_commercial_team):>6} SKUs | NMV Gap: {df_commercial_team['nmv_gap'].sum():>15,.0f} EGP\")\n",
    "    return filename\n",
    "\n",
    "# Export:\n",
    "export_aggregate_analysis(df_result, aggregate_results, 'pricing_aggregate_analysis.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b62d6d-c2b7-475a-b0fc-b8d8748ff0b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
