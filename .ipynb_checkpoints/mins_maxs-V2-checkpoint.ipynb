{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a2e32a9-ef1a-474a-a2f7-e291561f6da3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# =============================================================================\n",
    "# PACKAGE INSTALLATION\n",
    "# =============================================================================\n",
    "!pip install --upgrade pip\n",
    "\n",
    "# Database Connectivity\n",
    "!pip install psycopg2-binary snowflake-connector-python==3.15.0 snowflake-sqlalchemy\n",
    "\n",
    "# Authentication & Cloud\n",
    "!pip install keyring==23.11.0 sqlalchemy==1.4.46 requests boto3\n",
    "!pip install oauth2client gspread==5.9.0 gspread_dataframe google.cloud\n",
    "\n",
    "# Data Processing\n",
    "!pip install pandas==2.2.1 numpy polars openpyxl xlsxwriter\n",
    "\n",
    "# Utilities\n",
    "!pip install tqdm db-dtypes pytz import-ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16b73aac-2b7a-491b-80f9-674d0b9a67a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/snowflake/connector/options.py:104: UserWarning: You have an incompatible version of 'pyarrow' installed (22.0.0), please install a version that adheres to: 'pyarrow<19.0.0; extra == \"pandas\"'\n",
      "  warn_incompatible_dep(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.Renviron\n",
      "/home/ec2-user/service_account_key.json\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS & ENVIRONMENT SETUP\n",
    "# =============================================================================\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import importlib\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import snowflake.connector\n",
    "\n",
    "import setup_environment_2\n",
    "import import_ipynb\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Initialize environment\n",
    "importlib.reload(setup_environment_2)\n",
    "setup_environment_2.initialize_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f9ec765-6d3d-436f-bd59-76eda7de4e25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Pricing status mode: \"min_market\" adjusts tiers down by 1\n",
    "STATUS = \"\"# \"min_market\"\n",
    "\n",
    "# Google Sheets API scope\n",
    "GSHEETS_SCOPE = [\n",
    "    \"https://spreadsheets.google.com/feeds\",\n",
    "    \"https://www.googleapis.com/auth/spreadsheets\",\n",
    "    \"https://www.googleapis.com/auth/drive.file\",\n",
    "    \"https://www.googleapis.com/auth/drive\"\n",
    "]\n",
    "\n",
    "# Brand/Category pricing rules\n",
    "BELOW_MARKET_BRANDS = ['شويبس', 'كوكا كولا']\n",
    "MIN_PRICE_BRANDS = ['فاميليا', 'مولبد', 'مولفيكس', 'اوكسي', 'جوي', 'ريفولي', 'البوادي', 'هارفست فوودز', 'هاينز', 'بيبسي']\n",
    "AVG_PRICE_BRANDS = ['بخيره', 'جود كير']\n",
    "MAX_PRICE_BRANDS = ['فيوري']\n",
    "MIN_PRICE_CATEGORIES = ['تونة و سمك']\n",
    "\n",
    "# Warehouse mapping\n",
    "WAREHOUSE_CONFIG = pd.DataFrame([\n",
    "    ('Cairo', 'El-Marg', 38, 700),\n",
    "    ('Cairo', 'Mostorod', 1, 700),\n",
    "    ('Giza', 'Barageel', 236, 701),\n",
    "    ('Delta West', 'El-Mahala', 337, 703),\n",
    "    ('Delta West', 'Tanta', 8, 703),\n",
    "    ('Delta East', 'Mansoura FC', 339, 704),\n",
    "    ('Delta East', 'Sharqya', 170, 704),\n",
    "    ('Upper Egypt', 'Assiut FC', 501, 1124),\n",
    "    ('Upper Egypt', 'Bani sweif', 401, 1126),\n",
    "    ('Upper Egypt', 'Menya Samalot', 703, 1123),\n",
    "    ('Upper Egypt', 'Sohag', 632, 1125),\n",
    "    ('Alexandria', 'Khorshed Alex', 797, 702),\n",
    "    ('Giza', 'Sakkarah', 962, 701)\n",
    "], columns=['region', 'warehouse', 'warehouse_id', 'cohort_id'])\n",
    "\n",
    "# Region to cohort mapping\n",
    "REGION_COHORT_MAP = pd.DataFrame({\n",
    "    'region': ['Cairo', 'Giza', 'Delta West', 'Delta East', 'Upper Egypt', \n",
    "               'Upper Egypt', 'Upper Egypt', 'Upper Egypt', 'Alexandria'],\n",
    "    'cohort_id': [700, 701, 703, 704, 1124, 1126, 1123, 1125, 702]\n",
    "})\n",
    "\n",
    "# Products to exclude from TGTG processing\n",
    "TGTG_EXCLUSIONS = pd.DataFrame([\n",
    "], columns=['product_id', 'warehouse_id'])\n",
    "TGTG_EXCLUSIONS['remove'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cc41c3e-734d-43f3-8964-1c9fd8e676bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def query_snowflake(query, columns=None):\n",
    "    \"\"\"Execute a query on Snowflake and return results as DataFrame.\"\"\"\n",
    "    con = snowflake.connector.connect(\n",
    "        user=os.environ[\"SNOWFLAKE_USERNAME\"],\n",
    "        account=os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "        password=os.environ[\"SNOWFLAKE_PASSWORD\"],\n",
    "        database=os.environ[\"SNOWFLAKE_DATABASE\"]\n",
    "    )\n",
    "    try:\n",
    "        cur = con.cursor()\n",
    "        cur.execute(\"USE WAREHOUSE COMPUTE_WH\")\n",
    "        cur.execute(query)\n",
    "        data = cur.fetchall()\n",
    "        return pd.DataFrame(data, columns=columns) if columns else pd.DataFrame(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Snowflake Error: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    finally:\n",
    "        cur.close()\n",
    "        con.close()\n",
    "\n",
    "\n",
    "def get_gsheets_client():\n",
    "    \"\"\"Get authenticated Google Sheets client.\"\"\"\n",
    "    creds = ServiceAccountCredentials.from_json_keyfile_dict(\n",
    "        json.loads(setup_environment_2.get_secret(\"prod/maxab-sheets\")), \n",
    "        GSHEETS_SCOPE\n",
    "    )\n",
    "    return gspread.authorize(creds)\n",
    "\n",
    "\n",
    "def to_numeric_columns(df):\n",
    "    \"\"\"Convert all columns to numeric where possible.\"\"\"\n",
    "    for col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='ignore')\n",
    "    return df\n",
    "\n",
    "\n",
    "def assign_tier(cumulative_contribution):\n",
    "    \"\"\"Assign pricing tier based on cumulative NMV contribution.\"\"\"\n",
    "    thresholds = [0.4, 0.6, 0.8, 0.95]\n",
    "    for i, threshold in enumerate(thresholds, 1):\n",
    "        if cumulative_contribution <= threshold:\n",
    "            return i\n",
    "    return 5\n",
    "\n",
    "\n",
    "def price_analysis(row):\n",
    "    \"\"\"Analyze prices and calculate percentiles for a product.\"\"\"\n",
    "    wac = row['wac_p']\n",
    "    avg_margin = row['avg_margin'] if row['avg_margin'] >= 0.01 else row['target_margin']\n",
    "    std = row['std']\n",
    "    \n",
    "    # Collect all price points\n",
    "    price_list = [\n",
    "        row['ben_soliman_price'], row['final_min_price'], row['final_mod_price'],\n",
    "        row['final_max_price'], row['min_scrapped'], row['median_scrapped'], row['max_scrapped']\n",
    "    ]\n",
    "    \n",
    "    # Filter valid prices within acceptable range\n",
    "    valid_prices = sorted({\n",
    "        x for x in price_list \n",
    "        if x and not pd.isna(x) and x != 0 \n",
    "        and wac / (1 - (avg_margin - 2.5 * std)) <= x <= wac / (1 - (avg_margin + 4 * std))\n",
    "        and x >= wac\n",
    "    })\n",
    "    \n",
    "    if not valid_prices:\n",
    "        return np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "    \n",
    "    return (\n",
    "        np.min(valid_prices),\n",
    "        np.percentile(valid_prices, 25),\n",
    "        np.percentile(valid_prices, 50),\n",
    "        np.percentile(valid_prices, 75),\n",
    "        np.max(valid_prices)\n",
    "    )\n",
    "\n",
    "\n",
    "def calculate_step_bounds(row):\n",
    "    \"\"\"Calculate below/above market bounds based on price steps.\"\"\"\n",
    "    wac = row['wac_p']\n",
    "    std = row['std']\n",
    "    prices = [row['minimum'], row['percentile_25'], row['percentile_50'], row['percentile_75'], row['maximum']]\n",
    "    \n",
    "    # Calculate valid steps between price points\n",
    "    valid_steps = []\n",
    "    for i in range(len(prices) - 1):\n",
    "        step = prices[i + 1] - prices[i]\n",
    "        if (step / wac) <= std * 1.2:\n",
    "            valid_steps.append(step)\n",
    "    \n",
    "    avg_step = np.mean(valid_steps) if valid_steps else min(2 * std, 0.2 * row['target_margin'])\n",
    "    \n",
    "    new_min = prices[0] - avg_step if (prices[0] - avg_step) >= wac else prices[0]\n",
    "    new_max = prices[-1] + avg_step if (prices[-1] + avg_step) >= wac else prices[-1]\n",
    "    \n",
    "    return new_min, new_max\n",
    "\n",
    "\n",
    "def convert_sku_id(row):\n",
    "    \"\"\"Convert SKU string to integer ID.\"\"\"\n",
    "    try:\n",
    "        return int(str(row.SKU).replace(\",\", \"\"))\n",
    "    except:\n",
    "        return row.SKU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4843f389-c610-4185-bec0-0978a722587f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snowflake timezone: America/Los_Angeles\n",
      "Loaded 8075 min_max records, 126 blue campaign brands\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DATA LOADING - Snowflake Timezone & Google Sheets\n",
    "# =============================================================================\n",
    "\n",
    "# Get Snowflake timezone\n",
    "zone_to_use = query_snowflake(\"SHOW PARAMETERS LIKE 'TIMEZONE'\")[1].values[0]\n",
    "print(f\"Snowflake timezone: {zone_to_use}\")\n",
    "\n",
    "# Initialize Google Sheets client\n",
    "client = get_gsheets_client()\n",
    "\n",
    "# Load min_max margin cohort data\n",
    "min_max_sheet = client.open('Demand Based Dynamic Pricing').worksheet('min_max_margin_cohort')\n",
    "min_max_df = to_numeric_columns(pd.DataFrame(min_max_sheet.get_all_records()))\n",
    "min_max_df = min_max_df[min_max_df['min_margin'] > 0.01]\n",
    "\n",
    "# Load Blue FD campaign brands\n",
    "blue_brands_sheet = client.open('Anniversary Campaign 2025 (Final)').worksheet('Suppliers Brands')\n",
    "blue_list = pd.DataFrame(blue_brands_sheet.get_all_records())[['Brands']].drop_duplicates()['Brands'].tolist()\n",
    "\n",
    "print(f\"Loaded {len(min_max_df)} min_max records, {len(blue_list)} blue campaign brands\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5198399c-507c-4f5b-b61e-b030873c2694",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA LOADING - Market Main Data Query\n",
    "# =============================================================================\n",
    "\n",
    "MARKET_DATA_QUERY = f'''\n",
    "WITH whs as (SELECT *\n",
    "             FROM   (values\n",
    "                            ('Cairo', 'El-Marg', 38,700),\n",
    "                            ('Cairo', 'Mostorod', 1,700),\n",
    "                            ('Giza', 'Barageel', 236,701),\n",
    "                            ('Delta West', 'El-Mahala', 337,703),\n",
    "                            ('Delta West', 'Tanta', 8,703),\n",
    "                            ('Delta East', 'Mansoura FC', 339,704),\n",
    "                            ('Delta East', 'Sharqya', 170,704),\n",
    "                            ('Upper Egypt', 'Assiut FC', 501,1124),\n",
    "                            ('Upper Egypt', 'Bani sweif', 401,1126),\n",
    "                            ('Upper Egypt', 'Menya Samalot', 703,1123),\n",
    "                            ('Upper Egypt', 'Sohag', 632,1125),\n",
    "                            ('Alexandria', 'Khorshed Alex', 797,702),\n",
    "                            ('Giza', 'Sakkarah', 962,701))\n",
    "                    x(region, wh, warehouse_id,cohort_id)),\n",
    "\n",
    "full_data as (\n",
    "    select products.id as product_id, region\n",
    "    from products, whs \n",
    "    where activation = 'true'\n",
    "),\n",
    "\n",
    "MP as (\n",
    "    select region, product_id,\n",
    "        min(min_price) as min_price, min(max_price) as max_price,\n",
    "        min(mod_price) as mod_price, min(true_min) as true_min, min(true_max) as true_max\n",
    "    from (\n",
    "        select mp.region, mp.product_id, mp.pu_id,\n",
    "            min_price/BASIC_UNIT_COUNT as min_price,\n",
    "            max_price/BASIC_UNIT_COUNT as max_price,\n",
    "            mod_price/BASIC_UNIT_COUNT as mod_price,\n",
    "            TRUE_MIN_PRICE/BASIC_UNIT_COUNT as true_min,\n",
    "            TRUE_MAX_PRICE/BASIC_UNIT_COUNT as true_max\n",
    "        from materialized_views.marketplace_prices mp \n",
    "        join packing_unit_products pup on pup.product_id = mp.product_id and pup.packing_unit_id = mp.pu_id\n",
    "        join finance.all_cogs f on f.product_id = mp.product_id and CURRENT_TIMESTAMP between f.from_date and f.to_date\n",
    "        where least(min_price, mod_price) between wac_p*0.9 and wac_p*1.3 \n",
    "    )\n",
    "    group by all \n",
    "),\n",
    "\n",
    "region_mapping AS (\n",
    "    SELECT * FROM (VALUES\n",
    "        ('Delta East', 'Delta West'), ('Delta West', 'Delta East'),\n",
    "        ('Alexandria', 'Cairo'), ('Alexandria', 'Giza'),\n",
    "        ('Upper Egypt', 'Cairo'), ('Upper Egypt', 'Giza'),\n",
    "        ('Cairo', 'Giza'), ('Giza', 'Cairo'),\n",
    "        ('Delta West', 'Cairo'), ('Delta East', 'Cairo'),\n",
    "        ('Delta West', 'Giza'), ('Delta East', 'Giza')\n",
    "    ) AS region_mapping(region, fallback_region)\n",
    "),\n",
    "\n",
    "final_mp as (\n",
    "    select region, product_id,\n",
    "        min(final_min_price) as final_min_price, min(final_max_price) as final_max_price,\n",
    "        min(final_mod_price) as final_mod_price, min(final_true_min) as final_true_min,\n",
    "        min(final_true_max) as final_true_max\n",
    "    from (\n",
    "        SELECT distinct w.region, w.product_id,\n",
    "            COALESCE(m1.min_price, m2.min_price) AS final_min_price,\n",
    "            COALESCE(m1.max_price, m2.max_price) AS final_max_price,\n",
    "            COALESCE(m1.mod_price, m2.mod_price) AS final_mod_price,\n",
    "            COALESCE(m1.true_min, m2.true_min) AS final_true_min,\n",
    "            COALESCE(m1.true_max, m2.true_max) AS final_true_max\n",
    "        FROM full_data w\n",
    "        LEFT JOIN MP m1 ON w.region = m1.region and w.product_id = m1.product_id\n",
    "        JOIN region_mapping rm ON w.region = rm.region\n",
    "        LEFT JOIN MP m2 ON rm.fallback_region = m2.region AND w.product_id = m2.product_id\n",
    "    )\n",
    "    where final_min_price is not null \n",
    "    group by all \n",
    "),\n",
    "\n",
    "ben_soliman as (\n",
    "    select z.* from (\n",
    "        select maxab_product_id as product_id, maxab_sku as sku, avg(bs_final_price) as ben_soliman_price\n",
    "        from (\n",
    "            select *, row_number() over(partition by maxab_product_id order by diff) as rnk_2 from (\n",
    "                select *, (bs_final_price-wac_p)/wac_p as diff_2 from (\n",
    "                    select *, bs_price/maxab_basic_unit_count as bs_final_price from (\n",
    "                        select *, row_number() over(partition by maxab_product_id, maxab_pu order by diff) as rnk from (\n",
    "                            select sm.*, max(INJECTION_DATE::date) over(partition by maxab_product_id, maxab_pu) as max_date,\n",
    "                                wac1, wac_p, abs(bs_price-(wac_p*maxab_basic_unit_count))/(wac_p*maxab_basic_unit_count) as diff \n",
    "                            from materialized_views.savvy_mapping sm \n",
    "                            join finance.all_cogs f on f.product_id = sm.maxab_product_id and current_timestamp between f.from_Date and f.to_date\n",
    "                            where bs_price is not null and INJECTION_DATE::date >= CURRENT_DATE - 5\n",
    "                            qualify INJECTION_DATE::date = max_date\n",
    "                        ) qualify rnk = 1 \n",
    "                    )\n",
    "                ) where diff_2 between -0.5 and 0.5 \n",
    "            ) qualify rnk_2 = 1 \n",
    "        ) group by all\n",
    "    ) z \n",
    "    join finance.all_cogs f on f.product_id = z.product_id and current_timestamp between f.from_Date and f.to_date\n",
    "    where ben_soliman_price between f.wac_p*0.7 and f.wac_p*1.3\n",
    "),\n",
    "\n",
    "scrapped_data as (\n",
    "    select product_id, cat, brand, region, max_date,\n",
    "        min(MARKET_PRICE) as min_scrapped, max(MARKET_PRICE) as max_scrapped, median(MARKET_PRICE) as median_scrapped\n",
    "    from (\n",
    "        select MATERIALIZED_VIEWS.CLEANED_MARKET_PRICES.*, max(date) over(partition by region, MATERIALIZED_VIEWS.CLEANED_MARKET_PRICES.product_id, competitor) as max_date\n",
    "        from MATERIALIZED_VIEWS.CLEANED_MARKET_PRICES\n",
    "        join finance.all_cogs f on f.product_id = MATERIALIZED_VIEWS.CLEANED_MARKET_PRICES.product_id and CURRENT_TIMESTAMP between f.from_date and f.to_date \n",
    "        where date >= current_date - 5 and MARKET_PRICE between f.wac_p * 0.7 and wac_p*1.3\n",
    "        qualify date = max_date \n",
    "    ) group by all \n",
    "),\n",
    "\n",
    "local_prices as (\n",
    "    SELECT case when cpu.cohort_id in (700) then 'Cairo'\n",
    "                when cpu.cohort_id in (701) then 'Giza'\n",
    "                when cpu.cohort_id in (704) then 'Delta East'\n",
    "                when cpu.cohort_id in (703) then 'Delta West'\n",
    "                when cpu.cohort_id in (1123,1124,1125,1126) then 'Upper Egypt'\n",
    "                when cpu.cohort_id in (702) then 'Alexandria'\n",
    "           end as region,\n",
    "           cohort_id, pu.product_id, pu.packing_unit_id, pu.basic_unit_count, avg(cpu.price) as price\n",
    "    FROM cohort_product_packing_units cpu\n",
    "    join PACKING_UNIT_PRODUCTS pu on pu.id = cpu.product_packing_unit_id\n",
    "    WHERE cpu.cohort_id in (700,701,702,703,704,1123,1124,1125,1126)\n",
    "        and cpu.created_at::date <> '2023-07-31' and cpu.is_customized = true\n",
    "    group by all \n",
    "),\n",
    "\n",
    "live_prices as (\n",
    "    select region, cohort_id, product_id, pu_id as packing_unit_id, buc as basic_unit_count, NEW_PRICE as price\n",
    "    from materialized_views.DBDP_PRICES\n",
    "    where created_at = Current_timestamp::date\n",
    "        and DATE_PART('hour', Current_timestamp::time) BETWEEN SPLIT_PART(time_slot, '-', 1)::int AND SPLIT_PART(time_slot, '-', 2)::int\n",
    "        and cohort_id in (700,701,702,703,704,696,695,698,697,699,1123,1124,1125,1126)\n",
    "),\n",
    "\n",
    "prices as (\n",
    "    select * from (\n",
    "        SELECT *, 1 AS priority FROM live_prices\n",
    "        UNION ALL\n",
    "        SELECT *, 2 AS priority FROM local_prices\n",
    "    )\n",
    "    QUALIFY ROW_NUMBER() OVER (PARTITION BY region, cohort_id, product_id, packing_unit_id ORDER BY priority) = 1\n",
    "),\n",
    "\n",
    "maxab_prices as (\n",
    "    select region, cohort_id, product_id, price from prices where basic_unit_count = 1 \n",
    "),\n",
    "\n",
    "sales as (\n",
    "    SELECT DISTINCT cpc.cohort_id, pso.product_id, sum(pso.total_price) as nmv\n",
    "    FROM product_sales_order pso\n",
    "    JOIN sales_orders so ON so.id = pso.sales_order_id   \n",
    "    join COHORT_PRICING_CHANGES cpc on cpc.id = pso.cohort_pricing_change_id\n",
    "    WHERE so.created_at::date between date_trunc('month', Current_timestamp::date - 120) and Current_timestamp::date - 1\n",
    "        AND so.sales_order_status_id not in (7,12)\n",
    "        AND so.channel IN ('telesales','retailer')\n",
    "        AND pso.purchased_item_count <> 0\n",
    "    GROUP BY ALL\n",
    "),\n",
    "\n",
    "margin_change as (\n",
    "    select product_id, cohort_id, (0.6*product_std) + (0.3*brand_std) + (0.1*cat_std) as std, avg_margin\n",
    "    from (\n",
    "        select product_id, cohort_id, stddev(product_margin) as product_std, stddev(brand_margin) as brand_std,\n",
    "            stddev(cat_margin) as cat_std, avg(product_margin) as avg_margin\n",
    "        from (\n",
    "            select distinct product_id, order_date, cohort_id,\n",
    "                (nmv-cogs_p)/nmv as product_margin, (brand_nmv-brand_cogs)/brand_nmv as brand_margin,\n",
    "                (cat_nmv-cat_cogs)/cat_nmv as cat_margin\n",
    "            from (\n",
    "                SELECT DISTINCT so.created_at::date as order_date, cpc.cohort_id, pso.product_id,\n",
    "                    brands.name_ar as brand, categories.name_ar as cat,\n",
    "                    sum(COALESCE(f.wac_p,0) * pso.purchased_item_count * pso.basic_unit_count) as cogs_p,\n",
    "                    sum(pso.total_price) as nmv,\n",
    "                    sum(nmv) over(partition by order_date, cat, brand) as brand_nmv,\n",
    "                    sum(cogs_p) over(partition by order_date, cat, brand) as brand_cogs,\n",
    "                    sum(nmv) over(partition by order_date, cat) as cat_nmv,\n",
    "                    sum(cogs_p) over(partition by order_date, cat) as cat_cogs\n",
    "                FROM product_sales_order pso\n",
    "                JOIN sales_orders so ON so.id = pso.sales_order_id   \n",
    "                join COHORT_PRICING_CHANGES cpc on cpc.id = pso.cohort_pricing_change_id\n",
    "                JOIN products on products.id=pso.product_id\n",
    "                JOIN brands on products.brand_id = brands.id \n",
    "                JOIN categories ON products.category_id = categories.id\n",
    "                JOIN finance.all_cogs f ON f.product_id = pso.product_id\n",
    "                    AND f.from_date::date <= so.created_at::date AND f.to_date::date > so.created_at::date\n",
    "                WHERE so.created_at::date between date_trunc('month', Current_timestamp::date - 120) and Current_timestamp::date\n",
    "                    AND so.sales_order_status_id not in (7,12)\n",
    "                    AND so.channel IN ('telesales','retailer')\n",
    "                    AND pso.purchased_item_count <> 0\n",
    "                GROUP BY ALL\n",
    "            )\n",
    "        ) group by all \n",
    "    )\n",
    "),\n",
    "\n",
    "cat_brand_target as (\n",
    "    SELECT DISTINCT cat, brand, margin as target_bm\n",
    "    FROM performance.commercial_targets cplan\n",
    "    QUALIFY CASE WHEN DATE_TRUNC('month', MAX(DATE) OVER()) = DATE_TRUNC('month', Current_timestamp::date) \n",
    "        THEN DATE_TRUNC('month', Current_timestamp::date)\n",
    "        ELSE DATE_TRUNC('month', Current_timestamp::date - INTERVAL '1 month') END = DATE_TRUNC('month', date)\n",
    "),\n",
    "\n",
    "cat_target as (\n",
    "    select cat, sum(target_bm * (target_nmv/cat_total)) as cat_target_margin\n",
    "    from (\n",
    "        select *, sum(target_nmv) over(partition by cat) as cat_total\n",
    "        from (\n",
    "            select cat, brand, avg(target_bm) as target_bm, sum(target_nmv) as target_nmv\n",
    "            from (\n",
    "                SELECT DISTINCT date, city as region, cat, brand, margin as target_bm, nmv as target_nmv\n",
    "                FROM performance.commercial_targets cplan\n",
    "                QUALIFY CASE WHEN DATE_TRUNC('month', MAX(DATE) OVER()) = DATE_TRUNC('month', Current_timestamp::date) \n",
    "                    THEN DATE_TRUNC('month', Current_timestamp::date)\n",
    "                    ELSE DATE_TRUNC('month', Current_timestamp::date - INTERVAL '1 month') END = DATE_TRUNC('month', date)\n",
    "            ) group by all\n",
    "        )\n",
    "    ) group by all \n",
    ")\n",
    "\n",
    "select distinct maxab.cohort_id, maxab.product_id,\n",
    "    CONCAT(products.name_ar,' ',products.size,' ',product_units.name_ar) as sku,\n",
    "    brands.name_ar as brand, categories.name_ar as cat, sections.name_ar as section_name,\n",
    "    maxab.price as maxab_price, bs.ben_soliman_price,\n",
    "    final_min_price, final_max_price, final_mod_price,\n",
    "    min_scrapped, median_scrapped, max_scrapped,\n",
    "    wac_p, coalesce(nmv,0) as nmv, coalesce(mc.std,0.01) as std,\n",
    "    coalesce(coalesce(cbt.target_bm, ct.cat_target_margin),0) as target_margin,\n",
    "    coalesce(avg_margin,0) as avg_margin\n",
    "from maxab_prices maxab\n",
    "left join ben_soliman bs on bs.product_id = maxab.product_id\n",
    "left join final_mp fmp on fmp.product_id = maxab.product_id and fmp.region = maxab.region\n",
    "left join sales s on s.product_id = maxab.product_id and s.cohort_id = maxab.cohort_id\n",
    "left join scrapped_data sd on sd.product_id = maxab.product_id and sd.region = maxab.region\n",
    "join finance.all_cogs f on f.product_id = maxab.product_id and CURRENT_TIMESTAMP between f.from_date and f.to_date\n",
    "JOIN products on products.id=maxab.product_id\n",
    "JOIN brands on products.brand_id = brands.id \n",
    "JOIN categories ON products.category_id = categories.id\n",
    "JOIN sections ON sections.id = categories.section_id\n",
    "JOIN product_units ON product_units.id = products.unit_id \n",
    "left join margin_change mc on mc.product_id = maxab.product_id and mc.cohort_id = maxab.cohort_id\n",
    "left join cat_brand_target cbt on cbt.brand = brands.name_ar and cbt.cat = categories.name_ar \n",
    "left join cat_target ct on ct.cat = categories.name_ar \n",
    "'''\n",
    "\n",
    "market_cols = ['cohort_id', 'product_id', 'sku', 'brand', 'cat', 'section_name', 'maxab_price',\n",
    "               'ben_soliman_price', 'final_min_price', 'final_max_price', 'final_mod_price',\n",
    "               'min_scrapped', 'median_scrapped', 'max_scrapped', 'wac_p', 'nmv', 'std', 'target_margin', 'avg_margin']\n",
    "\n",
    "market_main_data = to_numeric_columns(query_snowflake(MARKET_DATA_QUERY, columns=market_cols))\n",
    "market_main_data = market_main_data[market_cols].drop_duplicates(subset=['cohort_id', 'product_id'])\n",
    "print(f\"Loaded {len(market_main_data)} market data records\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87883b4b-bad9-4d21-9f69-6685e716ea4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA LOADING - Additional Queries (Groups, Price Ups, Sales, WAC, Stocks, Stats)\n",
    "# =============================================================================\n",
    "\n",
    "# Product commercial groups from PostgreSQL\n",
    "groups = setup_environment_2.dwh_pg_query(\n",
    "    \"SELECT * FROM materialized_views.sku_commercial_groups\", \n",
    "    columns=['product_id', 'group']\n",
    ")\n",
    "groups.columns = groups.columns.str.lower()\n",
    "groups = to_numeric_columns(groups)\n",
    "\n",
    "# Price ups data\n",
    "price_ups = to_numeric_columns(query_snowflake('''\n",
    "    SELECT region, product_id, new_pp, forecasted_date\n",
    "    FROM materialized_views.DBDP_PRICE_UPS\n",
    "''', columns=['region', 'product_id', 'new_pp', 'forcasted_date']))\n",
    "\n",
    "# Sales data (120-day history)\n",
    "sales = to_numeric_columns(query_snowflake('''\n",
    "    SELECT DISTINCT cpc.cohort_id, pso.product_id,\n",
    "        CONCAT(products.name_ar,' ',products.size,' ',product_units.name_ar) as sku,\n",
    "        brands.name_ar as brand, categories.name_ar as cat,\n",
    "        sum(pso.total_price) as nmv\n",
    "    FROM product_sales_order pso\n",
    "    JOIN sales_orders so ON so.id = pso.sales_order_id\n",
    "    JOIN COHORT_PRICING_CHANGES cpc ON cpc.id = pso.COHORT_PRICING_CHANGE_id\n",
    "    JOIN products ON products.id = pso.product_id\n",
    "    JOIN brands ON products.brand_id = brands.id \n",
    "    JOIN categories ON products.category_id = categories.id\n",
    "    JOIN product_units ON product_units.id = products.unit_id \n",
    "    WHERE so.created_at::date BETWEEN current_date - 120 AND current_date - 1 \n",
    "        AND so.sales_order_status_id NOT IN (7, 12)\n",
    "        AND so.channel IN ('telesales', 'retailer')\n",
    "        AND pso.purchased_item_count <> 0\n",
    "        AND cpc.cohort_id IN (700,701,702,703,704,1123,1124,1125,1126)\n",
    "    GROUP BY ALL\n",
    "''', columns=['cohort_id', 'product_id', 'sku', 'brand', 'cat', 'nmv']))\n",
    "\n",
    "# WAC (Weighted Average Cost) data\n",
    "wacs = to_numeric_columns(query_snowflake(f'''\n",
    "    SELECT product_id, wac_p\n",
    "    FROM finance.all_cogs f \n",
    "    WHERE CONVERT_TIMEZONE('{zone_to_use}', 'Africa/Cairo', CURRENT_TIMESTAMP()) BETWEEN f.from_date AND f.to_date \n",
    "''', columns=['product_id', 'wac_p']))\n",
    "\n",
    "# Current stocks\n",
    "stocks = to_numeric_columns(query_snowflake('''\n",
    "    SELECT DISTINCT product_warehouse.warehouse_id, product_warehouse.product_id,\n",
    "        (product_warehouse.available_stock)::integer as stocks\n",
    "    FROM product_warehouse \n",
    "    JOIN products ON product_warehouse.product_id = products.id\n",
    "    JOIN product_units ON products.unit_id = product_units.id\n",
    "    WHERE product_warehouse.warehouse_id NOT IN (6, 9, 10)\n",
    "        AND product_warehouse.activation = 'true'\n",
    "        AND product_warehouse.is_basic_unit = 1\n",
    "''', columns=['warehouse_id', 'product_id', 'cu_stocks']))\n",
    "\n",
    "# Product statistics\n",
    "stats = to_numeric_columns(query_snowflake('''\n",
    "    SELECT region, product_id, optimal_bm, MIN_BOUNDARY, MAX_BOUNDARY, MEDIAN_BM\n",
    "    FROM (\n",
    "        SELECT region, product_id, target_bm, optimal_bm, MIN_BOUNDARY, MAX_BOUNDARY, MEDIAN_BM,\n",
    "            MAX(created_at) OVER(PARTITION BY product_id, region) as max_date, created_at\n",
    "        FROM materialized_views.PRODUCT_STATISTICS\n",
    "        WHERE created_at::date >= date_trunc('month', current_date - 60)\n",
    "        QUALIFY max_date = created_at\n",
    "    )\n",
    "''', columns=['region', 'product_id', 'optimal_bm', 'min_boundary', 'max_boundary', 'median_bm']))\n",
    "\n",
    "print(f\"Loaded: {len(groups)} groups, {len(price_ups)} price_ups, {len(sales)} sales, {len(wacs)} wacs, {len(stocks)} stocks, {len(stats)} stats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade36b5b-e61d-4d32-a310-6f5f610f838b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA LOADING - TGTG Aging Monitor (Google Sheets)\n",
    "# =============================================================================\n",
    "\n",
    "# Get current and recent week numbers for sheet lookup\n",
    "week_number = datetime.now().isocalendar()[1]\n",
    "week_candidates = [str(week_number), str(week_number - 1), str(week_number - 2)]\n",
    "\n",
    "# Find the most recent TGTG sheet\n",
    "tgtg_worksheets = client.open('Egypt SKUs Aging Monitor').worksheets()\n",
    "worksheet_names = [ws.title for ws in tgtg_worksheets]\n",
    "\n",
    "sheet_name = None\n",
    "for week_str in week_candidates:\n",
    "    for name in worksheet_names:\n",
    "        if week_str in name:\n",
    "            sheet_name = name\n",
    "            break\n",
    "    if sheet_name:\n",
    "        break\n",
    "\n",
    "# Load TGTG data\n",
    "tgtg_sheet = client.open('Egypt SKUs Aging Monitor').worksheet(sheet_name)\n",
    "tgtg_data = tgtg_sheet.get_all_values()\n",
    "\n",
    "if tgtg_data:\n",
    "    tgtg_df = pd.DataFrame(tgtg_data[2:], columns=tgtg_data[1]).iloc[:, :21]\n",
    "    tgtg_df = to_numeric_columns(tgtg_df)\n",
    "    tgtg_df = tgtg_df[tgtg_df['Fulfillment confirmation'] == 'confirmed']\n",
    "    \n",
    "    # Select relevant warehouse columns\n",
    "    warehouse_cols = ['SKU', 'Sharqya', 'Khorshed Alex', 'Bani sweif', 'Mostorod', 'Barageel', \n",
    "                      'El-Mahala', 'Sohag', 'Mansoura FC', 'Assiut FC', 'Menya Samalot', 'Tanta']\n",
    "    tgtg_df = tgtg_df[warehouse_cols]\n",
    "    \n",
    "    # Melt to long format (SKU x warehouse -> stocks)\n",
    "    tgtg_long = tgtg_df.melt(id_vars=['SKU'], var_name='warehouse', value_name='stocks')\n",
    "    tgtg_long['product_id'] = tgtg_long.apply(convert_sku_id, axis=1)\n",
    "    tgtg_long = tgtg_long.drop(columns='SKU')\n",
    "    tgtg_long = tgtg_long[~tgtg_long['stocks'].isna()]\n",
    "else:\n",
    "    tgtg_long = pd.DataFrame(columns=['warehouse', 'stocks', 'product_id'])\n",
    "\n",
    "print(f\"Loaded TGTG data from sheet '{sheet_name}': {len(tgtg_long)} warehouse-product records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0490c145-f6da-49d8-9ab8-bd241f6a0e16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA PROCESSING - Sales Tier Assignment\n",
    "# =============================================================================\n",
    "\n",
    "# Calculate NMV contribution and cumulative contribution\n",
    "sales['total_nmv'] = sales.groupby('cohort_id')['nmv'].transform('sum')\n",
    "sales['cntrb_nmv'] = sales['nmv'] / sales['total_nmv']\n",
    "sales = sales.sort_values(['cohort_id', 'nmv'], ascending=[True, False])\n",
    "sales['nmv_cumulative_cntrb'] = sales.groupby('cohort_id')['cntrb_nmv'].cumsum()\n",
    "\n",
    "# Assign base tier from cumulative contribution\n",
    "sales['tier'] = sales['nmv_cumulative_cntrb'].apply(assign_tier)\n",
    "\n",
    "# Apply brand/category tier adjustments\n",
    "sales.loc[sales['cat'].isin(MIN_PRICE_CATEGORIES), 'tier'] = np.maximum(sales['tier'] - 1, 1)\n",
    "#sales.loc[sales['brand'].isin(blue_list), 'tier'] = np.maximum(sales['tier'] - 1, 1)\n",
    "sales.loc[sales['brand'].isin(MIN_PRICE_BRANDS), 'tier'] = 1\n",
    "sales.loc[sales['brand'].isin(BELOW_MARKET_BRANDS), 'tier'] = 0\n",
    "sales.loc[sales['brand'].isin(AVG_PRICE_BRANDS), 'tier'] = 3\n",
    "sales.loc[sales['brand'].isin(MAX_PRICE_BRANDS), 'tier'] = 5\n",
    "\n",
    "# Apply status-based adjustment (reduce tier by 1 if \"min\" mode)\n",
    "if 'min' in STATUS:\n",
    "    sales['tier'] = np.maximum(sales['tier'] - 1, 0)\n",
    "\n",
    "print(f\"Tier distribution:\\n{sales['tier'].value_counts().sort_index()}\")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a60ce8-9362-4bbd-8d62-e969f2135d8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA PROCESSING - Market Data with Groups\n",
    "# =============================================================================\n",
    "\n",
    "# Merge market data with product groups\n",
    "market_data = market_main_data.copy()\n",
    "market_data = market_data.merge(groups, on='product_id', how='left')\n",
    "\n",
    "# Calculate group-level aggregated prices for products with group assignments\n",
    "groups_data = market_data[~market_data['group'].isna()].copy()\n",
    "groups_data['group_nmv'] = groups_data.groupby(['group', 'cohort_id'])['nmv'].transform('sum')\n",
    "groups_data['cntrb'] = (groups_data['nmv'] / groups_data['group_nmv']).fillna(1)\n",
    "\n",
    "# Aggregate group prices\n",
    "groups_agg = groups_data.groupby(['group', 'cohort_id']).agg({\n",
    "    'ben_soliman_price': 'median', 'final_min_price': 'median', 'final_max_price': 'median',\n",
    "    'final_mod_price': 'median', 'min_scrapped': 'median', 'median_scrapped': 'median', 'max_scrapped': 'median'\n",
    "}).reset_index()\n",
    "\n",
    "# Fill missing prices with group-level prices\n",
    "merged = market_data.merge(groups_agg, on=['group', 'cohort_id'], how='left', suffixes=('', '_group'))\n",
    "price_cols = ['ben_soliman_price', 'final_min_price', 'final_max_price', 'final_mod_price', \n",
    "              'min_scrapped', 'median_scrapped', 'max_scrapped']\n",
    "for col in price_cols:\n",
    "    merged[col] = merged[col].fillna(merged[f'{col}_group'])\n",
    "\n",
    "market_data = merged.drop(columns=[f'{c}_group' for c in price_cols])\n",
    "\n",
    "print(f\"Market data after group processing: {len(market_data)} records\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986e7772-bc1f-4e0c-8165-8fcac88753ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA PROCESSING - Price Analysis & Margin Calculation\n",
    "# =============================================================================\n",
    "\n",
    "# Apply price analysis to calculate price percentiles\n",
    "market_data[['minimum', 'percentile_25', 'percentile_50', 'percentile_75', 'maximum']] = \\\n",
    "    market_data.apply(price_analysis, axis=1, result_type='expand')\n",
    "\n",
    "# Filter out records without valid price analysis\n",
    "market_data = market_data[~market_data['minimum'].isna()]\n",
    "\n",
    "# Calculate below/above market bounds\n",
    "market_data[['below_market', 'above_market']] = market_data.apply(calculate_step_bounds, axis=1, result_type='expand')\n",
    "\n",
    "# Calculate margin metrics\n",
    "market_data = market_data[['cohort_id', 'product_id', 'maxab_price', 'wac_p', 'minimum', \n",
    "                           'percentile_25', 'percentile_50', 'percentile_75', 'maximum', \n",
    "                           'below_market', 'above_market']]\n",
    "\n",
    "# Convert prices to margins\n",
    "market_data['below_market'] = (market_data['below_market'] - market_data['wac_p']) / market_data['below_market']\n",
    "market_data['market_min'] = (market_data['minimum'] - market_data['wac_p']) / market_data['minimum']\n",
    "market_data['market_25'] = (market_data['percentile_25'] - market_data['wac_p']) / market_data['percentile_25']\n",
    "market_data['market_50'] = (market_data['percentile_50'] - market_data['wac_p']) / market_data['percentile_50']\n",
    "market_data['market_75'] = (market_data['percentile_75'] - market_data['wac_p']) / market_data['percentile_75']\n",
    "market_data['market_max'] = (market_data['maximum'] - market_data['wac_p']) / market_data['maximum']\n",
    "market_data['above_market'] = (market_data['above_market'] - market_data['wac_p']) / market_data['above_market']\n",
    "market_data['current_margin'] = (market_data['maxab_price'] - market_data['wac_p']) / market_data['maxab_price']\n",
    "\n",
    "market_data = market_data[['cohort_id', 'product_id', 'current_margin', 'below_market', 'market_min', \n",
    "                           'market_25', 'market_50', 'market_75', 'market_max', 'above_market']]\n",
    "\n",
    "print(f\"Market data after price analysis: {len(market_data)} records\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f390dc2b-c772-4d2e-b7cc-bdcc2695a411",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA PROCESSING - Calculate Min/Max Margins (Found Products)\n",
    "# =============================================================================\n",
    "\n",
    "# Merge with existing min_max constraints and sales tiers\n",
    "found = min_max_df.merge(market_data, on=['cohort_id', 'product_id'])\n",
    "found = found.merge(sales[['cohort_id', 'product_id', 'tier']], on=['cohort_id', 'product_id'])\n",
    "\n",
    "# Select min/max margins based on tier\n",
    "tier_conditions = [found['tier'] == i for i in range(6)]\n",
    "tier_min_choices = [found['below_market'], found['market_min'], found['market_25'], \n",
    "                    found['market_50'], found['market_75'], found['market_max']]\n",
    "tier_max_choices = [found['market_min'], found['market_25'], found['market_50'], \n",
    "                    found['market_75'], found['market_max'], found['market_max'] * 1.2]\n",
    "\n",
    "found['selected_min'] = np.select(tier_conditions, tier_min_choices, default=found['market_min'])\n",
    "found['selected_max'] = np.select(tier_conditions, tier_max_choices, default=found['market_min'])\n",
    "\n",
    "# Filter based on margin difference thresholds\n",
    "found['min_cu_diff'] = (found['selected_min'] - found['current_margin']) / found['current_margin']\n",
    "found['min_min_diff'] = (found['selected_min'] - found['min_margin']) / found['min_margin']\n",
    "found = found[((found['min_cu_diff'].between(-0.55, 0.55)) | (found['min_min_diff'].between(-0.55, 0.55)))]\n",
    "\n",
    "# Calculate final new min/max\n",
    "found['diff'] = (found['max_margin'] - found['min_margin']) / found['min_margin']\n",
    "found['new_min'] = found['selected_min']\n",
    "found['new_max'] = np.minimum(\n",
    "    np.maximum(np.maximum((found['diff'] + 1) * found['selected_min'], found['selected_max']), \n",
    "               found['selected_min'] + 0.01),\n",
    "    found['selected_min'] + 0.04\n",
    ")\n",
    "found = found[['cohort_id', 'product_id', 'new_min', 'new_max']]\n",
    "found['type'] = 'both'\n",
    "\n",
    "print(f\"Found (products with existing min_max): {len(found)} records\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2f8918-f784-4364-8b3c-b6fc7c0cc47a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA PROCESSING - Calculate Min/Max Margins (New Products - MP Only)\n",
    "# =============================================================================\n",
    "\n",
    "# Identify products not in existing min_max\n",
    "min_max_df['flag'] = 1\n",
    "not_found = market_data.merge(min_max_df[['cohort_id', 'product_id', 'flag']], on=['cohort_id', 'product_id'], how='left')\n",
    "not_found = not_found.merge(sales[['cohort_id', 'product_id', 'tier']], on=['cohort_id', 'product_id'])\n",
    "not_found = not_found[not_found['flag'].isna()]\n",
    "\n",
    "# Select margins based on tier\n",
    "tier_conditions = [not_found['tier'] == i for i in range(6)]\n",
    "tier_min_choices = [not_found['below_market'], not_found['market_min'], not_found['market_25'],\n",
    "                    not_found['market_50'], not_found['market_75'], not_found['market_max']]\n",
    "tier_max_choices = [not_found['market_min'], not_found['market_25'], not_found['market_50'],\n",
    "                    not_found['market_75'], not_found['market_max'], not_found['market_max'] * 1.2]\n",
    "\n",
    "not_found['selected_min'] = np.select(tier_conditions, tier_min_choices, default=not_found['market_min'])\n",
    "not_found['selected_max'] = np.select(tier_conditions, tier_max_choices, default=not_found['market_min'])\n",
    "\n",
    "# Filter based on margin difference\n",
    "not_found['min_cu_diff'] = (not_found['selected_min'] - not_found['current_margin']) / not_found['current_margin']\n",
    "not_found = not_found[not_found['min_cu_diff'].between(-2, 2)]\n",
    "\n",
    "# Calculate new min/max\n",
    "not_found['new_min'] = not_found['selected_min']\n",
    "not_found['new_max'] = np.minimum(\n",
    "    np.maximum(not_found['selected_max'], not_found['selected_min'] + 0.01),\n",
    "    not_found['selected_min'] + 0.04\n",
    ")\n",
    "not_found = not_found[['cohort_id', 'product_id', 'new_min', 'new_max']]\n",
    "not_found['type'] = 'MP_only'\n",
    "\n",
    "print(f\"Not found (new products): {len(not_found)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aab29e4-c8fc-42d9-a816-616d55f867da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA PROCESSING - Combine Results & Add Region Mapping\n",
    "# =============================================================================\n",
    "\n",
    "# Combine found and not_found\n",
    "final_df = pd.concat([found, not_found], axis=0).drop_duplicates()\n",
    "\n",
    "# Add region mapping\n",
    "final_df = final_df.merge(REGION_COHORT_MAP, on='cohort_id')\n",
    "final_df = final_df[['cohort_id', 'product_id', 'new_min', 'new_max', 'type']].drop_duplicates()\n",
    "final_df.columns = ['cohort_id', 'product_id', 'min_margin', 'max_margin', 'type']\n",
    "\n",
    "print(f\"Combined dataframe: {len(final_df)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8846fc1c-3e90-484e-8233-5e1b24369d15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA PROCESSING - TGTG (Too Good To Go) Products\n",
    "# =============================================================================\n",
    "\n",
    "# Process TGTG aging products with special margin rules\n",
    "tgtg = tgtg_long.merge(wacs, on='product_id')\n",
    "tgtg = tgtg.merge(WAREHOUSE_CONFIG, on='warehouse')\n",
    "tgtg = tgtg.merge(TGTG_EXCLUSIONS, on=['product_id', 'warehouse_id'], how='left')\n",
    "tgtg = tgtg[tgtg['remove'].isna()]\n",
    "tgtg = tgtg.merge(stocks, on=['warehouse_id', 'product_id'])\n",
    "tgtg = tgtg[tgtg['cu_stocks'] > 0]\n",
    "\n",
    "# Calculate stock value and filter\n",
    "tgtg['stock_value'] = tgtg['cu_stocks'] * tgtg['wac_p']\n",
    "tgtg = tgtg.sort_values(by='stock_value', ascending=False)\n",
    "tgtg = tgtg[tgtg['stock_value'] > 100]\n",
    "\n",
    "# Merge with market data and stats\n",
    "tgtg = tgtg.merge(market_data, on=['cohort_id', 'product_id'], how='left')\n",
    "tgtg = tgtg.merge(stats, on=['region', 'product_id'])\n",
    "tgtg = tgtg.merge(market_main_data[['cohort_id', 'product_id', 'target_margin']], on=['cohort_id', 'product_id'], how='left')\n",
    "tgtg = tgtg.fillna(1000)\n",
    "\n",
    "# Calculate aggressive min margins for TGTG products\n",
    "tgtg['min_margin'] = np.minimum(\n",
    "    np.minimum(\n",
    "        np.minimum(tgtg['market_min'] * 0.8, tgtg['target_margin'] / 4),\n",
    "        tgtg['min_boundary'] * 0.9\n",
    "    ),\n",
    "    tgtg['optimal_bm'] * 0.75\n",
    ")\n",
    "tgtg['max_margin'] = tgtg['min_margin']\n",
    "\n",
    "# Save TGTG data for reference\n",
    "tgtg.to_excel(\"Min_max_data/tgtg.xlsx\", index=False)\n",
    "\n",
    "# Aggregate TGTG by cohort/product\n",
    "tgtg = tgtg[['cohort_id', 'product_id', 'min_margin', 'max_margin']]\n",
    "tgtg = tgtg.groupby(['cohort_id', 'product_id']).agg({'min_margin': 'min', 'max_margin': 'min'}).reset_index()\n",
    "tgtg['type'] = 'TGTG'\n",
    "\n",
    "print(f\"TGTG products: {len(tgtg)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f459c984-aff5-484e-9e3c-c61da9f9c290",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA PROCESSING - Merge TGTG with Final DataFrame\n",
    "# =============================================================================\n",
    "\n",
    "# Remove TGTG products from main final_df to avoid duplicates\n",
    "result = final_df.merge(tgtg[['product_id', 'cohort_id']], on=['product_id', 'cohort_id'], how='left', indicator=True)\n",
    "result = result[result['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "# Add TGTG products to final\n",
    "final_df = pd.concat([result, tgtg], axis=0)\n",
    "\n",
    "print(f\"Final dataframe after TGTG: {len(final_df)} records\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc0fe18-8f02-4be7-a0f6-86695bdc957e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA PROCESSING - Price Ups & Final Adjustments\n",
    "# =============================================================================\n",
    "\n",
    "# Merge price_ups with region mapping\n",
    "price_ups = price_ups.merge(REGION_COHORT_MAP, on='region')\n",
    "\n",
    "# Merge with final_df\n",
    "final_df = final_df.merge(price_ups, on=['product_id', 'cohort_id'], how='left')\n",
    "\n",
    "# Adjust max_margin for products with price ups (except TGTG)\n",
    "mask = (~final_df['new_pp'].isna()) & (final_df['type'] != 'TGTG')\n",
    "final_df.loc[mask, 'max_margin'] = np.minimum(\n",
    "    final_df.loc[mask, 'max_margin'] + 0.15, \n",
    "    final_df.loc[mask, 'min_margin'] + 0.2\n",
    ")\n",
    "\n",
    "# Add enforce flag for price_ups products\n",
    "final_df['enforce'] = np.where(~final_df['new_pp'].isna(), 1, np.nan)\n",
    "\n",
    "# Merge with sales for SKU info\n",
    "final_df = final_df.drop_duplicates()\n",
    "final_df = final_df.merge(sales, on=['cohort_id', 'product_id'], how='left')\n",
    "final_df = final_df[['cohort_id', 'product_id', 'sku', 'min_margin', 'max_margin', 'enforce', 'brand', 'type']]\n",
    "\n",
    "# Add comparison with existing min_max\n",
    "final_df = final_df.merge(\n",
    "    min_max_df[['cohort_id', 'product_id', 'min_margin', 'max_margin']].rename(\n",
    "        columns={'min_margin': 'old_min', 'max_margin': 'old_max'}\n",
    "    ),\n",
    "    on=['cohort_id', 'product_id'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"Final dataframe ready: {len(final_df)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbd040b-9e06-49c4-848d-d93627c2da49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# OUTPUT - Export Final Results\n",
    "# =============================================================================\n",
    "\n",
    "# Save to Excel\n",
    "output_path = 'Min_max_data/min_max_data.xlsx'\n",
    "final_df.to_excel(output_path, index=False)\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"MIN/MAX MARGIN CALCULATION COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total records: {len(final_df)}\")\n",
    "print(f\"\\nBreakdown by type:\")\n",
    "print(final_df['type'].value_counts())\n",
    "print(f\"\\nOutput saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b45b608-51c2-48d9-ae5f-aa32e2e328d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
